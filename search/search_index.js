var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Lakehouse Engine","text":""},{"location":"index.html#lakehouse-engine","title":"Lakehouse Engine","text":"<p>A configuration driven Spark framework, written in Python, serving as a scalable and distributed engine for several lakehouse algorithms, data flows and utilities for Data Products.</p> <p>Note: whenever you read Data Product or Data Product team, we want to refer to Teams and use cases, whose main focus is on  leveraging the power of data, on a particular topic, end-to-end (ingestion, consumption...) to achieve insights, supporting faster and better decisions,  which generate value for their businesses. These Teams should not be focusing on building reusable frameworks, but on re-using the existing frameworks to achieve their goals.</p>"},{"location":"index.html#main-goals","title":"Main Goals","text":"<p>The goal of the Lakehouse Engine is to bring some advantages, such as:</p> <ul> <li>offer cutting-edge, standard, governed and battle-tested foundations that several Data Product teams can benefit from;</li> <li>avoid that Data Product teams develop siloed solutions, reducing technical debts and high operating costs (redundant developments across teams);</li> <li>allow Data Product teams to focus mostly on data-related tasks, avoiding wasting time &amp; resources on developing the same code for different use cases;</li> <li>benefit from the fact that many teams are reusing the same code, which increases the likelihood that common issues are surfaced and solved faster;</li> <li>decrease the dependency and learning curve to Spark and other technologies that the Lakehouse Engine abstracts;</li> <li>speed up repetitive tasks;</li> <li>reduced vendor lock-in.</li> </ul> <p>Note: even though you will see a focus on AWS and Databricks, this is just due to the lack of use cases for other technologies like GCP and Azure, but we are open for contribution.</p>"},{"location":"index.html#key-features","title":"Key Features","text":"<p>\u2b50 Data Loads: perform data loads from diverse source types and apply transformations and data quality validations,  ensuring trustworthy data, before integrating it into distinct target types. Additionally, people can also define termination  actions like optimisations or notifications. On the usage section you will find an example using all the supported keywords for data loads.</p> <p>Note: The Lakehouse  Engine supports different types of sources and targets, such as, kafka, jdbc, dataframes, files (csv, parquet, json, delta...), sftp, sap bw, sap b4...</p> <p>\u2b50 Transformations: configuration driven transformations without the need to write any spark code. Transformations can be applied by using the <code>transform_specs</code> in the Data Loads.</p> <p>Note: you can search all the available transformations, as well as checking implementation details and examples here.</p> <p>\u2b50 Data Quality Validations: the Lakehouse Engine uses Great Expectations as a backend and abstracts any implementation details by offering people the capability to specify what validations to apply on the data, solely using dict/json based configurations. The Data Quality validations can be applied on:</p> <ul> <li>post-mortem (static) data, using the DQ Validator algorithm (<code>execute_dq_validation</code>)</li> <li>data in-motion, using the <code>dq_specs</code> keyword in the Data Loads, to add it as one more step while loading data.  On the usage section you will find an example using this type of Data Quality validations.</li> </ul> <p>\u2b50 Reconciliation: useful algorithm to compare two source of data, by defining one version of the <code>truth</code> to compare against the <code>current</code> version of the data. It can be particularly useful during migrations phases, two compare a few KPIs and ensure the new version of a table (<code>current</code>), for example, delivers the same vision of the data as the old one (<code>truth</code>). Find usage examples here.</p> <p>\u2b50 Sensors: an abstraction to otherwise complex spark code that can be executed in very small single-node clusters to check if an upstream system or Data Product contains new data since the last execution. With this feature, people can trigger jobs to run in more frequent intervals and if the upstream does not contain new data, then the rest of the job exits without creating bigger clusters to execute more intensive data ETL (Extraction, Transformation, and Loading). Find usage examples here.</p> <p>\u2b50 Terminators: this feature allow people to specify what to do as a last action, before finishing a Data Load. Some examples of actions are: optimising target table, vacuum, compute stats, expose change data feed to external location or even send e-mail notifications. Thus, it is specifically used in Data Loads, using the <code>terminate_specs</code> keyword. On the usage section you will find an example using terminators.</p> <p>\u2b50 Table Manager: function <code>manage_table</code>, offers a set of actions to manipulate tables/views in several ways, such as:</p> <ul> <li>compute table statistics;</li> <li>create/drop tables and views;</li> <li>delete/truncate/repair tables;</li> <li>vacuum delta tables or locations;</li> <li>optimize table;</li> <li>describe table;</li> <li>show table properties;</li> <li>execute sql.</li> </ul> <p>\u2b50 File Manager: function <code>manage_files</code>, offers a set of actions to manipulate files in several ways, such as:</p> <ul> <li>delete Objects in S3;</li> <li>copy Objects in S3;</li> <li>restore Objects from S3 Glacier;</li> <li>check the status of a restore from S3 Glacier;</li> <li>request a restore of objects from S3 Glacier and wait for them to be copied to a destination.</li> </ul> <p>\u2b50 Notifications: you can configure and send email notifications.</p> <p>Note: it can be used as an independent function (<code>send_notification</code>) or as a <code>terminator_spec</code>, using the function <code>notify</code>.</p> <p>\ud83d\udcd6 In case you want to check further details you can check the documentation of the Lakehouse Engine facade.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>As the Lakehouse Engine is built as wheel (look into our build and deploy make targets) you can install it as any other python package using pip.</p> <pre><code>pip install lakehouse-engine\n</code></pre> <p>Alternatively, you can also upload the wheel to any target of your like (e.g. S3) and perform a pip installation pointing to that target location.</p> <p>Note: The Lakehouse Engine is packaged with plugins or optional dependencies, which are not installed by default. The goal is to make its installation lighter and to avoid unnecessary dependencies. You can check all the optional dependencies in the [tool.setuptools.dynamic] section of the pyproject.toml file. They are currently: os, dq, azure and sftp. So, in case you want to make usage of the Data Quality features offered in the Lakehouse Engine, instead of running the previous command, you should run the command below, which will bring the core functionalities, plus DQ. <pre><code>pip install lakehouse-engine[dq]\n</code></pre> In case you are in an environment without pre-install spark and delta, you will also want to install the <code>os</code> optional dependencies, like so: <pre><code>pip install lakehouse-engine[os]\n</code></pre> And in case you want to install several optional dependencies, you can run a command like: <pre><code>pip install lakehouse-engine[dq,sftp]\n</code></pre> It is advisable for a Data Product to pin a specific version of the Lakehouse Engine (and have recurring upgrading activities) to avoid breaking changes in a new release. In case you don't want to be so conservative, you can pin to a major version, which usually shouldn't include changes that break backwards compatibility.</p>"},{"location":"index.html#how-data-products-use-the-lakehouse-engine-framework","title":"How Data Products use the Lakehouse Engine Framework?","text":"<p>The Lakehouse Engine is a configuration-first Data Engineering framework, using the concept of ACONs to configure algorithms.  An ACON, stands for Algorithm Configuration and is a JSON representation, as the Load Data Usage Example demonstrates. </p> <p>Below you find described the main keywords you can use to configure and ACON for a Data Load.</p> <p>Note: the usage logic for the other algorithms/features presented will always be similar, but using different keywords,  which you can search for in the examples and documentation provided in the Key Features and Community Support and Contributing sections.</p> <ul> <li>Input specifications (input_specs): specify how to read data. This is a mandatory keyword.</li> <li>Transform specifications (transform_specs): specify how to transform data.</li> <li>Data quality specifications (dq_specs): specify how to execute the data quality process.</li> <li>Output specifications (output_specs): specify how to write data to the target. This is a mandatory keyword.</li> <li>Terminate specifications (terminate_specs): specify what to do after writing into the target (e.g., optimising target table, vacuum, compute stats, expose change data feed to external location, etc).</li> <li>Execution environment (exec_env): custom Spark session configurations to be provided for your algorithm (configurations can also be provided from your job/cluster configuration, which we highly advise you to do instead of passing performance related configs here for example).</li> </ul>"},{"location":"index.html#load-data-usage-example","title":"Load Data Usage Example","text":"<p>You can use the Lakehouse Engine in a pyspark script or notebook. Below you can find an example on how to execute a Data Load using the Lakehouse Engine, which is doing the following:</p> <ol> <li>Read CSV files, from a specified location, in a streaming fashion and providing a specific schema and some additional  options for properly read the files (e.g. header, delimiter...);</li> <li>Apply two transformations on the input data:<ol> <li>Add a new column having the Row ID;</li> <li>Add a new column <code>extraction_date</code>, which extracts the date from the <code>lhe_extraction_filepath</code>, based on a regex.</li> </ol> </li> <li>Apply Data Quality validations and store the result of their execution in the table <code>your_database.order_events_dq_checks</code>:<ol> <li>Check if the column <code>omnihub_locale_code</code> is not having null values;</li> <li>Check if the distinct value count for the column <code>product_division</code> is between 10 and 100;</li> <li>Check if the max of the column <code>so_net_value</code> is between 10 and 1000;</li> <li>Check if the length of the values in the column <code>omnihub_locale_code</code> is between 1 and 10;</li> <li>Check if the mean of the values for the column <code>coupon_code</code> is between 15 and 20.</li> </ol> </li> <li>Write the output into the table <code>your_database.order_events_with_dq</code> in a delta format, partitioned by <code>order_date_header</code> and applying a merge predicate condition, ensuring the data is only inserted into the table if it does not match the predicate (meaning the data is not yet available in the table). Moreover, the <code>insert_only</code> flag is used to specify that there should not  be any updates or deletes in the target table, only inserts;</li> <li>Optimize the Delta Table that we just wrote in (e.g. z-ordering);</li> <li>Specify 3 custom Spark Session configurations.</li> </ol> <p>\u26a0\ufe0f Note: <code>spec_id</code> is one of the main concepts to ensure you can chain the steps of the algorithm, so, for example, you can specify the transformations (in <code>transform_specs</code>) of a DataFrame that was read in the <code>input_specs</code>.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"orders_bronze\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",\n\"schema_path\": \"s3://my-data-product-bucket/artefacts/metadata/bronze/schemas/orders.json\",\n\"with_filepath\": True,\n\"options\": {\n\"badRecordsPath\": \"s3://my-data-product-bucket/badrecords/order_events_with_dq/\",\n\"header\": False,\n\"delimiter\": \"\\u005E\",\n\"dateFormat\": \"yyyyMMdd\",\n},\n\"location\": \"s3://my-data-product-bucket/bronze/orders/\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"orders_bronze_with_extraction_date\",\n\"input_id\": \"orders_bronze\",\n\"transformers\": [\n{\"function\": \"with_row_id\"},\n{\n\"function\": \"with_regex_value\",\n\"args\": {\n\"input_col\": \"lhe_extraction_filepath\",\n\"output_col\": \"extraction_date\",\n\"drop_input_col\": True,\n\"regex\": \".*WE_SO_SCL_(\\\\d+).csv\",\n},\n},\n],\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"check_orders_bronze_with_extraction_date\",\n\"input_id\": \"orders_bronze_with_extraction_date\",\n\"dq_type\": \"validator\",\n\"result_sink_db_table\": \"your_database.order_events_dq_checks\",\n\"fail_on_error\": False,\n\"dq_functions\": [\n{\n\"dq_function\": \"expect_column_values_to_not_be_null\", \n\"args\": {\n\"column\": \"omnihub_locale_code\"\n}\n},\n{\n\"dq_function\": \"expect_column_unique_value_count_to_be_between\",\n\"args\": {\n\"column\": \"product_division\", \n\"min_value\": 10,\n\"max_value\": 100\n},\n},\n{\n\"dq_function\": \"expect_column_max_to_be_between\", \n\"args\": {\n\"column\": \"so_net_value\", \n\"min_value\": 10, \n\"max_value\": 1000\n}\n},\n{\n\"dq_function\": \"expect_column_value_lengths_to_be_between\",\n\"args\": {\n\"column\": \"omnihub_locale_code\", \n\"min_value\": 1, \n\"max_value\": 10\n},\n},\n{\n\"dq_function\": \"expect_column_mean_to_be_between\", \n\"args\": {\n\"column\": \"coupon_code\", \n\"min_value\": 15, \n\"max_value\": 20\n}\n},\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"orders_silver\",\n\"input_id\": \"check_orders_bronze_with_extraction_date\",\n\"data_format\": \"delta\",\n\"write_type\": \"merge\",\n\"partitions\": [\"order_date_header\"],\n\"merge_opts\": {\n\"merge_predicate\": \"\"\"\n                    new.sales_order_header = current.sales_order_header\n                    AND new.sales_order_schedule = current.sales_order_schedule\n                    AND new.sales_order_item=current.sales_order_item\n                    AND new.epoch_status=current.epoch_status\n                    AND new.changed_on=current.changed_on\n                    AND new.extraction_date=current.extraction_date\n                    AND new.lhe_batch_id=current.lhe_batch_id\n                    AND new.lhe_row_id=current.lhe_row_id\n                \"\"\",\n\"insert_only\": True,\n},\n\"db_table\": \"your_database.order_events_with_dq\",\n\"options\": {\n\"checkpointLocation\": \"s3://my-data-product-bucket/checkpoints/template_order_events_with_dq/\"\n},\n}\n],\n\"terminate_specs\": [\n{\n\"function\": \"optimize_dataset\",\n\"args\": {\n\"db_table\": \"your_database.order_events_with_dq\"\n}\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre> <p>Note: Although it is possible to interact with the Lakehouse Engine functions directly from your python code,  instead of relying on creating an ACON dict and use the engine api, we do not ensure the stability across new  Lakehouse Engine releases when calling internal functions (not exposed in the facade) directly.</p> <p>Note: ACON structure might change across releases, please test your Data Product first before updating to a  new version of the Lakehouse Engine in your Production environment.</p>"},{"location":"index.html#overwriting-default-configurations","title":"Overwriting default configurations","text":"<p>We use a YAML file to specify various configurations needed for different functionalities. You can overwrite these  configurations using a dictionary with new settings or by providing a path to a YAML file.</p> <p>This functionality can be particularly useful for the open-source community as it unlocks  the usage several functionalities like Prisma and engine usage logs.</p> <p>Check default configurations. <pre><code>from lakehouse_engine.core import exec_env\nprint(exec_env.ExecEnv.ENGINE_CONFIG.dq_dev_bucket)\n   &gt; default-bucket\n</code></pre></p> <p>Change the dq_dev_bucket configuration. <pre><code>exec_env.ExecEnv.set_default_engine_config(custom_configs_dict={\"dq_dev_bucket\": \"your-dq-bucket\"})\nprint(exec_env.ExecEnv.ENGINE_CONFIG.dq_dev_bucket)\n   &gt; your-dq-bucket\n</code></pre> Reset to default configurations. <pre><code>exec_env.ExecEnv.set_default_engine_config()\nprint(exec_env.ExecEnv.ENGINE_CONFIG.dq_dev_bucket)\n   &gt; default-bucket\n</code></pre></p>"},{"location":"index.html#who-maintains-the-lakehouse-engine","title":"Who maintains the Lakehouse Engine?","text":"<p>The Lakehouse Engine is under active development and production usage by the Adidas Lakehouse Foundations Engineering team. </p>"},{"location":"index.html#community-support-and-contributing","title":"Community Support and Contributing","text":"<p>\ud83e\udd1d Do you want to contribute or need any support? Check out all the details in CONTRIBUTING.md.</p>"},{"location":"index.html#license-and-software-information","title":"License and Software Information","text":"<p>\u00a9 adidas AG</p> <p>adidas AG publishes this software and accompanied documentation (if any) subject to the terms of the license with the aim of helping the community with our tools and libraries which we think can be also useful for other people. You will find a copy of the license in the root folder of this package. All rights not explicitly granted to you under the license remain the sole and exclusive property of adidas AG.</p> <p>NOTICE: The software has been designed solely for the purposes described in this ReadMe file. The software is NOT designed, tested or verified for productive use whatsoever, nor or for any use related to high risk environments, such as health care, highly or fully autonomous driving, power plants, or other critical infrastructures or services.</p> <p>If you want to contact adidas regarding the software, you can mail us at software.engineering@adidas.com.</p> <p>For further information open the adidas terms and conditions page.</p>"},{"location":"lakehouse_engine_usage/lakehouse_engine_usage.html","title":"How to use the Lakehouse Engine?","text":"<p>Lakehouse engine usage examples for all the algorithms and other core functionalities.</p> <ul> <li>Data Loader</li> <li>Data Quality</li> <li>Reconciliator</li> <li>Sensor</li> <li>GAB</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html","title":"Data Loader","text":""},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#how-to-configure-a-dataloader-algorithm-in-the-lakehouse-engine-by-using-an-acon-file","title":"How to configure a DataLoader algorithm in the lakehouse-engine by using an ACON file?","text":"<p>An algorithm (e.g., data load) in the lakehouse-engine is configured using an ACON. The lakehouse-engine is a configuration-driven framework, so people don't have to write code to execute a Spark algorithm. In contrast, the algorithm is written in pyspark and accepts configurations through a JSON file (an ACON - algorithm configuration). The ACON is the configuration providing the behaviour of a lakehouse engine algorithm. You can check the algorithm code, and how it interprets the ACON here. In this page we will go through the structure of an ACON file and what are the most suitable ACON files for common data engineering scenarios. Check the underneath pages to find several ACON examples that cover many data extraction, transformation and loading scenarios.</p>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#overview-of-the-structure-of-the-acon-file-for-dataloads","title":"Overview of the Structure of the ACON file for DataLoads","text":"<p>An ACON-based algorithm needs several specifications to work properly, but some of them might be optional. The available specifications are:</p> <ul> <li>Input specifications (input_specs): specify how to read data. This is a mandatory keyword.</li> <li>Transform specifications (transform_specs): specify how to transform data.</li> <li>Data quality specifications (dq_specs): specify how to execute the data quality process.</li> <li>Output specifications (output_specs): specify how to write data to the target. This is a mandatory keyword.</li> <li>Terminate specifications (terminate_specs): specify what to do after writing into the target (e.g., optimising target table, vacuum, compute stats, expose change data feed to external location, etc.).</li> <li>Execution environment (exec_env): custom Spark session configurations to be provided for your algorithm (configurations can also be provided from your job/cluster configuration, which we highly advise you to do instead of passing performance related configs here for example).</li> </ul> <p>Below is an example of a complete ACON file that reads from a s3 folder with CSVs and incrementally loads that data (using a merge) into a delta lake table.</p> <p>What is the spec_id?</p> <p>spec_id is one of the main concepts to ensure you can chain the steps of the algorithm, so, for example, you can specify the transformations (in transform_specs) of a DataFrame that was read in the input_specs. Check ACON below to see how the spec_id of the input_specs is used as input_id in one transform specification.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"orders_bronze\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",\n\"schema_path\": \"s3://my-data-product-bucket/artefacts/metadata/bronze/schemas/orders.json\",\n\"with_filepath\": True,\n\"options\": {\n\"badRecordsPath\": \"s3://my-data-product-bucket/badrecords/order_events_with_dq/\",\n\"header\": False,\n\"delimiter\": \"\\u005E\",\n\"dateFormat\": \"yyyyMMdd\"\n},\n\"location\": \"s3://my-data-product-bucket/bronze/orders/\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"orders_bronze_with_extraction_date\",\n\"input_id\": \"orders_bronze\",\n\"transformers\": [\n{\n\"function\": \"with_row_id\"\n},\n{\n\"function\": \"with_regex_value\",\n\"args\": {\n\"input_col\": \"lhe_extraction_filepath\",\n\"output_col\": \"extraction_date\",\n\"drop_input_col\": True,\n\"regex\": \".*WE_SO_SCL_(\\\\d+).csv\"\n}\n}\n]\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"check_orders_bronze_with_extraction_date\",\n\"input_id\": \"orders_bronze_with_extraction_date\",\n\"dq_type\": \"validator\",\n\"result_sink_db_table\": \"my_database.my_table_dq_checks\",\n\"fail_on_error\": False,\n\"dq_functions\": [\n{\n\"dq_function\": \"expect_column_values_to_not_be_null\",\n\"args\": {\n\"column\": \"omnihub_locale_code\"\n}\n},\n{\n\"dq_function\": \"expect_column_unique_value_count_to_be_between\",\n\"args\": {\n\"column\": \"product_division\",\n\"min_value\": 10,\n\"max_value\": 100\n}\n},\n{\n\"dq_function\": \"expect_column_max_to_be_between\",\n\"args\": {\n\"column\": \"so_net_value\",\n\"min_value\": 10,\n\"max_value\": 1000\n}\n},\n{\n\"dq_function\": \"expect_column_value_lengths_to_be_between\",\n\"args\": {\n\"column\": \"omnihub_locale_code\",\n\"min_value\": 1,\n\"max_value\": 10\n}\n},\n{\n\"dq_function\": \"expect_column_mean_to_be_between\",\n\"args\": {\n\"column\": \"coupon_code\",\n\"min_value\": 15,\n\"max_value\": 20\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"orders_silver\",\n\"input_id\": \"check_orders_bronze_with_extraction_date\",\n\"data_format\": \"delta\",\n\"write_type\": \"merge\",\n\"partitions\": [\n\"order_date_header\"\n],\n\"merge_opts\": {\n\"merge_predicate\": \"\"\"\n            new.sales_order_header = current.sales_order_header\n            and new.sales_order_schedule = current.sales_order_schedule\n            and new.sales_order_item=current.sales_order_item\n            and new.epoch_status=current.epoch_status\n            and new.changed_on=current.changed_on\n            and new.extraction_date=current.extraction_date\n            and new.lhe_batch_id=current.lhe_batch_id\n            and new.lhe_row_id=current.lhe_row_id\n        \"\"\",\n\"insert_only\": True\n},\n\"db_table\": \"my_database.my_table_with_dq\",\n\"location\": \"s3://my-data-product-bucket/silver/order_events_with_dq/\",\n\"with_batch_id\": True,\n\"options\": {\n\"checkpointLocation\": \"s3://my-data-product-bucket/checkpoints/order_events_with_dq/\"\n}\n}\n],\n\"terminate_specs\": [\n{\n\"function\": \"optimize_dataset\",\n\"args\": {\n\"db_table\": \"my_database.my_table_with_dq\"\n}\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True\n}\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#input-specifications","title":"Input Specifications","text":"<p>You specify how to read the data by providing a list of Input Specifications. Usually there's just one element in that list, as, in the lakehouse, you are generally focused on reading data from one layer (e.g., source, bronze, silver, gold) and put it on the next layer. However, there may be scenarios where you would like to combine two datasets (e.g., joins or incremental filtering on one dataset based on the values of another one), therefore you can use one or more elements. More information about InputSpecs.</p>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#relevant-notes","title":"Relevant notes","text":"<ul> <li>A spec id is fundamental, so you can use the input data later on in any step of the algorithm (transform, write, dq process, terminate).</li> <li>You don't have to specify <code>db_table</code> and <code>location</code> at the same time. Depending on the data_format sometimes you read from a table (e.g., jdbc or deltalake table) sometimes you read from a location (e.g., files like deltalake, parquet, json, avro... or kafka topic).</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#transform-specifications","title":"Transform Specifications","text":"<p>In the lakehouse engine, you transform data by providing a transform specification, which contains a list of transform functions (transformers). So the transform specification acts upon on input, and it can execute multiple lakehouse engine transformation functions (transformers) upon that input.</p> <p>If you look into the example above we ask the lakehouse engine to execute two functions on the <code>orders_bronze</code> input data: <code>with_row_id</code> and <code>with_regex_value</code>. Those functions can of course receive arguments. You can see a list of all available transformation functions (transformers) here <code>lakehouse_engine.transformers</code>. Then, you just invoke them in your ACON as demonstrated above, following exactly the same function name and parameters name as described in the code documentation.  More information about TransformSpec.</p>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#relevant-notes_1","title":"Relevant notes","text":"<ul> <li>This stage is fully optional, you can omit it from the ACON.</li> <li>There is one relevant option <code>force_streaming_foreach_batch_processing</code> that can be used to force the transform to be   executed in the foreachBatch function to ensure non-supported streaming operations can be properly executed. You don't   have to worry about this if you are using regular lakehouse engine transformers. But if you are providing your custom   logic in pyspark code via our lakehouse engine   custom_transformation (<code>lakehouse_engine.transformers.custom_transformers</code>) then sometimes your logic may contain   Spark functions that are not compatible with Spark Streaming, and therefore this flag can enable all of your   computation to be streaming-compatible by pushing down all the logic into the foreachBatch() function.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#data-quality-specifications","title":"Data Quality Specifications","text":"<p>One of the most relevant features of the lakehouse engine is that you can have data quality guardrails that prevent you from loading bad data into your target layer (e.g., bronze, silver or gold). The lakehouse engine data quality process includes one main feature at the moment:</p> <ul> <li>Validator: The capability to perform data quality checks on that data (e.g., is the max value of a column bigger   than x?) and even tag your data with the results of the DQ checks.</li> </ul> <p>The output of the data quality process can be written into a Result Sink target (e.g. table or files) and is integrated with a Data Docs website, which can be a company-wide available website for people to check the quality of their data and share with others.</p> <p>To achieve all of this functionality the lakehouse engine uses Great Expectations internally. To hide the Great Expectations internals from our user base and provide friendlier abstractions using the ACON, we have developed the concept of DQSpec that can contain many DQFunctionSpec objects, which is very similar to the relationship between the TransformSpec and TransformerSpec, which means you can have multiple Great Expectations functions executed inside a single data quality specification (as in the ACON above).</p> <p>Note</p> <p>The names of the functions and args are a 1 to 1 match of Great Expectations API.</p> <p>More information about DQSpec.</p>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#relevant-notes_2","title":"Relevant notes","text":"<ul> <li>You can write the outputs of the DQ process to a sink through the result_sink* parameters of the   DQSpec. <code>result_sink_options</code> takes any Spark options for a DataFrame writer, which means you can specify the options   according to your sink format (e.g., delta, parquet, json, etc.). We usually recommend using <code>\"delta\"</code> as format.</li> <li>You can use the results of the DQ checks to tag the data that you are validating. When configured, these details will   appear as a new column (like any other), as part of the tables of your Data Product.</li> <li>To be able to make an analysis with the data of <code>result_sink*</code>, we have available an approach in which you   set <code>result_sink_explode</code> as true (which is the default) and then you have some columns expanded. Those are:<ul> <li>General columns: Those are columns that have the basic information regarding <code>dq_specs</code> and will have always values   and does not depend on the expectation types chosen.     -   Columns: <code>checkpoint_config</code>, <code>run_name</code>, <code>run_time</code>, <code>run_results</code>, <code>success</code>, <code>validation_result_identifier</code>, <code>spec_id</code>, <code>input_id</code>, <code>validation_results</code>, <code>run_time_year</code>, <code>run_time_month</code>, <code>run_time_day</code>.</li> <li>Statistics columns: Those are columns that have information about the runs of expectations, being those values for   the run and not for each expectation. Those columns come from <code>run_results.validation_result.statistics.*</code>.<ul> <li>Columns: <code>evaluated_expectations</code>, <code>success_percent</code>, <code>successful_expectations</code>, <code>unsuccessful_expectations</code>.</li> </ul> </li> <li>Expectations columns: Those are columns that have information about the expectation executed.<ul> <li>Columns: <code>expectation_type</code>, <code>batch_id</code>, <code>expectation_success</code>, <code>exception_info</code>. Those columns are exploded   from <code>run_results.validation_result.results</code>   inside <code>expectation_config.expectation_type</code>, <code>expectation_config.kwargs.batch_id</code>, <code>success as expectation_success</code>,   and <code>exception_info</code>. Moreover, we also include <code>unexpected_index_list</code>, <code>observed_value</code> and <code>kwargs</code>.</li> </ul> </li> <li>Arguments of Expectations columns: Those are columns that will depend on the expectation_type selected. Those   columns are exploded from <code>run_results.validation_result.results</code> inside <code>expectation_config.kwargs.*</code>.<ul> <li>We can have for   example: <code>column</code>, <code>column_A</code>, <code>column_B</code>, <code>max_value</code>, <code>min_value</code>, <code>value</code>, <code>value_pairs_set</code>, <code>value_set</code>,   and others.</li> </ul> </li> <li>More columns desired? Those can be added, using <code>result_sink_extra_columns</code> in which you can select columns   like <code>&lt;name&gt;</code> and/or explode columns like <code>&lt;name&gt;.*</code>.</li> </ul> </li> <li>Use the parameter <code>\"source\"</code> to identify the data used for an easier analysis.</li> <li>By default, Great Expectation will also provide a site presenting the history of the DQ validations that you have performed on your data.</li> <li>You can make an analysis of all your expectations and create a dashboard aggregating all that information.</li> <li>This stage is fully optional, you can omit it from the ACON.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#output-specifications","title":"Output Specifications","text":"<p>The output_specs section of an ACON is relatively similar to the input_specs section, but of course focusing on how to write the results of the algorithm, instead of specifying the input for the algorithm, hence the name output_specs (output specifications). More information about OutputSpec.</p>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#relevant-notes_3","title":"Relevant notes","text":"<ul> <li>Respect the supported write types and output formats.</li> <li>One of the most relevant options to specify in the options parameter is the <code>checkpoint_location</code> when in streaming   read mode, because that location will be responsible for storing which data you already read and transformed from the   source, when the source is a Spark Streaming compatible source (e.g., Kafka or S3 files).</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#terminate-specifications","title":"Terminate Specifications","text":"<p>The terminate_specs section of the ACON is responsible for some \"wrapping up\" activities like optimising a table, vacuuming old files in a delta table, etc. With time the list of available terminators will likely increase (e.g., reconciliation processes), but for now we have the following terminators. This stage is fully optional, you can omit it from the ACON. The most relevant now in the context of the lakehouse initiative are the following:</p> <ul> <li>dataset_optimizer</li> <li>cdf_processor</li> <li>sensor_terminator</li> <li>notifier_terminator</li> </ul> <p>More information about TerminatorSpec.</p>"},{"location":"lakehouse_engine_usage/data_loader/data_loader.html#execution-environment","title":"Execution Environment","text":"<p>In the exec_env section of the ACON you can pass any Spark Session configuration that you want to define for the execution of your algorithm. This is basically just a JSON structure that takes in any Spark Session property, so no custom lakehouse engine logic. This stage is fully optional, you can omit it from the ACON.</p> <p>Note</p> <p>Please be aware that Spark Session configurations that are not allowed to be changed when the Spark cluster is already running need to be passed in the configuration of the job/cluster that runs this algorithm, not here in this section. This section only accepts Spark Session configs that can be changed in runtime. Whenever you introduce an option make sure that it takes effect during runtime, as to the best of our knowledge there's no list of allowed Spark properties to be changed after the cluster is already running. Moreover, typically Spark algorithms fail if you try to modify a config that can only be set up before the cluster is running.</p>"},{"location":"lakehouse_engine_usage/data_loader/append_load_from_jdbc_with_permissive_mode/append_load_from_jdbc_with_permissive_mode.html","title":"Append Load from JDBC with PERMISSIVE mode (default)","text":"<p>This scenario is an append load from a JDBC source (e.g., SAP BW, Oracle Database, SQL Server Database...).</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"jdbc:sqlite:/app/tests/lakehouse/in/feature/append_load/jdbc_permissive/tests.db\",\n\"table\": \"jdbc_permissive\",\n\"properties\": {\n\"driver\": \"org.sqlite.JDBC\"\n}\n},\n\"options\": {\n\"numPartitions\": 1\n}\n},\n{\n\"spec_id\": \"sales_bronze\",\n\"read_type\": \"batch\",\n\"db_table\": \"test_db.jdbc_permissive_table\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_sales_bronze_date\",\n\"input_id\": \"sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"get_max_value\",\n\"args\": {\n\"input_col\": \"date\"\n}\n}\n]\n},\n{\n\"spec_id\": \"appended_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\n\"input_col\": \"date\",\n\"increment_df\": \"max_sales_bronze_date\"\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"appended_sales\",\n\"write_type\": \"append\",\n\"db_table\": \"test_db.jdbc_permissive_table\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\"\n],\n\"location\": \"file:///app/tests/lakehouse/out/feature/append_load/jdbc_permissive/data\"\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/append_load_from_jdbc_with_permissive_mode/append_load_from_jdbc_with_permissive_mode.html#relevant-notes","title":"Relevant notes","text":"<ul> <li>The ReadMode is PERMISSIVE in this scenario, which is the default in Spark, hence we don't need to specify it. Permissive means don't enforce any schema on the input data. </li> <li>From a JDBC source the ReadType needs to be \"batch\" always as \"streaming\" is not available for a JDBC source.</li> <li>In this scenario we do an append load by getting the max date (transformer_spec \"get_max_value\") on bronze and use that date to filter the source to only get data with a date greater than that max date on bronze (transformer_spec \"incremental_filter\"). That is the standard way we do incremental batch loads in the lakehouse engine. For streaming incremental loads we rely on Spark Streaming checkpoint feature (check a streaming append load ACON example).</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/append_load_with_failfast/append_load_with_failfast.html","title":"Append Load with FAILFAST","text":"<p>This scenario is an append load enforcing the schema (using the schema of the target table to enforce the schema of the source, i.e., the schema of the source needs to exactly match the schema of the target table) and FAILFASTING if the schema of the input data does not match the one we specified.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"enforce_schema_from_table\": \"test_db.failfast_table\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"mode\": \"FAILFAST\"\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/append_load/failfast/data\"\n},\n{\n\"spec_id\": \"sales_bronze\",\n\"read_type\": \"batch\",\n\"db_table\": \"test_db.failfast_table\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_sales_bronze_date\",\n\"input_id\": \"sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"get_max_value\",\n\"args\": {\n\"input_col\": \"date\"\n}\n}\n]\n},\n{\n\"spec_id\": \"appended_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\n\"input_col\": \"date\",\n\"increment_df\": \"max_sales_bronze_date\"\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"appended_sales\",\n\"write_type\": \"append\",\n\"db_table\": \"test_db.failfast_table\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\"\n],\n\"location\": \"file:///app/tests/lakehouse/out/feature/append_load/failfast/data\"\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/append_load_with_failfast/append_load_with_failfast.html#relevant-notes","title":"Relevant notes","text":"<ul> <li>The ReadMode is FAILFAST in this scenario, i.e., fail the algorithm if the schema of the input data does not match the one we specified via schema_path, read_schema_from_table or schema Input_specs variables.</li> <li>In this scenario we do an append load by getting the max date (transformer_spec \"get_max_value\") on bronze and use that date to filter the source to only get data with a date greater than that max date on bronze (transformer_spec \"incremental_filter\"). That is the standard way we do incremental batch loads in the lakehouse engine. For streaming incremental loads we rely on Spark Streaming checkpoint feature (check a streaming append load ACON example).</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html","title":"Batch Delta Load Init, Delta and Backfill with Merge","text":"<p>This scenario illustrates the process of implementing a delta load algorithm by first using an ACON to perform an initial load, then another one to perform the regular deltas that will be triggered on a recurrent basis, and finally an ACON for backfilling specific parcels if ever needed.</p>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html#init-load","title":"Init Load","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/backfill/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"condensed_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"condense_record_mode_cdc\",\n\"args\": {\n\"business_key\": [\n\"salesorder\",\n\"item\"\n],\n\"ranking_key_desc\": [\n\"extraction_timestamp\",\n\"actrequest_timestamp\",\n\"datapakid\",\n\"partno\",\n\"record\"\n],\n\"record_mode_col\": \"recordmode\",\n\"valid_record_modes\": [\n\"\",\n\"N\",\n\"R\",\n\"D\",\n\"X\"\n]\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"condensed_sales\",\n\"write_type\": \"merge\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data\",\n\"merge_opts\": {\n\"merge_predicate\": \"current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date\"\n}\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html#relevant-notes","title":"Relevant Notes","text":"<ul> <li>We can see that even though this is an init load we still have chosen to condense the records through our \"condense_record_mode_cdc\" transformer. This is a condensation step capable of handling SAP BW style changelogs based on actrequest_timestamps, datapakid, record_mode, etc...</li> <li>In the init load we actually did a merge in this case because we wanted to test locally if a merge with an empty target table works, but you don't have to do it, as an init load usually can be just a full load. If a merge of init data with an empty table has any performance implications when compared to a regular insert remains to be tested, but we don't have any reason to recommend a merge over an insert for an init load, and as said, this was done solely for local testing purposes, you can just use <code>write_type: \"overwrite\"</code></li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html#delta-load","title":"Delta Load","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/backfill/data\"\n},\n{\n\"spec_id\": \"sales_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_sales_bronze_timestamp\",\n\"input_id\": \"sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"get_max_value\",\n\"args\": {\n\"input_col\": \"actrequest_timestamp\"\n}\n}\n]\n},\n{\n\"spec_id\": \"condensed_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\n\"input_col\": \"actrequest_timestamp\",\n\"increment_df\": \"max_sales_bronze_timestamp\"\n}\n},\n{\n\"function\": \"condense_record_mode_cdc\",\n\"args\": {\n\"business_key\": [\n\"salesorder\",\n\"item\"\n],\n\"ranking_key_desc\": [\n\"extraction_timestamp\",\n\"actrequest_timestamp\",\n\"datapakid\",\n\"partno\",\n\"record\"\n],\n\"record_mode_col\": \"recordmode\",\n\"valid_record_modes\": [\n\"\",\n\"N\",\n\"R\",\n\"D\",\n\"X\"\n]\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"condensed_sales\",\n\"write_type\": \"merge\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data\",\n\"merge_opts\": {\n\"merge_predicate\": \"current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date\",\n\"delete_predicate\": \"new.recordmode in ('R','D','X')\",\n\"insert_predicate\": \"new.recordmode is null or new.recordmode not in ('R','D','X')\"\n}\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html#relevant-notes_1","title":"Relevant Notes","text":"<ul> <li> <p>The merge predicate and the insert, delete or update predicates should reflect the reality of your data, and it's up to each data product to figure out which predicates better match their reality:</p> <ul> <li>The merge predicate usually involves making sure that the \"primary key\" for your data matches.</li> </ul> <p>Performance Tip!!!</p> <p>Ideally, in order to get a performance boost in your merges, you should also place a filter in your merge predicate (e.g., certain technical or business date in the target table &gt;= x days ago), based on the assumption that the rows in that specified interval will never change in the future. This can drastically decrease the merge times of big tables.</p> <ul> <li>The insert, delete and update predicates will always depend on the structure of your changelog, and also how you expect your updates to arrive (e.g., in certain data products you know that you will never get out of order data or late arriving data, while in other you can never ensure that). These predicates should reflect that in order to prevent you from doing unwanted changes to the target delta lake table.<ul> <li>For example, in this scenario, we delete rows that have the R, D or X record_mode values, because we know that if after condensing the rows that is the latest status of that row from the changelog, they should be deleted, and we never insert rows with those status (note: we use this guardrail in the insert to prevent out of order changes, which is likely not the case in SAP BW).</li> <li>Because the <code>insert_predicate</code> is fully optional, in your scenario you may not require that.</li> </ul> </li> <li>In this scenario, we don't pass an <code>update_predicate</code> in the ACON, because both <code>insert_predicate</code> and update_predicate are fully optional, i.e., if you don't pass them the algorithm will update any data that matches the <code>merge_predicate</code> and insert any data that does not match it. The predicates in these cases just make sure the algorithm does not insert or update any data that you don't want, as in the late arriving changes scenario where a deleted row may arrive first from the changelog then the update row, and to prevent your target table to have inconsistent data for a certain period of time (it will eventually get consistent when you receive the latest correct status from the changelog though) you can have this guardrail in the insert or update predicates. Again, for most sources this will not happen but sources like Kafka for example cannot 100% ensure order, for example.</li> <li>In order to understand how we can cover different scenarios (e.g., late arriving changes, out of order changes, etc.), please go here.</li> <li>The order of the predicates in the ACON does not matter, is the logic in the lakehouse engine DeltaMergeWriter's \"_merge\" function that matters.</li> <li>Notice the \"&lt;=&gt;\" operator? In Spark SQL that's the null safe equal.</li> </ul> </li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html#backfilling","title":"Backfilling","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/backfill/data\"\n},\n{\n\"spec_id\": \"sales_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_sales_bronze_timestamp\",\n\"input_id\": \"sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"get_max_value\",\n\"args\": {\n\"input_col\": \"actrequest_timestamp\"\n}\n}\n]\n},\n{\n\"spec_id\": \"condensed_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\n\"input_col\": \"actrequest_timestamp\",\n\"increment_value\": \"20180110120052t\",\n\"greater_or_equal\": True\n}\n},\n{\n\"function\": \"condense_record_mode_cdc\",\n\"args\": {\n\"business_key\": [\n\"salesorder\",\n\"item\"\n],\n\"ranking_key_desc\": [\n\"extraction_timestamp\",\n\"actrequest_timestamp\",\n\"datapakid\",\n\"partno\",\n\"record\"\n],\n\"record_mode_col\": \"recordmode\",\n\"valid_record_modes\": [\n\"\",\n\"N\",\n\"R\",\n\"D\",\n\"X\"\n]\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"condensed_sales\",\n\"write_type\": \"merge\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data\",\n\"merge_opts\": {\n\"merge_predicate\": \"current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date\",\n\"delete_predicate\": \"new.recordmode in ('R','D','X')\",\n\"insert_predicate\": \"new.recordmode is null or new.recordmode not in ('R','D','X')\"\n}\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/batch_delta_load_init_delta_backfill_with_merge/batch_delta_load_init_delta_backfill_with_merge.html#relevant-notes_2","title":"Relevant Notes","text":"<ul> <li>The backfilling process depicted here is fairly similar to the init load, but it is relevant to highlight  by using a static value (that can be modified accordingly to the backfilling needs) in the incremental_filter function.</li> <li>Other relevant functions for backfilling may include the expression_filter function, where you can use a custom SQL filter to filter the input data.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer/custom_transformer.html","title":"Custom Transformer","text":"<p>There may appear a scenario where the data product dev team faces the need to perform complex data transformations that are either not yet available in the lakehouse engine or the logic is just too complex to chain in an ACON file. In the context of the lakehouse, the only layers that usually can impose that complexity is silver+ and gold. This page targets exactly those cases.</p> <p>Below you'll find a notebook where you can pass your own PySpark or Spark SQL logic into the ACON, by dynamically injecting a python function into the ACON dictionary. The lakehouse engine will take care of executing those transformations in the transformation step of the data loader algorithm. Please read the notebook's comments carefully to understand how it works, or simply open it in your notebook environment, which will make the notebook's code and comments more readable.</p> <p>Force Streaming Micro Batch Processing.</p> <p>When you use streaming mode, with a custom transformer, it\u2019s highly advisable that you set the <code>force_streaming_microbatch_processing</code> flag to <code>True</code> in the transform specification, as explained above!</p>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer/custom_transformer.html#what-is-a-custom-transformer-in-the-lakehouse-engine-and-how-you-can-use-it-to-write-your-own-pyspark-logic","title":"What is a custom transformer in the Lakehouse Engine and how you can use it to write your own pyspark logic?","text":"<p>We highly promote the Lakehouse Engine for creating Data Products aligned with the data source (bronze/silver layer), pumping data into silver so our Data Scientists and Analysts can leverage the value of the data in silver, as close as it comes from the source. The low-code and configuration-driven nature of the lakehouse engine makes it a compelling framework to use in such cases, where the transformations that are done from bronze to silver are not that many, as we want to keep the data close to the source.</p> <p>However, when it comes to Data Products enriched in some way or for insights (silver+, gold), they are typically heavy on transformations (they are the T of the overall ELT process), so the nature of the lakehouse engine may would have get into the way of adequately building it. Considering this, and considering our user base that prefers an ACON-based approach and all the nice off-the-shelf features of the lakehouse engine, we have developed a feature that allows us to pass custom transformers where you put your entire pyspark logic and can pass it as an argument in the ACON (the configuration file that configures every lakehouse engine algorithm).</p> <p>Motivation</p> <p>Doing that, you let the ACON guide your read, data quality, write and terminate processes, and you just focus on transforming data :)</p>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer/custom_transformer.html#custom-transformation-function","title":"Custom transformation Function","text":"<p>The function below is the one that encapsulates all your defined pyspark logic and sends it as a python function to the lakehouse engine. This function will then be invoked internally in the lakehouse engine via a df.transform() function. If you are interested in checking the internals of the lakehouse engine, our codebase is openly available here: https://github.com/adidas/lakehouse-engine</p> <p>Attention!!!</p> <p>For this process to work, your function defined below needs to receive a DataFrame and return a DataFrame. Attempting any other method signature (e.g., defining more parameters) will not work, unless you use something like python partials, for example.</p> <pre><code>def get_new_data(df: DataFrame) -&gt; DataFrame:\n\"\"\"Get the new data from the lakehouse engine reader and prepare it.\"\"\"\nreturn (\ndf.withColumn(\"amount\", when(col(\"_change_type\") == \"delete\", lit(0)).otherwise(col(\"amount\")))\n.select(\"article_id\", \"order_date\", \"amount\")\n.groupBy(\"article_id\", \"order_date\")\n.agg(sum(\"amount\").alias(\"amount\"))\n)\ndef get_joined_data(new_data_df: DataFrame, current_data_df: DataFrame) -&gt; DataFrame:\n\"\"\"Join the new data with the current data already existing in the target dataset.\"\"\"\nreturn (\nnew_data_df.alias(\"new_data\")\n.join(\ncurrent_data_df.alias(\"current_data\"),\n[\nnew_data_df.article_id == current_data_df.article_id,\nnew_data_df.order_date == current_data_df.order_date,\n],\n\"left_outer\",\n)\n.withColumn(\n\"current_amount\", when(col(\"current_data.amount\").isNull(), lit(0)).otherwise(\"current_data.amount\")\n)\n.withColumn(\"final_amount\", col(\"current_amount\") + col(\"new_data.amount\"))\n.select(col(\"new_data.article_id\"), col(\"new_data.order_date\"), col(\"final_amount\").alias(\"amount\"))\n)\ndef calculate_kpi(df: DataFrame) -&gt; DataFrame:\n\"\"\"Calculate KPI through a custom transformer that will be provided in the ACON.\n    Args:\n        df: DataFrame passed as input.\n    Returns:\n        DataFrame: the transformed DataFrame.\n    \"\"\"\nnew_data_df = get_new_data(df)\n# we prefer if you use 'ExecEnv.SESSION' instead of 'spark', because is the internal object the\n# lakehouse engine uses to refer to the spark session. But if you use 'spark' should also be fine.\ncurrent_data_df = ExecEnv.SESSION.table(\n\"my_database.my_table\"\n)\ntransformed_df = get_joined_data(new_data_df, current_data_df)\nreturn transformed_df\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer/custom_transformer.html#dont-like-pyspark-api-write-sql","title":"Don't like pyspark API? Write SQL","text":"<p>You don't have to comply to the pyspark API if you prefer SQL. Inside the function above (or any of the auxiliary functions you decide to develop) you can write something like:</p> <pre><code>def calculate_kpi(df: DataFrame) -&gt; DataFrame:\ndf.createOrReplaceTempView(\"new_data\")\n# we prefer if you use 'ExecEnv.SESSION' instead of 'spark', because is the internal object the\n# lakehouse engine uses to refer to the spark session. But if you use 'spark' should also be fine.\nExecEnv.SESSION.sql(\n\"\"\"\n          CREATE OR REPLACE TEMP VIEW my_kpi AS\n          SELECT ... FROM new_data ...\n        \"\"\"\n)\nreturn ExecEnv.SESSION.table(\"my_kpi\")\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer/custom_transformer.html#just-your-regular-acon","title":"Just your regular ACON","text":"<p>If you notice the ACON below, everything is the same as you would do in a Data Product, but the <code>transform_specs</code> section of the ACON has a difference, which is a function called <code>\"custom_transformation\"</code> where we supply as argument the function defined above with the pyspark code.</p> <p>Attention!!!</p> <p>Do not pass the function as calculate_kpi(), but as calculate_kpi, otherwise you are telling python to invoke the function right away, as opposed to pass it as argument to be invoked later by the lakehouse engine.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales\",\n\"read_type\": \"streaming\",\n\"data_format\": \"delta\",\n\"db_table\": \"my_database.dummy_sales\",\n\"options\": {\"readChangeFeed\": \"true\"},\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"transformed_sales_kpi\",\n\"input_id\": \"sales\",\n# because we are using streaming, this allows us to make sure that\n# all the computation in our custom transformer gets pushed to\n# Spark's foreachBatch method in a stream, which allows us to\n# run all Spark functions in a micro batch DataFrame, as there\n# are some Spark functions that are not supported in streaming.\n\"force_streaming_foreach_batch_processing\": True,\n\"transformers\": [\n{\n\"function\": \"custom_transformation\",\n\"args\": {\"custom_transformer\": calculate_kpi},\n},\n],\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"my_table_quality\",\n\"input_id\": \"transformed_sales_kpi\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_dq_bucket\",\n\"data_docs_bucket\": \"my_data_product_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"expectations_store_prefix\": \"dq/expectations/\",\n\"validations_store_prefix\": \"dq/validations/\",\n\"checkpoint_store_prefix\": \"dq/checkpoints/\",\n\"tbl_to_derive_pk\": \"my_table\",\n\"dq_functions\": [\n{\"function\": \"expect_column_values_to_not_be_null\", \"args\": {\"column\": \"article_id\"}},\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_kpi\",\n\"input_id\": \"transformed_sales_kpi\",\n\"write_type\": \"merge\",\n\"data_format\": \"delta\",\n\"db_table\": \"my_database.my_table\",\n\"options\": {\n\"checkpointLocation\": \"s3://my_data_product_bucket/gold/my_table\",\n},\n\"merge_opts\": {\n\"merge_predicate\": \"new.article_id = current.article_id AND new.order_date = current.order_date\"\n},\n}\n],\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer/sql_custom_transformer.html","title":"SQL Custom Transformer","text":"<p>The SQL Custom Transformer executes a SQL transformation provided by the user.This transformer can be very useful whenever the user wants to perform SQL-based transformations that are not natively supported by the lakehouse engine transformers.</p> <p>The transformer receives the SQL query to be executed. This can read from any table or view from the catalog, or any dataframe registered as a temp view.</p> <p>To register a dataframe as a temp view you can use the \"temp_view\" config in the input_specs, as shown below.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\"mode\": \"FAILFAST\", \"header\": True, \"delimiter\": \"|\"},\n\"schema_path\": \"file:///app/tests/lakehouse/in/feature/\"\n\"data_loader_custom_transformer/sql_transformation/\"\n\"source_schema.json\",\n\"location\": \"file:///app/tests/lakehouse/in/feature/\"\n\"data_loader_custom_transformer/sql_transformation/data\",\n\"temp_view\": \"sales_sql\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"calculated_kpi\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"sql_transformation\",\n\"args\": {\"sql\": SQL},\n}\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"calculated_kpi\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/\"\n\"data_loader_custom_transformer/sql_transformation/data\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/custom_transformer_sql/custom_transformer_sql.html","title":"SQL Custom Transformer","text":"<p>The SQL Custom Transformer executes a SQL transformation provided by the user.This transformer can be very useful whenever the user wants to perform SQL-based transformations that are not natively supported by the lakehouse engine transformers.</p> <p>The transformer receives the SQL query to be executed. This can read from any table or view from the catalog, or any dataframe registered as a temp view.</p> <p>To register a dataframe as a temp view you can use the \"temp_view\" config in the input_specs, as shown below.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\"mode\": \"FAILFAST\", \"header\": True, \"delimiter\": \"|\"},\n\"schema_path\": \"file:///app/tests/lakehouse/in/feature/\"\n\"data_loader_custom_transformer/sql_transformation/\"\n\"source_schema.json\",\n\"location\": \"file:///app/tests/lakehouse/in/feature/\"\n\"data_loader_custom_transformer/sql_transformation/data\",\n\"temp_view\": \"sales_sql\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"calculated_kpi\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"sql_transformation\",\n\"args\": {\"sql\": SQL},\n}\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"calculated_kpi\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/\"\n\"data_loader_custom_transformer/sql_transformation/data\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html","title":"Extract from SAP B4 ADSOs","text":"<p>A custom sap_b4 reader and a few utils are offered in the lakehouse-engine framework so that consumption of data from SAP B4 DSOs can be easily created. The framework abstracts all the logic behind the init/delta extractions (AQ vs CL, active table, changelog table, requests status table, how to identify the next delta timestamp...), only requiring a few parameters that are explained and exemplified in the template scenarios that we have created.</p> <p>Note</p> <p>This custom reader is very similar and uses most features from the sap_bw reader, so if you were using specific filters/parameters with the sap_bw reader, there is a high chance you can keep using it in a very similar way with the sap_b4 reader. The main concepts are applied to both readers, as the strategies on how to parallelize the extractions, for example.</p> <p>How can I find a good candidate column for partitioning the extraction from S4Hana?</p> <p>Parallelization Limitations</p> <p>There are no limits imposed by the Lakehouse-Engine framework, but you need to consider that there might be differences imposed by the source.</p> <p>E.g. Each User might be restricted on utilisation of about 100GB memory at a time from the source.</p> <p>Parallel extractions can bring a jdbc source down if a lot of stress is put on the system. Be careful choosing the number of partitions. Spark is a distributed system and can lead to many connections.</p> <p>Danger</p> <p>In case you want to perform further filtering in the REQTSN field, please be aware that it is not being pushed down to SAP B4 by default (meaning it will have bad performance).  In that case, you will need to use customSchema option while reading, so that you are able to enable filter push down for those.</p> <p>You can check the code documentation of the reader below:</p> <p>SAP B4 Reader</p> <p>JDBC Extractions arguments</p> <p>SAP B4 Extractions arguments</p> <p>Note</p> <p>For extractions using the SAP B4 reader, you can use the arguments listed in the SAP B4 arguments, but also the ones listed in the JDBC extractions, as those are inherited as well.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#extraction-from-sap-b4-adsos-template","title":"Extraction from SAP B4 ADSOs Template","text":"<p>This template covers the following scenarios of extractions from the SAP B4Hana ADSOs:</p> <ul> <li>1 - The Simplest Scenario (Not parallel - Not Recommended)</li> <li>2 - Parallel extraction</li> <li>2.1 - Simplest Scenario</li> <li>2.2 - Provide upperBound (Recommended)</li> <li>2.3 - Automatic upperBound (Recommended)</li> <li>2.4 - Provide predicates (Recommended)</li> <li>2.5 - Generate predicates (Recommended)</li> </ul> <p>Note</p> <p>The template will cover two ADSO Types:</p> <ul> <li>AQ: ADSO which is of append type and for which a single ADSO/tables holds all the information, like an event table. For this type, the same ADSO is used for reading data both for the inits and deltas. Usually, these ADSOs end with the digit \"6\".</li> <li>CL: ADSO which is split into two ADSOs, one holding the change log events, the other having the active data (current version of the truth for a particular source). For this type, the ADSO having the active data is used for the first extraction (init) and the change log ADSO is used for the subsequent extractions (deltas). Usually, these ADSOs are split into active table ending with the digit \"2\" and changelog table ending with digit \"3\".</li> </ul> <p>For each of these ADSO types, the lakehouse-engine abstracts the logic to get the delta extractions. This logic basically consists of joining the <code>db_table</code> (for <code>AQ</code>) or the <code>changelog_table</code> (for <code>CL</code>) with the table having the requests status (<code>my_database.requests_status_table</code>). One of the fields used for this joining is the <code>data_target</code>, which has a relationship with the ADSO (<code>db_table</code>/<code>changelog_table</code>), being basically the same identifier without considering parts of it.</p> <p>Based on the previous insights, the queries that the lakehouse-engine generates under the hood translate to (this is a simplified version, for more details please refer to the lakehouse-engine code documentation):</p> <p>AQ Init Extraction: <code>SELECT t.*, CAST({self._SAP_B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp FROM my_database.my_table t</code></p> <p>AQ Delta Extraction: <code>SELECT tbl.*, CAST({self._B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp FROM my_database.my_table AS tbl JOIN my_database.requests_status_table AS req WHERE STORAGE = 'AQ' AND REQUEST_IS_IN_PROCESS = 'N' AND LAST_OPERATION_TYPE IN ('C', 'U') AND REQUEST_STATUS IN ('GG', 'GR') AND UPPER(DATATARGET) = UPPER('my_identifier') AND req.REQUEST_TSN &gt; max_timestamp_in_bronze AND req.REQUEST_TSN &lt;= max_timestamp_in_requests_status_table</code></p> <p>CL Init Extraction: <code>SELECT t.*,     {self._SAP_B4_EXTRACTION.extraction_timestamp}000000000 AS reqtsn,     '0' AS datapakid,     0 AS record,     CAST({self._SAP_B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp FROM my_database.my_table_2 t</code></p> <p>CL Delta Extraction: <code>SELECT tbl.*, CAST({self._SAP_B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp</code> FROM my_database.my_table_3 AS tbl JOIN my_database.requests_status_table AS req WHERE STORAGE = 'AT' AND REQUEST_IS_IN_PROCESS = 'N' AND LAST_OPERATION_TYPE IN ('C', 'U') AND REQUEST_STATUS IN ('GG') AND UPPER(DATATARGET) = UPPER('my_data_target') AND req.REQUEST_TSN &gt; max_timestamp_in_bronze AND req.REQUEST_TSN &lt;= max_timestamp_in_requests_status_table`</p> <p>Introductory Notes</p> <p>If you want to have a better understanding about JDBC Spark optimizations, here you have a few useful links:</p> <ul> <li>https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</li> <li>https://docs.databricks.com/en/connect/external-systems/jdbc.html</li> <li>https://bit.ly/3x2eCEm</li> <li>https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#1-the-simplest-scenario-not-parallel-not-recommended","title":"1 - The Simplest Scenario (Not parallel - Not Recommended)","text":"<p>This scenario is the simplest one, not taking any advantage of Spark JDBC optimisation techniques and using a single connection to retrieve all the data from the source. It should only be used in case the ADSO you want to extract from SAP B4Hana is a small one, with no big requirements in terms of performance to fulfill. When extracting from the source ADSO, there are two options:</p> <ul> <li>Delta Init - full extraction of the source ADSO. You should use it in the first time you extract from the ADSO or any time you want to re-extract completely. Similar to a so-called full load.</li> <li>Delta - extracts the portion of the data that is new or has changed in the source, since the last extraction (using the <code>max_timestamp</code> value in the location of the data already extracted <code>latest_timestamp_data_location</code>).</li> </ul> <p>Below example is composed of two cells.</p> <ul> <li>The first cell is only responsible to define the variables <code>extraction_type</code> and <code>write_type</code>, depending on the extraction type: Delta Init (<code>load_type = \"init\"</code>) or a Delta (<code>load_type = \"delta\"</code>). The variables in this cell will also be referenced by other acons/examples in this notebook, similar to what you would do in your pipelines/jobs, defining this centrally and then re-using it.</li> <li>The second cell is where the acon to be used is defined (which uses the two variables <code>extraction_type</code> and <code>write_type</code> defined) and the <code>load_data</code> algorithm is executed to perform the extraction.</li> </ul> <p>Note</p> <p>There may be cases where you might want to always extract fully from the source ADSO. In these cases, you only need to use a Delta Init every time, meaning you would use <code>\"extraction_type\": \"init\"</code> and <code>\"write_type\": \"overwrite\"</code> as it is shown below. The explanation about what it is a Delta Init/Delta is applicable for all the scenarios presented in this notebook.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_b4\",\n\"options\": {\n\"url\": \"my_sap_b4_url\",\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"dbtable\": \"my_database.my_table\",\n\"extraction_type\": extraction_type,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"adso_type\": \"AQ\",\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#2-parallel-extraction","title":"2 - Parallel extraction","text":"<p>In this section, 5 possible scenarios for parallel extractions from SAP B4Hana ADSOs are presented.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#21-parallel-extraction-simplest-scenario","title":"2.1 - Parallel Extraction, Simplest Scenario","text":"<p>This scenario provides the simplest example you can have for a parallel extraction from SAP B4Hana, only using the property <code>numPartitions</code>. The goal of the scenario is to cover the case in which people do not have much knowledge around how to optimize the extraction from JDBC sources or cannot identify a column that can be used to split the extraction in several tasks. This scenario can also be used if the use case does not have big performance requirements/concerns, meaning you do not feel the need to optimize the performance of the extraction to its maximum potential.</p> <p>On the example below, <code>\"numPartitions\": 10</code> is specified, meaning that Spark will open 10 parallel connections to the source ADSO and automatically decide how to parallelize the extraction upon that requirement. This is the only change compared to the example provided in the scenario 1.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_b4\",\n\"options\": {\n\"url\": \"my_sap_b4_url\",\n\"user\": \"my_user\",\n\"password\": \"my_sap_b4_pwd\",\n\"dbtable\": \"my_database.my_table\",\n\"extraction_type\": extraction_type,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier_par_simple/\",\n\"adso_type\": \"AQ\",\n\"numPartitions\": 10,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/my_identifier_par_simple/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#22-parallel-extraction-provide-upper_bound-recommended","title":"2.2 - Parallel Extraction, Provide upper_bound (Recommended)","text":"<p>This scenario performs the extraction from the SAP B4 ADSO in parallel, but is more concerned with trying to optimize and have more control (compared to 2.1 example) on how the extraction is split and performed, using the following options:</p> <ul> <li><code>numPartitions</code> - number of Spark partitions to split the extraction.</li> <li><code>partitionColumn</code> - column used to split the extraction. It must be a numeric, date, or timestamp. It should be a column that is able to split the extraction evenly in several tasks. An auto-increment column is usually a very good candidate.</li> <li><code>lowerBound</code> - lower bound to decide the partition stride.</li> <li><code>upperBound</code> - upper bound to decide the partition stride.</li> </ul> <p>This is an adequate example for you to follow if you have/know a column in the ADSO that is good to be used as the <code>partitionColumn</code>. If you compare with the previous example, you'll notice that now <code>numPartitions</code> and three additional options are provided to fine tune the extraction (<code>partitionColumn</code>, <code>lowerBound</code>, <code>upperBound</code>).</p> <p>When these 4 properties are used, Spark will use them to build several queries to split the extraction.</p> <p>Example: for <code>\"numPartitions\": 10</code>, <code>\"partitionColumn\": \"record\"</code>, <code>\"lowerBound: 1\"</code>, <code>\"upperBound: 100\"</code>, Spark will generate 10 queries like this:</p> <ul> <li><code>SELECT * FROM dummy_table WHERE RECORD &lt; 10 OR RECORD IS NULL</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 10 AND RECORD &lt; 20</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 20 AND RECORD &lt; 30</code></li> <li>...</li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 100</code></li> </ul> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_b4\",\n\"options\": {\n\"url\": \"my_sap_b4_url\",\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"dbtable\": \"my_database.my_table\",\n\"extraction_type\": extraction_type,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier_par_prov_upper/\",\n\"adso_type\": \"AQ\",\n\"partitionColumn\": \"RECORD\",\n\"numPartitions\": 10,\n\"lowerBound\": 1,\n\"upperBound\": 1000000,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/my_identifier_par_prov_upper/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#23-parallel-extraction-automatic-upper_bound-recommended","title":"2.3 - Parallel Extraction, Automatic upper_bound (Recommended)","text":"<p>This scenario is very similar to 2.2, the only difference being that <code>upperBound</code> is not provided. Instead, the property <code>calculate_upper_bound</code> equals to true is used to benefit from the automatic calculation of the <code>upperBound</code> (derived from the <code>partitionColumn</code>) offered by the lakehouse-engine framework, which is useful, as in most of the cases you will probably not be aware of the max value for the column. The only thing you need to consider is that if you use this automatic calculation of the upperBound you will be doing an initial query to the SAP B4 ADSO to retrieve the max value for the <code>partitionColumn</code>, before doing the actual query to perform the extraction.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_b4\",\n\"calculate_upper_bound\": True,\n\"options\": {\n\"url\": \"my_sap_b4_url\",\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"dbtable\": \"my_database.my_table\",\n\"extraction_type\": extraction_type,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier_par_calc_upper/\",\n\"adso_type\": \"AQ\",\n\"partitionColumn\": \"RECORD\",\n\"numPartitions\": 10,\n\"lowerBound\": 1,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/my_identifier_par_calc_upper/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#24-parallel-extraction-provide-predicates-recommended","title":"2.4 - Parallel Extraction, Provide Predicates (Recommended)","text":"<p>This scenario performs the extraction from SAP B4 ADSO in parallel, useful in contexts in which there is no numeric, date or timestamp column to parallelize the extraction (e.g. when extracting from ADSO of Type <code>CL</code>, the active table does not have the <code>RECORD</code> column, which is usually a good option for scenarios 2.2 and 2.3):</p> <ul> <li><code>partitionColumn</code> - column used to split the extraction. It can be of any type.</li> </ul> <p>This is an adequate example for you to follow if you have/know a column in the ADSO that is good to be used as the <code>partitionColumn</code>, specially if these columns are not complying with the scenario 2.2 or 2.3.</p> <p>When this property is used all predicates need to be provided to Spark, otherwise it will leave data behind.</p> <p>Below the lakehouse function to generate predicate list automatically is presented.</p> <p>This function needs to be used carefully, specially on predicates_query and predicates_add_null variables.</p> <p>predicates_query: At the sample below the whole table is being considered (<code>select distinct(x) from table</code>), but it is possible to filter predicates list here, specially if you are applying filter on transformations spec, and you know entire table won't be necessary, so you can change it to something like this: <code>select distinct(x) from table where x &gt; y</code>.</p> <p>predicates_add_null: You can decide if you want to consider null on predicates list or not, by default this property is <code>True</code>.</p> <p>Example: for <code>\"partition_column\": \"CALMONTH\"</code></p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\n# import the lakehouse_engine ExecEnv class, so that you can use the functions it offers\n# import the lakehouse_engine extraction utils, so that you can use the JDBCExtractionUtils offered functions\nfrom lakehouse_engine.core.exec_env import ExecEnv\nfrom lakehouse_engine.utils.extraction.jdbc_extraction_utils import (\nJDBCExtraction,\nJDBCExtractionUtils,\n)\nExecEnv.get_or_create()\npartition_column = \"CALMONTH\"\ndbtable = \"my_database.my_table_3\"\npredicates_query = f\"\"\"(SELECT DISTINCT({partition_column}) FROM {dbtable})\"\"\"\nuser = \"my_user\"\npassword = \"my_b4_hana_pwd\"\nurl = \"my_sap_b4_url\"\npredicates_add_null = True\njdbc_util = JDBCExtractionUtils(\nJDBCExtraction(\nuser=user,\npassword=password,\nurl=url,\npredicates_add_null=predicates_add_null,\npartition_column=partition_column,\ndbtable=dbtable,\n)\n)\npredicates = jdbc_util.get_predicates(predicates_query)\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_2_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_b4\",\n\"options\": {\n\"url\": \"my_sap_b4_url\",\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n\"dbtable\": \"my_database.my_table_2\",\n\"changelog_table\": \"my_database.my_table_3\",\n\"extraction_type\": extraction_type,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier_2_prov_predicates/\",\n\"adso_type\": \"CL\",\n\"predicates\": predicates,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_2_bronze\",\n\"input_id\": \"my_identifier_2_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/my_identifier_2_prov_predicates/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso/extract_from_sap_b4_adso.html#25-parallel-extraction-generate-predicates","title":"2.5 - Parallel Extraction, Generate Predicates","text":"<p>This scenario is very similar to the scenario 2.4, with the only difference that it automatically generates the predicates (<code>\"generate_predicates\": True</code>).</p> <p>This is an adequate example for you to follow if you have/know a column in the ADSO that is good to be used as the <code>partitionColumn</code>, specially if these columns are not complying with the scenarios 2.2 and 2.3 (otherwise  those would probably be recommended).</p> <p>When this property is used, the lakehouse engine will generate the predicates to be used to extract data from the source. What the lakehouse engine does is to check for the init/delta portion of the data, what are the distinct values of the <code>partitionColumn</code> serving that data. Then, these values will be used by Spark to generate several queries to extract from the source in a parallel fashion. Each distinct value of the <code>partitionColumn</code> will be a query, meaning that you will not have control over the number of partitions used for the extraction. For example, if you face a scenario in which you are using a <code>partitionColumn</code> <code>LOAD_DATE</code> and for today's delta, all the data (let's suppose 2 million rows) is served by a single <code>LOAD_DATE = 20200101</code>, that would mean Spark would use a single partition to extract everything. In this extreme case you would probably need to change your <code>partitionColumn</code>. Note: these extreme cases are harder to happen when you use the strategy of the scenarios 2.2/2.3.</p> <p>Example: for <code>\"partitionColumn\": \"record\"</code></p> <p>Generate predicates: - <code>SELECT DISTINCT(RECORD) as RECORD FROM dummy_table</code> - <code>1</code> - <code>2</code> - <code>3</code> - ... - <code>100</code> - Predicates List: ['RECORD=1','RECORD=2','RECORD=3',...,'RECORD=100']</p> <p>Spark will generate 100 queries like this:</p> <ul> <li><code>SELECT * FROM dummy_table WHERE RECORD = 1</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD = 2</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD = 3</code></li> <li>...</li> <li><code>SELECT * FROM dummy_table WHERE RECORD = 100</code></li> </ul> <p>Generate predicates will also consider null by default:</p> <ul> <li><code>SELECT * FROM dummy_table WHERE RECORD IS NULL</code></li> </ul> <p>To disable this behaviour the following variable value should be changed to false: <code>\"predicates_add_null\": False</code></p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_2_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_b4\",\n\"generate_predicates\": True,\n\"options\": {\n\"url\": \"my_sap_b4_url\",\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n\"dbtable\": \"my_database.my_table_2\",\n\"changelog_table\": \"my_database.my_table_3\",\n\"extraction_type\": extraction_type,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier_2_gen_predicates/\",\n\"adso_type\": \"CL\",\n\"partitionColumn\": \"CALMONTH\",\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_2_bronze\",\n\"input_id\": \"my_identifier_2_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/my_identifier_2_gen_predicates/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html","title":"Extract from SAP BW DSOs","text":"<p>Parallelization Limitations</p> <p>Parallel extractions can bring a jdbc source down if a lot of stress is put on the system. Be careful choosing the number of partitions. Spark is a distributed system and can lead to many connections.</p> <p>A custom sap_bw reader and a few utils are offered in the lakehouse-engine framework so that consumption of data from  SAP BW DSOs can be easily created. The framework abstracts all the logic behind the init/delta extractions  (active table, changelog table, activation requests table, how to identify the next delta timestamp...),  only requiring a few parameters that are explained and exemplified in the  template scenarios that we have created.</p> <p>This page also provides you a section to help you figure out a good candidate for partitioning the extraction from SAP BW.</p> <p>You can check the code documentation of the reader below:</p> <p>SAP BW Reader</p> <p>JDBC Extractions arguments</p> <p>SAP BW Extractions arguments</p> <p>Note</p> <p>For extractions using the SAP BW reader, you can use the arguments listed in the SAP BW arguments, but also  the ones listed in the JDBC extractions, as those are inherited as well. </p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#extraction-from-sap-bw-template","title":"Extraction from SAP-BW template","text":"<p>This template covers the following scenarios of extractions from the SAP BW DSOs:</p> <ul> <li>1 - The Simplest Scenario (Not parallel - Not Recommended)</li> <li>2 - Parallel extraction</li> <li>2.1 - Simplest Scenario</li> <li>2.2 - Provide upperBound (Recommended)</li> <li>2.3 - Automatic upperBound (Recommended)</li> <li>2.4 - Backfilling</li> <li>2.5 - Provide predicates (Recommended)</li> <li>2.6 - Generate predicates (Recommended)</li> <li>3 - Extraction from Write Optimized DSO</li> <li>3.1 - Get initial actrequest_timestamp from Activation Requests Table</li> </ul> <p>Introductory Notes</p> <p>If you want to have a better understanding about JDBC Spark optimizations,  here you have a few useful links: - https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html - https://docs.databricks.com/en/connect/external-systems/jdbc.html - https://bit.ly/3x2eCEm - https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#1-the-simplest-scenario-not-parallel-not-recommended","title":"1 - The Simplest Scenario (Not parallel - Not Recommended)","text":"<p>This scenario is the simplest one, not taking any advantage of Spark JDBC optimisation techniques  and using a single connection to retrieve all the data from the source. It should only be used in case the DSO  you want to extract from SAP BW is a small one, with no big requirements in terms of performance to fulfill.</p> <p>When extracting from the source DSO, there are two options:</p> <ul> <li>Delta Init - full extraction of the source DSO. You should use it in the first time you extract from the  DSO or any time you want to re-extract completely. Similar to a so-called full load.</li> <li>Delta - extracts the portion of the data that is new or has changed in the source, since the last extraction (using the max <code>actrequest_timestamp</code> value in the location of the data already extracted, by default).</li> </ul> <p>Below example is composed of two cells.</p> <ul> <li>The first cell is only responsible to define the variables <code>extraction_type</code> and <code>write_type</code>, depending on the extraction type Delta Init (<code>LOAD_TYPE = INIT</code>) or a Delta (<code>LOAD_TYPE = DELTA</code>). The variables in this cell will also be referenced by other acons/examples in this notebook, similar to what you would do in your pipelines/jobs, defining this centrally and then re-using it.</li> <li>The second cell is where the acon to be used is defined (which uses the two variables <code>extraction_type</code> and <code>write_type</code> defined) and the <code>load_data</code> algorithm is executed to perform the extraction.</li> </ul> <p>Note</p> <p>There may be cases where you might want to always extract fully from the source DSO. In these cases, you only need to use a Delta Init every time, meaning you would use <code>\"extraction_type\": \"init\"</code> and <code>\"write_type\": \"overwrite\"</code> as it is shown below. The explanation about what it is a Delta Init/Delta is applicable for all the scenarios presented in this notebook.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n# You should use this custom reader to benefit from the lakehouse-engine utils for extractions from SAP BW\n\"data_format\": \"sap_bw\",\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"changelog_table\": \"my_database.my_changelog_table\",\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#2-parallel-extraction","title":"2 - Parallel extraction","text":"<p>In this section, 6 possible scenarios for parallel extractions from SAP BW DSOs.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#21-parallel-extraction-simplest-scenario","title":"2.1 - Parallel Extraction, Simplest Scenario","text":"<p>This scenario provides the simplest example you can have for a parallel extraction from SAP BW, only using the property <code>numPartitions</code>. The goal of the scenario is to cover the case in which people does not have much knowledge around how to optimize the extraction from JDBC sources or cannot identify a column that can be used to split the extraction in several tasks. This scenario can also be used if the use case does not have big performance requirements/concerns, meaning you do not feel the need to optimize the performance of the extraction to its maximum potential.  On the example below, <code>\"numPartitions\": 10</code> is specified, meaning that Spark will open 10 parallel connections to the source DSO and automatically decide how to parallelize the extraction upon that requirement. This is the only change compared to the example provided in the example 1.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"changelog_table\": \"my_database.my_changelog_table\",\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n\"numPartitions\": 10,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#22-parallel-extraction-provide-upper_bound-recommended","title":"2.2 - Parallel Extraction, Provide upper_bound (Recommended)","text":"<p>This scenario performs the extraction from the SAP BW DSO in parallel, but is more concerned with trying to optimize and have more control (compared to 2.1 example) on how the extraction is split and performed, using the following options:</p> <ul> <li><code>numPartitions</code> - number of Spark partitions to split the extraction.</li> <li><code>partitionColumn</code> - column used to split the extraction. It must be a numeric, date, or timestamp. It should be a column that is able to split the extraction evenly in several tasks. An auto-increment column is usually a very good candidate.</li> <li><code>lowerBound</code> - lower bound to decide the partition stride.</li> <li><code>upperBound</code> - upper bound to decide the partition stride. It can either be provided (as it is done in this example) or derived automatically by our upperBound optimizer (example 2.3).</li> </ul> <p>This is an adequate example for you to follow if you have/know a column in the DSO that is good to be used as the <code>partitionColumn</code>. If you compare with the previous example, you'll notice that now <code>numPartitions</code> and three additional options are provided to fine tune the extraction (<code>partitionColumn</code>, <code>lowerBound</code>, <code>upperBound</code>).</p> <p>When these 4 properties are used, Spark will use them to build several queries to split the extraction.</p> <p>Example: for <code>\"numPartitions\": 10</code>, <code>\"partitionColumn\": \"record\"</code>, <code>\"lowerBound: 1\"</code>, <code>\"upperBound: 100\"</code>, Spark will generate 10 queries like this:</p> <ul> <li><code>SELECT * FROM dummy_table WHERE RECORD &lt; 10 OR RECORD IS NULL</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 10 AND RECORD &lt; 20</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 20 AND RECORD &lt; 30</code></li> <li>...</li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 100</code></li> </ul> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"changelog_table\": \"my_database.my_changelog_table\",\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n\"numPartitions\": 3,\n\"partitionColumn\": \"my_partition_col\",\n\"lowerBound\": 1,\n\"upperBound\": 42,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#23-parallel-extraction-automatic-upper_bound-recommended","title":"2.3 - Parallel Extraction, Automatic upper_bound (Recommended)","text":"<p>This scenario is very similar to 2.2, the only difference being that upper_bound is not provided. Instead, the property <code>calculate_upper_bound</code> equals to true is used to benefit from the automatic calculation of the upperBound (derived from the <code>partitionColumn</code>) offered by the lakehouse-engine framework, which is useful, as in most of the cases you will probably not be aware of the max value for the column. The only thing you need to consider is that if you use this automatic calculation of the upperBound you will be doing an initial query to the SAP BW DSO to retrieve the max value for the <code>partitionColumn</code>, before doing the actual query to perform the extraction.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"calculate_upper_bound\": True,\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"changelog_table\": \"my_database.my_changelog_table\",\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n\"numPartitions\": 10,\n\"partitionColumn\": \"my_partition_col\",\n\"lowerBound\": 1,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#24-parallel-extraction-backfilling","title":"2.4 - Parallel Extraction, Backfilling","text":"<p>This scenario covers the case, in which you might want to backfill the data extracted from a SAP BW DSO and made available in the bronze layer. By default, the delta extraction considers the max value of the column <code>actrequest_timestamp</code> on the data already extracted. However, there might be cases, in which you might want to extract a delta from a particular timestamp onwards or for a particular interval of time. For this, you can use the properties <code>min_timestamp</code> and <code>max_timestamp</code>.</p> <p>Below, a very similar example to the previous one is provided, the only differences being that the properties <code>\"min_timestamp\": \"20210910000000\"</code> and <code>\"max_timestamp\": \"20210913235959\"</code> are not provided, meaning it will extract the data from the changelog table, using a filter <code>\"20210910000000\" &gt; actrequest_timestamp &lt;= \"20210913235959\"</code>, ignoring if some of the data is already available in the destination or not. Moreover, note that the property <code>latest_timestamp_data_location</code> does not need to be provided, as the timestamps to be considered are being directly provided (if both the timestamps and the <code>latest_timestamp_data_location</code> are provided, the last parameter will have no effect). Additionally, <code>\"extraction_type\": \"delta\"</code> and <code>\"write_type\": \"append\"</code> is forced, instead of using the variables as in the other  examples, because the backfilling scenario only makes sense for delta extractions.</p> <p>Note</p> <p>Note: be aware that the backfilling example being shown has no mechanism to enforce that you don't generate duplicated data in bronze. For your scenarios, you can either use this example and solve any duplication in the silver layer or extract the delta with a merge strategy while writing to bronze, instead of appending.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"calculate_upper_bound\": True,\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"changelog_table\": \"my_database.my_changelog_table\",\n\"extraction_type\": \"delta\",\n\"numPartitions\": 10,\n\"partitionColumn\": \"my_partition_col\",\n\"lowerBound\": 1,\n\"min_timestamp\": \"20210910000000\",\n\"max_timestamp\": \"20210913235959\",\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#25-parallel-extraction-provide-predicates-recommended","title":"2.5 - Parallel Extraction, Provide Predicates (Recommended)","text":"<p>This scenario performs the extraction from SAP BW DSO in parallel, useful in contexts in which there is no numeric, date or timestamp column to parallelize the extraction:</p> <ul> <li><code>partitionColumn</code> - column used to split the extraction. It can be of any type. </li> </ul> <p>This is an adequate example for you to follow if you have/know a column in the DSO that is good to be used as the <code>partitionColumn</code>, specially if these columns are not complying with the scenarios 2.2 and 2.3 (otherwise those would probably be recommended).</p> <p>When this property is used all predicates need to be provided to Spark, otherwise it will leave data behind.</p> <p>Below the lakehouse function to generate predicate list automatically is presented.</p> <p>This function needs to be used carefully, specially on predicates_query and predicates_add_null variables.</p> <p>predicates_query: At the sample below the whole table is being considered (<code>select distinct(x) from table</code>), but it is possible to filter predicates list here, specially if you are applying filter on transformations spec, and you know entire table won't be necessary, so you can change it to something like this: <code>select distinct(x) from table where x &gt; y</code>.</p> <p>predicates_add_null: You can decide if you want to consider null on predicates list or not, by default this property is True.</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\n# import the lakehouse_engine ExecEnv class, so that you can use the functions it offers\n# import the lakehouse_engine extraction utils, so that you can use the JDBCExtractionUtils offered functions\nfrom lakehouse_engine.core.exec_env import ExecEnv\nfrom lakehouse_engine.utils.extraction.jdbc_extraction_utils import (\nJDBCExtraction,\nJDBCExtractionUtils,\n)\nExecEnv.get_or_create()\npartition_column = \"my_partition_column\"\ndbtable = \"my_database.my_table\"\npredicates_query = f\"\"\"(SELECT DISTINCT({partition_column}) FROM {dbtable})\"\"\"\ncolumn_for_predicates = partition_column\nuser = \"my_user\"\npassword = \"my_hana_pwd\"\nurl = \"my_bw_url\"\npredicates_add_null = True\njdbc_util = JDBCExtractionUtils(\nJDBCExtraction(\nuser=user,\npassword=password,\nurl=url,\ndbtable=dbtable,\npartition_column=partition_column,\n)\n)\npredicates = jdbc_util.get_predicates(predicates_query)\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n\"predicates\": predicates,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#26-parallel-extraction-generate-predicates-recommended","title":"2.6 - Parallel Extraction, Generate Predicates (Recommended)","text":"<p>This scenario performs the extraction from SAP BW DSO in parallel, useful in contexts in which there is no numeric, date or timestamp column to parallelize the extraction:</p> <ul> <li><code>partitionColumn</code> - column used to split the extraction. It can be of any type.</li> </ul> <p>This is an adequate example for you to follow if you have/know a column in the DSO that is good to be used as the <code>partitionColumn</code>, specially if these columns are not complying with the scenarios 2.2 and 2.3 (otherwise those would probably be recommended).</p> <p>When this property is used, the lakehouse engine will generate the predicates to be used to extract data from the source. What the lakehouse engine does is to check for the init/delta portion of the data, what are the distinct values of the <code>partitionColumn</code> serving that data. Then, these values will be used by Spark to generate several queries to extract from the source in a parallel fashion. Each distinct value of the <code>partitionColumn</code> will be a query, meaning that you will not have control over the number of partitions used for the extraction. For example, if you face a scenario in which you are using a <code>partitionColumn</code> <code>LOAD_DATE</code> and for today's delta, all the data (let's suppose 2 million rows) is served by a single <code>LOAD_DATE = 20200101</code>, that would mean Spark would use a single partition to extract everything. In this extreme case you would probably need to change your <code>partitionColumn</code>. Note: these extreme cases are harder to happen when you use the strategy of the scenarios 2.2/2.3.</p> <p>Example: for <code>\"partitionColumn\": \"record\"</code> Generate predicates:</p> <ul> <li><code>SELECT DISTINCT(RECORD) as RECORD FROM dummy_table</code></li> <li><code>1</code></li> <li><code>2</code></li> <li><code>3</code></li> <li>...</li> <li><code>100</code></li> <li>Predicates List: ['RECORD=1','RECORD=2','RECORD=3',...,'RECORD=100']</li> </ul> <p>Spark will generate 100 queries like this:</p> <ul> <li><code>SELECT * FROM dummy_table WHERE RECORD = 1</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD = 2</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD = 3</code></li> <li>...</li> <li><code>SELECT * FROM dummy_table WHERE RECORD = 100</code></li> </ul> <p>Generate predicates will also consider null by default: - <code>SELECT * FROM dummy_table WHERE RECORD IS NULL</code></p> <p>To disable this behaviour the following variable value should be changed to false: <code>\"predicates_add_null\": False</code></p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"generate_predicates\": True,\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n\"partitionColumn\": \"my_partition_col\",\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#3-extraction-from-write-optimized-dsos","title":"3 - Extraction from Write Optimized DSOs","text":"<p>This scenario is based on the best practices of the scenario 2.2, but it is ready to extract data from Write Optimized DSOs, which have the changelog embedded in the active table, instead of having a separate changelog table. Due to this reason, you need to specify that the <code>changelog_table</code> parameter value is equal to the <code>dbtable</code> parameter value. Moreover, these tables usually already include the changelog technical columns like <code>RECORD</code> and <code>DATAPAKID</code>, for example, that the framework adds by default. Thus, you need to specify <code>\"include_changelog_tech_cols\": False</code> to change this behaviour. Finally, you also need to specify the name of the column in the table that can be used to join with the activation requests table to get the timestamp of the several requests/deltas, which is <code>\"actrequest\"</code> by default (<code>\"request_col_name\": 'request'</code>).</p> <pre><code>from lakehouse_engine.engine import load_data\nLOAD_TYPE = \"INIT\" or \"DELTA\"\nif LOAD_TYPE == \"INIT\":\nextraction_type = \"init\"\nwrite_type = \"overwrite\"\nelse:\nextraction_type = \"delta\"\nwrite_type = \"append\"\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"changelog_table\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"request_col_name\": \"request\",\n\"include_changelog_tech_cols\": False,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier/\",\n\"extraction_type\": extraction_type,\n\"numPartitions\": 2,\n\"partitionColumn\": \"RECORD\",\n\"lowerBound\": 1,\n\"upperBound\": 50000,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": write_type,\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#31-extraction-from-write-optimized-dsos-get-actrequest_timestamp-from-activation-requests-table","title":"3.1 - Extraction from Write Optimized DSOs, Get ACTREQUEST_TIMESTAMP from Activation Requests Table","text":"<p>By default, the act_request_timestamp has being hardcoded (either assumes a given extraction_timestamp or the current timestamp) in the init extraction, however this may be causing problems when merging changes in silver, for write optimised DSOs. So, a new possibility to choose when to retrieve this timestamp from the act_req_table was added.</p> <p>This scenario performs the data extraction from Write Optimized DSOs, forcing the actrequest_timestamp to assume the value from the activation requests table (timestamp column).</p> <p>This feature is only available for WODSOs and to use it you need to specify <code>\"get_timestamp_from_actrequest\": True</code>.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sap_bw\",\n\"options\": {\n\"user\": \"my_user\",\n\"password\": \"my_hana_pwd\",\n\"url\": \"my_sap_bw_url\",\n\"dbtable\": \"my_database.my_table\",\n\"changelog_table\": \"my_database.my_table\",\n\"odsobject\": \"my_ods_object\",\n\"request_col_name\": \"request\",\n\"include_changelog_tech_cols\": False,\n\"latest_timestamp_data_location\": \"s3://my_path/my_identifier_ACTREQUEST_TIMESTAMP/\",\n\"extraction_type\": \"init\",\n\"numPartitions\": 2,\n\"partitionColumn\": \"RECORD\",\n\"lowerBound\": 1,\n\"upperBound\": 50000,\n\"get_timestamp_from_act_request\": True,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\"actrequest_timestamp\"],\n\"location\": \"s3://my_path/my_identifier_ACTREQUEST_TIMESTAMP\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso/extract_from_sap_bw_dso.html#how-can-we-decide-the-partitioncolumn","title":"How can we decide the partitionColumn?","text":"<p>Compatible partitionColumn for upperBound/lowerBound Spark options:</p> <p>It needs to be int, date, timestamp \u2192 https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</p> <p>If you don't have any column to partition on those formats, you can use predicates to partition the table \u2192 https://docs.databricks.com/en/connect/external-systems/jdbc.html#manage-parallelism</p> <p>One of the most important parameters to optimise the extraction is the partitionColumn, as you can see in the template. Thus, this section helps you figure out if a column is a good candidate or not. </p> <p>Basically the partition column needs to be a column which is able to adequately split the processing, which means we can use it to \"create\" different queries with intervals/filters, so that the Spark tasks process similar amounts of rows/volume. Usually a good candidate is an integer auto-increment technical column.</p> <p>Note</p> <p>Although RECORD is usually a good candidate, it is usually available on the changelog table only. Meaning that you would need to use a different strategy for the init. In case you don't have good candidates for partitionColumn, you can use the sample acon provided in the scenario 2.1 in the template above. It might make sense to use scenario 2.1 for the init and then scenario 2.2 or 2.3 for the subsequent deltas.</p> <p>When there is no int, date or timestamp good candidate for partitionColumn:</p> <p>In this case you can opt by the scenario 2.5 - Generate Predicates, which supports any kind of column to be defined as partitionColumn.</p> <p>However, you should still analyse if the column you are thinking about is a good candidate or not. In this scenario, Spark will create one query per distinct value of the partitionColumn, so you can perform some analysis.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sftp/extract_from_sftp.html","title":"Extract from SFTP","text":"<p>Secure File Transfer Protocol (SFTP) is a file protocol for transferring files over the web.</p> <p>This feature is available in the Lakehouse Engine with the purpose of having a mechanism to read data directly from SFTP directories without moving those files manually/physically to a S3 bucket.</p> <p>The engine uses Pandas to read the files and converts them into a Spark dataframe, which makes the available resources of an Acon usable, such as <code>dq_specs</code>, <code>output_specs</code>, <code>terminator_specs</code> and <code>transform_specs</code>.</p> <p>Furthermore, this feature provides several filters on the directories that makes easier to control the extractions.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sftp/extract_from_sftp.html#introductory-notes","title":"Introductory Notes:","text":"<p>There are important parameters that must be added to input specs in order to make the SFTP extraction work properly:</p> <p>Read type</p> <p>The engine supports only BATCH mode for this feature.</p> <p>sftp_files_format - File format that will be used to read data from SFTP. The engine supports: CSV, FWF, JSON and XML.</p> <p>location - The SFTP directory to be extracted. If it is necessary to filter a specific file, it can be made using the <code>file_name_contains</code> option.</p> <p>options - Arguments used to set the Paramiko SSH client connection (hostname, username, password, port...), set the filter to retrieve files and set the file parameters (separators, headers, cols...). For more information about the file parameters, please go to the Pandas link in the useful links section.</p> <p>The options allowed are:</p> Property type Detail Example Comment Connection add_auto_policy(str) true of false Indicates to allow an SFTP connection using no host key. When a connection attempt is being made using no host key, then the engine will throw an exception if the auto_add_policy property is false. The purpose of this flag is to make the user conscientiously choose a lesser secure connection. Connection key_type (str) \"Ed25519\" or \"RSA\" Indicates the key type to be used for the connection (SSH, Ed25519). Connection key_filename (str) \"/path/to/private_key/private_key.ppk\" The filename, or list of filenames, of optional private(keys), and/or certs to try for authentication. It must be used with a pkey in order to add a policy. If a pkey is not provided, then use <code>add_auto_policy</code>. Connection pkey (str) \"AAAAC3MidD1lVBI1NTE5AAAAIKssLqd6hjahPi9FBH4GPDqMqwxOMsfxTgowqDCQAeX+\" Value to use for the host key when connecting to the remote SFTP server. Filter date_time_gt (str) \"1900-01-01\" or \"1900-01-01 08:59:59\" Filter the files greater than the string datetime formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\" Filter date_time_lt (str) \"3999-12-31\" or \"3999-12-31 20:59:59\" Filter the files lower than the string datetime formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\" Filter earliest_file (bool) true or false Filter the earliest dated file in the directory. Filter file_name_contains (str) \"part_of_filename\" Filter files when match the pattern. Filter latest_file (bool) true or false Filter the most recent dated file in the directory. Read data from subdirectories sub_dir (bool) true or false The engine will search files into subdirectories of the location. It will consider one level below the root location given.When <code>sub_dir</code> is used with latest_file/earliest_file argument, the engine will retrieve the latest/earliest file for each subdirectory. Add metadata info file_metadata (bool) true or false When this option is set as True, the dataframe retrieves the filename with location and the modification_time from the original files in sftp. It attaches these two columns adding the information to respective records. <p>Useful Info &amp; Links: 1. Paramiko SSH Client 2. Pandas documentation</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sftp/extract_from_sftp.html#scenario-1","title":"Scenario 1","text":"<p>The scenario below shows the extraction of a CSV file using most part of the available filter options. Also, as an example, the column \"created_on\" is created in the transform_specs in order to store the processing date for every record. As the result, it will have in the output table the original file date (provided by the option <code>file_metadata</code>) and the processing date from the engine.</p> <p>For an incremental load approach, it is advised to use the \"modification_time\" column created by the option <code>file_metadata</code>. Since it has the original file date of modification, this date can be used in the logic to control what is new and has been changed recently.</p> <p>Note</p> <p>Below scenario uses \"add_auto_policy\": true, which is not recommended.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sftp_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sftp\",\n\"sftp_files_format\": \"csv\",\n\"location\": \"my_sftp_data_path\",\n\"options\": {\n\"hostname\": \"my_sftp_hostname\",\n\"username\": \"my_sftp_username\",\n\"password\": \"my_sftp_password\",\n\"port\": \"my_port\",\n\"add_auto_policy\": True,\n\"file_name_contains\": \"test_pattern\",\n\"args\": {\"sep\": \"|\"},\n\"latest_file\": True,\n\"file_metadata\": True\n}\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"sftp_transformations\",\n\"input_id\": \"sftp_source\",\n\"transformers\": [\n{\n\"function\": \"with_literals\",\n\"args\": {\"literals\": {\"created_on\": datetime.now()}},\n},\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sftp_bronze\",\n\"input_id\": \"sftp_transformations\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_path/dummy_table\"\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_from_sftp/extract_from_sftp.html#scenario-2","title":"Scenario 2","text":"<p>The following scenario shows the extraction of a JSON file using an RSA pkey authentication instead of auto_add_policy. The engine supports Ed25519Key and RSA for pkeys.</p> <p>For the pkey file location, it is important to have the file in a location accessible by the cluster. This can be achieved either by mounting the location or with volumes.</p> <p>Note</p> <p>This scenario uses a more secure authentication, thus it is the recommended option, instead of the previous scenario.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"sftp_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"sftp\",\n\"sftp_files_format\": \"json\",\n\"location\": \"my_sftp_data_path\",\n\"options\": {\n\"hostname\": \"my_sftp_hostname\",\n\"username\": \"my_sftp_username\",\n\"password\": \"my_sftp_password\",\n\"port\": \"my_port\",\n\"key_type\": \"RSA\",\n\"key_filename\": \"dbfs_mount_location/my_file_key.ppk\",\n\"pkey\": \"my_key\",\n\"latest_file\": True,\n\"file_metadata\": True,\n\"args\": {\"lines\": True, \"orient\": \"columns\"},\n},\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"sftp_transformations\",\n\"input_id\": \"sftp_source\",\n\"transformers\": [\n{\n\"function\": \"with_literals\",\n\"args\": {\"literals\": {\"lh_created_on\": datetime.now()}},\n},\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sftp_bronze\",\n\"input_id\": \"sftp_transformations\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_path/dummy_table\"\n}\n]\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html","title":"Extract using JDBC connection","text":"<p>SAP Extraction</p> <p>SAP is only used as an example to demonstrate how we can use a JDBC connection to extract data.</p> <p>If you are looking to extract data from SAP, please use our sap_b4 or sap_bw reader.</p> <p>You can find the sap_b4 reader documentation: Extract from SAP B4 ADSOs and the sap_bw reader documentarion: Extract from SAP BW DSOs</p> <p>Parallel Extraction</p> <p>Parallel extractions can bring a jdbc source down if a lot of stress is put on the system. Be careful choosing the number of partitions. Spark is a distributed system and can lead to many connections.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#introduction","title":"Introduction","text":"<p>Many databases allow a JDBC connection to extract data. Our engine has one reader where you can configure all the necessary definitions to connect to a database using JDBC.</p> <p>In the next section you will find several examples about how to do it.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#the-simplest-scenario-using-sqlite","title":"The Simplest Scenario using sqlite","text":"<p>Not parallel</p> <p>Recommended for smaller datasets only, or when stressing the source system is a high concern</p> <p>This scenario is the simplest one we can have, not taking any advantage of Spark JDBC optimisation techniques and using a single connection to retrieve all the data from the source.</p> <p>Here we use a sqlite database where any connection is allowed. Due to that, we do not specify any username or password.</p> <p>Same as spark, we provide two different ways to run jdbc reader.</p> <p>1 - We can use the jdbc() function, passing inside all the arguments needed for Spark to work, and we can even combine this with additional options passed through .options().</p> <p>2 - Other way is using .format(\"jdbc\") and pass all necessary arguments through .options(). It's important to say by choosing jdbc() we can also add options() to the execution.</p> <p>You can find and run the following code in our local test for the engine.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#jdbc-function","title":"jdbc() function","text":"<p>As we can see in the next cell, all the arguments necessary to establish the jdbc connection are passed inside the <code>jdbc_args</code> object. Here we find the url, the table, and the driver. Besides that, we can add options, such as the partition number. The partition number will impact in the queries' parallelism.</p> <p>The below code is an example in how to use jdbc() function in our ACON. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_function/correct_arguments/tests.db\",\n\"table\": \"jdbc_function\",\n\"properties\": {\n\"driver\": \"org.sqlite.JDBC\"\n}\n},\n\"options\": {\n\"numPartitions\": 1\n}\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"overwrite\",\n\"db_table\": \"test_db.jdbc_function_table\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\"\n],\n\"location\": \"file:///app/tests/lakehouse/out/feature/jdbc_reader/jdbc_function/correct_arguments/data\"\n}\n]\n</code></pre></p> <p>This is same as using the following code in pyspark:</p> <pre><code>spark.read.jdbc(\nurl=\"jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_function/correct_arguments/tests.db\",\ntable=\"jdbc_function\",\nproperties={\"driver\":\"org.sqlite.JDBC\"})\n.option(\"numPartitions\", 1)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#formatjdbc","title":".format(\"jdbc\")","text":"<p>In this example we do not use the <code>jdbc_args</code> object. All the jdbc connection parameters are inside the dictionary with the object options. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"options\": {\n\"url\": \"jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_format/correct_arguments/tests.db\",\n\"dbtable\": \"jdbc_format\",\n\"driver\": \"org.sqlite.JDBC\",\n\"numPartitions\": 1\n}\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"overwrite\",\n\"db_table\": \"test_db.jdbc_format_table\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\"\n],\n\"location\": \"file:///app/tests/lakehouse/out/feature/jdbc_reader/jdbc_format/correct_arguments/data\"\n}\n]\n</code></pre></p> <p>This is same as using the following code in pyspark:</p> <pre><code>spark.read.format(\"jdbc\")\n.option(\"url\", \"jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_format/correct_arguments/tests.db\")\n.option(\"driver\", \"org.sqlite.JDBC\")\n.option(\"dbtable\", \"jdbc_format\")\n.option(\"numPartitions\", 1)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#template-with-more-complete-and-runnable-examples","title":"Template with more complete and runnable examples","text":"<p>In this template we will use a SAP as example for a more complete and runnable example. These definitions can be used in several databases that allow JDBC connection.</p> <p>The following scenarios of extractions are covered:</p> <ul> <li>1 - The Simplest Scenario (Not parallel -  Recommended for smaller datasets only, or when stressing the source system is a high concern)</li> <li>2 - Parallel extraction</li> <li>2.1 - Simplest Scenario </li> <li>2.2 - Provide upperBound (Recommended)</li> <li>2.3 - Provide predicates (Recommended)</li> </ul> <p>Disclaimer</p> <p>This template only uses SAP as demonstration example for JDBC connection. This isn't a SAP template!!! If you are looking to extract data from SAP, please use our sap_b4 reader or the sap_bw reader.</p> <p>The JDBC connection has 2 main sections to be filled, the jdbc_args and options:</p> <ul> <li>jdbc_args - Here you need to fill everything related to jdbc connection itself, like table/query, url, user, ..., password.</li> <li>options - This section is more flexible, and you can provide additional options like \"fetchSize\", \"batchSize\", \"numPartitions\", ..., upper and \"lowerBound\".</li> </ul> <p>If you want to know more regarding jdbc spark options you can follow the link below:</p> <ul> <li>https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</li> </ul> <p>If you want to have a better understanding about JDBC Spark optimizations, you can find them in the following:</p> <ul> <li>https://docs.databricks.com/en/connect/external-systems/jdbc.html</li> <li>https://stackoverflow.com/questions/41085238/what-is-the-meaning-of-partitioncolumn-lowerbound-upperbound-numpartitions-pa</li> <li>https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#1-the-simplest-scenario-not-parallel-recommended-for-smaller-datasets-or-for-not-stressing-the-source","title":"1 - The Simplest Scenario (Not parallel - Recommended for smaller datasets, or for not stressing the source)","text":"<p>This scenario is the simplest one we can have, not taking any advantage of Spark JDBC optimisation techniques and using a single connection to retrieve all the data from the source. It should only be used in case the data you want to extract from is a small one, with no big requirements in terms of performance to fulfill.</p> <p>When extracting from the source, we can have two options:</p> <ul> <li>Delta Init - full extraction of the source. You should use it in the first time you extract from the source or any time you want to re-extract completely. Similar to a so-called full load.</li> <li>Delta - extracts the portion of the data that is new or has changed in the source, since the last extraction (for that, the logic at the transformation step needs to be applied). On the examples below, the logic using REQTSN column is applied, which means that the maximum value on bronze is filtered and its value is used to filter incoming data from the data source.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#init-load-data-into-the-bronze-bucket","title":"Init - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"fetchSize\": 100000,\n\"compress\": True,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/jdbc_template/no_parallel/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#delta-load-data-into-the-bronze-bucket","title":"Delta - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_jdbc_url\",\n\"table\": \"my_database.my_table\",\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"fetchSize\": 100000,\n\"compress\": True,\n},\n},\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_path/jdbc_template/no_parallel/my_identifier/\",\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_my_identifier_bronze_date\",\n\"input_id\": \"my_identifier_bronze\",\n\"transformers\": [{\"function\": \"get_max_value\", \"args\": {\"input_col\": \"REQTSN\"}}],\n},\n{\n\"spec_id\": \"appended_my_identifier\",\n\"input_id\": \"my_identifier_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\"input_col\": \"REQTSN\", \"increment_df\": \"max_my_identifier_bronze_date\"},\n}\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"appended_my_identifier\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/jdbc_template/no_parallel/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#2-parallel-extraction","title":"2 - Parallel extraction","text":"<p>On this section we present 3 possible scenarios for parallel extractions from JDBC sources.</p> <p>Disclaimer for parallel extraction</p> <p>Parallel extractions can bring a jdbc source down if a lot of stress is put on the system. Be careful when choosing the number of partitions.  Spark is a distributed system and can lead to many connections.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#21-parallel-extraction-simplest-scenario","title":"2.1 - Parallel Extraction, Simplest Scenario","text":"<p>This scenario provides the simplest example you can have for a parallel extraction from JDBC sources, only using the property <code>numPartitions</code>. The goal of the scenario is to cover the case in which people do not have much experience around how to optimize the extraction from JDBC sources or cannot identify a column that can be used to split the extraction in several tasks. This scenario can also be used if the use case does not have big performance requirements/concerns, meaning you do not feel the need to optimize the performance of the extraction to its maximum potential.</p> <p>On the example bellow, <code>\"numPartitions\": 10</code> is specified, meaning that Spark will open 10 parallel connections to the source and automatically decide how to parallelize the extraction upon that requirement. This is the only change compared to the example provided in the scenario 1.</p>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#delta-init-load-data-into-the-bronze-bucket","title":"Delta Init - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"fetchSize\": 100000,\n\"compress\": True,\n\"numPartitions\": 10,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/jdbc_template/parallel_1/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#delta-load-data-into-the-bronze-bucket_1","title":"Delta - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"fetchSize\": 100000,\n\"compress\": True,\n\"numPartitions\": 10,\n},\n},\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_path/jdbc_template/parallel_1/my_identifier/\",\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_my_identifier_bronze_date\",\n\"input_id\": \"my_identifier_bronze\",\n\"transformers\": [{\"function\": \"get_max_value\", \"args\": {\"input_col\": \"REQTSN\"}}],\n},\n{\n\"spec_id\": \"appended_my_identifier\",\n\"input_id\": \"my_identifier_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\"input_col\": \"REQTSN\", \"increment_df\": \"max_my_identifier_bronze_date\"},\n}\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"appended_my_identifier\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"partitions\": [\"REQTSN\"],\n\"location\": \"s3://my_path/jdbc_template/parallel_1/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#22-parallel-extraction-provide-upper_bound-recommended","title":"2.2 - Parallel Extraction, Provide upper_bound (Recommended)","text":"<p>This scenario performs the extraction from the JDBC source in parallel, but has more concerns trying to optimize and have more control (compared to 2.1 example) on how the extraction is split and performed, using the following options:</p> <ul> <li><code>numPartitions</code> - number of Spark partitions to split the extraction.</li> <li><code>partitionColumn</code> - column used to split the extraction. It must be a numeric, date, or timestamp. It should be a column that is able to split the extraction evenly in several tasks. An auto-increment column is usually a very good candidate.</li> <li><code>lowerBound</code> - lower bound to decide the partition stride.</li> <li><code>upperBound</code> - upper bound to decide the partition stride.</li> </ul> <p>This is an adequate example to be followed if there is a column in the data source that is good to be used as the <code>partitionColumn</code>. Comparing with the previous example, the <code>numPartitions</code> and three additional options to fine tune the extraction (<code>partitionColumn</code>, <code>lowerBound</code>, <code>upperBound</code>) are provided.</p> <p>When these 4 properties are used, Spark will use them to build several queries to split the extraction. Example: for <code>\"numPartitions\": 10</code>, <code>\"partitionColumn\": \"record\"</code>, <code>\"lowerBound: 1\"</code>, <code>\"upperBound: 100\"</code>, Spark will generate 10 queries like:</p> <ul> <li><code>SELECT * FROM dummy_table WHERE RECORD &lt; 10 OR RECORD IS NULL</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 10 AND RECORD &lt; 20</code></li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 20 AND RECORD &lt; 30</code></li> <li>...</li> <li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 100</code></li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#init-load-data-into-the-bronze-bucket_1","title":"Init - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"partitionColumn\": \"RECORD\",\n\"numPartitions\": 10,\n\"lowerBound\": 1,\n\"upperBound\": 2000,\n\"fetchSize\": 100000,\n\"compress\": True,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\"RECORD\"],\n\"location\": \"s3://my_path/jdbc_template/parallel_2/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#delta-load-data-into-the-bronze-bucket_2","title":"Delta - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"partitionColumn\": \"RECORD\",\n\"numPartitions\": 10,\n\"lowerBound\": 1,\n\"upperBound\": 2000,\n\"fetchSize\": 100000,\n\"compress\": True,\n},\n},\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_path/jdbc_template/parallel_2/my_identifier/\",\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_my_identifier_bronze_date\",\n\"input_id\": \"my_identifier_bronze\",\n\"transformers\": [{\"function\": \"get_max_value\", \"args\": {\"input_col\": \"RECORD\"}}],\n},\n{\n\"spec_id\": \"appended_my_identifier\",\n\"input_id\": \"my_identifier_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\"input_col\": \"RECORD\", \"increment_df\": \"max_my_identifier_bronze_date\"},\n}\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"appended_my_identifier\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"partitions\": [\"RECORD\"],\n\"location\": \"s3://my_path/jdbc_template/parallel_2/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#23-parallel-extraction-with-predicates-recommended","title":"2.3 - Parallel Extraction with Predicates (Recommended)","text":"<p>This scenario performs the extraction from JDBC source in parallel, useful in contexts where there aren't numeric, date or timestamp columns to parallelize the extraction:</p> <ul> <li> <p><code>partitionColumn</code> - column used to split the extraction (can be of any type).</p> </li> <li> <p>This is an adequate example to be followed if there is a column in the data source that is good to be used as the <code>partitionColumn</code>, specially if these columns are not complying with the scenario 2.2.</p> </li> </ul> <p>When this property is used, all predicates to Spark need to be provided, otherwise it will leave data behind.</p> <p>Bellow, a lakehouse function to generate predicate list automatically, is presented.</p> <p>By using this function one needs to be careful specially on predicates_query and predicates_add_null variables.</p> <p>predicates_query: At the sample below the whole table (<code>select distinct(x) from table</code>) is being considered, but it is possible to filter using predicates list here, specially if you are applying filter on transformations spec, and you know entire table won't be necessary, so you can change it to something like this: <code>select distinct(x) from table where x &gt; y</code>.</p> <p>predicates_add_null: One can consider if null on predicates list or not. By default, this property is True. Example: for <code>\"partitionColumn\": \"record\"</code></p>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#init-load-data-into-the-bronze-bucket_2","title":"Init - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nfrom lakehouse_engine.core.exec_env import ExecEnv\nfrom lakehouse_engine.utils.extraction.jdbc_extraction_utils import (\nJDBCExtraction,\nJDBCExtractionUtils,\n)\nExecEnv.get_or_create()\npartitionColumn = \"my_partition_col\"\ndbtable = \"my_database.my_table\"\npredicates_query = f\"\"\"(SELECT DISTINCT({partitionColumn}) FROM {dbtable})\"\"\"\ncolumn_for_predicates = partitionColumn\nuser = \"my_user\"\npassword = \"my_b4_hana_pwd\"\nurl = \"my_sap_b4_url\"\ndriver = \"com.sap.db.jdbc.Driver\"\npredicates_add_null = True\njdbc_util = JDBCExtractionUtils(\nJDBCExtraction(\nuser=user,\npassword=password,\nurl=url,\npredicates_add_null=predicates_add_null,\npartition_column=partitionColumn,\ndbtable=dbtable,\n)\n)\npredicates = jdbc_util.get_predicates(predicates_query)\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"predicates\": predicates,\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"fetchSize\": 100000,\n\"compress\": True,\n},\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"my_identifier_source\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\"RECORD\"],\n\"location\": \"s3://my_path/jdbc_template/parallel_3/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/extract_using_jdbc_connection/extract_using_jdbc_connection.html#delta-load-data-into-the-bronze-bucket_3","title":"Delta - Load data into the Bronze Bucket","text":"<pre><code>from lakehouse_engine.engine import load_data\nfrom lakehouse_engine.core.exec_env import ExecEnv\nfrom lakehouse_engine.utils.extraction.jdbc_extraction_utils import (\nJDBCExtraction,\nJDBCExtractionUtils,\n)\nExecEnv.get_or_create()\npartitionColumn = \"my_partition_col\"\ndbtable = \"my_database.my_table\"\npredicates_query = f\"\"\"(SELECT DISTINCT({partitionColumn}) FROM {dbtable})\"\"\"\ncolumn_for_predicates = partitionColumn\nuser = \"my_user\"\npassword = \"my_b4_hana_pwd\"\nurl = \"my_sap_b4_url\"\ndriver = \"com.sap.db.jdbc.Driver\"\npredicates_add_null = True\njdbc_util = JDBCExtractionUtils(\nJDBCExtraction(\nuser=user,\npassword=password,\nurl=url,\npredicates_add_null=predicates_add_null,\npartition_column=partitionColumn,\ndbtable=dbtable,\n)\n)\npredicates = jdbc_util.get_predicates(predicates_query)\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"my_identifier_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"my_sap_b4_url\",\n\"table\": \"my_database.my_table\",\n\"predicates\": predicates,\n\"properties\": {\n\"user\": \"my_user\",\n\"password\": \"my_b4_hana_pwd\",\n\"driver\": \"com.sap.db.jdbc.Driver\",\n},\n},\n\"options\": {\n\"fetchSize\": 100000,\n\"compress\": True,\n},\n},\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_path/jdbc_template/parallel_3/my_identifier/\",\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"max_my_identifier_bronze_date\",\n\"input_id\": \"my_identifier_bronze\",\n\"transformers\": [{\"function\": \"get_max_value\", \"args\": {\"input_col\": \"RECORD\"}}],\n},\n{\n\"spec_id\": \"appended_my_identifier\",\n\"input_id\": \"my_identifier_source\",\n\"transformers\": [\n{\n\"function\": \"incremental_filter\",\n\"args\": {\"input_col\": \"RECORD\", \"increment_df\": \"max_my_identifier_bronze_date\"},\n}\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"my_identifier_bronze\",\n\"input_id\": \"appended_my_identifier\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"partitions\": [\"RECORD\"],\n\"location\": \"s3://my_path/jdbc_template/parallel_3/my_identifier/\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/filtered_full_load/filtered_full_load.html","title":"Filtered Full Load","text":"<p>This scenario is very similar to the full load, but it filters the data coming from the source, instead of doing a complete full load. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": true,\n\"delimiter\": \"|\",\n\"inferSchema\": true\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/full_load/with_filter/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"filtered_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"expression_filter\",\n\"args\": {\n\"exp\": \"date like '2016%'\"\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"filtered_sales\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"parquet\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/full_load/with_filter/data\"\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/filtered_full_load/filtered_full_load.html#relevant-notes","title":"Relevant notes:","text":"<ul> <li>As seen in the ACON, the filtering capabilities are provided by a transformer called <code>expression_filter</code>, where you can provide a custom Spark SQL filter.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/filtered_full_load_with_selective_replace/filtered_full_load_with_selective_replace.html","title":"Filtered Full Load with Selective Replace","text":"<p>This scenario is very similar to the Filtered Full Load, but we only replace a subset of the partitions, leaving the other ones untouched, so we don't replace the entire table. This capability is very useful for backfilling scenarios. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": true,\n\"delimiter\": \"|\",\n\"inferSchema\": true\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/full_load/with_filter_partition_overwrite/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"filtered_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"expression_filter\",\n\"args\": {\n\"exp\": \"date like '2016%'\"\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"filtered_sales\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\",\n\"customer\"\n],\n\"location\": \"file:///app/tests/lakehouse/out/feature/full_load/with_filter_partition_overwrite/data\",\n\"options\": {\n\"replaceWhere\": \"date like '2016%'\"\n}\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/filtered_full_load_with_selective_replace/filtered_full_load_with_selective_replace.html#relevant-notes","title":"Relevant notes:","text":"<ul> <li>The key option for this scenario in the ACON is the <code>replaceWhere</code>, which we use to only overwrite a specific period of time, that realistically can match a subset of all the partitions of the table. Therefore, this capability is very useful for backfilling scenarios.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/flatten_schema_and_explode_columns/flatten_schema_and_explode_columns.html","title":"Flatten Schema and Explode Columns","text":"<p>Related with schema, we can make two kind of operations:</p> <ul> <li> <p>Flatten Schema: transformer named \"flatten_schema\" used to flatten the schema of dataframe.</p> <ul> <li>Parameters to be defined:<ul> <li>max_level: 2 =&gt; this sets the level until you want to flatten the schema.</li> <li>shorten_names: True =&gt; this flag is when you want to shorten the name of the prefixes of the fields.</li> <li>alias: True =&gt; this flag is used when you want to define a prefix for the column to be flattened.</li> <li>num_chars: 7 =&gt; this sets the number of characters to consider when shortening the names of the fields.</li> <li>ignore_cols: True =&gt; this list value should be set to specify the columns you don't want to flatten.</li> </ul> </li> </ul> </li> <li> <p>Explode Columns: transformer named \"explode_columns\" used to explode columns with types ArrayType and MapType. </p> <ul> <li>Parameters to be defined:<ul> <li>explode_arrays: True =&gt; this flag should be set to true to explode all array columns present in the dataframe.</li> <li>array_cols_to_explode: [\"sample_col\"] =&gt; this list value should be set when to specify the array columns desired to explode.</li> <li>explode_maps: True =&gt; this flag should be set to true to explode all map columns present in the dataframe.</li> <li>map_cols_to_explode: [\"map_col\"] =&gt; this list value should be set when to specify the map columns desired to explode.</li> </ul> </li> <li>Recommendation: use array_cols_to_explode and map_cols_to_explode to specify the columns desired to explode and do not do it for all of them.</li> </ul> </li> </ul> <p>The below scenario of flatten_schema is transforming one or more columns and dividing the content nested in more columns, as desired. We defined the number of levels we want to flatten in the schema, regarding the nested values. In this case, we are just setting <code>max_level</code> of <code>2</code>. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"json\",\n\"schema_path\": \"file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_schema/source_schema.json\",\n\"location\": \"file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_schema/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": {\n\"date\": \"date2\",\n\"customer\": \"customer2\"\n}\n}\n},\n{\n\"function\": \"with_expressions\",\n\"args\": {\n\"cols_and_exprs\": {\n\"constant\": \"'just a constant'\",\n\"length_customer2\": \"length(customer2)\"\n}\n}\n},\n{\n\"function\": \"from_json\",\n\"args\": {\n\"input_col\": \"sample\",\n\"schema\": {\n\"type\": \"struct\",\n\"fields\": [\n{\n\"name\": \"field1\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"field2\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"field3\",\n\"type\": \"double\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"field4\",\n\"type\": {\n\"type\": \"struct\",\n\"fields\": [\n{\n\"name\": \"field1\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"field2\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n}\n]\n},\n\"nullable\": true,\n\"metadata\": {}\n}\n]\n}\n}\n},\n{\n\"function\": \"to_json\",\n\"args\": {\n\"in_cols\": [\n\"item\",\n\"amount\"\n],\n\"out_col\": \"item_amount_json\"\n}\n},\n{\n\"function\": \"flatten_schema\",\n\"args\": {\n\"max_level\": 2\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/transformations/column_reshapers/flatten_schema/batch/data\"\n}\n]\n</code></pre></p> <p>The scenario of explode_arrays is transforming the arrays columns in one or more rows, depending on the number of elements, so, it replicates the row for each array value. In this case we are using explode to all array columns, using <code>explode_arrays</code> as <code>true</code>. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"json\",\n\"schema_path\": \"file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/explode_arrays/source_schema.json\",\n\"location\": \"file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/explode_arrays/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": {\n\"date\": \"date2\",\n\"customer\": \"customer2\"\n}\n}\n},\n{\n\"function\": \"with_expressions\",\n\"args\": {\n\"cols_and_exprs\": {\n\"constant\": \"'just a constant'\",\n\"length_customer2\": \"length(customer2)\"\n}\n}\n},\n{\n\"function\": \"to_json\",\n\"args\": {\n\"in_cols\": [\n\"item\",\n\"amount\"\n],\n\"out_col\": \"item_amount_json\"\n}\n},\n{\n\"function\": \"explode_columns\",\n\"args\": {\n\"explode_arrays\": true\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/transformations/column_reshapers/explode_arrays/batch/data\"\n}\n]\n</code></pre></p> <p>The scenario of flatten_and_explode_arrays_and_maps is using <code>flatten_schema</code> and <code>explode_columns</code> to have the desired output. In this case, the desired output is to flatten all schema and explode maps and arrays, even having an array inside a struct. Steps:</p> <pre><code>1. In this case, we have an array column inside a struct column, so first we need to use the `flatten_schema` transformer to extract the columns inside that struct;\n2. Then, we are able to explode all the array columns desired and map columns, using `explode_columns` transformer.\n3. To be able to have the map column in 2 columns, we use again the `flatten_schema` transformer.\n</code></pre> <p>As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"json\",\n\"schema_path\": \"file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_and_explode_arrays_and_maps/source_schema.json\",\n\"location\": \"file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_and_explode_arrays_and_maps/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": {\n\"date\": \"date2\",\n\"customer\": \"customer2\"\n}\n}\n},\n{\n\"function\": \"with_expressions\",\n\"args\": {\n\"cols_and_exprs\": {\n\"constant\": \"'just a constant'\",\n\"length_customer2\": \"length(customer2)\"\n}\n}\n},\n{\n\"function\": \"from_json\",\n\"args\": {\n\"input_col\": \"agg_fields\",\n\"schema\": {\n\"type\": \"struct\",\n\"fields\": [\n{\n\"name\": \"field1\",\n\"nullable\": true,\n\"metadata\": {},\n\"type\": {\n\"containsNull\": true,\n\"elementType\": \"string\",\n\"type\": \"array\"\n}\n},\n{\n\"name\": \"field2\",\n\"type\": {\n\"type\": \"struct\",\n\"fields\": [\n{\n\"name\": \"field1\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"field2\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n}\n]\n},\n\"nullable\": true,\n\"metadata\": {}\n}\n]\n}\n}\n},\n{\n\"function\": \"to_json\",\n\"args\": {\n\"in_cols\": [\n\"item\",\n\"amount\"\n],\n\"out_col\": \"item_amount_json\"\n}\n},\n{\n\"function\": \"flatten_schema\",\n\"args\": {\n\"max_level\": 2\n}\n},\n{\n\"function\": \"explode_columns\",\n\"args\": {\n\"explode_arrays\": true,\n\"map_cols_to_explode\": [\n\"sample\"\n]\n}\n},\n{\n\"function\": \"flatten_schema\",\n\"args\": {\n\"max_level\": 2\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"append\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/transformations/column_reshapers/flatten_and_explode_arrays_and_maps/batch/data\"\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/full_load/full_load.html","title":"Full Load","text":"<p>This scenario reads CSV data from a path and writes in full to another path with delta lake files.</p>"},{"location":"lakehouse_engine_usage/data_loader/full_load/full_load.html#relevant-notes","title":"Relevant notes","text":"<ul> <li>This ACON infers the schema automatically through the option <code>inferSchema</code> (we use it for local tests only). This is usually not a best practice using CSV files, and you should provide a schema through the InputSpec variables <code>schema_path</code>, <code>read_schema_from_table</code> or <code>schema</code>.</li> <li>The <code>transform_specs</code> in this case are purely optional, and we basically use the repartition transformer to create one partition per combination of date and customer. This does not mean you have to use this in your algorithm.</li> <li>A full load is also adequate for an init load (initial load).</li> </ul> <p>As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": true,\n\"delimiter\": \"|\",\n\"inferSchema\": true\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/full_load/full_overwrite/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"repartitioned_sales\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"repartition\",\n\"args\": {\n\"num_partitions\": 1,\n\"cols\": [\"date\", \"customer\"]\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\",\n\"customer\"\n],\n\"location\": \"file:///app/tests/lakehouse/out/feature/full_load/full_overwrite/data\"\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/read_from_dataframe/read_from_dataframe.html","title":"Read from Dataframe","text":"<p>Danger</p> <p>Don't use this feature if the Lakehouse Engine already has a supported data format for your use case, as in that case it is preferred to use the dedicated data formats which are more extensively tested and predictable. Check the supported data formats here.</p> <p>Reading from a Spark DataFrame is very simple using our framework. You just need to define the input_specs as follows: </p> <pre><code>{\n\"input_spec\": {\n\"spec_id\": \"my_df\",\n\"read_type\": \"batch\",\n\"data_format\": \"dataframe\",\n\"df_name\": df,\n}\n}\n</code></pre> <p>Why is it relevant?</p> <p>With this capability of reading a dataframe you can deal with sources that do not yet officially have a reader (e.g., REST api, XML files, etc.).</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_append_load_with_malformed/streaming_append_load_with_malformed.html","title":"Streaming Append Load with DROPMALFORMED","text":"<p>This scenario illustrates an append load done via streaming instead of batch, providing an efficient way of picking up new files from an S3 folder, instead of relying on the incremental filtering from the source needed from a batch based process (see append loads in batch from a JDBC source to understand the differences between streaming and batch append loads). However, not all sources (e.g., JDBC) allow streaming. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": true,\n\"delimiter\": \"|\",\n\"mode\": \"DROPMALFORMED\"\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/append_load/streaming_dropmalformed/data\",\n\"schema\": {\n\"type\": \"struct\",\n\"fields\": [\n{\n\"name\": \"salesorder\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"item\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"date\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"customer\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"article\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"amount\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n}\n]\n}\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"append\",\n\"db_table\": \"test_db.streaming_dropmalformed_table\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\"\n],\n\"options\": {\n\"checkpointLocation\": \"file:///app/tests/lakehouse/out/feature/append_load/streaming_dropmalformed/checkpoint\"\n},\n\"location\": \"file:///app/tests/lakehouse/out/feature/append_load/streaming_dropmalformed/data\"\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_append_load_with_malformed/streaming_append_load_with_malformed.html#relevant-notes","title":"Relevant notes:","text":"<ul> <li>In this scenario, we use DROPMALFORMED read mode, which drops rows that do not comply with the provided schema;</li> <li>In this scenario, the schema is provided through the <code>input_spec</code> \"schema\" variable. This removes the need of a separate JSON Spark schema file, which may be more convenient in certain cases.</li> <li>As can be seen, we use the <code>output_spec</code> Spark option <code>checkpointLocation</code> to specify where to save the checkpoints indicating what we have already consumed from the input data. This allows fault-tolerance if the streaming job fails, but more importantly, it allows us to run a streaming job using AvailableNow and the next job automatically picks up the stream state since the last checkpoint, allowing us to do efficient append loads without having to manually specify incremental filters as we do for batch append loads.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/streaming_append_load_with_terminator/streaming_append_load_with_terminator.html","title":"Streaming Append Load with Optimize Dataset Terminator","text":"<p>This scenario includes a terminator which optimizes a dataset (table), being able of vacuuming the table, optimising it with z-order or not, computing table statistics and more. You can find more details on the Terminator here.</p> <p>As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": true,\n\"delimiter\": \"|\",\n\"mode\": \"DROPMALFORMED\"\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/append_load/streaming_with_terminators/data\",\n\"schema\": {\n\"type\": \"struct\",\n\"fields\": [\n{\n\"name\": \"salesorder\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"item\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"date\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"customer\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"article\",\n\"type\": \"string\",\n\"nullable\": true,\n\"metadata\": {}\n},\n{\n\"name\": \"amount\",\n\"type\": \"integer\",\n\"nullable\": true,\n\"metadata\": {}\n}\n]\n}\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"sales_source\",\n\"write_type\": \"append\",\n\"db_table\": \"test_db.streaming_with_terminators_table\",\n\"data_format\": \"delta\",\n\"partitions\": [\n\"date\"\n],\n\"options\": {\n\"checkpointLocation\": \"file:///app/tests/lakehouse/out/feature/append_load/streaming_with_terminators/checkpoint\"\n},\n\"location\": \"file:///app/tests/lakehouse/out/feature/append_load/streaming_with_terminators/data\"\n}\n],\n\"terminate_specs\": [\n{\n\"function\": \"optimize_dataset\",\n\"args\": {\n\"db_table\": \"test_db.streaming_with_terminators_table\",\n\"debug\": true\n}\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_load_with_group_and_rank_condensation/streaming_delta_load_with_group_and_rank_condensation.html","title":"Streaming Delta Load with Group and Rank Condensation","text":"<p>This scenario is useful for when we want to do delta loads based on changelogs that need to be first condensed based on a group by and then a rank only, instead of the record mode logic in the record mode based change data capture. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",\n\"schema_path\": \"file:///app/tests/lakehouse/in/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/source_schema.json\",\n\"with_filepath\": true,\n\"options\": {\n\"mode\": \"FAILFAST\",\n\"header\": true,\n\"delimiter\": \"|\"\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"sales_bronze_with_extraction_date\",\n\"input_id\": \"sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"with_regex_value\",\n\"args\": {\n\"input_col\": \"lhe_extraction_filepath\",\n\"output_col\": \"extraction_date\",\n\"drop_input_col\": true,\n\"regex\": \".*WE_SO_SCL_(\\\\d+).csv\"\n}\n},\n{\n\"function\": \"with_auto_increment_id\"\n},\n{\n\"function\": \"group_and_rank\",\n\"args\": {\n\"group_key\": [\n\"salesorder\",\n\"item\"\n],\n\"ranking_key\": [\n\"extraction_date\",\n\"changed_on\",\n\"lhe_row_id\"\n]\n}\n},\n{\n\"function\": \"repartition\",\n\"args\": {\n\"num_partitions\": 1\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_silver\",\n\"input_id\": \"sales_bronze_with_extraction_date\",\n\"write_type\": \"merge\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/data\",\n\"options\": {\n\"checkpointLocation\": \"file:///app/tests/lakehouse/out/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/checkpoint\"\n},\n\"with_batch_id\": true,\n\"merge_opts\": {\n\"merge_predicate\": \"current.salesorder = new.salesorder and current.item = new.item\",\n\"update_predicate\": \"new.extraction_date &gt;= current.extraction_date and new.changed_on &gt;= current.changed_on\",\n\"delete_predicate\": \"new.extraction_date &gt;= current.extraction_date and new.changed_on &gt;= current.changed_on and new.event = 'deleted'\"\n}\n}\n]\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_load_with_group_and_rank_condensation/streaming_delta_load_with_group_and_rank_condensation.html#relevant-notes","title":"Relevant notes:","text":"<ul> <li>This type of delta load with this type of condensation is useful when the source changelog can be condensed based on dates, instead of technical fields like <code>datapakid</code>, <code>record</code>, <code>record_mode</code>, etc., as we see in SAP BW DSOs.An example of such system is Omnihub Tibco orders and deliveries files.</li> </ul>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html","title":"Streaming Delta Load with Late Arriving and Out of Order Events (with and without watermarking)","text":""},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#how-to-deal-with-late-arriving-data-without-using-watermark","title":"How to Deal with Late Arriving Data without using Watermark","text":"<p>This scenario covers a delta load in streaming mode that is able to deal with late arriving and out of order events. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre> Example of ACON configuration: <pre><code>{\n\"input_specs\": [\n{\n\"spec_id\": \"sales_source\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": true,\n\"delimiter\": \"|\"\n},\n\"location\": \"file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/late_arriving_changes/streaming/data\"\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"transformed_sales_source\",\n\"input_id\": \"sales_source\",\n\"transformers\": [\n{\n\"function\": \"condense_record_mode_cdc\",\n\"args\": {\n\"business_key\": [\n\"salesorder\",\n\"item\"\n],\n\"ranking_key_desc\": [\n\"extraction_timestamp\",\n\"actrequest_timestamp\",\n\"datapakid\",\n\"partno\",\n\"record\"\n],\n\"record_mode_col\": \"recordmode\",\n\"valid_record_modes\": [\n\"\",\n\"N\",\n\"R\",\n\"D\",\n\"X\"\n]\n}\n}\n]\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"transformed_sales_source\",\n\"write_type\": \"merge\",\n\"data_format\": \"delta\",\n\"location\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/late_arriving_changes/streaming/data\",\n\"options\": {\n\"checkpointLocation\": \"file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/late_arriving_changes/streaming/checkpoint\"\n},\n\"merge_opts\": {\n\"merge_predicate\": \"current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date\",\n\"update_predicate\": \"new.extraction_timestamp &gt; current.extraction_timestamp or new.actrequest_timestamp &gt; current.actrequest_timestamp or ( new.actrequest_timestamp = current.actrequest_timestamp and new.datapakid &gt; current.datapakid) or ( new.actrequest_timestamp = current.actrequest_timestamp and new.datapakid = current.datapakid and new.partno &gt; current.partno) or ( new.actrequest_timestamp = current.actrequest_timestamp and new.datapakid = current.datapakid and new.partno = current.partno and new.record &gt;= current.record)\",\n\"delete_predicate\": \"new.recordmode in ('R','D','X')\",\n\"insert_predicate\": \"new.recordmode is null or new.recordmode not in ('R','D','X')\"\n}\n}\n],\n\"exec_env\": {\n\"spark.sql.streaming.schemaInference\": true\n}\n</code></pre></p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#relevant-notes","title":"Relevant notes:","text":"<ul> <li>First question we can impose is: Do we need such complicated update predicate to handle late arriving and out of order events? Simple answer is no. Because we expect that the latest event (e.g., latest status of a record in the source) will eventually arrive, and therefore the target delta lake table will eventually be consistent. However, when will that happen? Do we want to have our target table inconsistent until the next update comes along? This of course is only true when your source cannot ensure the order of the changes and cannot avoid late arriving changes (e.g., some changes that should have come in this changelog extraction, will only arrive in the next changelog extraction). From previous experiences, this is not the case with SAP BW, for example (as SAP BW is ACID compliant, and it will extract data from an SAP source and only have the updated changelog available when the extraction goes through, so theoretically we should not be able to extract data from the SAP BW changelog while SAP BW is still extracting data). </li> <li>However, when the source cannot fully ensure ordering (e.g., Kafka) and we want to make sure we don't load temporarily inconsistent data into the target table, we can pay extra special attention, as we do here, to our update and insert predicates, that will enable us to only insert or update data if the new event meets the respective predicates:<ul> <li>In this scenario, we will only update if the <code>update_predicate</code> is true, and that long predicate we have here ensures that the change that we are receiving is likely the latest one;</li> <li>In this scenario, we will only insert the record if the record is not marked for deletion (this can happen if the new event is a record that is marked for deletion, but the record was not in the target table (late arriving changes where the delete came before the insert), and therefore, without the <code>insert_predicate</code>, the algorithm would still try to insert the row, even if the <code>record_mode</code> indicates that that row is for deletion. By using the <code>insert_predicate</code> above we avoid that to happen. However, even in such scenario, to prevent the algorithm to insert the data that comes later (which is old, as we said, the delete came before the insert and was actually the latest status), we would even need a more complex predicate based on your data's nature. Therefore, please read the disclaimer below. !!! note \"Disclaimer!\" The scenario illustrated in this page is purely fictional, designed for the Lakehouse Engine local tests specifically. Your data source changelogs may be different and the scenario and predicates discussed here may not make sense to you. Consequently, the data product team should reason about the adequate merge predicate and insert, update and delete predicates, that better reflect how they want to handle the delta loads for their data.</li> </ul> </li> <li>We use spark.sql.streaming.schemaInference in our local tests only. We don't encourage you to use it in your data product.</li> </ul> <p>Documentation</p> <p>Feature Deep Dive: Watermarking in Apache Spark Structured Streaming - The Databricks Blog</p> <p>Structured Streaming Programming Guide - Spark 3.4.0 Documentation</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#how-to-deal-with-late-arriving-data-using-watermark","title":"How to Deal with Late Arriving Data using Watermark","text":"<p>When building real-time pipelines, one of the realities that teams have to work with is that distributed data ingestion is inherently unordered. Additionally, in the context of stateful streaming operations, teams need to be able to properly track event time progress in the stream of data they are ingesting for the proper calculation of time-window aggregations and other stateful operations. While working with real-time streaming data there will be delays between event time and processing time due to how data is ingested and whether the overall application experiences issues like downtime. Due to these potential variable delays, the engine that you use to process this data needs to have some mechanism to decide when to close the aggregate windows and produce the aggregate result.</p> <p>Imagine a scenario where we will need to perform stateful aggregations on the streaming data to understand and identify problems in the machines. This is where we need to leverage Structured Streaming and Watermarking to produce the necessary stateful aggregations.</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#approach-1-use-a-pre-defined-fixed-window-bad","title":"Approach 1 - Use a pre-defined fixed window (Bad)","text":"<p>Credits: Image source</p> <p>To explain this visually let\u2019s take a scenario where we are receiving data at various times from around 10:50 AM \u2192 11:20 AM. We are creating 10-minute tumbling windows that calculate the average of the temperature and pressure readings that came in during the windowed period.</p> <p>In this first picture, we have the tumbling windows trigger at 11:00 AM, 11:10 AM and 11:20 AM leading to the result tables shown at the respective times. When the second batch of data comes around 11:10 AM with data that has an event time of 10:53 AM this gets incorporated into the temperature and pressure averages calculated for the 11:00 AM \u2192 11:10 AM window that closes at 11:10 AM, which does not give the correct result.</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#approach-2-watermark","title":"Approach 2 - Watermark","text":"<p>We can define a watermark that will allow Spark to understand when to close the aggregate window and produce the correct aggregate result. In Structured Streaming applications, we can ensure that all relevant data for the aggregations we want to calculate is collected by using a feature called watermarking. In the most basic sense, by defining a watermark Spark Structured Streaming then knows when it has ingested all data up to some time, T, (based on a set lateness expectation) so that it can close and produce windowed aggregates up to timestamp T.</p> <p></p> <p>Credits: Image source</p> <p>Unlike the first scenario where Spark will emit the windowed aggregation for the previous ten minutes every ten minutes (i.e. emit the 11:00 AM \u219211:10 AM window at 11:10 AM), Spark now waits to close and output the windowed aggregation once the max event time seen minus the specified watermark is greater than the upper bound of the window.</p> <p>In other words, Spark needed to wait until it saw data points where the latest event time seen minus 10 minutes was greater than 11:00 AM to emit the 10:50 AM \u2192 11:00 AM aggregate window. At 11:00 AM, it does not see this, so it only initialises the aggregate calculation in Spark\u2019s internal state store. At 11:10 AM, this condition is still not met, but we have a new data point for 10:53 AM so the internal state gets updated, just not emitted. Then finally by 11:20 AM Spark has seen a data point with an event time of 11:15 AM and since 11:15 AM minus 10 minutes is 11:05 AM which is later than 11:00 AM the 10:50 AM \u2192 11:00 AM window can be emitted to the result table.</p> <p>This produces the correct result by properly incorporating the data based on the expected lateness defined by the watermark. Once the results are emitted the corresponding state is removed from the state store.</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#watermarking-and-different-output-modes","title":"Watermarking and Different Output Modes","text":"<p>It is important to understand how state, late-arriving records, and the different output modes could lead to different behaviours of your application running on Spark. The main takeaway here is that in both append and update modes, once the watermark indicates that all data is received for an aggregate time window, the engine can trim the window state. In append mode the aggregate is produced only at the closing of the time window plus the watermark delay while in update mode it is produced on every update to the window.</p> <p>Lastly, by increasing your watermark delay window you will cause the pipeline to wait longer for data and potentially drop less data \u2013 higher precision, but also higher latency to produce the aggregates. On the flip side, smaller watermark delay leads to lower precision but also lower latency to produce the aggregates.</p> <p>Watermarks can only be used when you are running your streaming application in append or update output modes. There is a third output mode, complete mode, in which the entire result table is written to storage. This mode cannot be used because it requires all aggregate data to be preserved, and hence cannot use watermarking to drop intermediate state.</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#joins-with-watermark","title":"Joins With Watermark","text":"<p>There are three types of stream-stream joins that can be implemented in Structured Streaming: inner, outer, and semi joins. The main problem with doing joins in streaming applications is that you may have an incomplete picture of one side of the join. Giving Spark an understanding of when there are no future matches to expect is similar to the earlier problem with aggregations where Spark needed to understand when there were no new rows to incorporate into the calculation for the aggregation before emitting it.</p> <p>To allow Spark to handle this, we can leverage a combination of watermarks and event-time constraints within the join condition of the stream-stream join. This combination allows Spark to filter out late records and trim the state for the join operation through a time range condition on the join.</p> <p>Spark has a policy for handling multiple watermark definitions. Spark maintains one global watermark that is based on the slowest stream to ensure the highest amount of safety when it comes to not missing data.</p> <p>We can change this behaviour by changing spark.sql.streaming.multipleWatermarkPolicy to max; however, this means that data from the slower stream will be dropped.</p>"},{"location":"lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events/streaming_delta_with_late_arriving_and_out_of_order_events.html#state-store-performance-considerations","title":"State Store Performance Considerations","text":"<p>As of Spark 3.2, Spark offers RocksDB state store provider.</p> <p>If you have stateful operations in your streaming query (for example, streaming aggregation, streaming dropDuplicates, stream-stream joins, mapGroupsWithState, or flatMapGroupsWithState) and you want to maintain millions of keys in the state, then you may face issues related to large JVM garbage collection (GC) pauses causing high variations in the micro-batch processing times. This occurs because, by the implementation of HDFSBackedStateStore, the state data is maintained in the JVM memory of the executors and large number of state objects puts memory pressure on the JVM causing high GC pauses.</p> <p>In such cases, you can choose to use a more optimized state management solution based on RocksDB. Rather than keeping the state in the JVM memory, this solution uses RocksDB to efficiently manage the state in the native memory and the local disk. Furthermore, any changes to this state are automatically saved by Structured Streaming to the checkpoint location you have provided, thus providing full fault-tolerance guarantees (the same as default state management).</p> <p>To enable the new build-in state store implementation, set <code>spark.sql.streaming.stateStore.providerClass</code> to <code>org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider</code>.</p> <p>For more details please visit Spark documentation: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#rocksdb-state-store-implementation</p> <p>You can enable this in your acons, by specifying it as part of the exec_env properties like below:</p> <pre><code>\"exec_env\": {\n\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n}\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html","title":"Write and Read Dataframe","text":"<p>DataFrame writer can give us some advantages by returning a dictionary containing the <code>spec_id</code> and the computed dataframe. In these examples we will cover the following scenarios of using the output <code>dataframe</code> format:</p> <ol> <li>Write to dataframe: Consuming the output spec as DataFrame;</li> <li>Write all dataframes: Consuming all DataFrames generated per specs;</li> <li>Read from and Write to dataframe: Making use of the DataFrame output spec to compose silver data.</li> </ol>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#main-advantages-of-using-this-output-writer","title":"Main advantages of using this output writer:","text":"<ul> <li>Debugging purposes: as we can access any dataframe used in any part of our ACON   we can observe what is happening with the computation and identify what might be wrong   or can be improved.</li> <li>Flexibility: in case we have some very specific need not covered yet by the lakehouse   engine capabilities, example: return the Dataframe for further processing like using a machine   learning model/prediction.</li> <li>Simplify ACONs: instead developing a single complex ACON, using the Dataframe writer,   we can compose our ACON from the output of another ACON. This allows us to identify   and split the notebook logic across ACONs.</li> </ul> <p>If you want/need, you can add as many dataframes as you want in the output spec referencing the spec_id you want to add.</p> <p>Warning</p> <p>This is not intended to replace the other capabilities offered by the lakehouse-engine and in case other feature can cover your use case, you should use it instead of using the Dataframe writer, as they are much more extensively tested on different type of operations.</p> <p>Additionally, please always introspect if the problem that you are trying to resolve and for which no lakehouse-engine feature is available, could be a common problem and thus deserve a common solution and feature.</p> <p>Moreover, Dataframe writer is not supported for the streaming trigger   types <code>processing time</code> and <code>continuous</code>.</p>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#1-write-to-dataframe-consuming-the-output-spec-as-dataframe","title":"1. Write to dataframe: Consuming the output spec as DataFrame","text":""},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#silver-dummy-sales-write-to-dataframe","title":"Silver Dummy Sales Write to DataFrame","text":"<p>In this example we will cover the Dummy Sales write to a result containing the output DataFrame.</p> <ul> <li>An ACON is used to read from bronze, apply silver transformations and write to a dictionary   containing the output spec as key and the dataframe as value through the following steps:<ul> <li>1 - Definition of how to read data (input data location, read type and data format);</li> <li>2 - Transformation of data (rename relevant columns);</li> <li>3 - Write the data to dict containing the dataframe;</li> </ul> </li> </ul> <p>Note</p> <p>If you are trying to retrieve more than once the same data using checkpoint it will return an empty dataframe with empty schema as we don't have new data to read.</p> <pre><code>from lakehouse_engine.engine import load_data\ncols_to_rename = {\"item\": \"ordered_item\", \"date\": \"order_date\", \"article\": \"article_id\"}\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_sales_bronze\",\n\"read_type\": \"streaming\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_sales\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_sales_transform\",\n\"input_id\": \"dummy_sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": cols_to_rename,\n},\n},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_sales_silver\",\n\"input_id\": \"dummy_sales_transform\",\n\"data_format\": \"dataframe\",\n\"options\": {\n\"checkpointLocation\": \"s3://my_data_product_bucket/checkpoints/bronze/dummy_sales\",\n},\n}\n],\n}\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#run-the-load-and-return-the-dictionary-with-the-dataframes-by-outputspec","title":"Run the Load and Return the Dictionary with the DataFrames by OutputSpec","text":"<p>This exploratory test will return a dictionary with the output spec and the dataframe that will be stored after transformations.</p> <pre><code>output = load_data(acon=acon)\ndisplay(output.keys())\ndisplay(output.get(\"dummy_sales_silver\"))\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#2-write-all-dataframes-consuming-all-dataframes-generated-per-specs","title":"2. Write all dataframes: Consuming all DataFrames generated per specs","text":""},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#silver-dummy-sales-write-to-dataframe_1","title":"Silver Dummy Sales Write to DataFrame","text":"<p>In this example we will cover the Dummy Sales write to a result containing the specs and related DataFrame.</p> <ul> <li>An ACON is used to read from bronze, apply silver transformations and write to a dictionary   containing the spec id as key and the DataFrames as value through the following steps:<ul> <li>Definition of how to read data (input data location, read type and data format);</li> <li>Transformation of data (rename relevant columns);</li> <li>Write the data to a dictionary containing all the spec ids and DataFrames computed per step;</li> </ul> </li> </ul> <pre><code>from lakehouse_engine.engine import load_data\ncols_to_rename = {\"item\": \"ordered_item\", \"date\": \"order_date\", \"article\": \"article_id\"}\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_sales_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_sales\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_sales_transform\",\n\"input_id\": \"dummy_sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": cols_to_rename,\n},\n},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"sales_bronze\",\n\"input_id\": \"dummy_sales_bronze\",\n\"data_format\": \"dataframe\",\n},\n{\n\"spec_id\": \"sales_silver\",\n\"input_id\": \"dummy_sales_transform\",\n\"data_format\": \"dataframe\",\n},\n],\n}\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#run-the-load-and-return-the-dictionary-with-the-related-dataframes-by-spec","title":"Run the Load and Return the Dictionary with the related DataFrames by Spec","text":"<p>This exploratory test will return a dictionary with all specs and the related dataframe. You can access the DataFrame you need by <code>output.get(&lt;spec_id&gt;)</code> for future developments and tests.</p> <pre><code>output = load_data(acon=acon)\ndisplay(output.keys())\ndisplay(output.get(\"sales_bronze\"))\ndisplay(output.get(\"sales_silver\"))\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#3-read-from-and-write-to-dataframe-making-use-of-the-dataframe-output-spec-to-compose-silver-data","title":"3. Read from and Write to dataframe: Making use of the DataFrame output spec to compose silver data","text":""},{"location":"lakehouse_engine_usage/data_loader/write_and_read_dataframe/write_and_read_dataframe.html#silver-load-dummy-deliveries","title":"Silver Load Dummy Deliveries","text":"<p>In this example we will cover the Dummy Deliveries table read and incremental load to silver composing the silver data to write using the DataFrame output spec:</p> <ul> <li>First ACON is used to get the latest data from bronze, in this step we are using more than one output because we will need the bronze data with the latest data in the next step.</li> <li>Second ACON is used to consume the bronze data and the latest data to perform silver transformation, in this ACON we are using as input the two dataframes computed by the first ACON.</li> <li>Third ACON is used to write the silver computed data from the previous ACON to the target.</li> </ul> <p>Note</p> <p>This example is not a recommendation on how to deal with incremental loads, the ACON was split in 3 for demo purposes.</p> <p>Consume bronze data, generate the latest data and return a dictionary with bronze and transformed dataframes:</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_sales\",\n},\n{\n\"spec_id\": \"dummy_deliveries_silver_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"db_table\": \"my_database.dummy_deliveries\",\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_table_max_value\",\n\"input_id\": \"dummy_deliveries_silver_source\",\n\"transformers\": [\n{\n\"function\": \"get_max_value\",\n\"args\": {\"input_col\": \"delivery_date\", \"output_col\": \"latest\"},\n},\n{\n\"function\": \"with_expressions\",\n\"args\": {\n\"cols_and_exprs\": {\"latest\": \"CASE WHEN latest IS NULL THEN 0 ELSE latest END\"},\n},\n},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"deliveries_bronze\",\n\"input_id\": \"dummy_deliveries_bronze\",\n\"data_format\": \"dataframe\",\n},\n{\n\"spec_id\": \"dummy_deliveries_transformed\",\n\"input_id\": \"dummy_deliveries_table_max_value\",\n\"data_format\": \"dataframe\",\n},\n],\n}\ndummy_deliveries_transformed = load_data(acon=acon)\ndummy_deliveries_transformed_df = dummy_deliveries_transformed.get(\"dummy_deliveries_transformed\")\ndummy_deliveries_bronze_df = dummy_deliveries_transformed.get(\"deliveries_bronze\")\n</code></pre> <p>Consume previous dataframes generated by the first ACON (bronze and latest bronze data) to generate the silver data. In this acon we are only using just one output because we only need the dataframe from the output for the next step.</p> <pre><code>from lakehouse_engine.engine import load_data\ncols_to_rename = {\"delivery_note_header\": \"delivery_note\", \"article\": \"article_id\"}\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"dataframe\",\n\"df_name\": dummy_deliveries_bronze_df,\n},\n{\n\"spec_id\": \"dummy_deliveries_table_max_value\",\n\"read_type\": \"batch\",\n\"data_format\": \"dataframe\",\n\"df_name\": dummy_deliveries_transformed_df,\n},\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_transform\",\n\"input_id\": \"dummy_deliveries_bronze\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": cols_to_rename,\n},\n},\n{\n\"function\": \"incremental_filter\",\n\"args\": {\n\"input_col\": \"delivery_date\",\n\"increment_df\": \"dummy_deliveries_table_max_value\",\n\"increment_col\": \"latest\",\n\"greater_or_equal\": False,\n},\n},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_silver\",\n\"input_id\": \"dummy_deliveries_transform\",\n\"data_format\": \"dataframe\",\n}\n],\n}\ndummy_deliveries_silver = load_data(acon=acon)\ndummy_deliveries_silver_df = dummy_deliveries_silver.get(\"dummy_deliveries_silver\")\n</code></pre> <p>Write the silver data generated by previous ACON into the target</p> <pre><code>from lakehouse_engine.engine import load_data\nwrite_silver_acon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_silver\",\n\"read_type\": \"batch\",\n\"data_format\": \"dataframe\",\n\"df_name\": dummy_deliveries_silver_df,\n},\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_quality\",\n\"input_id\": \"dummy_deliveries_silver\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"expectations_store_prefix\": \"dq/expectations/\",\n\"validations_store_prefix\": \"dq/validations/\",\n\"data_docs_prefix\": \"dq/data_docs/site/\",\n\"checkpoint_store_prefix\": \"dq/checkpoints/\",\n\"result_sink_db_table\": \"my_database.dummy_deliveries_dq\",\n\"result_sink_location\": \"my_data_product_bucket/dq/dummy_deliveries\",\n\"fail_on_error\": False,\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"dq_functions\": [\n{\n\"function\": \"expect_column_values_to_not_be_null\",\n\"args\": {\"column\": \"delivery_note\"},\n},\n{\n\"function\": \"expect_table_row_count_to_be_between\",\n\"args\": {\"min_value\": 19},\n},\n{\n\"function\": \"expect_column_max_to_be_between\",\n\"args\": {\"column\": \"delivery_item\", \"min_value\": 2},\n},\n],\n},\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_silver\",\n\"input_id\": \"dummy_deliveries_quality\",\n\"write_type\": \"append\",\n\"location\": \"s3://my_data_product_bucket/silver/dummy_deliveries_df_writer\",\n\"data_format\": \"delta\",\n}\n],\n\"exec_env\": {\n\"spark.databricks.delta.schema.autoMerge.enabled\": True,\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.databricks.delta.autoCompact.enabled\": True,\n},\n}\nload_data(acon=write_silver_acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_to_console/write_to_console.html","title":"Write to Console","text":"<p>Console writer is an interesting feature to debug / validate what have been done on lakehouse engine. Before moving forward and store data somewhere, it is possible to show / print the final dataframe to the console, which means it is possible to transform the data as many times as you want and display the final result to validate if it is as expected.</p>"},{"location":"lakehouse_engine_usage/data_loader/write_to_console/write_to_console.html#silver-dummy-sales-write-to-console-example","title":"Silver Dummy Sales Write to Console Example","text":"<p>In this template we will cover the Dummy Sales write to console. An ACON is used to read from bronze, apply silver transformations and write on console through the following steps:</p> <ol> <li>Definition of how to read data (input data location, read type and data format);</li> <li>Transformation of data (rename relevant columns);</li> <li>Definition of how to print to console (limit, truncate, vertical options);</li> </ol> <p>For this, the ACON specs are :</p> <ul> <li>input_specs (MANDATORY): specify how to read data;</li> <li>transform specs (OPTIONAL): specify how to transform data;</li> <li>output_specs (MANDATORY): specify how to write data to the target.</li> </ul> <p>Note</p> <p>Writer to console is a wrapper for spark.show() function, if you want to know more about the function itself or the available options, please check the spark documentation here.</p> <pre><code>from lakehouse_engine.engine import load_data\ncols_to_rename = {\"item\": \"ordered_item\", \"date\": \"order_date\", \"article\": \"article_id\"}\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_sales_bronze\",\n\"read_type\": \"streaming\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_sales\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_sales_transform\",\n\"input_id\": \"dummy_sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": cols_to_rename,\n},\n},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_sales_silver\",\n\"input_id\": \"dummy_sales_transform\",\n\"data_format\": \"console\",\n\"options\": {\"limit\": 8, \"truncate\": False, \"vertical\": False},\n}\n],\n}\n</code></pre> <p>And then, Run the Load and Exit the Notebook: This exploratory test will write to the console, which means the final dataframe will be displayed.</p> <pre><code>load_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_loader/write_to_rest_api/write_to_rest_api.html","title":"Write to REST API","text":"<p>REST API writer is an interesting feature to send data from Spark to a REST API within the data pipeline context. It uses the Python requests library to execute the REST calls.</p> <p>It is possible to configure a few aspects of the writer, like if the payload should be sent via JSON body or via file, or configure additional JSON body parameters to add to the payload generated via Spark.</p> <p>In the current implementation of the writer, each row will generate a request to the API, so it is important that you prepare your dataframe accordingly (check example below).</p>"},{"location":"lakehouse_engine_usage/data_loader/write_to_rest_api/write_to_rest_api.html#silver-dummy-sales-write-to-rest-api-example","title":"Silver Dummy Sales Write to REST API Example","text":"<p>In this template we will cover the Dummy Sales write to a REST API. An ACON is used to read from bronze, apply silver transformations to prepare the REST api payload and write to the API through the following steps:</p> <ol> <li>Definition of how to read data (input data location, read type and data format);</li> <li>Transformation of the data so that we form a payload column per each row.     Important Note: In the current implementation of the writer, each row will generate a request to the API, so <code>create_payload</code> is a lakehouse engine custom transformer function that creates a JSON string with the payload to be sent to the API. The column name should be exactly \"payload\", so that the lakehouse engine further processes that column accordingly, in order to correctly write the data to the REST API.</li> <li>Definition of how to write to a REST api (url, authentication, payload format configuration, ...);</li> </ol> <p>For this, the ACON specs are :</p> <ul> <li>input_specs (MANDATORY): specify how to read data;</li> <li>transform specs (MANDATORY): specify how to transform data to prepare the payload;</li> <li>output_specs (MANDATORY): specify how to write data to the target.</li> </ul> <pre><code>from lakehouse_engine.engine import load_data\ndef create_payload(df: DataFrame) -&gt; DataFrame:\npayload_df = payload_df.withColumn(\n\"payload\",\nlit('{\"just a dummy key\": \"just a dummy value\"}')\n)\nreturn payload_df\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_sales_bronze\",\n\"read_type\": \"streaming\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_sales\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_sales_transform\",\n\"input_id\": \"dummy_sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"custom_transformation\",\n\"args\": {\n\"custom_transformer\": create_payload,\n},\n}\n],\n},\n],\n\"output_specs\": [\n{ \n\"spec_id\": \"data_to_send_to_api\",\n\"input_id\": \"dummy_sales_transform\",\n\"data_format\": \"rest_api\",\n\"options\": {\n\"rest_api_url\": \"https://foo.bar.com\",\n\"rest_api_method\": \"post\",\n\"rest_api_basic_auth\": {\n\"username\": \"...\",\n\"password\": \"...\",\n},\n\"rest_api_is_file_payload\": False, # True if payload is to be sent via JSON file instead of JSON body (application/json)\n\"rest_api_file_payload_name\": \"custom_file\", # this is the name of the file to be sent in cases where the payload uses file uploads rather than JSON body.\n\"rest_api_extra_json_payload\": {\"x\": \"y\"}\n}\n}\n],\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html","title":"Data Quality","text":"<p>The Data Quality framework is based on Great Expectations (GX) and other custom-made  developments, providing a very light abstraction on top of the GX open source framework and the Spark framework.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#how-to-use-data-quality","title":"How to use Data Quality?","text":""},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#data-loader","title":"Data Loader","text":"<p>You can define data quality rules inside the DataLoader algorithm that you use to load data.</p> <p>Note</p> <p>The DataLoader algorithm allows you to store the results of the data quality checks inside your custom location using the result_sink options (e.g., a delta table on your data product). Using result sink unlocks the  capability to store DQ results having history over all the DQ executions, which can be used for debugging,  to create DQ dashboards on top of the data, and much more.</p> <p>Examples: In these examples, dummy sales local data is used to cover a few example usages of the DQ Framework (based on Great Expectations).</p> <p>The main difference between the sample acons is on the usage of <code>dq_specs</code>.</p> <ul> <li>1 - Minimal Example applying DQ with the Required Parameters</li> <li>2 - Configure Result Sink</li> <li>3 - Validations Failing</li> <li>4 - Row Tagging</li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#data-quality-validator","title":"Data Quality Validator","text":"<p>The DQValidator algorithm focuses on validating data (e.g., spark DataFrames, Files or Tables). In contrast to the <code>dq_specs</code> inside the DataLoader algorithm, the DQValidator focuses on validating data at rest  (post-mortem) instead of validating data in-transit (before it is loaded to the destination).</p> <p>Note</p> <p>The DQValidator algorithm allows you to store the results of the data quality checks inside your custom location using the result_sink options (e.g., a delta table on your data product). Using result sink unlocks the capability to store DQ results having history over all the DQ executions, which can be used for debugging, to create DQ dashboards on top of the data, and much more.</p> <p>Here you can find more information regarding DQValidator and examples.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#reconciliator","title":"Reconciliator","text":"<p>Similarly to the Data Quality Validator algorithm, the Reconciliator algorithm focuses on  validating data at rest (post-mortem). In contrast to the DQValidator algorithm, the Reconciliator always compares a  truth dataset (e.g., spark DataFrames, Files or Tables) with the current dataset (e.g., spark DataFrames, Files or  Tables), instead of executing DQ rules defined by the teams.  Here you can find more information regarding reconciliator and examples.</p> <p>Note</p> <p>Reconciliator does not use Great Expectations, therefore Data Docs and Result Sink and others native methods are not available.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#custom-expectations","title":"Custom Expectations","text":"<p>If your data has a data quality check that cannot be done with the expectations provided by Great Expectations you  can create a custom expectation to make this verification.</p> <p>Note</p> <p>Before creating a custom expectation check if there is an expectation already created to address your needs,  both in Great Expectations and the Lakehouse Engine. Any Custom Expectation that is too specific (using hardcoded table/column names) will be rejected. Expectations should be generic by definition.</p> <p>Here you can find more information regarding custom expectations and examples.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#row-tagging","title":"Row Tagging","text":"<p>The row tagging strategy allows users to tag the rows that failed to be easier to identify the problems  in the validations. Here you can find all the details and examples.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#prisma","title":"Prisma","text":"<p>Prisma is part of the Lakehouse Engine DQ Framework, and it allows users to read DQ functions dynamically from a table instead of writing them explicitly in the Acons. Here you can find more information regarding Prisma.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#how-to-check-the-results-of-the-data-quality-process","title":"How to check the results of the Data Quality Process?","text":""},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#1-tablelocation-analysis","title":"1. Table/location analysis","text":"<p>The possibility to configure a Result Sink allows you to store the history of executions of the DQ process.  You can query the table or the location to search through data and analyse history.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#2-power-bi-dashboard","title":"2. Power BI Dashboard","text":"<p>With the information expanded, interactive analysis can be built on top of the history of the DQ process. A dashboard can be created with the results that we have in <code>dq_specs</code>. To be able to have this information you  need to use arguments <code>result_sink_db_table</code> and/or <code>result_sink_location</code>.</p> <p>Through having a dashboard, the runs and expectations can be analysed, filtered by year, month, source and  run name, and you will have information about the number of runs, some statistics, status of expectations and more.  Analysis such as biggest failures per expectation type, biggest failures by columns, biggest failures per source,  and others can be made, using the information in the <code>result_sink_db_table</code>/<code>result_sink_location</code>.</p> <p>Note</p> <p>The recommendation is to use the same result sink table/location for all your dq_specs and  in the dashboard you will get a preview of the status of all of them.</p> <p></p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality.html#3-data-docs-website","title":"3. Data Docs Website","text":"<p>A site that is auto generated to present you all the relevant information can also be used. If you choose to define  the parameter <code>data_docs_bucket</code> you will be able to store the GX documentation in the defined bucket, and therefore make your data docs available in the DQ Web App (GX UI) visible to everyone.  The <code>data_docs_bucket</code> property supersedes the <code>bucket</code> property only for data docs storage.</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html","title":"Custom Expectations","text":""},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#defining-custom-expectations","title":"Defining Custom Expectations","text":"<p>Custom expectations are defined in python and need to follow a structure to correctly integrate with Great Expectations.</p> <p>Follow the documentation of GX on Creating Custom Expectations  and find information about the existing types of expectations. </p> <p>Here is an example of custom expectation. As for other cases, the acon configuration should be executed with <code>load_data</code> using: <pre><code>from lakehouse_engine.engine import load_data\nacon = {...}\nload_data(acon=acon)\n</code></pre></p> <p>Example of ACON configuration:</p> <pre><code>\"\"\"Expectation to check if column 'a' is lower or equal than column 'b'.\"\"\"\nfrom typing import Any, Dict, Optional\nfrom great_expectations.core import ExpectationConfiguration\nfrom great_expectations.execution_engine import ExecutionEngine, SparkDFExecutionEngine\nfrom great_expectations.expectations.expectation import ColumnPairMapExpectation\nfrom great_expectations.expectations.metrics.map_metric_provider import (\nColumnPairMapMetricProvider,\ncolumn_pair_condition_partial,\n)\nfrom lakehouse_engine.utils.expectations_utils import validate_result\nclass ColumnPairCustom(ColumnPairMapMetricProvider):\n\"\"\"Asserts that column 'A' is lower or equal than column 'B'.\n    Additionally, the 'margin' parameter can be used to add a margin to the\n    check between column 'A' and 'B': 'A' &lt;= 'B' + 'margin'.\n    \"\"\"\ncondition_metric_name = \"column_pair_values.a_smaller_or_equal_than_b\"\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n)\ncondition_value_keys = (\"margin\",)\n@column_pair_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnPairMapMetricProvider,\ncolumn_A: Any,\ncolumn_B: Any,\nmargin: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Args:\n            column_A: Value of the row of column_A.\n            column_B: Value of the row of column_B.\n            margin: margin value to be added to column_b.\n            kwargs: dict with additional parameters.\n        Returns:\n            If the condition is met.\n        \"\"\"\nif margin is None:\napprox = 0\nelif not isinstance(margin, (int, float, complex)):\nraise TypeError(\nf\"margin must be one of int, float, complex.\"\nf\" Found: {margin} as {type(margin)}\"\n)\nelse:\napprox = margin  # type: ignore\nreturn column_A &lt;= column_B + approx  # type: ignore\nclass ExpectColumnPairAToBeSmallerOrEqualThanB(ColumnPairMapExpectation):\n\"\"\"Expect values in column A to be lower or equal than column B.\n    Args:\n        column_A: The first column name.\n        column_B: The second column name.\n        margin: additional approximation to column B value.\n    Keyword Args:\n        allow_cross_type_comparisons: If True, allow\n            comparisons between types (e.g. integer and string).\n            Otherwise, attempting such comparisons will raise an exception.\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the expectation config\n            as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [11, 22, 50],\n\"b\": [10, 21, 100],\n\"c\": [9, 21, 30],\n},\n\"schemas\": {\n\"spark\": {\n\"a\": \"IntegerType\",\n\"b\": \"IntegerType\",\n\"c\": \"IntegerType\",\n}\n},\n}\n],\n\"tests\": [\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"c\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"c\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\"c\": 9, \"a\": 11},\n{\"c\": 21, \"a\": 22},\n{\"c\": 30, \"a\": 50},\n],\n},\n},\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"b\",\n\"margin\": 1,\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\n\"success\": True,\n\"unexpected_index_list\": [],\n},\n},\n],\n},\n]\nmap_metric = \"column_pair_values.a_smaller_or_equal_than_b\"\nsuccess_keys = (\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n\"margin\",\n\"mostly\",\n)\ndefault_kwarg_values = {\n\"mostly\": 1.0,\n\"ignore_row_if\": \"neither\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nColumnPairMapExpectation,\n)\n\"\"\"Mandatory block of code. If it is removed the expectation will not be available.\"\"\"\nif __name__ == \"__main__\":\n# test the custom expectation with the function `print_diagnostic_checklist()`\nExpectColumnPairAToBeSmallerOrEqualThanB().print_diagnostic_checklist()\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#naming-conventions","title":"Naming Conventions","text":"<p>Your expectation's name should start with expect.</p> <p>The name of the file must be the name of the expectation written in snake case. Ex: <code>expect_column_length_match_input_length</code></p> <p>The name of the class must be the name of the expectation written in camel case. Ex: <code>ExpectColumnLengthMatchInputLength</code></p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#file-structure","title":"File Structure","text":"<p>The file contains two main sections:</p> <ul> <li>the definition of the metric that we are tracking (where we define the logic of the expectation);</li> <li>the definition of the expectation</li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#metric-definition","title":"Metric Definition","text":"<p>In this section we define the logic of the expectation. This needs to follow a certain structure:</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#code-structure","title":"Code Structure","text":"<p>1) The class you define needs to extend one of the Metric Providers defined by Great Expectations that corresponds  to your expectation's type. More info on the metric providers. </p> <p>2) You need to define the name of your metric. This name must be unique and must follow the following structure:  type of expectation.name of metric. Ex.: <code>column_pair_values.a_smaller_or_equal_than_b</code> Types of expectations: <code>column_values</code>, <code>multicolumn_values</code>, <code>column_pair_values</code>, <code>table_rows</code>, <code>table_columns</code>.</p> <p>3) Any GX default parameters that are necessary to calculate your metric must be defined as \"condition_domain_keys\".</p> <p>4) Any additional parameters that are necessary to calculate your metric must be defined as \"condition_value_keys\".</p> <p>5) The logic of your expectation must be defined for the SparkDFExecutionEngine in order to be run on the Lakehouse.</p> <pre><code>1) class ColumnMapMetric(ColumnMapMetricProvider):\n\"\"\"Asserts that a column matches a pattern.\"\"\"\n2) condition_metric_name = \"column_pair_values.a_smaller_or_equal_than_b\"\n3) condition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n)\n4) condition_value_keys = (\"margin\",)\n5) @column_pair_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnPairMapMetricProvider,\ncolumn_A: Any,\ncolumn_B: Any,\nmargin: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Args:\n            column_A: Value of the row of column_A.\n            column_B: Value of the row of column_B.\n            margin: margin value to be added to column_b.\n            kwargs: dict with additional parameters.\n        Returns:\n            If the condition is met.\n        \"\"\"\nif margin is None:\napprox = 0\nelif not isinstance(margin, (int, float, complex)):\nraise TypeError(\nf\"margin must be one of int, float, complex.\"\nf\" Found: {margin} as {type(margin)}\"\n)\nelse:\napprox = margin  # type: ignore\nreturn column_A &lt;= column_B + approx  # type: ignore\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#expectation-definition","title":"Expectation Definition","text":"<p>In this section we define the expectation. This needs to follow a certain structure:</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#code-structure_1","title":"Code Structure","text":"<p>1) The class you define needs to extend one of the Expectations defined by Great Expectations that corresponds to your expectation's type. </p> <p>2) You must define an \"examples\" object where you define at least one success and one failure of your expectation to  demonstrate its logic. The result format must be set to complete, and you must set the unexpected_index_name variable.</p> <p>Note</p> <p>For any examples where you will have unexpected results you must define  unexpected_index_list in your \"out\" element. This will be validated during the testing phase.</p> <p>3) The metric must be the same you defined in the metric definition.</p> <p>4) You must define all additional parameters that the user has to/should provide to the expectation. </p> <p>5) You should define any default values for your expectations parameters. </p> <p>6) You must define the <code>_validate</code> method like shown in the example. You must call the <code>validate_result</code> function  inside your validate method, this process adds a validation to the unexpected index list in the examples.</p> <p>Note</p> <p>If your custom expectation requires any extra validations, or you require additional fields to be returned on  the final dataframe, you can add them in this function.  The validate_result method has two optional parameters (<code>partial_success</code> and `partial_result) that can be used to  pass the result of additional validations and add more information to the result key of the returned dict respectively.</p> <pre><code>1) class ExpectColumnPairAToBeSmallerOrEqualThanB(ColumnPairMapExpectation):\n\"\"\"Expect values in column A to be lower or equal than column B.\n    Args:\n        column_A: The first column name.\n        column_B: The second column name.\n        margin: additional approximation to column B value.\n    Keyword Args:\n        allow_cross_type_comparisons: If True, allow\n            comparisons between types (e.g. integer and string).\n            Otherwise, attempting such comparisons will raise an exception.\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the expectation config\n            as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\n2) examples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": {\n\"a\": [11, 22, 50],\n\"b\": [10, 21, 100],\n\"c\": [9, 21, 30],\n},\n\"schemas\": {\n\"spark\": {\"a\": \"IntegerType\", \"b\": \"IntegerType\", \"c\": \"IntegerType\"}\n},\n\"tests\": [\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"c\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"c\"],\n\"include_unexpected_rows\": True,\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\"c\": 9, \"a\": 11},\n{\"c\": 21, \"a\": 22},\n{\"c\": 30, \"a\": 50},\n],\n},\n},\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"b\",\n\"margin\": 1,\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\"success\": True},\n},\n],\n},\n]\n3) map_metric = \"column_values.pattern_match\"\n4) success_keys = (\n\"validation_regex\",\n\"mostly\",\n)\n5) default_kwarg_values = {\n\"ignore_row_if\": \"never\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n\"mostly\": 1,\n}\n6) def _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; dict:\n\"\"\"Custom implementation of the GX _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GX test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(self, configuration, metrics)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#printing-the-expectation-diagnostics","title":"Printing the Expectation Diagnostics","text":"<p>Your expectations must include the ability to call the Great Expectations diagnostic function in order to be validated.</p> <p>In order to do this code must be present.</p> <pre><code>\"\"\"Mandatory block of code. If it is removed the expectation will not be available.\"\"\"\nif __name__ == \"__main__\":\n# test the custom expectation with the function `print_diagnostic_checklist()`\nExpectColumnPairAToBeSmallerOrEqualThanB().print_diagnostic_checklist()\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#creation-process","title":"Creation Process","text":"<p>1) Create a branch from lakehouse engine.</p> <p>2) Create a custom expectation with your specific logic:</p> <ol> <li>All new expectations must be placed inside folder <code>/lakehouse_engine/dq_processors/custom_expectations</code>.</li> <li>The name of the expectation must be added to the file <code>/lakehouse_engine/core/definitions.py</code>, to the variable: <code>CUSTOM_EXPECTATION_LIST</code>.</li> <li> <p>All new expectations must be tested on <code>/tests/feature/custom_expectations/test_custom_expectations.py</code>.    In order to create a new test for your custom expectation it is necessary to:</p> </li> <li> <p>Copy one of the expectation folders in <code>tests/resources/feature/custom_expectations</code> renaming it to your custom expectation.</p> </li> <li>Make any necessary changes on the data/schema file present.</li> <li>On <code>/tests/feature/custom_expectations/test_custom_expectations.py</code> add a scenario to test your expectation, all expectations     must be tested on batch and streaming. The test is implemented to generate an acon based on each scenario data. </li> <li>Test your developments to check that everything is working as intended.</li> </ol> <p>3) When the development is completed, create a pull request with your changes.</p> <p>4) Your expectation will be available with the next release of the lakehouse engine that happens after you pull request is approved.  This means that you need to upgrade your version of the lakehouse engine in order to use it.</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#usage","title":"Usage","text":"<p>Custom Expectations are available to use like any other expectations provided by Great Expectations.</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#parameters","title":"Parameters","text":"<p>Depending on the type of expectation you are defining some parameters are expected by default.  Ex: A ColumnMapExpectation has a default \"column\" parameter.</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#mostly","title":"Mostly","text":"<p>Mostly is a standard  parameter for a subset of expectations that is used to define a threshold for the failure of an expectation.  Ex: A mostly value of 0.7 makes it so that the expectation only fails if more than 70% of records have  a negative result.</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#result-format","title":"Result Format","text":"<p>Great Expectations has several different types of result formats  for the expectations results. The lakehouse engine requires the result format to be set to \"COMPLETE\" in order to tag  the lines where the expectations failed.</p>"},{"location":"lakehouse_engine_usage/data_quality/custom_expectations/custom_expectations.html#unexpected_index_column_names","title":"<code>unexpected_index_column_names</code>","text":"<p>Inside this key you must define what columns are used as an index inside your data. If this is set and the result  format is set to \"COMPLETE\" a list with the indexes of the lines that failed the validation will be returned by  Great Expectations. This information is used by the Lakehouse Engine to tag the lines in error after the fact. The additional tests  inside the <code>_validate</code> method verify that the custom expectation is tagging these lines correctly.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html","title":"Data Quality Validator","text":"<p>DQValidator algorithm allows DQ Validations isolated from the data load (only read and apply data quality validations). With this algorithm you have the capacity to apply the Lakehouse-Engine Data Quality Process, using Great Expectations functions directly into a specific dataset also making use of all the InputSpecs available in the engine.</p> <p>Validating the Data Quality, using this algorithm, is a matter of defining the data you want to read and the validations you want to do to your data, detailing the great expectations functions you want to apply on the data to assess its quality.</p> <p>Warning</p> <p>This algorithm also gives the possibility to restore a previous version of a delta table or delta files in case the DQ process raises any exception. Please use it carefully!! You may lose important commits and data. Moreover, this will highly depend on the frequency that you run your Data Quality validations. If you run your data loads daily and Data Quality validations weekly, and you define the restore_prev_version to true, this means that the table will be restored to the previous version, but the error could have happened 4 or 5 versions before.</p>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#when-to-use","title":"When to use?","text":"<ul> <li>Post-Load validation: check quality of data already loaded to a table/location</li> <li>Pre-Load validation: check quality of the data you want to load (check DQ by reading a set of files in a specific   location...)</li> <li>Validation of a DataFrame computed in the notebook itself (e.g. check data quality after joining or filtering   datasets, using the computed DataFrame as input for the validation)</li> </ul> <p>This algorithm also gives teams some freedom to:</p> <ul> <li>Schedule isolated DQ Validations to run periodically, with the frequency they need;</li> <li>Define a DQ Validation process as an end-to-end test of the respective data product.</li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#how-to-use","title":"How to use?","text":"<p>All of these configurations are passed via the ACON to instantiate a DQValidatorSpec object. The DQValidator algorithm uses an ACON to configure its execution. In DQValidatorSpec you can find the meaning of each ACON property.</p> <p>Here is an example of ACON configuration:</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_spec\": {\n\"spec_id\": \"sales_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"table\",\n\"db_table\": \"my_database.my_table\"\n},\n\"dq_spec\": {\n\"spec_id\": \"dq_sales\",\n\"input_id\": \"sales_source\",\n\"dq_type\": \"validator\",\n\"store_backend\": \"file_system\",\n\"local_fs_root_dir\": \"/app/tests/lakehouse/in/feature/dq_validator/dq\",\n\"result_sink_db_table\": \"my_database.dq_validator\",\n\"result_sink_format\": \"json\",\n\"fail_on_error\": False,\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"article\"}},\n{\n\"function\": \"expect_table_row_count_to_be_between\",\n\"args\": {\"min_value\": 3, \"max_value\": 11},\n},\n],\n},\n\"restore_prev_version\": True,\n}\nload_data(acon=acon)\n</code></pre> <p>On this page you will also find the following examples of usage:</p> <ol> <li>Dataframe as input &amp; Success on the DQ Validation</li> <li>Table as input &amp; Failure on DQ Validation &amp; Restore previous version</li> <li>Files as input &amp; Failure on DQ Validation &amp; Fail_on_error disabled</li> <li>Files as input &amp; Failure on DQ Validation &amp; Critical functions defined</li> <li>Files as input &amp; Failure on DQ Validation &amp; Max failure percentage defined</li> </ol>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#example-1-dataframe-as-input-success-on-the-dq-validation","title":"Example 1 : Dataframe as input &amp; Success on the DQ Validation","text":"<p>This example focuses on using a dataframe, computed in this notebook, directly in the input spec. First, a new DataFrame is generated as a result of the join of data from two tables (dummy_deliveries and dummy_pd_article) and some DQ Validations are applied on top of this dataframe.</p> <pre><code>from lakehouse_engine.engine import execute_dq_validation\ninput_df = spark.sql(\"\"\"\n        SELECT a.*, b.article_category, b.article_color\n        FROM my_database.dummy_deliveries a\n        JOIN my_database.dummy_pd_article b\n            ON a.article_id = b.article_id\n        \"\"\"\n)\nacon = {\n\"input_spec\": {\n\"spec_id\": \"deliveries_article_input\",\n\"read_type\": \"batch\",\n\"data_format\": \"dataframe\",\n\"df_name\": input_df,\n},\n\"dq_spec\": {\n\"spec_id\": \"deliveries_article_dq\",\n\"input_id\": \"deliveries_article_input\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"result_sink_db_table\": \"my_database.dq_validator_deliveries\",\n\"result_sink_location\": \"my_dq_path/dq_validator/dq_validator_deliveries/\",\n\"expectations_store_prefix\": \"dq/dq_validator/expectations/\",\n\"validations_store_prefix\": \"dq/dq_validator/validations/\",\n\"data_docs_prefix\": \"dq/dq_validator/data_docs/site/\",\n\"checkpoint_store_prefix\": \"dq/dq_validator/checkpoints/\",\n\"unexpected_rows_pk\": [\"salesorder\", \"delivery_item\", \"article_id\"],\n\"dq_functions\": [{\"function\": \"expect_column_values_to_not_be_null\", \"args\": {\"column\": \"delivery_date\"}}],\n},\n\"restore_prev_version\": False,\n}\nexecute_dq_validation(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#example-2-table-as-input-failure-on-dq-validation-restore-previous-version","title":"Example 2: Table as input &amp; Failure on DQ Validation &amp; Restore previous version","text":"<p>In this example we are using a table as input to validate the data that was loaded. Here, we are forcing the DQ Validations to fail in order to show the possibility of restoring the table to the previous version.</p> <p>Warning</p> <p>Be careful when using the feature of restoring a previous version of a delta table or delta files. You may lose important commits and data. Moreover, this will highly depend on the frequency that you run your Data Quality validations. If you run your data loads daily and Data Quality validations weekly, and you define the restore_prev_version to true, this means that the table will be restored to the previous version, but the error could have happened 4 or 5 versions before (because loads are daily, validations are weekly).</p> <p>Steps followed in this example to show how the restore_prev_version feature works.</p> <ol> <li>Insert rows into the dummy_deliveries table to adjust the total numbers of rows and make the DQ process fail.</li> <li>Use the \"DESCRIBE HISTORY\" statement to check the number of versions available on the table and check the version    number resulting from the insertion to the table.</li> <li>Execute the DQ Validation, using the configured acon (based on reading the dummy_deliveries table and setting the  <code>restore_prev_version</code> to <code>true</code>). Checking the logs of the process, you can see that the data did not pass all the  expectations defined and that the table version restore process was triggered.</li> <li>Re-run a \"DESCRIBE HISTORY\" statement to check that the previous version of the table was restored and thus, the row inserted in the beginning of the process is no longer present in the table.</li> </ol> <pre><code>from lakehouse_engine.engine import execute_dq_validation\n# Force failure of data quality by adding new row\nspark.sql(\"\"\"INSERT INTO my_database.dummy_deliveries VALUES (7, 1, 20180601, 71, \"article1\", \"delivered\")\"\"\")\n# Check history of the table\nspark.sql(\"\"\"DESCRIBE HISTORY my_database.dummy_deliveries\"\"\")\nacon = {\n\"input_spec\": {\n\"spec_id\": \"deliveries_input\",\n\"read_type\": \"batch\",\n\"db_table\": \"my_database.dummy_deliveries\",\n},\n\"dq_spec\": {\n\"spec_id\": \"dq_deliveries\",\n\"input_id\": \"deliveries_input\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"dq_functions\": [\n{\"function\": \"expect_column_values_to_not_be_null\", \"args\": {\"column\": \"delivery_date\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 19}},\n],\n},\n\"restore_prev_version\": True,\n}\nexecute_dq_validation(acon=acon)\n# Check that the previous version of the table was restored\nspark.sql(\"\"\"DESCRIBE HISTORY my_database.dummy_deliveries\"\"\")\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#example-3-files-as-input-failure-on-dq-validation-fail_on_error-disabled","title":"Example 3: Files as input &amp; Failure on DQ Validation &amp; Fail_on_error disabled","text":"<p>In this example we are using a location as input to validate the files in a specific folder. Here, we are forcing the DQ Validations to fail, however disabling the \"fail_on_error\" configuration, so the algorithm warns about the expectations that failed but the process/the execution of the algorithm doesn't fail.</p> <pre><code>from lakehouse_engine.engine import execute_dq_validation\nacon = {\n\"input_spec\": {\n\"spec_id\": \"deliveries_input\",\n\"data_format\": \"delta\",\n\"read_type\": \"streaming\",\n\"location\": \"s3://my_data_product_bucket/silver/dummy_deliveries/\",\n},\n\"dq_spec\": {\n\"spec_id\": \"dq_deliveries\",\n\"input_id\": \"deliveries_input\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"fail_on_error\": False,\n\"dq_functions\": [\n{\"function\": \"expect_column_values_to_not_be_null\", \"args\": {\"column\": \"delivery_date\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 17}},\n],\n},\n\"restore_prev_version\": False,\n}\nexecute_dq_validation(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#example-4-files-as-input-failure-on-dq-validation-critical-functions-defined","title":"Example 4: Files as input &amp; Failure on DQ Validation &amp; Critical functions defined","text":"<p>In this example we are using a location as input to validate the files in a specific folder. Here, we are forcing the DQ Validations to fail by using the critical functions feature, which will throw an error if any of the functions fails.</p> <pre><code>from lakehouse_engine.engine import execute_dq_validation\nacon = {\n\"input_spec\": {\n\"spec_id\": \"deliveries_input\",\n\"data_format\": \"delta\",\n\"read_type\": \"streaming\",\n\"location\": \"s3://my_data_product_bucket/silver/dummy_deliveries/\",\n},\n\"dq_spec\": {\n\"spec_id\": \"dq_deliveries\",\n\"input_id\": \"deliveries_input\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"fail_on_error\": True,\n\"dq_functions\": [\n{\"function\": \"expect_column_values_to_not_be_null\", \"args\": {\"column\": \"delivery_date\"}},\n],\n\"critical_functions\": [\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 17}},\n],\n},\n\"restore_prev_version\": False,\n}\nexecute_dq_validation(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#example-5-files-as-input-failure-on-dq-validation-max-failure-percentage-defined","title":"Example 5: Files as input &amp; Failure on DQ Validation &amp; Max failure percentage defined","text":"<p>In this example we are using a location as input to validate the files in a specific folder. Here, we are forcing the DQ Validations to fail by using the max_percentage_failure, which will throw an error if the percentage of failures surpasses the defined maximum threshold.</p> <pre><code>from lakehouse_engine.engine import execute_dq_validation\nacon = {\n\"input_spec\": {\n\"spec_id\": \"deliveries_input\",\n\"data_format\": \"delta\",\n\"read_type\": \"streaming\",\n\"location\": \"s3://my_data_product_bucket/silver/dummy_deliveries/\",\n},\n\"dq_spec\": {\n\"spec_id\": \"dq_deliveries\",\n\"input_id\": \"deliveries_input\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"fail_on_error\": True,\n\"dq_functions\": [\n{\"function\": \"expect_column_values_to_not_be_null\", \"args\": {\"column\": \"delivery_date\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 17}},\n],\n\"max_percentage_failure\": 0.2,\n},\n\"restore_prev_version\": False,\n}\nexecute_dq_validation(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/data_quality_validator/data_quality_validator.html#limitations","title":"Limitations","text":"<p>Unlike DataLoader, this new DQValidator algorithm only allows, for now, one input_spec (instead of a list of input_specs) and one dq_spec (instead of a list of dq_specs). There are plans and efforts already initiated to make this available in the input_specs and one dq_spec (instead of a list of dq_specs). However, you can prepare a Dataframe which joins more than a source, and use it as input, in case you need to assess the Data Quality from different sources at the same time. Alternatively, you can also show interest on any enhancement on this feature, as well as contributing yourself.</p>"},{"location":"lakehouse_engine_usage/data_quality/minimal_example/minimal_example.html","title":"Minimal Example","text":"<p>This scenario illustrates the minimal configuration that you can have to use <code>dq_specs</code>, in which it uses required parameters: <code>spec_id, input_id, dq_type, bucket, dq_functions</code> and the optional parameter <code>data_docs_bucket</code>. This parameter allows you to store the GX documentation in another bucket that can be used to make your data docs available, in DQ Web App (GX UI), without giving users access to your bucket. The<code>data_docs_bucket</code> property supersedes the <code>bucket</code> property only for data docs storage.</p> <p>Regarding the dq_functions, it uses 3 functions (retrieved from the expectations supported by GX), which check:</p> <ul> <li>expect_column_to_exist - if a column exist in the data;</li> <li>expect_table_row_count_to_be_between - if the row count of the data is between the defined interval;</li> <li>expect_table_column_count_to_be_between - if the number of columns in the data is bellow the max value defined.</li> </ul> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True,\n},\n\"location\": \"s3://my_data_product_bucket/dummy_deliveries/\",\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator\",\n\"input_id\": \"dummy_deliveries_source\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"salesorder\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 25}},\n{\"function\": \"expect_table_column_count_to_be_between\", \"args\": {\"max_value\": 7}},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"input_id\": \"dq_validator\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/prisma/prisma.html","title":"Prisma","text":"<p>Prisma is part of the Lakehouse Engine DQ Framework, and it allows users to read DQ functions dynamically from a table instead of writing them explicitly in the Acons.</p>"},{"location":"lakehouse_engine_usage/data_quality/prisma/prisma.html#how-to-use-prisma","title":"How to use Prisma?","text":"<ul> <li>Use the Lakehouse Engine version: 1.22.0 or later;</li> <li>Use DBR 13.3 or later.  If you are not using Databricks, ensure a similar environment with Spark 3.4.1 and Delta 2.4.0.</li> <li>Create the DQ Checks in a table in your Data Product:</li> <li>Each data quality check conducted in Prisma will be hosted within the bucket defined in the engine config file (lakehouse_engine/configs/engine.yaml). Consequently, the result sink location will receive the results of their assessments at the granularity of each \"run\", capturing all records generated during every operation. The DQ Checks table is located in the demanding data product and can have any name (i.e: data_quality_checks).</li> <li>The idea is for it to be a central bucket for all DPs to ensure easier and better observability and unlock offering of easier insights over the Data Quality of the Lakehouse.</li> </ul> <p>Below you find a DDL example with the expected schema and description for the fields: <pre><code>DROP TABLE IF EXISTS my_database.data_quality_checks;\nCREATE EXTERNAL TABLE my_database.data_quality_checks (\ndq_rule_id STRING COMMENT 'DQ Rule ID.',\ndq_tech_function STRING COMMENT 'Great Expectations function type to apply according to the DQ rules type. Example: expect_column_to_exist.',\nexecution_point STRING COMMENT 'In motion/At rest.',\nschema STRING COMMENT 'The database schema on which the check is to be applied.',\ntable STRING COMMENT 'The table on which the check is to be applied.',\ncolumn STRING COMMENT 'The column (either on Lakehouse or in other accessible source systems, such as FDP or SAP BW) on which the check is to be applied.',\nfilters STRING COMMENT 'General filters to the data set (where part of the statement). Note: this is purely descriptive at this point as there is no automated action/filtering of the Lakehouse Engine or PRISMA upon it.',\narguments STRING COMMENT 'Additional arguments to run the Great Expectation Function in the same order as they appear in the function. Example: {\"column\": \"amount\", \"min_value\": 0}.',\ndimension STRING COMMENT 'Data Quality dimension.'\n)\nUSING DELTA\nLOCATION 's3://my-data-product-bucket/inbound/data_quality_checks'\nCOMMENT 'Table with dummy data mapping DQ Checks.'\nTBLPROPERTIES(\n'lakehouse.primary_key'='dq_rule_id',\n'delta.enableChangeDataFeed'='true'\n)\n</code></pre> Data sample:</p> dq_rule_id dq_tech_function execution_point schema table column filters arguments dimension 1 expect_column_values_to_not_be_null at_rest my_database_schema dummy_sales ordered_item {\"column\": \"ordered_item\"} Completeness 2 expect_column_min_to_be_between in_motion my_database_schema dummy_sales ordered_item {\"column\": \"amount\", \"min_value\": 0} Completeness 3 expect_column_values_to_not_be_in_set in_motion my_database_schema dummy_sales ordered_item {\"column\": \"amount\", \"value_set\": [1,2,3]} Completeness 4 expect_column_pair_a_to_be_not_equal_to_b at_rest my_database_schema dummy_sales ordered_item {\"column_A\": \"amount\",\"column_B\": \"ordered_item\"} Completeness 5 expect_table_row_count_to_be_between at_rest my_database_schema dummy_sales ordered_item {\"min_value\": 1, \"max_value\": 10} Completeness <p>Table definition:</p> Column Name Definition dq_rule_id The identifier of a data quality rule. dq_tech_function Type of Great Expectations function to apply according to the DQ rules type. See the values here: Gallery of Expectations and Packages execution_point The way how validations will be performed on top the the data set. List of values: at_rest, in_motion. schema The schema on which the check is to be applied. table The table on which the check is to be applied. column The column on which the check is to be applied. filters General filters to the data set (where part of the statement). Note: this is purely descriptive at this point as there is no automated action/filtering of the Lakehouse Engine or PRISMA upon it. arguments Additional arguments to run the Great Expectation Function in the same order as they appear in the function. dimension Categorisation of a DQ rule related to one of the dimensions. List of values: Completeness, Uniqueness, Timeliness, Validity, Consistency, Accuracy. Note: these values are purely descriptive. <p>Execution behaviour - The value of the execution_point column determines the type of Acon execution:</p> <ul> <li> <p>For records at_rest, they will only be processed when the Lakehouse engine is called by the execute_dq_validation() function.</p> </li> <li> <p>For records in_motion, they will only be processed when the Lakehouse engine is called by load_data() function.</p> </li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/prisma/prisma.html#what-are-the-main-changes-on-my-acon-if-i-already-implemented-dq","title":"What are the main changes on my ACON if I already implemented DQ?","text":"<p>The following configurations represent the minimum requirements to make Prisma DQ work.</p> <ul> <li>dq_type: \"prisma\" - the value must be set in order for the engine process the DQ with Prisma;</li> <li>store_backend: \"file_system\" or \"s3\" - which store backend to use;</li> <li>bucket - the bucket name to consider for the store_backend (store DQ artefacts). Note: only applicable and mandatory for store_backend s3.</li> <li>local_fs_root_dir: path of the root directory. Notes: only applicable for store_backend file_system;</li> <li>dq_db_table: the DQ Check table that is located in the demanding data product;</li> <li>dq_table_table_filter: name of the table which rules are to be applied in the validations. The table name must match with the values inserted in the column \"table\" from dq_db_table;</li> <li>data_product_name: the name of the data product;</li> <li>tbl_to_derive_pk or unexpected_rows_pk:</li> <li>tbl_to_derive_pk - automatically derive the primary keys from a given database table. Note: the primary keys are derived from the lakehouse.primary_key property of a table.   </li> <li>unexpected_rows_pk - the list of columns composing the primary key of the source data to identify the rows failing the DQ validations.</li> </ul> <p>DQ Prisma Acon example <pre><code>\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator_in_motion\",\n\"input_id\": \"dummy_sales_transform\",\n\"dq_type\": \"prisma\",\n\"store_backend\": \"file_system\",\n\"local_fs_root_dir\": \"/my-data-product/artefacts/dq\",\n\"dq_db_table\": DQ_DB_TABLE,\n\"dq_table_table_filter\": \"dummy_sales\",\n\"data_product_name\": DATA_PRODUCT_NAME,\n\"tbl_to_derive_pk\": DB_TABLE,\n}\n],\n</code></pre></p> <p>Note</p> <p>Available extra parameters to use in the DQ Specs for Prisma:</p> <ul> <li>data_docs_local_fs - the path for data docs. The parameter is useful in case you want your DQ Results to be reflected on the automatic Data Docs site;</li> <li>data_docs_prefix - prefix where to store data_docs' data. This parameter must be used together with <code>data_docs_local_fs</code>;</li> <li>dq_table_extra_filters - extra filters to be used when deriving DQ functions. This is an SQL expression to be applied to <code>dq_db_table</code> which means that the statements must use one of the available columns in the table. For example: dq_rule_id in ('rule1','rule2');</li> <li>data_docs_bucket - the bucket name for data docs only. When defined, it will supersede bucket parameter. Note: only applicable for store_backend s3;</li> <li>expectations_store_prefix - prefix where to store expectations' data. Note: only applicable for store_backend s3;</li> <li>validations_store_prefix - prefix where to store validations' data. Note: only applicable for store_backend s3;</li> <li>checkpoint_store_prefix - prefix where to store checkpoints' data. Note: only applicable for store_backend s3;</li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/prisma/prisma.html#end2end-example","title":"End2End Example","text":"<p>Below you can also find an End2End and detailed example of loading data into the DQ Checks table and then using PRISMA both with load_data() and execute_dq_validation().</p> 1 - Load the DQ Checks Table <p>This example shows how to insert data into the data_quality_checks table using an Acon with a csv file as a source. The location provided is just an example of a place to store the csv. It is also important that the source file contains the data_quality_checks schema. <pre><code>acon = {\n\"input_specs\": [\n{\n\"spec_id\": \"read_dq_checks\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\"header\": True, \"delimiter\": \";\"},\n\"location\": \"s3://my-data-product/local_data/data_quality_checks/\",\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"write_dq_checks\",\n\"input_id\": \"read_dq_checks\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my-data-product-bucket/inbound/data_quality_checks\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre></p> 2 - PRISMA - IN MOTION (load_data) <pre><code>cols_to_rename = {\"item\": \"ordered_item\", \"date\": \"order_date\", \"article\": \"article_id\"}   \nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_sales_bronze\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my-data-product-bucket/bronze/dummy_sales\",\n}\n],\n\"transform_specs\": [\n{\n\"spec_id\": \"dummy_sales_transform\",\n\"input_id\": \"dummy_sales_bronze\",\n\"transformers\": [\n{\n\"function\": \"rename\",\n\"args\": {\n\"cols\": cols_to_rename,\n},\n},\n],\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator_in_motion\",\n\"input_id\": \"dummy_sales_transform\",\n\"dq_type\": \"prisma\",\n\"store_backend\": \"file_system\",\n\"local_fs_root_dir\": \"/my-data-product/artefacts/dq\",\n\"dq_db_table\": DQ_DB_TABLE,\n\"dq_table_table_filter\": \"dummy_sales\",\n\"dq_table_extra_filters\": \"1 = 1\",\n\"data_docs_local_fs\": \"my-data-product/my-data-product-dq-site\",\n\"data_docs_prefix\": \"{}/my-data-product-bucket/data_docs/site/\".format(DQ_PREFIX),\n\"data_product_name\": DATA_PRODUCT_NAME,\n\"tbl_to_derive_pk\": DB_TABLE,\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_sales_silver\",\n\"input_id\": \"dq_validator_in_motion\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my-data-product-bucket/silver/dummy_sales_dq_template_in_motion\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre> 3 - PRISMA - AT REST (exec_dq_validation) <pre><code>acon = {\n\"input_spec\": {\n\"spec_id\": \"dummy_sales_source\",\n\"read_type\": \"batch\",\n\"db_table\": DB_TABLE,\n},\n\"dq_spec\": {\n\"spec_id\": \"dq_validator_at_rest\",\n\"input_id\": \"sales_input\",\n\"dq_type\": \"prisma\",\n\"store_backend\": \"file_system\",\n\"local_fs_root_dir\": \"/my-data-product/artefacts/dq\",\n\"dq_db_table\": DQ_DB_TABLE,\n\"dq_table_table_filter\": \"dummy_sales\",\n\"data_docs_local_fs\": \"my-data-product/my-data-product-dq-site\",\n\"data_docs_prefix\": \"{}/my-data-product-bucket/data_docs/site/\".format(DQ_PREFIX),\n\"data_product_name\": DATA_PRODUCT_NAME,\n\"tbl_to_derive_pk\": DB_TABLE,\n},\n}\nexecute_dq_validation(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/data_quality/prisma/prisma.html#troubleshootingcommon-issues","title":"Troubleshooting/Common issues","text":"<p>This section provides a summary of common issues and resolutions.</p> Error type: filter does not get rules from DQ Checks table. <p></p> <p>Solution: make sure the records in your DQ Checks table are well-defined. In the Acon, ensure that you have the dq_table_table_filter with the correct table name.</p> Error type: missing expectation. <p></p> <p>Solution: make sure that you are using a valid expectation. See the valid ones on: Gallery of Expectations and Packages</p> Error type: missing expectation parameters. <p></p> <p>Solution: make sure that your \"arguments\" column in the DQ CHECKS table has all necessary parameters for the expectation. For example, the expectation expect_column_values_to_not_be_null needs one argument (column (str): The column name).</p>"},{"location":"lakehouse_engine_usage/data_quality/result_sink/result_sink.html","title":"Result Sink","text":"<p>These scenarios store the results of the dq_specs into a result sink. For that, both scenarios include parameters defining the specific table and location (<code>result_sink_db_table</code> and <code>result_sink_location</code>) where the results are expected to be stored. With this configuration, people can, later on, check the history of the DQ executions using the configured table/location, as shown bellow. You can configure saving the output of the results in the result sink following two approaches:</p> <ul> <li>Denormalized/exploded Data Model (recommended) - the results are stored in a detailed format in which people are able to analyse them by Data Quality Run, by expectation_type and by keyword arguments.</li> </ul> ... source column max_value min_value expectation_type expectation_success observed_value run_time_year ... all columns from raw + more deliveries salesorder null null expect_column_to_exist TRUE null 2023 ... all columns from raw + more deliveries null null null expect_table_row_count_to_be_between TRUE 23 2023 ... all columns from raw + more deliveries null null null expect_table_column_count_to_be_between TRUE 6 2023 ... <ul> <li>Raw Format Data Model (not recommended) - the results are stored in the raw format that Great Expectations outputs. This is not recommended as the data will be highly nested and in a string format (to prevent problems with schema changes), which makes analysis and the creation of a dashboard on top way  harder.</li> </ul> checkpoint_config run_name run_time run_results success validation_result_identifier spec_id input_id entire configuration 20230323-...-dq_validation 2023-03-23T15:11:32.225354+00:00 results of the 3 expectations true/false for the run identifier spec_id input_id <p>Note</p> <ul> <li>More configurations can be applied in the result sink, as the file format and partitions.</li> <li> <p>It is recommended to:</p> <ul> <li>Use the same result sink table/location for all dq_specs across different data loads, from different  sources, in the same Data Product.</li> <li>Use the parameter <code>source</code> (only available with <code>\"result_sink_explode\": True</code>), in the dq_specs, as used in both scenarios, with the name of the data source, to be easier to distinguish sources in the analysis. If not specified, the <code>input_id</code> of the dq_spec will be considered as the <code>source</code>.</li> <li>These recommendations will enable more rich analysis/dashboard at Data Product level, considering all the different sources and data loads that the Data Product is having.</li> </ul> </li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/result_sink/result_sink.html#1-result-sink-exploded-recommended","title":"1. Result Sink Exploded (Recommended)","text":"<p>This scenario stores DQ Results (results produces by the execution of the dq_specs) in the Result Sink, in a detailed format, in which people are able to analyse them by Data Quality Run, by expectation_type and by keyword arguments. This is the recommended approach since it makes the analysis on top of the result sink way easier and faster.</p> <p>For achieving the exploded data model, this scenario introduces the parameter <code>result_sink_explode</code>, which is a flag to determine if the output table/location should have the columns exploded (as <code>True</code>) or not (as <code>False</code>). Default: <code>True</code>, but it is still provided explicitly in this scenario for demo purposes. The table/location will include a schema which contains general columns, statistic columns, arguments of expectations, and others, thus part of the schema will be always with values and other part will depend on the expectations chosen.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True,\n},\n\"location\": \"s3://my_data_product_bucket/dummy_deliveries/\",\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator\",\n\"input_id\": \"dummy_deliveries_source\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"result_sink_db_table\": \"my_database.dq_result_sink\",\n\"result_sink_location\": \"my_dq_path/dq_result_sink/\",\n\"result_sink_explode\": True,\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"source\": \"deliveries_success\",\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"salesorder\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 25}},\n{\"function\": \"expect_table_column_count_to_be_between\", \"args\": {\"max_value\": 7}},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"input_id\": \"dq_validator\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre> <p>To check the history of the DQ results, you can run commands like:</p> <ul> <li>the table: <code>display(spark.table(\"my_database.dq_result_sink\"))</code></li> <li>the location: <code>display(spark.read.format(\"delta\").load(\"my_dq_path/dq_result_sink/\"))</code></li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/result_sink/result_sink.html#2-raw-result-sink","title":"2. Raw Result Sink","text":"<p>This scenario is very similar to the previous one, but it changes the parameter <code>result_sink_explode</code> to <code>False</code> so that it produces a raw result sink output containing only one row representing the full run of <code>dq_specs</code> (no matter the amount of expectations/dq_functions defined there). Being a raw output, it is not a recommended approach, as it will be more complicated to analyse and make queries on top of it.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True,\n},\n\"location\": \"s3://my_data_product_bucket/dummy_deliveries/\",\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator\",\n\"input_id\": \"dummy_deliveries_source\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"result_sink_db_table\": \"my_database.dq_result_sink_raw\",\n\"result_sink_location\": \"my_dq_path/dq_result_sink_raw/\",\n\"result_sink_explode\": False,\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"source\": \"deliveries_success_raw\",\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"salesorder\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 25}},\n{\"function\": \"expect_table_column_count_to_be_between\", \"args\": {\"max_value\": 7}},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"input_id\": \"dq_validator\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre> <p>To check the history of the DQ results, you can run commands like:</p> <ul> <li>the table: <code>display(spark.table(\"my_database.dq_result_sink_raw\"))</code></li> <li>the location: <code>display(spark.read.format(\"delta\").load(\"my_dq_path/dq_result_sink_raw/\"))</code></li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/row_tagging/row_tagging.html","title":"Row Tagging","text":"<p>Data quality is essential for any organisation that relies on data to make informed decisions.  High-quality data provides accurate, reliable, and timely information that enables organisations to identify opportunities, mitigate risks, and optimize their operations. In contrast, low-quality data can lead to incorrect conclusions, faulty decisions, and wasted resources.</p> <p>There are several common issues that can compromise data quality, such as:</p> <ul> <li>data entry errors; </li> <li>data duplication; </li> <li>incomplete / inconsistent data; </li> <li>changes where data is collected (e.g. sources); </li> <li>faulty data processing, such as inaccurate data cleansing or transformations.</li> </ul> <p>Therefore, implementing data quality controls, such as data validation rules, and regularly monitoring data for  accuracy and completeness is key for any organisation.</p> <p>One of these controls that can be applied is the DQ Row Tagging Strategy so that you not only apply validations on  your data to ensure Data Quality, but you also tag your data with the results of the Data Quality validations  providing advantages like:</p> <ul> <li>Transparency for downstream and upstream consumers; </li> <li>Data Observability and Reliability; </li> <li>More trust over the data; </li> <li>Anomaly Detection; </li> <li>Easier and faster discovery of Data Quality problems, and, consequently faster resolution; </li> <li>Makes it easier to deal with integrations with other systems and migrations (you can have validations capturing that a column was changed or simply disappeared);</li> </ul> <p>Note</p> <p>When using the DQ Row Tagging approach data availability will take precedence over Data Quality, meaning  that all the data will be introduced into the final target (e.g. table or location) no matter what Data Quality issues it is having.</p> <p>Different Types of Expectations:</p> <ul> <li>Table Level </li> <li>Column Aggregated Level </li> <li>Query Level </li> <li>Column Values (row level)</li> <li>Column Pair Value (row level)</li> <li>Multicolumn Values (row level)</li> </ul> <p>The expectations highlighted as row level will be the ones enabling to Tag failures on specific rows and adding  the details about each failure (they affect the field run_row_result inside dq_validations). The expectations  with other levels (not row level) influence the overall result of the Data Quality execution, but won't be used to tag specific rows (they affect the field run_success only, so you can even have situations for which you get  run_success False and run_row_success True for all rows).</p>"},{"location":"lakehouse_engine_usage/data_quality/row_tagging/row_tagging.html#how-does-the-strategy-work","title":"How does the Strategy work?","text":"<p>The strategy relies mostly on the 6 below arguments.</p> <p>Note</p> <p>When you specify <code>\"tag_source_data\": True</code> the arguments fail_on_error, gx_result_format and  result_sink_explode are set to the expected values. </p> <ul> <li>unexpected_rows_pk - the list columns composing the primary key of the source data to use to identify the rows  failing the DQ validations. </li> <li>tbl_to_derive_pk - <code>db.table</code> to automatically derive the unexpected_rows_pk from. </li> <li>gx_result_format - great expectations result format. Default: <code>COMPLETE</code>. </li> <li>tag_source_data - flag to enable the tagging strategy in the source data, adding the information of  the DQ results in a column <code>dq_validations</code>. This column makes it possible to identify if the DQ run was succeeded in general and, if not, it unlocks the insights to know what specific rows have made the DQ validations fail and why. Default: <code>False</code>.</li> </ul> <p>Note</p> <p>It only works if result_sink_explode is <code>True</code>, result_format is <code>COMPLETE</code> and  fail_on_error is `False. </p> <ul> <li>fail_on_error - whether to fail the algorithm if the validations of your data in the DQ process failed. </li> <li>result_sink_explode - flag to determine if the output table/location should have the columns exploded (as <code>True</code>) or not (as <code>False</code>). Default: <code>True</code>.</li> </ul> <p>Note</p> <p>It is mandatory to provide one of the arguments (unexpected_rows_pk or tbl_to_derive_pk) when using  tag_source_data as True.  When tag_source_data is False, this is not mandatory, but still recommended. </p> <p></p> <p>Note</p> <p>The tagging strategy only works when <code>tag_source_data</code> is <code>True</code>, which automatically assigns the expected values for the parameters <code>result_sink_explode</code> (True), <code>fail_on_error</code> (False) and <code>gx_result_format</code> (\"COMPLETE\").</p> <p>Note</p> <p>For the DQ Row Tagging to work, in addition to configuring the aforementioned arguments in the dq_specs,  you will also need to add the dq_validations field into your table (your DDL statements, recommended) or  enable schema evolution.</p> <p>Note</p> <p>Kwargs field is a string, because it can assume different schemas for different expectations and runs.  It is useful to provide the complete picture of the row level failure and to allow filtering/joining with  the result sink table, when there is one. Some examples of kwargs bellow:</p> <ul> <li><code>{\"column\": \"country\", \"min_value\": 1, \"max_value\": 2, \"batch_id\": \"o723491yyr507ho4nf3\"}</code> \u2192 example for  expectations starting with <code>expect_column_values</code> (they always make use of \"column\", the other arguments vary). </li> <li><code>{\"column_A: \"country\", \"column_B\": \"city\", \"batch_id\": \"o723491yyr507ho4nf3\"}</code> \u2192 example for expectations  starting with <code>expect_column_pair</code> (they make use of \"column_A\" and \"column_B\", the other arguments vary). </li> <li><code>{\"column_list\": [\"col1\", \"col2\", \"col3\"], \"batch_id\": \"o723491yyr507ho4nf3\"}</code> \u2192 example for expectations  starting with <code>expect_multicolumn</code> (they make use of \"column_list\", the other arguments vary). <code>batch_id</code> is common to all expectations, and it is an identifier for the batch of data being validated by Great Expectations.</li> </ul>"},{"location":"lakehouse_engine_usage/data_quality/row_tagging/row_tagging.html#example","title":"Example","text":"<p>This scenario uses the row tagging strategy which allow users to tag the rows that failed to be easier to identify the problems in the validations.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True,\n},\n\"location\": \"s3://my_data_product_bucket/dummy_deliveries/\",\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator\",\n\"input_id\": \"dummy_deliveries_source\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket }}\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"result_sink_db_table\": \"my_database.dq_result_sink\",\n\"result_sink_location\": \"my_dq_path/dq_result_sink/\",\n\"tag_source_data\": True,\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"source\": \"deliveries_tag\",\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"salesorder\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 25}},\n{\n\"function\": \"expect_column_values_to_be_in_set\",\n\"args\": {\"column\": \"salesorder\", \"value_set\": [\"37\"]},\n},\n{\n\"function\": \"expect_column_pair_a_to_be_smaller_or_equal_than_b\",\n\"args\": {\"column_A\": \"salesorder\", \"column_B\": \"delivery_item\"},\n},\n{\n\"function\": \"expect_multicolumn_sum_to_equal\",\n\"args\": {\"column_list\": [\"salesorder\", \"delivery_item\"], \"sum_total\": 100},\n},\n],\n\"critical_functions\": [\n{\"function\": \"expect_table_column_count_to_be_between\", \"args\": {\"max_value\": 6}},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"input_id\": \"dq_validator\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre> <p>Running bellow cell shows the new column created, named <code>dq_validations</code> with information about DQ validations. <code>display(spark.read.format(\"delta\").load(\"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\"))</code></p>"},{"location":"lakehouse_engine_usage/data_quality/row_tagging/row_tagging.html#performance-and-limitations-trade-offs","title":"Performance and Limitations Trade-offs","text":"<p>When using the DQ Row Tagging Strategy, by default we are using Great Expectations Result Format \"Complete\" with  Unexpected Index Column Names (a primary key for the failures), meaning that for each failure, we are getting all  the distinct values for the primary key. After getting all the failures, we are applying some needed transformations  and joining them with the source data, so that it can be tagged by filling the \"dq_validations\" column.</p> <p>Hence, this can definitely be a heavy and time-consuming operation on your data loads. To reduce this disadvantage  you can cache the dataframe by passing the <code>\"cache_df\": True</code> in your DQ Specs. In addition to this, always have in  mind that each expectation (dq_function) that you add into your DQ Specs, is more time that you are adding into your  data loads, so always balance performance vs amount of validations that you need.</p> <p>Moreover, Great Expectations is currently relying on the driver node to capture the results of the execution and  return/store them. Thus, in case you have huge amounts of rows failing (let's say 500k or more) Great Expectations  might raise exceptions.</p> <p>On these situations, the data load will still happen and the data will still be tagged with the Data Quality  validations information, however you won't have the complete picture of the failures, so the raised_exceptions  field is filled as True, so that you can easily notice it and debug it.</p> <p>Most of the time, if you have such an amount of rows failing, it will probably mean that you did something wrong  and want to fix it as soon as possible (you are not really caring about tagging specific rows, because you will  not want your consumers to be consuming a million of defective rows). However, if you still want to try to make it  pass, you can try to increase your driver and play with some spark configurations like:</p> <ul> <li><code>spark.driver.maxResultSize</code></li> <li><code>spark.task.maxFailures</code></li> </ul> <p>For debugging purposes, you can also use a different Great Expectations Result Format like \"SUMMARY\" (adding in your DQ Spec <code>\"gx_result_format\": \"SUMMARY\"</code>), so that you get only a partial list of the failures, avoiding surpassing the driver capacity. </p> <p>Note</p> <p>When using a Result Format different from the default (\"COMPLETE\"), the flag \"tag_source_data\" will be  overwritten to <code>False</code>, as the results of the tagging wouldn't be complete which could lead to erroneous  conclusions from stakeholders (but you can always get the details about the result of the DQ execution in the <code>result_sink_location</code> or <code>result_sink_db_table</code> that you have configured).</p>"},{"location":"lakehouse_engine_usage/data_quality/validations_failing/validations_failing.html","title":"Validations Failing","text":"<p>The scenarios presented on this page are similar, but their goal is to show what happens when a DQ expectation fails the validations. The logs generated by the execution of the code will contain information regarding which expectation(s) have failed and why.</p>"},{"location":"lakehouse_engine_usage/data_quality/validations_failing/validations_failing.html#1-fail-on-error","title":"1. Fail on Error","text":"<p>In this scenario is specified below two parameters:</p> <ul> <li><code>\"fail_on_error\": False</code> - this parameter is what controls what happens if a DQ expectation fails. In case this is set to <code>true</code> (default), your job will fail/be aborted and an exception will be raised. In case this is set to <code>false, a log message will be printed about the error (as shown in this scenario) and the result status will also be available in result sink (if configured) and in the [data docs great expectation site](../data_quality.html#3-data-docs-website). On this scenario it is set to</code>false`  to avoid failing the execution of the notebook.</li> <li>the <code>max_value</code> of the function <code>expect_table_column_count_to_be_between</code> is defined with specific value so that this expectation fails the validations.</li> </ul> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True,\n},\n\"location\": \"s3://my_data_product_bucket/dummy_deliveries/\",\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator\",\n\"input_id\": \"dummy_deliveries_source\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"result_sink_db_table\": \"my_database.dq_result_sink\",\n\"result_sink_location\": \"my_dq_path/dq_result_sink/\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"source\": \"deliveries_fail\",\n\"fail_on_error\": False,\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"salesorder\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 20}},\n{\"function\": \"expect_table_column_count_to_be_between\", \"args\": {\"max_value\": 5}},\n{\"function\": \"expect_column_values_to_be_null\", \"args\": {\"column\": \"article\"}},\n{\"function\": \"expect_column_values_to_be_unique\", \"args\": {\"column\": \"status\"}},\n{\n\"function\": \"expect_column_min_to_be_between\",\n\"args\": {\"column\": \"delivery_item\", \"min_value\": 1, \"max_value\": 15},\n},\n{\n\"function\": \"expect_column_max_to_be_between\",\n\"args\": {\"column\": \"delivery_item\", \"min_value\": 15, \"max_value\": 30},\n},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"input_id\": \"dq_validator\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre> <p>If you run bellow command, you would be able to see the <code>success</code> column has the value <code>false</code> for the last execution. <code>display(spark.table(RENDER_UTILS.render_content(\"my_database.dq_result_sink\")))</code></p>"},{"location":"lakehouse_engine_usage/data_quality/validations_failing/validations_failing.html#2-critical-functions","title":"2. Critical Functions","text":"<p>In this scenario, alternative parameters to <code>fail_on_error</code> are used:</p> <ul> <li><code>critical_functions</code> - this parameter defaults to <code>None</code> if not defined. It controls what DQ functions are considered a priority and as such, it stops the validation and throws an execution error whenever a function defined as critical doesn't pass the test. If any other function that is not defined in this parameter fails, an error message is printed in the logs. This parameter has priority over <code>fail_on_error</code>. In this specific example, after defining the <code>expect_table_column_count_to_be_between</code> as critical, it is made sure that the execution is stopped whenever the conditions for the function are not met.</li> </ul> <p>Additionally, it can also be defined additional parameters like:</p> <ul> <li><code>max_percentage_failure</code> - this parameter defaults to <code>None</code> if not defined. It controls what percentage of the total functions can fail without stopping the execution of the validation. If the threshold is surpassed the execution stops and a failure error is thrown. This parameter has priority over <code>fail_on_error</code> and <code>critical_functions</code>.</li> </ul> <p>You can also pair <code>critical_functions</code> with <code>max_percentage_failure</code> by defining something like a 0.6 max percentage of failure and also defining some critical function. In this case even if the threshold is respected, the list defined on <code>critical_functions</code> still is checked.</p> <pre><code>from lakehouse_engine.engine import load_data\nacon = {\n\"input_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_source\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"options\": {\n\"header\": True,\n\"delimiter\": \"|\",\n\"inferSchema\": True,\n},\n\"location\": \"s3://my_data_product_bucket/dummy_deliveries/\",\n}\n],\n\"dq_specs\": [\n{\n\"spec_id\": \"dq_validator\",\n\"input_id\": \"dummy_deliveries_source\",\n\"dq_type\": \"validator\",\n\"bucket\": \"my_data_product_bucket\",\n\"data_docs_bucket\": \"my_dq_data_docs_bucket\",\n\"data_docs_prefix\": \"dq/my_data_product/data_docs/site/\",\n\"result_sink_db_table\": \"my_database.dq_result_sink\",\n\"result_sink_location\": \"my_dq_path/dq_result_sink/\",\n\"source\": \"deliveries_critical\",\n\"tbl_to_derive_pk\": \"my_database.dummy_deliveries\",\n\"dq_functions\": [\n{\"function\": \"expect_column_to_exist\", \"args\": {\"column\": \"salesorder\"}},\n{\"function\": \"expect_table_row_count_to_be_between\", \"args\": {\"min_value\": 15, \"max_value\": 25}},\n],\n\"critical_functions\": [\n{\"function\": \"expect_table_column_count_to_be_between\", \"args\": {\"max_value\": 5}},\n],\n}\n],\n\"output_specs\": [\n{\n\"spec_id\": \"dummy_deliveries_bronze\",\n\"input_id\": \"dq_validator\",\n\"write_type\": \"overwrite\",\n\"data_format\": \"delta\",\n\"location\": \"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\",\n}\n],\n}\nload_data(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/gab/gab.html","title":"GAB - Gold Asset Builder","text":"<p>GAB stands for Gold Asset Builder and, technically, it is a SQL-first transformation workflow that allows teams to, quickly and collaboratively, deploy aggregate tables on top of base fact tables, which can then be used for empowering analytics over different perspectives on dashboards or exploratory queries.</p> <p>Main benefits:</p> <ul> <li>Efficiency and speed: It reduces the efforts and time to production for new aggregate tables (gold layer assets).</li> <li>Simple pipeline: It simplifies the cluster decision by having just 3 cluster types (small, medium, large), there's no need to create a separated pipeline for each case.</li> <li>Low-code and mainly SQL language.</li> </ul> <p>Warning</p> <p>Before deciding whether your use case will fit into GAB or not, please read the instructions in the sections below carefully. If there is any doubt about certain metrics which might deviate from our framework support please reach us before starting your development. GAB may not be a one size fit for all your requirements so please make use of the framework only if it satisfies your criteria.</p> <p></p>"},{"location":"lakehouse_engine_usage/gab/gab.html#advantages","title":"Advantages","text":"<ul> <li>More flexibility to define any type of complex sql queries.</li> <li>Only need to touch the sql file for the execution</li> <li>Quick production rollout and adaptability</li> <li>Innersourcing model really works, a data analyst can work on sql and hand it over to the engineering team. Engineering team can then adapt the templating and make the production ready code quickly after the data validation.</li> <li>As shown in the image below, it's possible to generate different perspectives of the same metric for a specific use case:<ol> <li>rollup (aggregated by dimension1(d1), dimension2(d2)) - Compute the same metrics at a upper level from the most granular level</li> <li>grouping set (aggregated by dimension1(d1), dimension2(d2), dimension3(d3)) - Compute the same metrics at the most granular level</li> <li>cube (aggregated by dimension1(d1)) - Compute the same metrics in the higher aggregation level</li> </ol> </li> </ul>"},{"location":"lakehouse_engine_usage/gab/gab.html#use-gab","title":"Use GAB","text":"<ul> <li>When an aggregate table is to be created for any analytical perspectives on dashboards or exploratory queries with some specific dimensions and metrics and can be done using normal SQL query.</li> <li>When metrics and dimensions are bound to the configured cadences and you are not calculating the whole universe data in your SQL query.</li> </ul>"},{"location":"lakehouse_engine_usage/gab/gab.html#dont-use-gab","title":"Don't Use GAB","text":"<ul> <li>When metrics and dimensions are not bound to cadences.</li> <li>When your table is not an aggregated i.e. table is at an acid granularity</li> <li>When your metrics are not calculated incrementally.</li> </ul>"},{"location":"lakehouse_engine_usage/gab/gab.html#gab-capabilities","title":"GAB Capabilities","text":""},{"location":"lakehouse_engine_usage/gab/gab.html#cadence","title":"Cadence","text":"<p>In which cadence (timeslice) you want the data (DAILY, WEEKLY, MONTHLY, QUARTERLY, YEARLY).</p>"},{"location":"lakehouse_engine_usage/gab/gab.html#dimensionsmetrics","title":"Dimensions/Metrics","text":""},{"location":"lakehouse_engine_usage/gab/gab.html#dimension","title":"Dimension","text":"<p>Usually directly mapped from the table without any transformation, it will be used to aggregate the metric, example: <code>product_category</code>.</p>"},{"location":"lakehouse_engine_usage/gab/gab.html#metric","title":"Metric","text":"<p>Aggregated value at the dimension level with the cadence as one of the dimension. There's some options to compute a metric:</p> <ul> <li>Directly from a table column, example: <code>sum(product_amount)</code></li> <li>Compute it in the same cadence but in an exactly 1 previous time window, example: In a <code>MONTHLY</code> cadence it will compute for the previous month.</li> <li>Compute it in the same cadence but from the last year, example: In a <code>QUARTERLY</code> cadence it will compute it in the same quarter but from the previous year.</li> <li>Compute it in the same cadence but with a custom window function, example: In a <code>QUARTER</code> cadence computing the last 2 quarters.</li> <li>Compute it in any SQL way possible: using any of the available columns, deriving a metric from another, using SQL functions, etc. Example: compute a metric for the last 6 months of data.</li> </ul> <p>Note</p> <p>each computation derives a new column on the output view.</p>"},{"location":"lakehouse_engine_usage/gab/gab.html#extended-window-calculator-reconciliation","title":"Extended Window Calculator &amp; Reconciliation","text":""},{"location":"lakehouse_engine_usage/gab/gab.html#extended-window-calculator","title":"Extended window Calculator","text":"<p>This feature aims to calculate the extended window of any cadence despite the user providing custom dates which are not the exact start and end dates of a cadence.</p> <ul> <li>For example, if the user wants to calculate the <code>MONTH</code> cadence but gives a date range of <code>2023-01-10</code> to <code>2023-01-29</code>, the computation window will be a window from <code>2023-01-01</code> to <code>2023-01-31</code>, which calculates the full data for the month and inserts it. This function handles any user error to efficiently integrate the data. <pre><code>{'DAY':{},'WEEK':{},'MONTH':{},'YEAR':{}}\n</code></pre> You can find an image below sampling how a recon_window config looks like. If we consider WEEK cadence, then it has a recon_window of MONTH and QUARTER. What this means is, at the start of a new Month or a Quarter, all the weeks in that cadence are recalculated to consider the late events. For example, <code>2023-01-01</code> is the start of a Month, Quarter and a Year. Here since Month and Quarter are given and Quarter is the greater cadence among the 2, all the weeks in Q4 2022 (with the extended cadence window using the above function) are recalculated i.e. instead of <code>2022-10-01</code> to <code>2022-12-31</code>, extended window is <code>2022-09-26</code> to <code>2023-01-01</code>.</li> </ul> <p></p>"},{"location":"lakehouse_engine_usage/gab/gab.html#reconciliation","title":"Reconciliation","text":"<p>Reconciliation enables the user to compute the data aggregated by the specified cadence but in a different frequency, this brings flexibility in the data computation as the use case can compute the data in a smaller or bigger frequency, example:</p> <ul> <li>There's a use case where the cadence is <code>WEEKLY</code> but we want the aggregated data with a <code>DAILY</code> frequency, so configuring the reconciliation window to be <code>DAILY</code> it will compute the data in <code>WEEK TO DATE</code> basis (In a case where the first day of week is monday, on monday it will have the data just for monday, on tuesday will be the computation of monday + tuesday, on wednesday will be the computation of monday + tuesday + wednesday, and so on until the end of week), configuration example: <pre><code>{'WEEK': {'recon_window': {'DAY': {'snapshot': 'N'}}}}\n```\n- You can also have the same frequency calculation but with in a bigger frequency, like `WEEKLY` cadence but compute it just with a `MONTHLY` frequency, configuration example:\n```python\n{'WEEK': {'recon_window': {'MONTH': {'snapshot': 'N'}}}}\n</code></pre></li> </ul>"},{"location":"lakehouse_engine_usage/gab/gab.html#snapshot","title":"Snapshot","text":"<p>It creates a snapshot of the computed data on the specified reconciliation cadence, example: in a case where we have <code>MONTHLY</code> cadence and snapshot enabled at <code>DAILY</code> basis, we are going to compute the use case for each day in the month. Use case configuration:  <pre><code>{'MONTH': {'recon_window': {'DAY': {'snapshot': 'Y'}}}}\n</code></pre></p>"},{"location":"lakehouse_engine_usage/gab/gab.html#next-steps","title":"Next Steps","text":"<p>If you are interested in using GAB you can check our step-by-step documentation that aims to help in the use case configuration and make easier to use this awesome feature :)</p>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html","title":"GAB step-by-step","text":""},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#1-data-product-setup-based-on-templated-files","title":"1. Data Product Setup based on templated files","text":"<ul> <li>Lakehouse engine: 1.20.0+</li> <li>Copy GAB assets from templated files to your data product:</li> <li>GAB tables:<ul> <li>Calendar table - dim_calendar</li> <li>Use case configuration table - lkp_query_builder</li> <li>Unified data table - gab_use_case_results</li> <li>GAB log events table - gab_log_events</li> </ul> </li> <li>GAB notebooks:<ul> <li>Feed Calendar table - gab_dim_calendar</li> <li>Use case creation - query_builder_helper</li> <li>GAB execution - gab</li> <li>GAB job manager - gab_job_manager</li> </ul> </li> </ul>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#2-use-case-setup","title":"2. Use case setup","text":""},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#templating-sql-query","title":"Templating SQL Query","text":"<p>The placeholder will always use pre-configured values from the use case (lkp_query_builder). </p>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#available-placeholders","title":"Available Placeholders:","text":"<ul> <li>Date Aggregation:<ul> <li>Reference date (the date column used to compute the cadences and the extended window)   <pre><code> ({{ project_date_column }} + interval '{{ offset_value }}' hour)\n</code></pre> Note: The <code>replace_offset_value</code> have the responsibility to use directly the date column or shift it to the specified timezone using the <code>offset_value</code> from the configured use case.</li> <li>Last day of the cadence and if snapshots are enabled then it contains the snapshot end day:   <pre><code>{{ to_date }}\n</code></pre></li> </ul> </li> <li>Filter:<ul> <li>It expects to have the data partitioned by date (year/month/day) and it replaces the placeholder with a filter like <code>year = **** and month = ** and day = **</code>:   <pre><code>{{ partition_filter }}\n</code></pre> Note: if your table does not have the Year, Month, Day columns you should not add this template</li> <li>Shift the data consumed on your use case to be between the specified offset timezone:   <pre><code>{{ filter_date_column }} &gt;= ('{{ start_date }}' + interval '{{ offset_value }}' hour) AND {{ filter_date_column }} &lt; ('{{ end_date }}' + interval '{{ offset_value }}' hour)\n</code></pre></li> </ul> </li> <li>The source database to consume the data:   <pre><code>{{ database }}\n</code></pre></li> <li>Calendar join:   <pre><code>{{ joins }}\n</code></pre> Note: Can be added after any of the table names in the <code>from</code> statement. The framework renders this <code>joins</code> with an internal calendar join and populates the <code>to_date</code> and the <code>project_date_column</code> as per the configured cadences, which means, it will need to be added when using Cadence, Extended Window Calculation, Reconciliation and Snapshot.</li> </ul>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#sample-combined-query-making-use-of-the-placeholders","title":"Sample combined query making use of the placeholders:","text":"<pre><code>SELECT\n({{ project_date_column }} + interval '{{ offset_value }}' hour)  AS order_date,  # date aggregation: computed cadence start date\n{{ to_date }} AS to_date,  # date aggregation: last day of the cadence or of the snapshot if enabled\nb.category_name,\nCOUNT(a.article_id) qty_articles,\nSUM(amount) total_amount\nFROM\n{{ database }}.dummy_sales_kpi a  # source database\n{{ joins }}  # calendar table join: used to compute the cadence start and end date\nLEFT JOIN\narticle_categories b ON a.article_id = b.article_id\nWHERE\n{{ partition_filter }}  # filter: partition filter\nAND\nTO_DATE({{ filter_date_column }}, 'yyyyMMdd') &gt;= (\n'{{ start_date }}' + interval '{{ offset_value }}' hour\n)  # filter by date column configured in the use case for this file and timezone shift\nAND\nTO_DATE({{ filter_date_column }}, 'yyyyMMdd') &lt; (\n'{{ end_date }}' + interval '{{ offset_value }}' hour\n)  # filter by date column configured in the use case for this file and timezone shift\nGROUP BY 1,2,3\n</code></pre> <p>Note: If there's just one sql file for the use case, the file should start with 1_. When the use case has several different intermediate stages/temp tables, please create them according to the sequence order, as shown in the screenshot, and a final combined script, example:</p> <p></p> <p>We suggest using the folder metadata/gab to use as the SQL use case folder but this is a parametrised property which you can override with the property gab_base_path. This property is used in the job manager as well.</p>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#use-case-configuration-using-the-query_builder_helper","title":"Use case configuration using the query_builder_helper","text":"1. General configuration <p>GAB will pull information from <code>lkp_query_builder</code> in order to retrieve information/configuration to execute the process. To help us on this task a query_builder_help notebook was provided. We will follow the notebook instructions to add necessary configuration.</p> <p></p> Variable Value Description Complexity Low Defines the complexity of your use case.You should mainly consider the volume of data.Possible values: Low, Medium and High. Database Name example_database Refers to the name of the development environment database where the lkp_query_builder table resides.This parameter is used at the end of the notebook to insert data into the lkp_query_builder table. How many dimensions 1 Number of dimensions (columns) expected in the use case.Note: Do not consider the reference date or metrics here, as they have their own parameters. How many views 1 Defines how many views to generate in the use case, it's possible to have as many as the use case need.All views will have the same structure (dimensions and metrics), the only difference possible to specify between the views is the view filter.Default value is 1.Note: This configuration have a direct impact in the <code>3. Configure View Name and Filters</code> configuration. Is Active Y Flag to make the use case active or not.Default value is Y. Market GLOBAL Used in the gab_job_manager to execute the use cases for each market. SQL File Names 1_article_category.sql,2_f_agg_dummy_sales_kpi.sql Name of the SQL files used in the use case. You can combine different layers of dependencies between them as shown in the example, where the 2_combined.sql file depends on 1_product_category.sql file. The file name should follow the pattern x_file_name (where x is an integer digit) and be separated by a comma (e.g.: 1_first_query.sql, 2_second_query.sql). Snapshot End Date to_date This parameter is used in the template, by default its value must be to_date.You can change it if you have managed this in your SQL files.The values stored in this column depend on the use case behavior:<ul><li>if snapshots are enabled, it will contain the snapshot end day.</li><li>If no snapshot is enabled, it will contain the last day of the cadence.</li></ul>The snapshot behaviour is set in the reconciliation steps. Timezone Offset 0 The time zone offset that you want to apply to the reference date column.It should be a number to decrement or add to the date (e.g., -8 or 8).The default value is zero, which means that any time zone transformation will be applied to the date. Use Case Name f_agg_dummy_sales_kpi Name of the use case.The suggestion is to use lowercase and underlined alphanumeric characters. Use Case Reference Date order_date Reference date of the use case.&lt;br /The parameter should be the column name and the selected column should have the date/datetime format. Week Start MONDAY The start of the business week of the use case.Possible values: SUNDAY or MONDAY. 2. Configure Dimension Names <p></p> 3. Configure View Name and Filters <p>This will be the name of the view at the end of the process. Filters should be applied at this step if needed.</p> <p></p> Variable Value Description View Filter Based on the dimensions defined in the previous step.Example: if you have set the country as <code>D1</code>, the filter here could be D1 = \"Germany\". The commands allowed for the filter step are the same as those supported in the where clause in SQL language. View Name vw_f_agg_dummy_sales_kpi Name of the view to output the computed data. 4. Configure RECON <p>This step is where we define which will be the cadence displayed at the view. </p> Variable Value Description Reconciliation Cadence YEAR Compute the data aggregated by the specified cadence but with the reconciliation frequency.Check more about it here 5. Configure METRICS <p>First question regarding metrics is how many metrics do we have on our SQL use case query.</p> <p>On our template we have two metrics (<code>qty_articles</code> and <code>total_amount</code>).</p> <p></p> <p></p> <p>The next paragraph will create widgets for us to be able to define if we want GAB to create secondary calculations for us based on the metric name. Note: metrics should follow the same order as defined on the SQL use case query.</p> <p></p> Variable Description Calculated Metric It's possible to derive (add secondary calculations) 4 new columns based on each metric.Those new columns will be based on cadences like last_cadence, last_year_cadence and window function.But also, you can create a derived column,which is a SQL statement that you can write on your own by selecting the derived_metric option. Metric Name Name of the base metric. Should have the same name as on the SQL use case query. <p>The next metrics configuration is where we configure secondary calculations. </p> Variable Description derived_metric.Formula Formula to calculate the metric referring any of previous configured metrics by the Metric Name.Example: <code>total_amount*0.56</code> derived_metric.Label Name of the generated metric by derived_metric. last_cadence.Label Name of the generated metric by last_cadence. last_cadence.Window Cadence lookback window, which means in this example,a lookback from the previous year (as the use case is on YEARLY cadence) window_function.Agg Func SQL Function to calculate the metric.Possible values: sum, avg, max, min, count window_function.Label Name of the generated metric by window_function. window_function.Window Interval Window interval to use on the metric generation. 6. Configure Stages <p>Stages are related to each SQL statement at the use case.</p> <p></p> Variable Description Script Filter Date Column It will be used to filter the data of your use case.This information will be replaced in the placeholder of the GAB template. Script Project Date Column It will be used as reference date for the query given.This information will be replaced in the placeholder of the GAB template. Script Repartition Type Type of repartitioning the data of the query.Possible values: Key and Number.When use Key, it expects column names separated by a comma.When use number it expects an integer of how many partitions the user want. Script Repartition Value This parameter only has effect when used with Repartition Type parameter.It sets the way of repartitioning the data while processing. Script Storage Level Defines the type of spark persistence storage levels you want to define(e.g. Memory Only, Memory and Disk etc). Script Table Alias The alias name of the sql file that will run, this name can be used to consume the output of the execution in the next stage(next sql file). 7. Build and Insert SQL Instruction <p></p> <p></p> <p>At the end of the process it should have data inserted on lkp_query_builder table: </p> Configured Use case <p>After configuring the use case, it would generate a SQL command to create it on the <code>lkp_query_builder</code>, like this: <pre><code>DELETE FROM example_database.lkp_query_builder WHERE QUERY_LABEL = 'f_agg_dummy_sales_kpi';\nINSERT INTO example_database.lkp_query_builder VALUES (\n1,\n'f_agg_dummy_sales_kpi',\n'GLOBAL',\n\"\"\"{\n    'vw_f_agg_dummy_sales_kpi': {\n      'dimensions': {\n        'from_date': 'order_date',\n        'to_date': 'to_date',\n        'd1': 'category_name'\n      },\n      'metric': {\n        'm1': {\n          'metric_name': 'qty_articles',\n          'calculated_metric': {},\n          'derived_metric': {}\n        },\n        'm2': {\n          'metric_name': 'total_amount',\n            'calculated_metric': {\n              'last_cadence': [\n                {\n                  'label': 'total_amount_last_year',\n                  'window': '1'\n                }\n              ],\n              'window_function': [\n                {\n                  'label': 'avg_total_amount_last_2_years',\n                  'window': [2, 1],\n                  'agg_func': 'avg'\n                }\n              ]\n            },\n            'derived_metric': [\n              {\n                'label': 'discounted_total_amount',\n                'formula': 'total_amount*0.56'\n              }\n            ]\n          }\n        },\n      'filter': {}\n    }\n  }\"\"\",\n\"\"\"{\n    '1': {\n        'file_path': 'f_agg_dummy_sales_kpi/1_article_category.sql',\n        'table_alias': 'article_categories',\n        'storage_level': 'MEMORY_ONLY',\n        'project_date_column': '',\n        'filter_date_column': '',\n        'repartition': {}\n    },\n    '2': {\n        'file_path': 'f_agg_dummy_sales_kpi/2_f_agg_dummy_sales_kpi.sql',\n        'table_alias': 'dummy_sales_kpi',\n        'storage_level': 'MEMORY_ONLY',\n        'project_date_column': 'order_date',\n        'filter_date_column': 'order_date',\n        'repartition': {}\n    }\n  }\"\"\",\n\"\"\"{'YEAR': {}}\"\"\",\n'0',\n'MONDAY',\n'Y',\n'Low',\ncurrent_timestamp()\n);\n</code></pre></p>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#3-use-case-execution","title":"3. Use case execution","text":"<p>After the initial setup and adding your use case to the lkp_query_builder you can schedule the gab_job_manager to manage the use case execution in any schedule time you want.</p>"},{"location":"lakehouse_engine_usage/gab/step_by_step/step_by_step.html#4-consuming-the-data","title":"4. Consuming the data","text":"<p>The data is available in the view you specified as output from the use-case, so you can normally consume the view as you would consume any other data (directly on power bi or in another pipeline).</p>"},{"location":"lakehouse_engine_usage/reconciliator/reconciliator.html","title":"Reconciliator","text":"<p>Checking if data reconciles, using this algorithm, is a matter of reading the truth data and the current data. You can use any input specification compatible with the lakehouse engine to read truth or current data. On top of that, you can pass a <code>truth_preprocess_query</code> and a <code>current_preprocess_query</code> so you can preprocess the data before it goes into the actual reconciliation process. The reconciliation process is focused on joining truth with <code>current</code> by all provided columns except the ones passed as <code>metrics</code>.</p> <p>In the table below, we present how a simple reconciliation would look like:</p> current_country current_count truth_country truth_count absolute_diff perc_diff yellow red recon_type Sweden 123 Sweden 120 3 0.025 0.1 0.2 percentage Germany 2946 Sweden 2946 0 0 0.1 0.2 percentage France 2901 France 2901 0 0 0.1 0.2 percentage Belgium 426 Belgium 425 1 0.002 0.1 0.2 percentage <p>The Reconciliator algorithm uses an ACON to configure its execution. You can find the meaning of each ACON property in ReconciliatorSpec object.</p> <p>Below there is an example of usage of reconciliator. <pre><code>from lakehouse_engine.engine import execute_reconciliation\ntruth_query = \"\"\"\n  SELECT\n    shipping_city,\n    sum(sales_order_qty) as qty,\n    order_date_header\n  FROM (\n    SELECT\n      ROW_NUMBER() OVER (\n        PARTITION BY sales_order_header, sales_order_schedule, sales_order_item, shipping_city\n        ORDER BY changed_on desc\n      ) as rank1,\n      sales_order_header,\n      sales_order_item,\n      sales_order_qty,\n      order_date_header,\n      shipping_city\n    FROM truth -- truth is a locally accessible temp view created by the lakehouse engine\n    WHERE order_date_header = '2021-10-01'\n  ) a\nWHERE a.rank1 = 1\nGROUP BY a.shipping_city, a.order_date_header\n\"\"\"\ncurrent_query = \"\"\"\n  SELECT\n    shipping_city,\n    sum(sales_order_qty) as qty,\n    order_date_header\n  FROM (\n    SELECT\n      ROW_NUMBER() OVER (\n        PARTITION BY sales_order_header, sales_order_schedule, sales_order_item, shipping_city\n        ORDER BY changed_on desc\n      ) as rank1,\n      sales_order_header,\n      sales_order_item,\n      sales_order_qty,\n      order_date_header,\n      shipping_city\n    FROM current -- current is a locally accessible temp view created by the lakehouse engine\n    WHERE order_date_header = '2021-10-01'\n  ) a\nWHERE a.rank1 = 1\nGROUP BY a.shipping_city, a.order_date_header\n\"\"\"\nacon = {\n\"metrics\": [{\"metric\": \"qty\", \"type\": \"percentage\", \"aggregation\": \"avg\", \"yellow\": 0.05, \"red\": 0.1}],\n\"truth_input_spec\": {\n\"spec_id\": \"truth\",\n\"read_type\": \"batch\",\n\"data_format\": \"csv\",\n\"schema_path\": \"s3://my_data_product_bucket/artefacts/metadata/schemas/bronze/orders.json\",\n\"options\": {\n\"delimiter\": \"^\",\n\"dateFormat\": \"yyyyMMdd\",\n},\n\"location\": \"s3://my_data_product_bucket/bronze/orders\",\n},\n\"truth_preprocess_query\": truth_query,\n\"current_input_spec\": {\n\"spec_id\": \"current\",\n\"read_type\": \"batch\",\n\"data_format\": \"delta\",\n\"db_table\": \"my_database.orders\",\n},\n\"current_preprocess_query\": current_query,\n}\nexecute_reconciliation(acon=acon)\n</code></pre></p>"},{"location":"lakehouse_engine_usage/sensor/sensor.html","title":"Sensor","text":""},{"location":"lakehouse_engine_usage/sensor/sensor.html#what-is-it","title":"What is it?","text":"<p>The lakehouse engine sensors are an abstraction to otherwise complex spark code that can be executed in very small single-node clusters to check if an upstream system or data product contains new data since the last execution of our job. With this feature, we can trigger a job to run in more frequent intervals and if the upstream does not contain new data, then the rest of the job exits without creating bigger clusters to execute more intensive data ETL (Extraction, Transformation, and Loading).</p>"},{"location":"lakehouse_engine_usage/sensor/sensor.html#how-do-sensor-based-jobs-work","title":"How do Sensor-based jobs work?","text":"<p>With the sensors capability, data products in the lakehouse can sense if another data product or an upstream system (source system) have new data since the last successful job. We accomplish this through the approach illustrated above, which can be interpreted as follows:</p> <ol> <li>A Data Product can check if Kafka, JDBC or any other Lakehouse Engine Sensors supported sources, contains new data using the respective sensors;</li> <li>The Sensor task may run in a very tiny single-node cluster to ensure cost    efficiency (check sensor cost efficiency);</li> <li>If the sensor has recognised that there is new data in the upstream, then you can start a different ETL Job Cluster    to process all the ETL tasks (data processing tasks).</li> <li>In the same way, a different Data Product can sense if an upstream Data Product has new data by using 1 of 2 options:<ol> <li>(Preferred) Sense the upstream Data Product sensor control delta table;</li> <li>Sense the upstream Data Product data files in s3 (files sensor) or any of their delta tables (delta table    sensor);</li> </ol> </li> </ol>"},{"location":"lakehouse_engine_usage/sensor/sensor.html#the-structure-and-relevance-of-the-data-products-sensors-control-table","title":"The Structure and Relevance of the Data Product\u2019s Sensors Control Table","text":"<p>The concept of a lakehouse engine sensor is based on a special delta table stored inside the data product that chooses to opt in for a sensor-based job. That table is used to control the status of the various sensors implemented by that data product. You can refer to the below table to understand the sensor delta table structure:</p> Column Name Type Description sensor_id STRING A unique identifier of the sensor in a specific job. This unique identifier is really important because it is used by the engine to identify if there is new data in the upstream.Each sensor in each job should have a different sensor_id.If you attempt to create 2 sensors with the same sensor_id, the engine will fail. assets ARRAY\\ A list of assets (e.g., tables or dataset folder) that are considered as available to consume downstream after the sensor has status PROCESSED_NEW_DATA. status STRING Status of the sensor. Can either be:<ul><li>ACQUIRED_NEW_DATA \u2013 when the sensor in a job has recognised that there is new data from the upstream but, the job where the sensor is, was still not successfully executed.</li><li>PROCESSED_NEW_DATA - when the job where the sensor is located has processed all the tasks in that job.</li></ul> status_change_timestamp STRING Timestamp when the status has changed for the last time. checkpoint_location STRING Base location of the Spark streaming checkpoint location, when applicable (i.e., when the type of sensor uses Spark streaming checkpoints to identify if the upstream has new data). E.g. Spark streaming checkpoints are used for Kafka, Delta and File sensors. upstream_key STRING Upstream key (e.g., used to store an attribute name from the upstream so that new data can be detected automatically).This is useful for sensors that do not rely on Spark streaming checkpoints, like the JDBC sensor, as it stores the name of a field in the JDBC upstream that contains the values that will allow us to identify new data (e.g., a timestamp in the upstream that tells us when the record was loaded into the database). upstream_value STRING Upstream value (e.g., used to store the max attribute value from the upstream so that new data can be detected automatically). This is the value for upstream_key. This is useful for sensors that do not rely on Spark streaming checkpoints, like the JDBC sensor, as it stores the value of a field in the JDBC upstream that contains the maximum value that was processed by the sensor, and therefore useful for recognizing that there is new data in the upstream (e.g., the value of a timestamp attribute in the upstream that tells us when the record was loaded into the database). <p>Note</p> <p>To make use of the sensors you will need to add this table to your data product.</p>"},{"location":"lakehouse_engine_usage/sensor/sensor.html#how-is-it-different-from-scheduled-jobs","title":"How is it different from scheduled jobs?","text":"<p>Sensor-based jobs are still scheduled, but they can be scheduled with higher frequency, as they are more cost-efficient than ramping up a multi-node cluster supposed to do heavy ETL, only to figure out that the upstream does not have new data.</p>"},{"location":"lakehouse_engine_usage/sensor/sensor.html#are-sensor-based-jobs-cost-efficient","title":"Are sensor-based jobs cost-efficient?","text":"<p>For the same schedule (e.g., 4 times a day), sensor-based jobs are more cost-efficient than scheduling a regular job, because with sensor-based jobs you can start a very tiny single-node cluster, and only if there is new data in the upstream the bigger ETL cluster is spin up. For this reason, they are considered more cost-efficient. Moreover, if you have very hard SLAs to comply with, you can also play with alternative architectures where you can have several sensors in a continuous (always running) cluster, which then keeps triggering the respective data processing jobs, whenever there is new data.</p>"},{"location":"lakehouse_engine_usage/sensor/sensor.html#sensor-steps","title":"Sensor Steps","text":"<ol> <li>Create your sensor task for the upstream source. Examples of available sources:<ul> <li>Delta Table</li> <li>Delta Upstream Sensor Table</li> <li>File</li> <li>JDBC</li> <li>Kafka</li> <li>SAP BW/B4</li> </ul> </li> <li>Setup/Execute your ETL task based in the Sensor Condition</li> <li>Update the Sensor Control table status with the Update Sensor Status</li> </ol>"},{"location":"lakehouse_engine_usage/sensor/delta_table/delta_table.html","title":"Sensor from Delta Table","text":"<p>This shows how to create a Sensor to detect new data from a Delta Table.</p>"},{"location":"lakehouse_engine_usage/sensor/delta_table/delta_table.html#configuration-required-to-have-a-sensor","title":"Configuration required to have a Sensor","text":"<ul> <li>sensor_id: A unique identifier of the sensor in a specific job.</li> <li>assets: List of assets considered for the sensor, which are considered as available once the   sensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li> <li>control_db_table_name: Name of the sensor control table.</li> <li>input_spec: Input spec with the upstream source.</li> <li>preprocess_query: Query to filter data returned by the upstream.</li> </ul> <p>Note</p> <p>This parameter is only needed when the upstream data have to be filtered, in this case a custom query should be created with the source table as <code>sensor_new_data</code>. If you want to view some examples of usage you can visit the delta upstream sensor table or the jdbc sensor.</p> <ul> <li>base_checkpoint_location: Spark streaming checkpoints to identify if the upstream has new data.</li> <li>fail_on_empty_result: Flag representing if it should raise <code>NoNewDataException</code> when there is no new data detected from upstream.</li> </ul> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"lakehouse_engine_usage/sensor/delta_table/delta_table.html#scenarios","title":"Scenarios","text":"<p>This covers the following scenarios of using the Sensor:</p> <ol> <li>The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</li> <li>The <code>fail_on_empty_result=False</code>.</li> </ol> <p>Data will be consumed from a delta table in streaming mode, so if there is any new data it will give condition to proceed to the next task.</p>"},{"location":"lakehouse_engine_usage/sensor/delta_table/delta_table.html#fail_on_empty_result-as-true-default-and-suggested","title":"<code>fail_on_empty_result</code> as True (default and SUGGESTED)","text":"<pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n\"sensor_id\": \"MY_SENSOR_ID\",\n\"assets\": [\"MY_SENSOR_ASSETS\"],\n\"control_db_table_name\": \"my_database.lakehouse_engine_sensors\",\n\"input_spec\": {\n\"spec_id\": \"sensor_upstream\",\n\"read_type\": \"streaming\",\n\"data_format\": \"delta\",\n\"db_table\": \"upstream_database.source_delta_table\",\n\"options\": {\n\"readChangeFeed\": \"true\", # to read changes in upstream table\n},\n},\n\"base_checkpoint_location\": \"s3://my_data_product_bucket/checkpoints\",\n\"fail_on_empty_result\": True,\n}\nexecute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/delta_table/delta_table.html#fail_on_empty_result-as-false","title":"<code>fail_on_empty_result</code> as False","text":"<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it  has acquired new data. This value can be used to execute or not the next steps.</p> <pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n[...],\n\"fail_on_empty_result\": False\n}\nacquired_data = execute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/delta_upstream_sensor_table/delta_upstream_sensor_table.html","title":"Sensor from other Sensor Delta Table","text":"<p>This shows how to create a Sensor to detect new data from another Sensor Delta Table.</p>"},{"location":"lakehouse_engine_usage/sensor/delta_upstream_sensor_table/delta_upstream_sensor_table.html#configuration-required-to-have-a-sensor","title":"Configuration required to have a Sensor","text":"<ul> <li>sensor_id: A unique identifier of the sensor in a specific job.</li> <li>assets: List of assets considered for the sensor, which are considered as available once the   sensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li> <li>control_db_table_name: Name of the sensor control table.</li> <li>input_spec: Input spec with the upstream source.</li> <li>preprocess_query: Query to filter data returned by the upstream.</li> </ul> <p>Note</p> <p>This parameter is only needed when the upstream data have to be filtered, in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p> <ul> <li>base_checkpoint_location: Spark streaming checkpoints to identify if the upstream has new data.</li> <li>fail_on_empty_result: Flag representing if it should raise <code>NoNewDataException</code> when there is no new data detected from upstream.</li> </ul> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"lakehouse_engine_usage/sensor/delta_upstream_sensor_table/delta_upstream_sensor_table.html#scenarios","title":"Scenarios","text":"<p>This covers the following scenarios of using the Sensor:</p> <ol> <li>The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</li> <li>The <code>fail_on_empty_result=False</code>.</li> </ol> <p>It makes use of <code>generate_sensor_query</code> to generate the <code>preprocess_query</code>, different from delta_table.</p> <p>Data from other sensor delta table, in streaming mode, will be consumed. If there is any new data it will trigger  the condition to proceed to the next task.</p>"},{"location":"lakehouse_engine_usage/sensor/delta_upstream_sensor_table/delta_upstream_sensor_table.html#fail_on_empty_result-as-true-default-and-suggested","title":"<code>fail_on_empty_result</code> as True (default and SUGGESTED)","text":"<pre><code>from lakehouse_engine.engine import execute_sensor, generate_sensor_query\nacon = {\n\"sensor_id\": \"MY_SENSOR_ID\",\n\"assets\": [\"MY_SENSOR_ASSETS\"],\n\"control_db_table_name\": \"my_database.lakehouse_engine_sensors\",\n\"input_spec\": {\n\"spec_id\": \"sensor_upstream\",\n\"read_type\": \"streaming\",\n\"data_format\": \"delta\",\n\"db_table\": \"upstream_database.lakehouse_engine_sensors\",\n\"options\": {\n\"readChangeFeed\": \"true\",\n},\n},\n\"preprocess_query\": generate_sensor_query(\"UPSTREAM_SENSOR_ID\"),\n\"base_checkpoint_location\": \"s3://my_data_product_bucket/checkpoints\",\n\"fail_on_empty_result\": True,\n}\nexecute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/delta_upstream_sensor_table/delta_upstream_sensor_table.html#fail_on_empty_result-as-false","title":"<code>fail_on_empty_result</code> as False","text":"<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it has acquired new data. This value can be used to execute or not the next steps.</p> <pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n[...],\n\"fail_on_empty_result\": False\n}\nacquired_data = execute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/file/file.html","title":"Sensor from Files","text":"<p>This shows how to create a Sensor to detect new data from a File Location.</p>"},{"location":"lakehouse_engine_usage/sensor/file/file.html#configuration-required-to-have-a-sensor","title":"Configuration required to have a Sensor","text":"<ul> <li>sensor_id: A unique identifier of the sensor in a specific job.</li> <li>assets: List of assets considered for the sensor, which are considered as available once the sensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li> <li>control_db_table_name: Name of the sensor control table.</li> <li>input_spec: Input spec with the upstream source.</li> <li>preprocess_query: Query to filter data returned by the upstream.</li> </ul> <p>Note</p> <p>This parameter is only needed when the upstream data have to be filtered, in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p> <ul> <li>base_checkpoint_location: Spark streaming checkpoints to identify if the upstream has new data.</li> <li>fail_on_empty_result: Flag representing if it should raise <code>NoNewDataException</code> when there is no new data detected from upstream.</li> </ul> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"lakehouse_engine_usage/sensor/file/file.html#scenarios","title":"Scenarios","text":"<p>This covers the following scenarios of using the Sensor:</p> <ol> <li>The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</li> <li>The <code>fail_on_empty_result=False</code>.</li> </ol> <p>Using these sensors and consuming the data in streaming mode, if any new file is added to the file location,  it will automatically trigger the proceeding task.</p>"},{"location":"lakehouse_engine_usage/sensor/file/file.html#fail_on_empty_result-as-true-default-and-suggested","title":"<code>fail_on_empty_result</code> as True (default and SUGGESTED)","text":"<pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n\"sensor_id\": \"MY_SENSOR_ID\",\n\"assets\": [\"MY_SENSOR_ASSETS\"],\n\"control_db_table_name\": \"my_database.lakehouse_engine_sensors\",\n\"input_spec\": {\n\"spec_id\": \"sensor_upstream\",\n\"read_type\": \"streaming\",\n\"data_format\": \"csv\",  # You can use any of the data formats supported by the lakehouse engine, e.g: \"avro|json|parquet|csv|delta|cloudfiles\"\n\"location\": \"s3://my_data_product_bucket/path\",\n},\n\"base_checkpoint_location\": \"s3://my_data_product_bucket/checkpoints\",\n\"fail_on_empty_result\": True,\n}\nexecute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/file/file.html#fail_on_empty_result-as-false","title":"<code>fail_on_empty_result</code> as False","text":"<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it has acquired new data. This value can be used to execute or not the next steps.</p> <pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n[...],\n\"fail_on_empty_result\": False\n}\nacquired_data = execute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/jdbc_table/jdbc_table.html","title":"Sensor from JDBC","text":"<p>This shows how to create a Sensor to detect new data from a JDBC table.</p>"},{"location":"lakehouse_engine_usage/sensor/jdbc_table/jdbc_table.html#configuration-required-to-have-a-sensor","title":"Configuration required to have a Sensor","text":"<ul> <li>jdbc_args: Arguments of the JDBC upstream.</li> <li>generate_sensor_query: Generates a Sensor query to consume data from the upstream, this function can be used on <code>preprocess_query</code> ACON option.<ul> <li>sensor_id: The unique identifier for the Sensor.</li> <li>filter_exp: Expression to filter incoming new data.   A placeholder <code>?upstream_key</code> and <code>?upstream_value</code> can be used, example: <code>?upstream_key &gt; ?upstream_value</code> so that it can be replaced by the respective values from the sensor <code>control_db_table_name</code> for this specific sensor_id.</li> <li>control_db_table_name: Sensor control table name.</li> <li>upstream_key: the key of custom sensor information to control how to identify new data from the upstream (e.g., a time column in the upstream).</li> <li>upstream_value: the first upstream value to identify new data from the upstream (e.g., the value of a time present in the upstream). Note: This parameter will have effect just in the first run to detect if the upstream have new data. If it's empty the default value applied is <code>-2147483647</code>.</li> <li>upstream_table_name: Table name to consume the upstream value. If it's empty the default value applied is <code>sensor_new_data</code>.</li> </ul> </li> </ul> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"lakehouse_engine_usage/sensor/jdbc_table/jdbc_table.html#scenarios","title":"Scenarios","text":"<p>This covers the following scenarios of using the Sensor:</p> <ol> <li>Generic JDBC template with <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</li> <li>Generic JDBC template with <code>fail_on_empty_result=False</code>.</li> </ol> <p>Data from JDBC, in batch mode, will be consumed. If there is new data based in the preprocess query from the source table, it will trigger the condition to proceed to the next task.</p>"},{"location":"lakehouse_engine_usage/sensor/jdbc_table/jdbc_table.html#fail_on_empty_result-as-true-default-and-suggested","title":"<code>fail_on_empty_result</code> as True (default and SUGGESTED)","text":"<pre><code>from lakehouse_engine.engine import execute_sensor, generate_sensor_query\nacon = {\n\"sensor_id\": \"MY_SENSOR_ID\",\n\"assets\": [\"MY_SENSOR_ASSETS\"],\n\"control_db_table_name\": \"my_database.lakehouse_engine_sensors\",\n\"input_spec\": {\n\"spec_id\": \"sensor_upstream\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"jdbc_args\": {\n\"url\": \"JDBC_URL\",\n\"table\": \"JDBC_DB_TABLE\",\n\"properties\": {\n\"user\": \"JDBC_USERNAME\",\n\"password\": \"JDBC_PWD\",\n\"driver\": \"JDBC_DRIVER\",\n},\n},\n\"options\": {\n\"compress\": True,\n},\n},\n\"preprocess_query\": generate_sensor_query(\nsensor_id=\"MY_SENSOR_ID\",\nfilter_exp=\"?upstream_key &gt; '?upstream_value'\",\ncontrol_db_table_name=\"my_database.lakehouse_engine_sensors\",\nupstream_key=\"UPSTREAM_COLUMN_TO_IDENTIFY_NEW_DATA\",\n),\n\"base_checkpoint_location\": \"s3://my_data_product_bucket/checkpoints\",\n\"fail_on_empty_result\": True,\n}\nexecute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/jdbc_table/jdbc_table.html#fail_on_empty_result-as-false","title":"<code>fail_on_empty_result</code> as False","text":"<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it has acquired new data. This value can be used to execute or not the next steps.</p> <pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n[...],\n\"fail_on_empty_result\": False\n}\nacquired_data = execute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/kafka/kafka.html","title":"Sensor from Kafka","text":"<p>This shows how to create a Sensor to detect new data from Kafka.</p>"},{"location":"lakehouse_engine_usage/sensor/kafka/kafka.html#configuration-required-to-have-a-sensor","title":"Configuration required to have a Sensor","text":"<ul> <li>sensor_id: A unique identifier of the sensor in a specific job.</li> <li>assets: List of assets considered for the sensor, which are considered as available once the   sensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li> <li>control_db_table_name: Name of the sensor control table.</li> <li>input_spec: Input spec with the upstream source.</li> <li>preprocess_query: Query to filter data returned by the upstream.</li> </ul> <p>Note</p> <p>This parameter is only needed when the upstream data have to be filtered, in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p> <ul> <li>base_checkpoint_location: Spark streaming checkpoints to identify if the upstream has new data.</li> <li>fail_on_empty_result: Flag representing if it should raise <code>NoNewDataException</code> when there is no new data detected from upstream.</li> </ul> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"lakehouse_engine_usage/sensor/kafka/kafka.html#scenarios","title":"Scenarios","text":"<p>This covers the following scenarios of using the Sensor:</p> <ol> <li>The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</li> <li>The <code>fail_on_empty_result=False</code>.</li> </ol> <p>Data from Kafka, in streaming mode, will be consumed, so if there is any new data in the kafka topic it will give condition to proceed to the next task.</p>"},{"location":"lakehouse_engine_usage/sensor/kafka/kafka.html#fail_on_empty_result-as-true-default-and-suggested","title":"<code>fail_on_empty_result</code> as True (default and SUGGESTED)","text":"<pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n\"sensor_id\": \"MY_SENSOR_ID\",\n\"assets\": [\"MY_SENSOR_ASSETS\"],\n\"control_db_table_name\": \"my_database.lakehouse_engine_sensors\",\n\"input_spec\": {\n\"spec_id\": \"sensor_upstream\",\n\"read_type\": \"streaming\",\n\"data_format\": \"kafka\",\n\"options\": {\n\"kafka.bootstrap.servers\": \"KAFKA_SERVER\",\n\"subscribe\": \"KAFKA_TOPIC\",\n\"startingOffsets\": \"earliest\",\n\"kafka.security.protocol\": \"SSL\",\n\"kafka.ssl.truststore.location\": \"TRUSTSTORE_LOCATION\",\n\"kafka.ssl.truststore.password\": \"TRUSTSTORE_PWD\",\n\"kafka.ssl.keystore.location\": \"KEYSTORE_LOCATION\",\n\"kafka.ssl.keystore.password\": \"KEYSTORE_PWD\",\n},\n},\n\"base_checkpoint_location\": \"s3://my_data_product_bucket/checkpoints\",\n\"fail_on_empty_result\": True,\n}\nexecute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/kafka/kafka.html#fail_on_empty_result-as-false","title":"<code>fail_on_empty_result</code> as False","text":"<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it has acquired new data. This value can be used to execute or not the next steps.</p> <pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n[...],\n\"fail_on_empty_result\": False\n}\nacquired_data = execute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/sap_bw_b4/sap_bw_b4.html","title":"Sensor from SAP","text":"<p>This shows how to create a Sensor to detect new data from a SAP LOGCHAIN table.</p>"},{"location":"lakehouse_engine_usage/sensor/sap_bw_b4/sap_bw_b4.html#configuration-required-to-have-a-sensor","title":"Configuration required to have a Sensor","text":"<ul> <li>sensor_id: A unique identifier of the sensor in a specific job.</li> <li>assets: List of assets considered for the sensor, which are considered as available once the   sensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li> <li>control_db_table_name: Name of the sensor control table.</li> <li>input_spec: Input spec with the upstream source.</li> <li>preprocess_query: Query to filter data returned by the upstream.</li> </ul> <p>Note</p> <p>This parameter is only needed when the upstream data have to be filtered, in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p> <ul> <li>base_checkpoint_location: Spark streaming checkpoints to identify if the upstream has new data.</li> <li>fail_on_empty_result: Flag representing if it should raise <code>NoNewDataException</code> when there is no new data detected from upstream.</li> </ul> <p>Specific configuration required to have a Sensor consuming a SAP BW/B4 upstream. The Lakehouse Engine provides two utility functions to make easier to consume SAP as upstream: <code>generate_sensor_sap_logchain_query</code> and <code>generate_sensor_query</code>.</p> <ul> <li> <p>generate_sensor_sap_logchain_query: This function aims   to create a temporary table with timestamp from the SAP LOGCHAIN table, which is a process control table.</p> <p>Note</p> <p>this temporary table only lives during runtime, and it is related with the sap process control table but has no relationship or effect on the sensor control table.</p> <ul> <li>chain_id: SAP Chain ID process.</li> <li>dbtable: SAP LOGCHAIN db table name, default: <code>my_database.RSPCLOGCHAIN</code>.</li> <li>status: SAP Chain Status of your process, default: <code>G</code>.</li> <li>engine_table_name: Name of the temporary table created from the upstream data,  default: <code>sensor_new_data</code>. This temporary table will be used as source in the <code>query</code> option.</li> </ul> </li> <li> <p>generate_sensor_query: Generates a Sensor query to consume data from the temporary table created in the <code>prepareQuery</code>.</p> <ul> <li>sensor_id: The unique identifier for the Sensor.</li> <li>filter_exp: Expression to filter incoming new data.     A placeholder <code>?upstream_key</code> and <code>?upstream_value</code> can be used, example: <code>?upstream_key &gt; ?upstream_value</code>     so that it can be replaced by the respective values from the sensor <code>control_db_table_name</code>     for this specific sensor_id.</li> <li>control_db_table_name: Sensor control table name.</li> <li>upstream_key: the key of custom sensor information to control how to identify     new data from the upstream (e.g., a time column in the upstream).</li> <li>upstream_value: the first upstream value to identify new data from the     upstream (e.g., the value of a time present in the upstream).     .. note:: This parameter will have effect just in the first run to detect if the upstream have new data. If it's empty the default value applied is <code>-2147483647</code>.</li> <li>upstream_table_name: Table name to consume the upstream value.     If it's empty the default value applied is <code>sensor_new_data</code>.     .. note:: In case of using the <code>generate_sensor_sap_logchain_query</code> the default value for the temp table is <code>sensor_new_data</code>, so if passing a different value in the <code>engine_table_name</code> this parameter should have the same value.</li> </ul> </li> </ul> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"lakehouse_engine_usage/sensor/sap_bw_b4/sap_bw_b4.html#scenarios","title":"Scenarios","text":"<p>This covers the following scenarios of using the Sensor:</p> <ol> <li>The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</li> <li>The <code>fail_on_empty_result=False</code>.</li> </ol> <p>Data from SAP, in streaming mode, will be consumed, so if there is any new data in the kafka topic it will give condition to proceed to the next task.</p>"},{"location":"lakehouse_engine_usage/sensor/sap_bw_b4/sap_bw_b4.html#fail_on_empty_result-as-true-default-and-suggested","title":"<code>fail_on_empty_result</code> as True (default and SUGGESTED)","text":"<pre><code>from lakehouse_engine.engine import execute_sensor, generate_sensor_query, generate_sensor_sap_logchain_query\nacon = {\n\"sensor_id\": \"MY_SENSOR_ID\",\n\"assets\": [\"MY_SENSOR_ASSETS\"],\n\"control_db_table_name\": \"my_database.lakehouse_engine_sensors\",\n\"input_spec\": {\n\"spec_id\": \"sensor_upstream\",\n\"read_type\": \"batch\",\n\"data_format\": \"jdbc\",\n\"options\": {\n\"compress\": True,\n\"driver\": \"JDBC_DRIVER\",\n\"url\": \"JDBC_URL\",\n\"user\": \"JDBC_USERNAME\",\n\"password\": \"JDBC_PWD\",\n\"prepareQuery\": generate_sensor_sap_logchain_query(chain_id=\"CHAIN_ID\", dbtable=\"JDBC_DB_TABLE\"),\n\"query\": generate_sensor_query(\nsensor_id=\"MY_SENSOR_ID\",\nfilter_exp=\"?upstream_key &gt; '?upstream_value'\",\ncontrol_db_table_name=\"my_database.lakehouse_engine_sensors\",\nupstream_key=\"UPSTREAM_COLUMN_TO_IDENTIFY_NEW_DATA\",\n),\n},\n},\n\"base_checkpoint_location\": \"s3://my_data_product_bucket/checkpoints\",\n\"fail_on_empty_result\": True,\n}\nexecute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/sap_bw_b4/sap_bw_b4.html#fail_on_empty_result-as-false","title":"<code>fail_on_empty_result</code> as False","text":"<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it has acquired new data. This value can be used to execute or not the next steps.</p> <pre><code>from lakehouse_engine.engine import execute_sensor\nacon = {\n[...],\n\"fail_on_empty_result\": False\n}\nacquired_data = execute_sensor(acon=acon)\n</code></pre>"},{"location":"lakehouse_engine_usage/sensor/update_sensor_status/update_sensor_status.html","title":"Update Sensor control delta table after processing the data","text":"<p>This shows how to update the status of your Sensor after processing the new data.</p> <p>Here is an example on how to update the status of your sensor in the Sensors Control Table: <pre><code>from lakehouse_engine.engine import update_sensor_status\nupdate_sensor_status(\nsensor_id=\"MY_SENSOR_ID\",\ncontrol_db_table_name=\"my_database.lakehouse_engine_sensors\",\nstatus=\"PROCESSED_NEW_DATA\",\nassets=[\"MY_SENSOR_ASSETS\"]\n)\n</code></pre></p> <p>If you want to know more please visit the definition of the class here.</p>"},{"location":"reference/index.html","title":"API Documentation","text":"<ul> <li>packages<ul> <li>algorithms<ul> <li>algorithm</li> <li>data_loader</li> <li>dq_validator</li> <li>exceptions</li> <li>gab</li> <li>reconciliator</li> <li>sensor</li> </ul> </li> <li>configs</li> <li>core<ul> <li>dbfs_file_manager</li> <li>definitions</li> <li>exec_env</li> <li>executable</li> <li>file_manager</li> <li>gab_manager</li> <li>gab_sql_generator</li> <li>s3_file_manager</li> <li>sensor_manager</li> <li>table_manager</li> </ul> </li> <li>dq_processors<ul> <li>custom_expectations<ul> <li>expect_column_pair_a_to_be_not_equal_to_b</li> <li>expect_column_pair_a_to_be_smaller_or_equal_than_b</li> <li>expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b</li> <li>expect_column_values_to_be_date_not_older_than</li> <li>expect_column_values_to_not_be_null_or_empty_string</li> <li>expect_multicolumn_column_a_must_equal_b_or_c</li> <li>expect_queried_column_agg_value_to_be</li> </ul> </li> <li>dq_factory</li> <li>exceptions</li> <li>validator</li> </ul> </li> <li>engine</li> <li>io<ul> <li>exceptions</li> <li>reader</li> <li>reader_factory</li> <li>readers<ul> <li>dataframe_reader</li> <li>file_reader</li> <li>jdbc_reader</li> <li>kafka_reader</li> <li>query_reader</li> <li>sap_b4_reader</li> <li>sap_bw_reader</li> <li>sftp_reader</li> <li>table_reader</li> </ul> </li> <li>writer</li> <li>writer_factory</li> <li>writers<ul> <li>console_writer</li> <li>dataframe_writer</li> <li>delta_merge_writer</li> <li>file_writer</li> <li>jdbc_writer</li> <li>kafka_writer</li> <li>rest_api_writer</li> <li>sharepoint_writer</li> <li>table_writer</li> </ul> </li> </ul> </li> <li>terminators<ul> <li>cdf_processor</li> <li>dataset_optimizer</li> <li>notifier</li> <li>notifier_factory</li> <li>notifiers<ul> <li>email_notifier</li> <li>notification_templates</li> </ul> </li> <li>sensor_terminator</li> <li>spark_terminator</li> <li>terminator_factory</li> </ul> </li> <li>transformers<ul> <li>aggregators</li> <li>column_creators</li> <li>column_reshapers</li> <li>condensers</li> <li>custom_transformers</li> <li>data_maskers</li> <li>date_transformers</li> <li>exceptions</li> <li>filters</li> <li>joiners</li> <li>null_handlers</li> <li>optimizers</li> <li>regex_transformers</li> <li>repartitioners</li> <li>transformer_factory</li> <li>unions</li> <li>watermarker</li> </ul> </li> <li>utils<ul> <li>acon_utils</li> <li>configs<ul> <li>config_utils</li> </ul> </li> <li>databricks_utils</li> <li>dq_utils</li> <li>engine_usage_stats</li> <li>expectations_utils</li> <li>extraction<ul> <li>jdbc_extraction_utils</li> <li>sap_b4_extraction_utils</li> <li>sap_bw_extraction_utils</li> <li>sftp_extraction_utils</li> </ul> </li> <li>file_utils</li> <li>gab_utils</li> <li>logging_handler</li> <li>rest_api</li> <li>schema_utils</li> <li>sharepoint_utils</li> <li>sql_parser_utils</li> <li>storage<ul> <li>dbfs_storage</li> <li>file_storage</li> <li>file_storage_functions</li> <li>local_fs_storage</li> <li>s3_storage</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/packages/index.html","title":"Packages","text":"<p>Lakehouse engine package containing all the system subpackages.</p>"},{"location":"reference/packages/engine.html","title":"Engine","text":"<p>Contract of the lakehouse engine with all the available functions to be executed.</p>"},{"location":"reference/packages/engine.html#packages.engine.build_data_docs","title":"<code>build_data_docs(store_backend=DQDefaults.STORE_BACKEND.value, local_fs_root_dir=None, data_docs_local_fs=None, data_docs_prefix=DQDefaults.DATA_DOCS_PREFIX.value, bucket=None, data_docs_bucket=None, expectations_store_prefix=DQDefaults.EXPECTATIONS_STORE_PREFIX.value, validations_store_prefix=DQDefaults.VALIDATIONS_STORE_PREFIX.value, checkpoint_store_prefix=DQDefaults.CHECKPOINT_STORE_PREFIX.value)</code>","text":"<p>Build Data Docs for the project.</p> <p>This function does a full build of data docs based on all the great expectations checkpoints in the specified location, getting all history of run/validations executed and results.</p> <p>Parameters:</p> Name Type Description Default <code>store_backend</code> <code>str</code> <p>which store_backend to use (e.g. s3 or file_system).</p> <code>DQDefaults.STORE_BACKEND.value</code> <code>local_fs_root_dir</code> <code>str</code> <p>path of the root directory. Note: only applicable for store_backend file_system</p> <code>None</code> <code>data_docs_local_fs</code> <code>str</code> <p>path of the root directory. Note: only applicable for store_backend file_system.</p> <code>None</code> <code>data_docs_prefix</code> <code>str</code> <p>prefix where to store data_docs' data.</p> <code>DQDefaults.DATA_DOCS_PREFIX.value</code> <code>bucket</code> <code>str</code> <p>the bucket name to consider for the store_backend (store DQ artefacts). Note: only applicable for store_backend s3.</p> <code>None</code> <code>data_docs_bucket</code> <code>str</code> <p>the bucket name for data docs only. When defined, it will supersede bucket parameter. Note: only applicable for store_backend s3.</p> <code>None</code> <code>expectations_store_prefix</code> <code>str</code> <p>prefix where to store expectations' data. Note: only applicable for store_backend s3.</p> <code>DQDefaults.EXPECTATIONS_STORE_PREFIX.value</code> <code>validations_store_prefix</code> <code>str</code> <p>prefix where to store validations' data. Note: only applicable for store_backend s3.</p> <code>DQDefaults.VALIDATIONS_STORE_PREFIX.value</code> <code>checkpoint_store_prefix</code> <code>str</code> <p>prefix where to store checkpoints' data. Note: only applicable for store_backend s3.</p> <code>DQDefaults.CHECKPOINT_STORE_PREFIX.value</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def build_data_docs(\nstore_backend: str = DQDefaults.STORE_BACKEND.value,\nlocal_fs_root_dir: str = None,\ndata_docs_local_fs: str = None,\ndata_docs_prefix: str = DQDefaults.DATA_DOCS_PREFIX.value,\nbucket: str = None,\ndata_docs_bucket: str = None,\nexpectations_store_prefix: str = DQDefaults.EXPECTATIONS_STORE_PREFIX.value,\nvalidations_store_prefix: str = DQDefaults.VALIDATIONS_STORE_PREFIX.value,\ncheckpoint_store_prefix: str = DQDefaults.CHECKPOINT_STORE_PREFIX.value,\n) -&gt; None:\n\"\"\"Build Data Docs for the project.\n    This function does a full build of data docs based on all the great expectations\n    checkpoints in the specified location, getting all history of run/validations\n    executed and results.\n    Args:\n        store_backend: which store_backend to use (e.g. s3 or file_system).\n        local_fs_root_dir: path of the root directory. Note: only applicable\n            for store_backend file_system\n        data_docs_local_fs: path of the root directory. Note: only applicable\n            for store_backend file_system.\n        data_docs_prefix: prefix where to store data_docs' data.\n        bucket: the bucket name to consider for the store_backend\n            (store DQ artefacts). Note: only applicable for store_backend s3.\n        data_docs_bucket: the bucket name for data docs only. When defined,\n            it will supersede bucket parameter.\n            Note: only applicable for store_backend s3.\n        expectations_store_prefix: prefix where to store expectations' data.\n            Note: only applicable for store_backend s3.\n        validations_store_prefix: prefix where to store validations' data.\n            Note: only applicable for store_backend s3.\n        checkpoint_store_prefix: prefix where to store checkpoints' data.\n            Note: only applicable for store_backend s3.\n    \"\"\"\nfrom lakehouse_engine.dq_processors.dq_factory import DQFactory\nDQFactory.build_data_docs(\nstore_backend=store_backend,\nlocal_fs_root_dir=local_fs_root_dir,\ndata_docs_local_fs=data_docs_local_fs,\ndata_docs_prefix=data_docs_prefix,\nbucket=bucket,\ndata_docs_bucket=data_docs_bucket,\nexpectations_store_prefix=expectations_store_prefix,\nvalidations_store_prefix=validations_store_prefix,\ncheckpoint_store_prefix=checkpoint_store_prefix,\n)\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.execute_dq_validation","title":"<code>execute_dq_validation(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Execute the DQValidator algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def execute_dq_validation(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Execute the DQValidator algorithm.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks or other\n            apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\nfrom lakehouse_engine.algorithms.dq_validator import DQValidator\ntry:\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(\napp_name=\"dq_validator\", config=acon.get(\"exec_env\", None)\n)\nacon = validate_and_resolve_acon(acon, \"at_rest\")\nfinally:\nEngineUsageStats.store_engine_usage(\nacon, execute_dq_validation.__name__, collect_engine_usage, spark_confs\n)\nDQValidator(acon).execute()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.execute_gab","title":"<code>execute_gab(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Execute the gold asset builder based on a GAB Algorithm Configuration.</p> <p>GaB is useful to build your gold assets with predefined functions for recurrent periods.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def execute_gab(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Execute the gold asset builder based on a GAB Algorithm Configuration.\n    GaB is useful to build your gold assets with predefined functions for recurrent\n    periods.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks\n            or other apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(app_name=\"execute_gab\", config=acon.get(\"exec_env\", None))\nEngineUsageStats.store_engine_usage(\nacon, execute_gab.__name__, collect_engine_usage, spark_confs\n)\nGAB(acon).execute()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.execute_reconciliation","title":"<code>execute_reconciliation(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Execute the Reconciliator algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def execute_reconciliation(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Execute the Reconciliator algorithm.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks or other\n            apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\ntry:\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(\napp_name=\"reconciliator\", config=acon.get(\"exec_env\", None)\n)\nacon = validate_and_resolve_acon(acon)\nfinally:\nEngineUsageStats.store_engine_usage(\nacon, execute_reconciliation.__name__, collect_engine_usage, spark_confs\n)\nReconciliator(acon).execute()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.execute_sensor","title":"<code>execute_sensor(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Execute a sensor based on a Sensor Algorithm Configuration.</p> <p>A sensor is useful to check if an upstream system has new data.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def execute_sensor(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; bool:\n\"\"\"Execute a sensor based on a Sensor Algorithm Configuration.\n    A sensor is useful to check if an upstream system has new data.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks\n            or other apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(app_name=\"execute_sensor\", config=acon.get(\"exec_env\", None))\nEngineUsageStats.store_engine_usage(\nacon, execute_sensor.__name__, collect_engine_usage, spark_confs\n)\nreturn Sensor(acon).execute()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.generate_sensor_query","title":"<code>generate_sensor_query(sensor_id, filter_exp=None, control_db_table_name=None, upstream_key=None, upstream_value=None, upstream_table_name=None)</code>","text":"<p>Generates a preprocess query to be used in a sensor configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id.</p> required <code>filter_exp</code> <code>str</code> <p>expression to filter incoming new data. You can use the placeholder ?default_upstream_key and ?default_upstream_value, so that it can be replaced by the respective values in the control_db_table_name for this specific sensor_id.</p> <code>None</code> <code>control_db_table_name</code> <code>str</code> <p><code>db.table</code> to retrieve the last status change timestamp. This is only relevant for the jdbc sensor.</p> <code>None</code> <code>upstream_key</code> <code>str</code> <p>the key of custom sensor information to control how to identify new data from the upstream (e.g., a time column in the upstream).</p> <code>None</code> <code>upstream_value</code> <code>str</code> <p>the upstream value to identify new data from the upstream (e.g., the value of a time present in the upstream).</p> <code>None</code> <code>upstream_table_name</code> <code>str</code> <p>value for custom sensor to query new data from the upstream If none we will set the default value, our <code>sensor_new_data</code> view.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The query string.</p> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def generate_sensor_query(\nsensor_id: str,\nfilter_exp: str = None,\ncontrol_db_table_name: str = None,\nupstream_key: str = None,\nupstream_value: str = None,\nupstream_table_name: str = None,\n) -&gt; str:\n\"\"\"Generates a preprocess query to be used in a sensor configuration.\n    Args:\n        sensor_id: sensor id.\n        filter_exp: expression to filter incoming new data.\n            You can use the placeholder ?default_upstream_key and\n            ?default_upstream_value, so that it can be replaced by the\n            respective values in the control_db_table_name for this specific\n            sensor_id.\n        control_db_table_name: `db.table` to retrieve the last status change\n            timestamp. This is only relevant for the jdbc sensor.\n        upstream_key: the key of custom sensor information to control how to\n            identify new data from the upstream (e.g., a time column in the\n            upstream).\n        upstream_value: the upstream value\n            to identify new data from the upstream (e.g., the value of a time\n            present in the upstream).\n        upstream_table_name: value for custom sensor\n            to query new data from the upstream\n            If none we will set the default value,\n            our `sensor_new_data` view.\n    Returns:\n        The query string.\n    \"\"\"\nExecEnv.get_or_create(app_name=\"generate_sensor_preprocess_query\")\nif filter_exp:\nreturn SensorUpstreamManager.generate_filter_exp_query(\nsensor_id=sensor_id,\nfilter_exp=filter_exp,\ncontrol_db_table_name=control_db_table_name,\nupstream_key=upstream_key,\nupstream_value=upstream_value,\nupstream_table_name=upstream_table_name,\n)\nelse:\nreturn SensorUpstreamManager.generate_sensor_table_preprocess_query(\nsensor_id=sensor_id\n)\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.generate_sensor_sap_logchain_query","title":"<code>generate_sensor_sap_logchain_query(chain_id, dbtable=SAPLogchain.DBTABLE.value, status=SAPLogchain.GREEN_STATUS.value, engine_table_name=SAPLogchain.ENGINE_TABLE.value)</code>","text":"<p>Generates a sensor query based in the SAP Logchain table.</p> <p>Parameters:</p> Name Type Description Default <code>chain_id</code> <code>str</code> <p>chain id to query the status on SAP.</p> required <code>dbtable</code> <code>str</code> <p><code>db.table</code> to retrieve the data to check if the sap chain is already finished.</p> <code>SAPLogchain.DBTABLE.value</code> <code>status</code> <code>str</code> <p><code>db.table</code> to retrieve the last status change timestamp.</p> <code>SAPLogchain.GREEN_STATUS.value</code> <code>engine_table_name</code> <code>str</code> <p>table name exposed with the SAP LOGCHAIN data. This table will be used in the jdbc query.</p> <code>SAPLogchain.ENGINE_TABLE.value</code> <p>Returns:</p> Type Description <code>str</code> <p>The query string.</p> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def generate_sensor_sap_logchain_query(\nchain_id: str,\ndbtable: str = SAPLogchain.DBTABLE.value,\nstatus: str = SAPLogchain.GREEN_STATUS.value,\nengine_table_name: str = SAPLogchain.ENGINE_TABLE.value,\n) -&gt; str:\n\"\"\"Generates a sensor query based in the SAP Logchain table.\n    Args:\n        chain_id: chain id to query the status on SAP.\n        dbtable: `db.table` to retrieve the data to\n            check if the sap chain is already finished.\n        status: `db.table` to retrieve the last status change\n            timestamp.\n        engine_table_name: table name exposed with the SAP LOGCHAIN data.\n            This table will be used in the jdbc query.\n    Returns:\n        The query string.\n    \"\"\"\nExecEnv.get_or_create(app_name=\"generate_sensor_sap_logchain_query\")\nreturn SensorUpstreamManager.generate_sensor_sap_logchain_query(\nchain_id=chain_id,\ndbtable=dbtable,\nstatus=status,\nengine_table_name=engine_table_name,\n)\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.load_data","title":"<code>load_data(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Load data using the DataLoader algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def load_data(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; Optional[OrderedDict]:\n\"\"\"Load data using the DataLoader algorithm.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks or other\n            apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\ntry:\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(app_name=\"data_loader\", config=acon.get(\"exec_env\", None))\nacon = validate_and_resolve_acon(acon, \"in_motion\")\nfinally:\nEngineUsageStats.store_engine_usage(\nacon, load_data.__name__, collect_engine_usage, spark_confs\n)\nreturn DataLoader(acon).execute()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.manage_files","title":"<code>manage_files(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Manipulate s3 files using File Manager algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def manage_files(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Manipulate s3 files using File Manager algorithm.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks\n            or other apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(app_name=\"manage_files\", config=acon.get(\"exec_env\", None))\nEngineUsageStats.store_engine_usage(\nacon, manage_files.__name__, collect_engine_usage, spark_confs\n)\nFileManagerFactory.execute_function(configs=acon)\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.manage_table","title":"<code>manage_table(acon_path=None, acon=None, collect_engine_usage=CollectEngineUsage.PROD_ONLY.value, spark_confs=None)</code>","text":"<p>Manipulate tables/views using Table Manager algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>CollectEngineUsage.PROD_ONLY.value</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def manage_table(\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ncollect_engine_usage: str = CollectEngineUsage.PROD_ONLY.value,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Manipulate tables/views using Table Manager algorithm.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks\n            or other apps).\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when collecting\n            the engine usage.\n    \"\"\"\nacon = ConfigUtils.get_acon(acon_path, acon)\nExecEnv.get_or_create(app_name=\"manage_table\", config=acon.get(\"exec_env\", None))\nEngineUsageStats.store_engine_usage(\nacon, manage_table.__name__, collect_engine_usage, spark_confs\n)\nTableManager(acon).get_function()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.send_notification","title":"<code>send_notification(args)</code>","text":"<p>Send a notification using a notifier.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict</code> <p>arguments for the notifier.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def send_notification(args: dict) -&gt; None:\n\"\"\"Send a notification using a notifier.\n    Args:\n        args: arguments for the notifier.\n    \"\"\"\nnotifier = NotifierFactory.get_notifier(\nspec=TerminatorSpec(function=\"notify\", args=args)\n)\nnotifier.create_notification()\nnotifier.send_notification()\n</code></pre>"},{"location":"reference/packages/engine.html#packages.engine.update_sensor_status","title":"<code>update_sensor_status(sensor_id, control_db_table_name, status=SensorStatus.PROCESSED_NEW_DATA.value, assets=None)</code>","text":"<p>Update internal sensor status.</p> <p>Update the sensor status in the control table, it should be used to tell the system that the sensor has processed all new data that was previously identified, hence updating the shifted sensor status. Usually used to move from <code>SensorStatus.ACQUIRED_NEW_DATA</code> to <code>SensorStatus.PROCESSED_NEW_DATA</code>, but there might be scenarios - still to identify - where we can update the sensor status from/to different statuses.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id.</p> required <code>control_db_table_name</code> <code>str</code> <p><code>db.table</code> to store sensor checkpoints.</p> required <code>status</code> <code>str</code> <p>status of the sensor.</p> <code>SensorStatus.PROCESSED_NEW_DATA.value</code> <code>assets</code> <code>List[str]</code> <p>a list of assets that are considered as available to consume downstream after this sensor has status PROCESSED_NEW_DATA.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/engine.py</code> <pre><code>def update_sensor_status(\nsensor_id: str,\ncontrol_db_table_name: str,\nstatus: str = SensorStatus.PROCESSED_NEW_DATA.value,\nassets: List[str] = None,\n) -&gt; None:\n\"\"\"Update internal sensor status.\n    Update the sensor status in the control table,\n    it should be used to tell the system\n    that the sensor has processed all new data that was previously identified,\n    hence updating the shifted sensor status.\n    Usually used to move from `SensorStatus.ACQUIRED_NEW_DATA` to\n    `SensorStatus.PROCESSED_NEW_DATA`,\n    but there might be scenarios - still to identify -\n    where we can update the sensor status from/to different statuses.\n    Args:\n        sensor_id: sensor id.\n        control_db_table_name: `db.table` to store sensor checkpoints.\n        status: status of the sensor.\n        assets: a list of assets that are considered as available to\n            consume downstream after this sensor has status\n            PROCESSED_NEW_DATA.\n    \"\"\"\nExecEnv.get_or_create(app_name=\"update_sensor_status\")\nSensorTerminator.update_sensor_status(\nsensor_id=sensor_id,\ncontrol_db_table_name=control_db_table_name,\nstatus=status,\nassets=assets,\n)\n</code></pre>"},{"location":"reference/packages/algorithms/index.html","title":"Algorithms","text":"<p>Package containing all the lakehouse engine algorithms.</p>"},{"location":"reference/packages/algorithms/algorithm.html","title":"Algorithm","text":"<p>Module containing the Algorithm class.</p>"},{"location":"reference/packages/algorithms/algorithm.html#packages.algorithms.algorithm.Algorithm","title":"<code>Algorithm</code>","text":"<p>         Bases: <code>Executable</code></p> <p>Class to define the behavior of every algorithm based on ACONs.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/algorithm.py</code> <pre><code>class Algorithm(Executable):\n\"\"\"Class to define the behavior of every algorithm based on ACONs.\"\"\"\ndef __init__(self, acon: dict):\n\"\"\"Construct Algorithm instances.\n        Args:\n            acon: algorithm configuration.\n        \"\"\"\nself.acon = acon\n@classmethod\ndef get_dq_spec(\ncls, spec: dict\n) -&gt; Tuple[DQSpec, List[DQFunctionSpec], List[DQFunctionSpec]]:\n\"\"\"Get data quality specification object from acon.\n        Args:\n            spec: data quality specifications.\n        Returns:\n            The DQSpec and the List of DQ Functions Specs.\n        \"\"\"\ndq_spec = DQSpec(\nspec_id=spec[\"spec_id\"],\ninput_id=spec[\"input_id\"],\ndq_type=spec[\"dq_type\"],\ndq_functions=[],\ndq_db_table=spec.get(\"dq_db_table\"),\ndq_table_table_filter=spec.get(\"dq_table_table_filter\"),\ndq_table_extra_filters=spec.get(\n\"dq_table_extra_filters\", DQSpec.dq_table_extra_filters\n),\nexecution_point=spec.get(\"execution_point\"),\nunexpected_rows_pk=spec.get(\n\"unexpected_rows_pk\", DQSpec.unexpected_rows_pk\n),\ngx_result_format=spec.get(\"gx_result_format\", DQSpec.gx_result_format),\ntbl_to_derive_pk=spec.get(\"tbl_to_derive_pk\", DQSpec.tbl_to_derive_pk),\nsort_processed_keys=spec.get(\n\"sort_processed_keys\", DQSpec.sort_processed_keys\n),\ntag_source_data=spec.get(\"tag_source_data\", DQSpec.tag_source_data),\ndata_asset_name=spec.get(\"data_asset_name\", DQSpec.data_asset_name),\nexpectation_suite_name=spec.get(\n\"expectation_suite_name\", DQSpec.expectation_suite_name\n),\nstore_backend=spec.get(\"store_backend\", DQDefaults.STORE_BACKEND.value),\nlocal_fs_root_dir=spec.get(\"local_fs_root_dir\", DQSpec.local_fs_root_dir),\ndata_docs_local_fs=spec.get(\n\"data_docs_local_fs\", DQSpec.data_docs_local_fs\n),\nbucket=spec.get(\"bucket\", DQSpec.bucket),\ndata_docs_bucket=spec.get(\"data_docs_bucket\", DQSpec.data_docs_bucket),\ncheckpoint_store_prefix=spec.get(\n\"checkpoint_store_prefix\", DQDefaults.CHECKPOINT_STORE_PREFIX.value\n),\nexpectations_store_prefix=spec.get(\n\"expectations_store_prefix\",\nDQDefaults.EXPECTATIONS_STORE_PREFIX.value,\n),\ndata_docs_prefix=spec.get(\n\"data_docs_prefix\", DQDefaults.DATA_DOCS_PREFIX.value\n),\nvalidations_store_prefix=spec.get(\n\"validations_store_prefix\",\nDQDefaults.VALIDATIONS_STORE_PREFIX.value,\n),\nresult_sink_db_table=spec.get(\n\"result_sink_db_table\", DQSpec.result_sink_db_table\n),\nresult_sink_location=spec.get(\n\"result_sink_location\", DQSpec.result_sink_location\n),\nresult_sink_partitions=spec.get(\n\"result_sink_partitions\", DQSpec.result_sink_partitions\n),\nresult_sink_format=spec.get(\n\"result_sink_format\", OutputFormat.DELTAFILES.value\n),\nresult_sink_options=spec.get(\n\"result_sink_options\", DQSpec.result_sink_options\n),\nresult_sink_explode=spec.get(\n\"result_sink_explode\", DQSpec.result_sink_explode\n),\nresult_sink_extra_columns=spec.get(\"result_sink_extra_columns\", []),\nsource=spec.get(\"source\", spec[\"input_id\"]),\nfail_on_error=spec.get(\"fail_on_error\", DQSpec.fail_on_error),\ncache_df=spec.get(\"cache_df\", DQSpec.cache_df),\ncritical_functions=spec.get(\n\"critical_functions\", DQSpec.critical_functions\n),\nmax_percentage_failure=spec.get(\n\"max_percentage_failure\", DQSpec.max_percentage_failure\n),\n)\ndq_functions = cls._get_dq_functions(spec, \"dq_functions\")\ncritical_functions = cls._get_dq_functions(spec, \"critical_functions\")\ncls._validate_dq_tag_strategy(dq_spec)\nreturn dq_spec, dq_functions, critical_functions\n@staticmethod\ndef _get_dq_functions(spec: dict, function_key: str) -&gt; List[DQFunctionSpec]:\n\"\"\"Get DQ Functions from a DQ Spec, based on a function_key.\n        Args:\n            spec: data quality specifications.\n            function_key: dq function key (\"dq_functions\" or\n                \"critical_functions\").\n        Returns:\n            a list of DQ Function Specs.\n        \"\"\"\nfunctions = []\nif spec.get(function_key, []):\nfor f in spec.get(function_key, []):\ndq_fn_spec = DQFunctionSpec(\nfunction=f[\"function\"],\nargs=f.get(\"args\", {}),\n)\nfunctions.append(dq_fn_spec)\nreturn functions\n@staticmethod\ndef _validate_dq_tag_strategy(spec: DQSpec) -&gt; None:\n\"\"\"Validate DQ Spec arguments related with the data tagging strategy.\n        Args:\n            spec: data quality specifications.\n        \"\"\"\nif spec.tag_source_data:\nspec.gx_result_format = DQSpec.gx_result_format\nspec.fail_on_error = False\nspec.result_sink_explode = DQSpec.result_sink_explode\nelif spec.gx_result_format != DQSpec.gx_result_format:\nspec.tag_source_data = False\n</code></pre>"},{"location":"reference/packages/algorithms/algorithm.html#packages.algorithms.algorithm.Algorithm.__init__","title":"<code>__init__(acon)</code>","text":"<p>Construct Algorithm instances.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>algorithm configuration.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/algorithm.py</code> <pre><code>def __init__(self, acon: dict):\n\"\"\"Construct Algorithm instances.\n    Args:\n        acon: algorithm configuration.\n    \"\"\"\nself.acon = acon\n</code></pre>"},{"location":"reference/packages/algorithms/algorithm.html#packages.algorithms.algorithm.Algorithm.get_dq_spec","title":"<code>get_dq_spec(spec)</code>  <code>classmethod</code>","text":"<p>Get data quality specification object from acon.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>dict</code> <p>data quality specifications.</p> required <p>Returns:</p> Type Description <code>Tuple[DQSpec, List[DQFunctionSpec], List[DQFunctionSpec]]</code> <p>The DQSpec and the List of DQ Functions Specs.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/algorithm.py</code> <pre><code>@classmethod\ndef get_dq_spec(\ncls, spec: dict\n) -&gt; Tuple[DQSpec, List[DQFunctionSpec], List[DQFunctionSpec]]:\n\"\"\"Get data quality specification object from acon.\n    Args:\n        spec: data quality specifications.\n    Returns:\n        The DQSpec and the List of DQ Functions Specs.\n    \"\"\"\ndq_spec = DQSpec(\nspec_id=spec[\"spec_id\"],\ninput_id=spec[\"input_id\"],\ndq_type=spec[\"dq_type\"],\ndq_functions=[],\ndq_db_table=spec.get(\"dq_db_table\"),\ndq_table_table_filter=spec.get(\"dq_table_table_filter\"),\ndq_table_extra_filters=spec.get(\n\"dq_table_extra_filters\", DQSpec.dq_table_extra_filters\n),\nexecution_point=spec.get(\"execution_point\"),\nunexpected_rows_pk=spec.get(\n\"unexpected_rows_pk\", DQSpec.unexpected_rows_pk\n),\ngx_result_format=spec.get(\"gx_result_format\", DQSpec.gx_result_format),\ntbl_to_derive_pk=spec.get(\"tbl_to_derive_pk\", DQSpec.tbl_to_derive_pk),\nsort_processed_keys=spec.get(\n\"sort_processed_keys\", DQSpec.sort_processed_keys\n),\ntag_source_data=spec.get(\"tag_source_data\", DQSpec.tag_source_data),\ndata_asset_name=spec.get(\"data_asset_name\", DQSpec.data_asset_name),\nexpectation_suite_name=spec.get(\n\"expectation_suite_name\", DQSpec.expectation_suite_name\n),\nstore_backend=spec.get(\"store_backend\", DQDefaults.STORE_BACKEND.value),\nlocal_fs_root_dir=spec.get(\"local_fs_root_dir\", DQSpec.local_fs_root_dir),\ndata_docs_local_fs=spec.get(\n\"data_docs_local_fs\", DQSpec.data_docs_local_fs\n),\nbucket=spec.get(\"bucket\", DQSpec.bucket),\ndata_docs_bucket=spec.get(\"data_docs_bucket\", DQSpec.data_docs_bucket),\ncheckpoint_store_prefix=spec.get(\n\"checkpoint_store_prefix\", DQDefaults.CHECKPOINT_STORE_PREFIX.value\n),\nexpectations_store_prefix=spec.get(\n\"expectations_store_prefix\",\nDQDefaults.EXPECTATIONS_STORE_PREFIX.value,\n),\ndata_docs_prefix=spec.get(\n\"data_docs_prefix\", DQDefaults.DATA_DOCS_PREFIX.value\n),\nvalidations_store_prefix=spec.get(\n\"validations_store_prefix\",\nDQDefaults.VALIDATIONS_STORE_PREFIX.value,\n),\nresult_sink_db_table=spec.get(\n\"result_sink_db_table\", DQSpec.result_sink_db_table\n),\nresult_sink_location=spec.get(\n\"result_sink_location\", DQSpec.result_sink_location\n),\nresult_sink_partitions=spec.get(\n\"result_sink_partitions\", DQSpec.result_sink_partitions\n),\nresult_sink_format=spec.get(\n\"result_sink_format\", OutputFormat.DELTAFILES.value\n),\nresult_sink_options=spec.get(\n\"result_sink_options\", DQSpec.result_sink_options\n),\nresult_sink_explode=spec.get(\n\"result_sink_explode\", DQSpec.result_sink_explode\n),\nresult_sink_extra_columns=spec.get(\"result_sink_extra_columns\", []),\nsource=spec.get(\"source\", spec[\"input_id\"]),\nfail_on_error=spec.get(\"fail_on_error\", DQSpec.fail_on_error),\ncache_df=spec.get(\"cache_df\", DQSpec.cache_df),\ncritical_functions=spec.get(\n\"critical_functions\", DQSpec.critical_functions\n),\nmax_percentage_failure=spec.get(\n\"max_percentage_failure\", DQSpec.max_percentage_failure\n),\n)\ndq_functions = cls._get_dq_functions(spec, \"dq_functions\")\ncritical_functions = cls._get_dq_functions(spec, \"critical_functions\")\ncls._validate_dq_tag_strategy(dq_spec)\nreturn dq_spec, dq_functions, critical_functions\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html","title":"Data loader","text":"<p>Module to define DataLoader class.</p>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader","title":"<code>DataLoader</code>","text":"<p>         Bases: <code>Algorithm</code></p> <p>Load data using an algorithm configuration (ACON represented as dict).</p> <p>This algorithm focuses on the cases where users will be specifying all the algorithm steps and configurations through a dict based configuration, which we name ACON in our framework.</p> <p>Since an ACON is a dict you can pass a custom transformer through a python function and, therefore, the DataLoader can also be used to load data with custom transformations not provided in our transformers package.</p> <p>As the algorithm base class of the lakehouse-engine framework is based on the concept of ACON, this DataLoader algorithm simply inherits from Algorithm, without overriding anything. We designed the codebase like this to avoid instantiating the Algorithm class directly, which was always meant to be an abstraction for any specific algorithm included in the lakehouse-engine framework.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>class DataLoader(Algorithm):\n\"\"\"Load data using an algorithm configuration (ACON represented as dict).\n    This algorithm focuses on the cases where users will be specifying all the algorithm\n    steps and configurations through a dict based configuration, which we name ACON\n    in our framework.\n    Since an ACON is a dict you can pass a custom transformer through a python function\n    and, therefore, the DataLoader can also be used to load data with custom\n    transformations not provided in our transformers package.\n    As the algorithm base class of the lakehouse-engine framework is based on the\n    concept of ACON, this DataLoader algorithm simply inherits from Algorithm,\n    without overriding anything. We designed the codebase like this to avoid\n    instantiating the Algorithm class directly, which was always meant to be an\n    abstraction for any specific algorithm included in the lakehouse-engine framework.\n    \"\"\"\ndef __init__(self, acon: dict):\n\"\"\"Construct DataLoader algorithm instances.\n        A data loader needs several specifications to work properly,\n        but some of them might be optional. The available specifications are:\n        - input specifications (mandatory): specify how to read data.\n        - transform specifications (optional): specify how to transform data.\n        - data quality specifications (optional): specify how to execute the data\n            quality process.\n        - output specifications (mandatory): specify how to write data to the\n            target.\n        - terminate specifications (optional): specify what to do after writing into\n            the target (e.g., optimizing target table, vacuum, compute stats, etc).\n        Args:\n            acon: algorithm configuration.\n        \"\"\"\nself._logger: Logger = LoggingHandler(self.__class__.__name__).get_logger()\nsuper().__init__(acon)\nself.input_specs: List[InputSpec] = self._get_input_specs()\n# the streaming transformers plan is needed to future change the\n# execution specification to accommodate streaming mode limitations in invoking\n# certain functions (e.g., sort, window, generate row ids/auto increments, ...).\nself._streaming_micro_batch_transformers_plan: dict = {}\nself.transform_specs: List[TransformSpec] = self._get_transform_specs()\n# our data quality process is not compatible with streaming mode, hence we\n# have to run it in micro batches, similar to what happens to certain\n# transformation functions not supported in streaming mode.\nself._streaming_micro_batch_dq_plan: dict = {}\nself.dq_specs: List[DQSpec] = self._get_dq_specs()\nself.output_specs: List[OutputSpec] = self._get_output_specs()\nself.terminate_specs: List[TerminatorSpec] = self._get_terminate_specs()\ndef read(self) -&gt; OrderedDict:\n\"\"\"Read data from an input location into a distributed dataframe.\n        Returns:\n             An ordered dict with all the dataframes that were read.\n        \"\"\"\nread_dfs: OrderedDict = OrderedDict({})\nfor spec in self.input_specs:\nself._logger.info(f\"Found input specification: {spec}\")\nread_dfs[spec.spec_id] = ReaderFactory.get_data(spec)\nreturn read_dfs\ndef transform(self, data: OrderedDict) -&gt; OrderedDict:\n\"\"\"Transform (optionally) the data that was read.\n        If there isn't a transformation specification this step will be skipped, and the\n        original dataframes that were read will be returned.\n        Transformations can have dependency from another transformation result, however\n        we need to keep in mind if we are using streaming source and for some reason we\n        need to enable micro batch processing, this result cannot be used as input to\n        another transformation. Micro batch processing in pyspark streaming is only\n        available in .write(), which means this transformation with micro batch needs\n        to be the end of the process.\n        Args:\n            data: input dataframes in an ordered dict.\n        Returns:\n            Another ordered dict with the transformed dataframes, according to the\n            transformation specification.\n        \"\"\"\nif not self.transform_specs:\nreturn data\nelse:\ntransformed_dfs = OrderedDict(data)\nfor spec in self.transform_specs:\nself._logger.info(f\"Found transform specification: {spec}\")\ntransformed_df = transformed_dfs[spec.input_id]\nfor transformer in spec.transformers:\ntransformed_df = transformed_df.transform(\nTransformerFactory.get_transformer(transformer, transformed_dfs)\n)\ntransformed_dfs[spec.spec_id] = transformed_df\nreturn transformed_dfs\ndef process_dq(\nself, data: OrderedDict\n) -&gt; tuple[OrderedDict, Optional[dict[str, str]]]:\n\"\"\"Process the data quality tasks for the data that was read and/or transformed.\n        It supports multiple input dataframes. Although just one is advisable.\n        It is possible to use data quality validators/expectations that will validate\n        your data and fail the process in case the expectations are not met. The DQ\n        process also generates and keeps updating a site containing the results of the\n        expectations that were done on your data. The location of the site is\n        configurable and can either be on file system or S3. If you define it to be\n        stored on S3, you can even configure your S3 bucket to serve the site so that\n        people can easily check the quality of your data. Moreover, it is also\n        possible to store the result of the DQ process into a defined result sink.\n        Args:\n            data: dataframes from previous steps of the algorithm that we which to\n                run the DQ process on.\n        Returns:\n            Another ordered dict with the validated dataframes and\n            a dictionary with the errors if they exist, or None.\n        \"\"\"\nif not self.dq_specs:\nreturn data, None\ndq_processed_dfs, error = self._verify_dq_rule_id_uniqueness(\ndata, self.dq_specs\n)\nif error:\nreturn dq_processed_dfs, error\nelse:\nfrom lakehouse_engine.dq_processors.dq_factory import DQFactory\ndq_processed_dfs = OrderedDict(data)\nfor spec in self.dq_specs:\ndf_processed_df = dq_processed_dfs[spec.input_id]\nself._logger.info(f\"Found data quality specification: {spec}\")\nif (\nspec.dq_type == DQType.PRISMA.value or spec.dq_functions\n) and spec.spec_id not in self._streaming_micro_batch_dq_plan:\nif spec.cache_df:\ndf_processed_df.cache()\ndq_processed_dfs[spec.spec_id] = DQFactory.run_dq_process(\nspec, df_processed_df\n)\nelse:\ndq_processed_dfs[spec.spec_id] = df_processed_df\nreturn dq_processed_dfs, None\ndef write(self, data: OrderedDict) -&gt; OrderedDict:\n\"\"\"Write the data that was read and transformed (if applicable).\n        It supports writing multiple datasets. However, we only recommend to write one\n        dataframe. This recommendation is based on easy debugging and reproducibility,\n        since if we start mixing several datasets being fueled by the same algorithm, it\n        would unleash an infinite sea of reproducibility issues plus tight coupling and\n        dependencies between datasets. Having said that, there may be cases where\n        writing multiple datasets is desirable according to the use case requirements.\n        Use it accordingly.\n        Args:\n            data: dataframes that were read and transformed (if applicable).\n        Returns:\n            Dataframes that were written.\n        \"\"\"\nwritten_dfs: OrderedDict = OrderedDict({})\nfor spec in self.output_specs:\nself._logger.info(f\"Found output specification: {spec}\")\nwritten_output = WriterFactory.get_writer(\nspec, data[spec.input_id], data\n).write()\nif written_output:\nwritten_dfs.update(written_output)\nelse:\nwritten_dfs[spec.spec_id] = data[spec.input_id]\nreturn written_dfs\ndef terminate(self, data: OrderedDict) -&gt; None:\n\"\"\"Terminate the algorithm.\n        Args:\n            data: dataframes that were written.\n        \"\"\"\nif self.terminate_specs:\nfor spec in self.terminate_specs:\nself._logger.info(f\"Found terminate specification: {spec}\")\nTerminatorFactory.execute_terminator(\nspec, data[spec.input_id] if spec.input_id else None\n)\ndef execute(self) -&gt; Optional[OrderedDict]:\n\"\"\"Define the algorithm execution behaviour.\"\"\"\ntry:\nself._logger.info(\"Starting read stage...\")\nread_dfs = self.read()\nself._logger.info(\"Starting transform stage...\")\ntransformed_dfs = self.transform(read_dfs)\nself._logger.info(\"Starting data quality stage...\")\nvalidated_dfs, errors = self.process_dq(transformed_dfs)\nself._logger.info(\"Starting write stage...\")\nwritten_dfs = self.write(validated_dfs)\nself._logger.info(\"Starting terminate stage...\")\nself.terminate(written_dfs)\nself._logger.info(\"Execution of the algorithm has finished!\")\nexcept Exception as e:\nNotifierFactory.generate_failure_notification(self.terminate_specs, e)\nraise e\nif errors:\nraise DQDuplicateRuleIdException(\n\"Data Written Successfully, but DQ Process Encountered an Issue.\\n\"\n\"We detected a duplicate dq_rule_id in the dq_spec definition. \"\n\"As a result, none of the Data Quality (DQ) processes (dq_spec) \"\n\"were executed.\\n\"\n\"Please review and verify the following dq_rules:\\n\"\nf\"{errors}\"\n)\nreturn written_dfs\ndef _get_input_specs(self) -&gt; List[InputSpec]:\n\"\"\"Get the input specifications from an acon.\n        Returns:\n            List of input specifications.\n        \"\"\"\nreturn [InputSpec(**spec) for spec in self.acon[\"input_specs\"]]\ndef _get_transform_specs(self) -&gt; List[TransformSpec]:\n\"\"\"Get the transformation specifications from an acon.\n        If we are executing the algorithm in streaming mode and if the\n        transformer function is not supported in streaming mode, it is\n        important to note that ONLY those unsupported operations will\n        go into the streaming_micro_batch_transformers (see if in the function code),\n        in the same order that they appear in the list of transformations. This means\n        that other supported transformations that appear after an\n        unsupported one continue to stay one the normal execution plan,\n        i.e., outside the foreachBatch function. Therefore, this may\n        make your algorithm to execute a different logic than the one you\n        originally intended. For this reason:\n            1) ALWAYS PLACE UNSUPPORTED STREAMING TRANSFORMATIONS AT LAST;\n            2) USE force_streaming_foreach_batch_processing option in transform_spec\n            section.\n            3) USE THE CUSTOM_TRANSFORMATION AND WRITE ALL YOUR TRANSFORMATION LOGIC\n            THERE.\n        Check list of unsupported spark streaming operations here:\n        https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\n        Returns:\n            List of transformation specifications.\n        \"\"\"\ninput_read_types = self._get_input_read_types(self.acon[\"input_specs\"])\ntransform_input_ids = self._get_transform_input_ids(\nself.acon.get(\"transform_specs\", [])\n)\nprev_spec_read_types = self._get_previous_spec_read_types(\ninput_read_types, transform_input_ids\n)\ntransform_specs = []\nfor spec in self.acon.get(\"transform_specs\", []):\ntransform_spec = TransformSpec(\nspec_id=spec[\"spec_id\"],\ninput_id=spec[\"input_id\"],\ntransformers=[],\nforce_streaming_foreach_batch_processing=spec.get(\n\"force_streaming_foreach_batch_processing\", False\n),\n)\nfor s in spec[\"transformers\"]:\ntransformer_spec = TransformerSpec(\nfunction=s[\"function\"], args=s.get(\"args\", {})\n)\nif (\nprev_spec_read_types[transform_spec.input_id]\n== ReadType.STREAMING.value\nand s[\"function\"]\nin TransformerFactory.UNSUPPORTED_STREAMING_TRANSFORMERS\n) or (\nprev_spec_read_types[transform_spec.input_id]\n== ReadType.STREAMING.value\nand transform_spec.force_streaming_foreach_batch_processing\n):\nself._move_to_streaming_micro_batch_transformers(\ntransform_spec, transformer_spec\n)\nelse:\ntransform_spec.transformers.append(transformer_spec)\ntransform_specs.append(transform_spec)\nreturn transform_specs\ndef _get_dq_specs(self) -&gt; List[DQSpec]:\n\"\"\"Get list of data quality specification objects from acon.\n        In streaming mode, we automatically convert the data quality specification in\n        the streaming_micro_batch_dq_processors list for the respective output spec.\n        This is needed because our dq process cannot be executed using native streaming\n        functions.\n        Returns:\n            List of data quality spec objects.\n        \"\"\"\ninput_read_types = self._get_input_read_types(self.acon[\"input_specs\"])\ntransform_input_ids = self._get_transform_input_ids(\nself.acon.get(\"transform_specs\", [])\n)\nprev_spec_read_types = self._get_previous_spec_read_types(\ninput_read_types, transform_input_ids\n)\ndq_specs = []\nfor spec in self.acon.get(\"dq_specs\", []):\ndq_spec, dq_functions, critical_functions = Algorithm.get_dq_spec(spec)\nif prev_spec_read_types[dq_spec.input_id] == ReadType.STREAMING.value:\n# we need to use deepcopy to explicitly create a copy of the dict\n# otherwise python only create binding for dicts, and we would be\n# modifying the original dict, which we don't want to.\nself._move_to_streaming_micro_batch_dq_processors(\ndeepcopy(dq_spec), dq_functions, critical_functions\n)\nelse:\ndq_spec.dq_functions = dq_functions\ndq_spec.critical_functions = critical_functions\nself._logger.info(\nf\"Streaming Micro Batch DQ Plan: \"\nf\"{str(self._streaming_micro_batch_dq_plan)}\"\n)\ndq_specs.append(dq_spec)\nreturn dq_specs\ndef _get_output_specs(self) -&gt; List[OutputSpec]:\n\"\"\"Get the output specifications from an acon.\n        Returns:\n            List of output specifications.\n        \"\"\"\nreturn [\nOutputSpec(\nspec_id=spec[\"spec_id\"],\ninput_id=spec[\"input_id\"],\nwrite_type=spec.get(\"write_type\", None),\ndata_format=spec.get(\"data_format\", OutputFormat.DELTAFILES.value),\ndb_table=spec.get(\"db_table\", None),\nlocation=spec.get(\"location\", None),\nmerge_opts=(\nMergeOptions(**spec[\"merge_opts\"])\nif spec.get(\"merge_opts\")\nelse None\n),\nsharepoint_opts=(\nSharepointOptions(**spec[\"sharepoint_opts\"])\nif spec.get(\"sharepoint_opts\")\nelse None\n),\npartitions=spec.get(\"partitions\", []),\nstreaming_micro_batch_transformers=self._get_streaming_transformer_plan(\nspec[\"input_id\"], self.dq_specs\n),\nstreaming_once=spec.get(\"streaming_once\", None),\nstreaming_processing_time=spec.get(\"streaming_processing_time\", None),\nstreaming_available_now=spec.get(\n\"streaming_available_now\",\n(\nFalse\nif (\nspec.get(\"streaming_once\", None)\nor spec.get(\"streaming_processing_time\", None)\nor spec.get(\"streaming_continuous\", None)\n)\nelse True\n),\n),\nstreaming_continuous=spec.get(\"streaming_continuous\", None),\nstreaming_await_termination=spec.get(\n\"streaming_await_termination\", True\n),\nstreaming_await_termination_timeout=spec.get(\n\"streaming_await_termination_timeout\", None\n),\nwith_batch_id=spec.get(\"with_batch_id\", False),\noptions=spec.get(\"options\", None),\nstreaming_micro_batch_dq_processors=(\nself._streaming_micro_batch_dq_plan.get(spec[\"input_id\"], [])\n),\n)\nfor spec in self.acon[\"output_specs\"]\n]\ndef _get_streaming_transformer_plan(\nself, input_id: str, dq_specs: Optional[List[DQSpec]]\n) -&gt; List[TransformerSpec]:\n\"\"\"Gets the plan for transformations to be applied on streaming micro batches.\n        When running both DQ processes and transformations in streaming micro batches,\n        the _streaming_micro_batch_transformers_plan to consider is the one associated\n        with the transformer spec_id and not with the dq spec_id. Thus, on those cases,\n        this method maps the input id of the output_spec (which is the spec_id of a\n        dq_spec) with the dependent transformer spec_id.\n        Args:\n            input_id: id of the corresponding input specification.\n            dq_specs: data quality specifications.\n        Returns:\n            a list of TransformerSpec, representing the transformations plan.\n        \"\"\"\ntransformer_id = (\n[dq_spec.input_id for dq_spec in dq_specs if dq_spec.spec_id == input_id][0]\nif self._streaming_micro_batch_dq_plan.get(input_id)\nand self._streaming_micro_batch_transformers_plan\nelse input_id\n)\nstreaming_micro_batch_transformers_plan: list[TransformerSpec] = (\nself._streaming_micro_batch_transformers_plan.get(transformer_id, [])\n)\nreturn streaming_micro_batch_transformers_plan\ndef _get_terminate_specs(self) -&gt; List[TerminatorSpec]:\n\"\"\"Get the terminate specifications from an acon.\n        Returns:\n            List of terminate specifications.\n        \"\"\"\nreturn [TerminatorSpec(**spec) for spec in self.acon.get(\"terminate_specs\", [])]\ndef _move_to_streaming_micro_batch_transformers(\nself, transform_spec: TransformSpec, transformer_spec: TransformerSpec\n) -&gt; None:\n\"\"\"Move the transformer to the list of streaming micro batch transformations.\n        If the transform specs contain functions that cannot be executed in streaming\n        mode, this function sends those functions to the output specs\n        streaming_micro_batch_transformers, where they will be executed inside the\n        stream foreachBatch function.\n        To accomplish that we use an instance variable that associates the\n        streaming_micro_batch_transformers to each output spec, in order to do reverse\n        lookup when creating the OutputSpec.\n        Args:\n            transform_spec: transform specification (overall\n                transformation specification - a transformation may contain multiple\n                transformers).\n            transformer_spec: the specific transformer function and arguments.\n        \"\"\"\nif transform_spec.spec_id not in self._streaming_micro_batch_transformers_plan:\nself._streaming_micro_batch_transformers_plan[transform_spec.spec_id] = []\nself._streaming_micro_batch_transformers_plan[transform_spec.spec_id].append(\ntransformer_spec\n)\ndef _move_to_streaming_micro_batch_dq_processors(\nself,\ndq_spec: DQSpec,\ndq_functions: List[DQFunctionSpec],\ncritical_functions: List[DQFunctionSpec],\n) -&gt; None:\n\"\"\"Move the dq function to the list of streaming micro batch transformations.\n        If the dq specs contain functions that cannot be executed in streaming mode,\n        this function sends those functions to the output specs\n        streaming_micro_batch_dq_processors, where they will be executed inside the\n        stream foreachBatch function.\n        To accomplish that we use an instance variable that associates the\n        streaming_micro_batch_dq_processors to each output spec, in order to do reverse\n        lookup when creating the OutputSpec.\n        Args:\n            dq_spec: dq specification (overall dq process specification).\n            dq_functions: the list of dq functions to be considered.\n            critical_functions: list of critical functions to be considered.\n        \"\"\"\nif dq_spec.spec_id not in self._streaming_micro_batch_dq_plan:\nself._streaming_micro_batch_dq_plan[dq_spec.spec_id] = []\ndq_spec.dq_functions = dq_functions\ndq_spec.critical_functions = critical_functions\nself._streaming_micro_batch_dq_plan[dq_spec.spec_id].append(dq_spec)\n@staticmethod\ndef _get_input_read_types(list_of_specs: List) -&gt; dict:\n\"\"\"Get a dict of spec ids and read types from a list of input specs.\n        Args:\n            list_of_specs: list of input specs ([{k:v}]).\n        Returns:\n            Dict of {input_spec_id: read_type}.\n        \"\"\"\nreturn {item[\"spec_id\"]: item[\"read_type\"] for item in list_of_specs}\n@staticmethod\ndef _get_transform_input_ids(list_of_specs: List) -&gt; dict:\n\"\"\"Get a dict of transform spec ids and input ids from list of transform specs.\n        Args:\n            list_of_specs: list of transform specs ([{k:v}]).\n        Returns:\n            Dict of {transform_spec_id: input_id}.\n        \"\"\"\nreturn {item[\"spec_id\"]: item[\"input_id\"] for item in list_of_specs}\n@staticmethod\ndef _get_previous_spec_read_types(\ninput_read_types: dict, transform_input_ids: dict\n) -&gt; dict:\n\"\"\"Get the read types of the previous specification: input and/or transform.\n        For the chaining transformations and for DQ process to work seamlessly in batch\n        and streaming mode, we have to figure out if the previous spec to the transform\n        or dq spec(e.g., input spec or transform spec) refers to a batch read type or\n        a streaming read type.\n        Args:\n            input_read_types: dict of {input_spec_id: read_type}.\n            transform_input_ids: dict of {transform_spec_id: input_id}.\n        Returns:\n            Dict of {input_spec_id or transform_spec_id: read_type}\n        \"\"\"\ncombined_read_types = input_read_types\nfor spec_id, input_id in transform_input_ids.items():\ncombined_read_types[spec_id] = combined_read_types[input_id]\nreturn combined_read_types\n@staticmethod\ndef _verify_dq_rule_id_uniqueness(\ndata: OrderedDict, dq_specs: list[DQSpec]\n) -&gt; tuple[OrderedDict, dict[str, str]]:\n\"\"\"Verify the uniqueness of dq_rule_id.\n        Verify the existence of duplicate dq_rule_id values\n        and prepare the DataFrame for the next stage.\n        Args:\n            data: dataframes.\n            dq_specs: a list of DQSpec to be validated.\n        Returns:\n             processed df and error if existed.\n        \"\"\"\nerror_dict = PrismaUtils.validate_rule_id_duplication(dq_specs)\ndq_processed_dfs = OrderedDict(data)\nfor spec in dq_specs:\ndf_processed_df = dq_processed_dfs[spec.input_id]\ndq_processed_dfs[spec.spec_id] = df_processed_df\nreturn dq_processed_dfs, error_dict\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.__init__","title":"<code>__init__(acon)</code>","text":"<p>Construct DataLoader algorithm instances.</p> <p>A data loader needs several specifications to work properly, but some of them might be optional. The available specifications are:</p> <ul> <li>input specifications (mandatory): specify how to read data.</li> <li>transform specifications (optional): specify how to transform data.</li> <li>data quality specifications (optional): specify how to execute the data     quality process.</li> <li>output specifications (mandatory): specify how to write data to the     target.</li> <li>terminate specifications (optional): specify what to do after writing into     the target (e.g., optimizing target table, vacuum, compute stats, etc).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>algorithm configuration.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def __init__(self, acon: dict):\n\"\"\"Construct DataLoader algorithm instances.\n    A data loader needs several specifications to work properly,\n    but some of them might be optional. The available specifications are:\n    - input specifications (mandatory): specify how to read data.\n    - transform specifications (optional): specify how to transform data.\n    - data quality specifications (optional): specify how to execute the data\n        quality process.\n    - output specifications (mandatory): specify how to write data to the\n        target.\n    - terminate specifications (optional): specify what to do after writing into\n        the target (e.g., optimizing target table, vacuum, compute stats, etc).\n    Args:\n        acon: algorithm configuration.\n    \"\"\"\nself._logger: Logger = LoggingHandler(self.__class__.__name__).get_logger()\nsuper().__init__(acon)\nself.input_specs: List[InputSpec] = self._get_input_specs()\n# the streaming transformers plan is needed to future change the\n# execution specification to accommodate streaming mode limitations in invoking\n# certain functions (e.g., sort, window, generate row ids/auto increments, ...).\nself._streaming_micro_batch_transformers_plan: dict = {}\nself.transform_specs: List[TransformSpec] = self._get_transform_specs()\n# our data quality process is not compatible with streaming mode, hence we\n# have to run it in micro batches, similar to what happens to certain\n# transformation functions not supported in streaming mode.\nself._streaming_micro_batch_dq_plan: dict = {}\nself.dq_specs: List[DQSpec] = self._get_dq_specs()\nself.output_specs: List[OutputSpec] = self._get_output_specs()\nself.terminate_specs: List[TerminatorSpec] = self._get_terminate_specs()\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.execute","title":"<code>execute()</code>","text":"<p>Define the algorithm execution behaviour.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def execute(self) -&gt; Optional[OrderedDict]:\n\"\"\"Define the algorithm execution behaviour.\"\"\"\ntry:\nself._logger.info(\"Starting read stage...\")\nread_dfs = self.read()\nself._logger.info(\"Starting transform stage...\")\ntransformed_dfs = self.transform(read_dfs)\nself._logger.info(\"Starting data quality stage...\")\nvalidated_dfs, errors = self.process_dq(transformed_dfs)\nself._logger.info(\"Starting write stage...\")\nwritten_dfs = self.write(validated_dfs)\nself._logger.info(\"Starting terminate stage...\")\nself.terminate(written_dfs)\nself._logger.info(\"Execution of the algorithm has finished!\")\nexcept Exception as e:\nNotifierFactory.generate_failure_notification(self.terminate_specs, e)\nraise e\nif errors:\nraise DQDuplicateRuleIdException(\n\"Data Written Successfully, but DQ Process Encountered an Issue.\\n\"\n\"We detected a duplicate dq_rule_id in the dq_spec definition. \"\n\"As a result, none of the Data Quality (DQ) processes (dq_spec) \"\n\"were executed.\\n\"\n\"Please review and verify the following dq_rules:\\n\"\nf\"{errors}\"\n)\nreturn written_dfs\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.process_dq","title":"<code>process_dq(data)</code>","text":"<p>Process the data quality tasks for the data that was read and/or transformed.</p> <p>It supports multiple input dataframes. Although just one is advisable.</p> <p>It is possible to use data quality validators/expectations that will validate your data and fail the process in case the expectations are not met. The DQ process also generates and keeps updating a site containing the results of the expectations that were done on your data. The location of the site is configurable and can either be on file system or S3. If you define it to be stored on S3, you can even configure your S3 bucket to serve the site so that people can easily check the quality of your data. Moreover, it is also possible to store the result of the DQ process into a defined result sink.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>OrderedDict</code> <p>dataframes from previous steps of the algorithm that we which to run the DQ process on.</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Another ordered dict with the validated dataframes and</p> <code>Optional[dict[str, str]]</code> <p>a dictionary with the errors if they exist, or None.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def process_dq(\nself, data: OrderedDict\n) -&gt; tuple[OrderedDict, Optional[dict[str, str]]]:\n\"\"\"Process the data quality tasks for the data that was read and/or transformed.\n    It supports multiple input dataframes. Although just one is advisable.\n    It is possible to use data quality validators/expectations that will validate\n    your data and fail the process in case the expectations are not met. The DQ\n    process also generates and keeps updating a site containing the results of the\n    expectations that were done on your data. The location of the site is\n    configurable and can either be on file system or S3. If you define it to be\n    stored on S3, you can even configure your S3 bucket to serve the site so that\n    people can easily check the quality of your data. Moreover, it is also\n    possible to store the result of the DQ process into a defined result sink.\n    Args:\n        data: dataframes from previous steps of the algorithm that we which to\n            run the DQ process on.\n    Returns:\n        Another ordered dict with the validated dataframes and\n        a dictionary with the errors if they exist, or None.\n    \"\"\"\nif not self.dq_specs:\nreturn data, None\ndq_processed_dfs, error = self._verify_dq_rule_id_uniqueness(\ndata, self.dq_specs\n)\nif error:\nreturn dq_processed_dfs, error\nelse:\nfrom lakehouse_engine.dq_processors.dq_factory import DQFactory\ndq_processed_dfs = OrderedDict(data)\nfor spec in self.dq_specs:\ndf_processed_df = dq_processed_dfs[spec.input_id]\nself._logger.info(f\"Found data quality specification: {spec}\")\nif (\nspec.dq_type == DQType.PRISMA.value or spec.dq_functions\n) and spec.spec_id not in self._streaming_micro_batch_dq_plan:\nif spec.cache_df:\ndf_processed_df.cache()\ndq_processed_dfs[spec.spec_id] = DQFactory.run_dq_process(\nspec, df_processed_df\n)\nelse:\ndq_processed_dfs[spec.spec_id] = df_processed_df\nreturn dq_processed_dfs, None\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.read","title":"<code>read()</code>","text":"<p>Read data from an input location into a distributed dataframe.</p> <p>Returns:</p> Type Description <code>OrderedDict</code> <p>An ordered dict with all the dataframes that were read.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def read(self) -&gt; OrderedDict:\n\"\"\"Read data from an input location into a distributed dataframe.\n    Returns:\n         An ordered dict with all the dataframes that were read.\n    \"\"\"\nread_dfs: OrderedDict = OrderedDict({})\nfor spec in self.input_specs:\nself._logger.info(f\"Found input specification: {spec}\")\nread_dfs[spec.spec_id] = ReaderFactory.get_data(spec)\nreturn read_dfs\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.terminate","title":"<code>terminate(data)</code>","text":"<p>Terminate the algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>OrderedDict</code> <p>dataframes that were written.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def terminate(self, data: OrderedDict) -&gt; None:\n\"\"\"Terminate the algorithm.\n    Args:\n        data: dataframes that were written.\n    \"\"\"\nif self.terminate_specs:\nfor spec in self.terminate_specs:\nself._logger.info(f\"Found terminate specification: {spec}\")\nTerminatorFactory.execute_terminator(\nspec, data[spec.input_id] if spec.input_id else None\n)\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.transform","title":"<code>transform(data)</code>","text":"<p>Transform (optionally) the data that was read.</p> <p>If there isn't a transformation specification this step will be skipped, and the original dataframes that were read will be returned. Transformations can have dependency from another transformation result, however we need to keep in mind if we are using streaming source and for some reason we need to enable micro batch processing, this result cannot be used as input to another transformation. Micro batch processing in pyspark streaming is only available in .write(), which means this transformation with micro batch needs to be the end of the process.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>OrderedDict</code> <p>input dataframes in an ordered dict.</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Another ordered dict with the transformed dataframes, according to the</p> <code>OrderedDict</code> <p>transformation specification.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def transform(self, data: OrderedDict) -&gt; OrderedDict:\n\"\"\"Transform (optionally) the data that was read.\n    If there isn't a transformation specification this step will be skipped, and the\n    original dataframes that were read will be returned.\n    Transformations can have dependency from another transformation result, however\n    we need to keep in mind if we are using streaming source and for some reason we\n    need to enable micro batch processing, this result cannot be used as input to\n    another transformation. Micro batch processing in pyspark streaming is only\n    available in .write(), which means this transformation with micro batch needs\n    to be the end of the process.\n    Args:\n        data: input dataframes in an ordered dict.\n    Returns:\n        Another ordered dict with the transformed dataframes, according to the\n        transformation specification.\n    \"\"\"\nif not self.transform_specs:\nreturn data\nelse:\ntransformed_dfs = OrderedDict(data)\nfor spec in self.transform_specs:\nself._logger.info(f\"Found transform specification: {spec}\")\ntransformed_df = transformed_dfs[spec.input_id]\nfor transformer in spec.transformers:\ntransformed_df = transformed_df.transform(\nTransformerFactory.get_transformer(transformer, transformed_dfs)\n)\ntransformed_dfs[spec.spec_id] = transformed_df\nreturn transformed_dfs\n</code></pre>"},{"location":"reference/packages/algorithms/data_loader.html#packages.algorithms.data_loader.DataLoader.write","title":"<code>write(data)</code>","text":"<p>Write the data that was read and transformed (if applicable).</p> <p>It supports writing multiple datasets. However, we only recommend to write one dataframe. This recommendation is based on easy debugging and reproducibility, since if we start mixing several datasets being fueled by the same algorithm, it would unleash an infinite sea of reproducibility issues plus tight coupling and dependencies between datasets. Having said that, there may be cases where writing multiple datasets is desirable according to the use case requirements. Use it accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>OrderedDict</code> <p>dataframes that were read and transformed (if applicable).</p> required <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Dataframes that were written.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/data_loader.py</code> <pre><code>def write(self, data: OrderedDict) -&gt; OrderedDict:\n\"\"\"Write the data that was read and transformed (if applicable).\n    It supports writing multiple datasets. However, we only recommend to write one\n    dataframe. This recommendation is based on easy debugging and reproducibility,\n    since if we start mixing several datasets being fueled by the same algorithm, it\n    would unleash an infinite sea of reproducibility issues plus tight coupling and\n    dependencies between datasets. Having said that, there may be cases where\n    writing multiple datasets is desirable according to the use case requirements.\n    Use it accordingly.\n    Args:\n        data: dataframes that were read and transformed (if applicable).\n    Returns:\n        Dataframes that were written.\n    \"\"\"\nwritten_dfs: OrderedDict = OrderedDict({})\nfor spec in self.output_specs:\nself._logger.info(f\"Found output specification: {spec}\")\nwritten_output = WriterFactory.get_writer(\nspec, data[spec.input_id], data\n).write()\nif written_output:\nwritten_dfs.update(written_output)\nelse:\nwritten_dfs[spec.spec_id] = data[spec.input_id]\nreturn written_dfs\n</code></pre>"},{"location":"reference/packages/algorithms/dq_validator.html","title":"Dq validator","text":"<p>Module to define Data Validator class.</p>"},{"location":"reference/packages/algorithms/dq_validator.html#packages.algorithms.dq_validator.DQValidator","title":"<code>DQValidator</code>","text":"<p>         Bases: <code>Algorithm</code></p> <p>Validate data using an algorithm configuration (ACON represented as dict).</p> <p>This algorithm focuses on isolate Data Quality Validations from loading, applying a set of data quality functions to a specific input dataset, without the need to define any output specification. You can use any input specification compatible with the lakehouse engine (dataframe, table, files, etc).</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/dq_validator.py</code> <pre><code>class DQValidator(Algorithm):\n\"\"\"Validate data using an algorithm configuration (ACON represented as dict).\n    This algorithm focuses on isolate Data Quality Validations from loading,\n    applying a set of data quality functions to a specific input dataset,\n    without the need to define any output specification.\n    You can use any input specification compatible with the lakehouse engine\n    (dataframe, table, files, etc).\n    \"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef __init__(self, acon: dict):\n\"\"\"Construct DQValidator algorithm instances.\n        A data quality validator needs the following specifications to work properly:\n        - input specification (mandatory): specify how and what data to\n        read.\n        - data quality specification (mandatory): specify how to execute\n        the data quality process.\n        - restore_prev_version (optional): specify if, having\n        delta table/files as input, they should be restored to the\n        previous version if the data quality process fails. Note: this\n        is only considered if fail_on_error is kept as True.\n        Args:\n            acon: algorithm configuration.\n        \"\"\"\nself.spec: DQValidatorSpec = DQValidatorSpec(\ninput_spec=InputSpec(**acon[\"input_spec\"]),\ndq_spec=self._get_dq_spec(acon[\"dq_spec\"]),\nrestore_prev_version=acon.get(\"restore_prev_version\", None),\n)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from an input location into a distributed dataframe.\n        Returns:\n             Dataframe with data that was read.\n        \"\"\"\ncurrent_df = ReaderFactory.get_data(self.spec.input_spec)\nreturn current_df\ndef process_dq(self, data: DataFrame) -&gt; DataFrame:\n\"\"\"Process the data quality tasks for the data that was read.\n        It supports a single input dataframe.\n        It is possible to use data quality validators/expectations that will validate\n        your data and fail the process in case the expectations are not met. The DQ\n        process also generates and keeps updating a site containing the results of the\n        expectations that were done on your data. The location of the site is\n        configurable and can either be on file system or S3. If you define it to be\n        stored on S3, you can even configure your S3 bucket to serve the site so that\n        people can easily check the quality of your data. Moreover, it is also\n        possible to store the result of the DQ process into a defined result sink.\n        Args:\n            data: input dataframe on which to run the DQ process.\n        Returns:\n            Validated dataframe.\n        \"\"\"\nreturn DQFactory.run_dq_process(self.spec.dq_spec, data)\ndef execute(self) -&gt; None:\n\"\"\"Define the algorithm execution behaviour.\"\"\"\nself._LOGGER.info(\"Starting read stage...\")\nread_df = self.read()\nself._LOGGER.info(\"Starting data quality validator...\")\nself._LOGGER.info(\"Validating DQ definitions\")\nerror_dict = PrismaUtils.validate_rule_id_duplication(specs=[self.spec.dq_spec])\nif error_dict:\nraise DQDuplicateRuleIdException(\n\"Duplicate dq_rule_id detected in dq_spec definition.\\n\"\n\"We have identified one or more duplicate dq_rule_id \"\n\"entries in the dq_spec definition. \"\n\"Please review and verify the following dq_rules:\\n\"\nf\"{error_dict}\"\n)\ntry:\nif read_df.isStreaming:\n# To handle streaming, and although we are not interested in\n# writing any data, we still need to start the streaming and\n# execute the data quality process in micro batches of data.\ndef write_dq_validator_micro_batch(\nbatch_df: DataFrame, batch_id: int\n) -&gt; None:\nself.process_dq(batch_df)\nread_df.writeStream.trigger(once=True).foreachBatch(\nwrite_dq_validator_micro_batch\n).start().awaitTermination()\nelse:\nself.process_dq(read_df)\nexcept (DQValidationsFailedException, StreamingQueryException):\nif not self.spec.input_spec.df_name and self.spec.restore_prev_version:\nself._LOGGER.info(\"Restoring delta table/files to previous version...\")\nself._restore_prev_version()\nraise DQValidationsFailedException(\n\"Data Quality Validations Failed! The delta \"\n\"table/files were restored to the previous version!\"\n)\nelif self.spec.dq_spec.fail_on_error:\nraise DQValidationsFailedException(\"Data Quality Validations Failed!\")\nelse:\nself._LOGGER.info(\"Execution of the algorithm has finished!\")\n@staticmethod\ndef _get_dq_spec(input_dq_spec: dict) -&gt; DQSpec:\n\"\"\"Get data quality specification from acon.\n        Args:\n            input_dq_spec: data quality specification.\n        Returns:\n            Data quality spec.\n        \"\"\"\ndq_spec, dq_functions, critical_functions = Algorithm.get_dq_spec(input_dq_spec)\ndq_spec.dq_functions = dq_functions\ndq_spec.critical_functions = critical_functions\nreturn dq_spec\ndef _restore_prev_version(self) -&gt; None:\n\"\"\"Restore delta table or delta files to previous version.\"\"\"\nif self.spec.input_spec.db_table:\ndelta_table = DeltaTable.forName(\nExecEnv.SESSION, self.spec.input_spec.db_table\n)\nelse:\ndelta_table = DeltaTable.forPath(\nExecEnv.SESSION, self.spec.input_spec.location\n)\nprevious_version = (\ndelta_table.history().agg({\"version\": \"max\"}).collect()[0][0] - 1\n)\ndelta_table.restoreToVersion(previous_version)\n</code></pre>"},{"location":"reference/packages/algorithms/dq_validator.html#packages.algorithms.dq_validator.DQValidator.__init__","title":"<code>__init__(acon)</code>","text":"<p>Construct DQValidator algorithm instances.</p> <p>A data quality validator needs the following specifications to work properly:</p> <ul> <li>input specification (mandatory): specify how and what data to read.</li> <li>data quality specification (mandatory): specify how to execute the data quality process.</li> <li>restore_prev_version (optional): specify if, having delta table/files as input, they should be restored to the previous version if the data quality process fails. Note: this is only considered if fail_on_error is kept as True.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>algorithm configuration.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/dq_validator.py</code> <pre><code>def __init__(self, acon: dict):\n\"\"\"Construct DQValidator algorithm instances.\n    A data quality validator needs the following specifications to work properly:\n    - input specification (mandatory): specify how and what data to\n    read.\n    - data quality specification (mandatory): specify how to execute\n    the data quality process.\n    - restore_prev_version (optional): specify if, having\n    delta table/files as input, they should be restored to the\n    previous version if the data quality process fails. Note: this\n    is only considered if fail_on_error is kept as True.\n    Args:\n        acon: algorithm configuration.\n    \"\"\"\nself.spec: DQValidatorSpec = DQValidatorSpec(\ninput_spec=InputSpec(**acon[\"input_spec\"]),\ndq_spec=self._get_dq_spec(acon[\"dq_spec\"]),\nrestore_prev_version=acon.get(\"restore_prev_version\", None),\n)\n</code></pre>"},{"location":"reference/packages/algorithms/dq_validator.html#packages.algorithms.dq_validator.DQValidator.execute","title":"<code>execute()</code>","text":"<p>Define the algorithm execution behaviour.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/dq_validator.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Define the algorithm execution behaviour.\"\"\"\nself._LOGGER.info(\"Starting read stage...\")\nread_df = self.read()\nself._LOGGER.info(\"Starting data quality validator...\")\nself._LOGGER.info(\"Validating DQ definitions\")\nerror_dict = PrismaUtils.validate_rule_id_duplication(specs=[self.spec.dq_spec])\nif error_dict:\nraise DQDuplicateRuleIdException(\n\"Duplicate dq_rule_id detected in dq_spec definition.\\n\"\n\"We have identified one or more duplicate dq_rule_id \"\n\"entries in the dq_spec definition. \"\n\"Please review and verify the following dq_rules:\\n\"\nf\"{error_dict}\"\n)\ntry:\nif read_df.isStreaming:\n# To handle streaming, and although we are not interested in\n# writing any data, we still need to start the streaming and\n# execute the data quality process in micro batches of data.\ndef write_dq_validator_micro_batch(\nbatch_df: DataFrame, batch_id: int\n) -&gt; None:\nself.process_dq(batch_df)\nread_df.writeStream.trigger(once=True).foreachBatch(\nwrite_dq_validator_micro_batch\n).start().awaitTermination()\nelse:\nself.process_dq(read_df)\nexcept (DQValidationsFailedException, StreamingQueryException):\nif not self.spec.input_spec.df_name and self.spec.restore_prev_version:\nself._LOGGER.info(\"Restoring delta table/files to previous version...\")\nself._restore_prev_version()\nraise DQValidationsFailedException(\n\"Data Quality Validations Failed! The delta \"\n\"table/files were restored to the previous version!\"\n)\nelif self.spec.dq_spec.fail_on_error:\nraise DQValidationsFailedException(\"Data Quality Validations Failed!\")\nelse:\nself._LOGGER.info(\"Execution of the algorithm has finished!\")\n</code></pre>"},{"location":"reference/packages/algorithms/dq_validator.html#packages.algorithms.dq_validator.DQValidator.process_dq","title":"<code>process_dq(data)</code>","text":"<p>Process the data quality tasks for the data that was read.</p> <p>It supports a single input dataframe.</p> <p>It is possible to use data quality validators/expectations that will validate your data and fail the process in case the expectations are not met. The DQ process also generates and keeps updating a site containing the results of the expectations that were done on your data. The location of the site is configurable and can either be on file system or S3. If you define it to be stored on S3, you can even configure your S3 bucket to serve the site so that people can easily check the quality of your data. Moreover, it is also possible to store the result of the DQ process into a defined result sink.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>input dataframe on which to run the DQ process.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Validated dataframe.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/dq_validator.py</code> <pre><code>def process_dq(self, data: DataFrame) -&gt; DataFrame:\n\"\"\"Process the data quality tasks for the data that was read.\n    It supports a single input dataframe.\n    It is possible to use data quality validators/expectations that will validate\n    your data and fail the process in case the expectations are not met. The DQ\n    process also generates and keeps updating a site containing the results of the\n    expectations that were done on your data. The location of the site is\n    configurable and can either be on file system or S3. If you define it to be\n    stored on S3, you can even configure your S3 bucket to serve the site so that\n    people can easily check the quality of your data. Moreover, it is also\n    possible to store the result of the DQ process into a defined result sink.\n    Args:\n        data: input dataframe on which to run the DQ process.\n    Returns:\n        Validated dataframe.\n    \"\"\"\nreturn DQFactory.run_dq_process(self.spec.dq_spec, data)\n</code></pre>"},{"location":"reference/packages/algorithms/dq_validator.html#packages.algorithms.dq_validator.DQValidator.read","title":"<code>read()</code>","text":"<p>Read data from an input location into a distributed dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with data that was read.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/dq_validator.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from an input location into a distributed dataframe.\n    Returns:\n         Dataframe with data that was read.\n    \"\"\"\ncurrent_df = ReaderFactory.get_data(self.spec.input_spec)\nreturn current_df\n</code></pre>"},{"location":"reference/packages/algorithms/exceptions.html","title":"Exceptions","text":"<p>Package defining all the algorithm custom exceptions.</p>"},{"location":"reference/packages/algorithms/exceptions.html#packages.algorithms.exceptions.NoNewDataException","title":"<code>NoNewDataException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when no new data is available.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/exceptions.py</code> <pre><code>class NoNewDataException(Exception):\n\"\"\"Exception for when no new data is available.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/algorithms/exceptions.html#packages.algorithms.exceptions.ReconciliationFailedException","title":"<code>ReconciliationFailedException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the reconciliation process fails.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/exceptions.py</code> <pre><code>class ReconciliationFailedException(Exception):\n\"\"\"Exception for when the reconciliation process fails.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/algorithms/exceptions.html#packages.algorithms.exceptions.RestoreTypeNotFoundException","title":"<code>RestoreTypeNotFoundException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the restore type is not found.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/exceptions.py</code> <pre><code>class RestoreTypeNotFoundException(Exception):\n\"\"\"Exception for when the restore type is not found.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/algorithms/exceptions.html#packages.algorithms.exceptions.SensorAlreadyExistsException","title":"<code>SensorAlreadyExistsException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a sensor with same sensor id already exists.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/exceptions.py</code> <pre><code>class SensorAlreadyExistsException(Exception):\n\"\"\"Exception for when a sensor with same sensor id already exists.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/algorithms/gab.html","title":"Gab","text":"<p>Module to define Gold Asset Builder algorithm behavior.</p>"},{"location":"reference/packages/algorithms/gab.html#packages.algorithms.gab.GAB","title":"<code>GAB</code>","text":"<p>         Bases: <code>Algorithm</code></p> <p>Class representing the gold asset builder.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/gab.py</code> <pre><code>class GAB(Algorithm):\n\"\"\"Class representing the gold asset builder.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n_SPARK_DEFAULT_PARALLELISM_CONFIG = (\n\"spark.sql.sources.parallelPartitionDiscovery.parallelism\"\n)\n_SPARK_DEFAULT_PARALLELISM_VALUE = \"10000\"\ndef __init__(self, acon: dict):\n\"\"\"Construct GAB instances.\n        Args:\n            acon: algorithm configuration.\n        \"\"\"\nself.spec: GABSpec = GABSpec.create_from_acon(acon=acon)\ndef execute(self) -&gt; None:\n\"\"\"Execute the Gold Asset Builder.\"\"\"\nself._LOGGER.info(f\"Reading {self.spec.lookup_table} as lkp_query_builder\")\nlookup_query_builder_df = ExecEnv.SESSION.read.table(self.spec.lookup_table)\nExecEnv.SESSION.read.table(self.spec.calendar_table).createOrReplaceTempView(\n\"df_cal\"\n)\nself._LOGGER.info(f\"Generating calendar from {self.spec.calendar_table}\")\nquery_label = self.spec.query_label_filter\nqueue = self.spec.queue_filter\ncadence = self.spec.cadence_filter\nself._LOGGER.info(f\"Query Label Filter {query_label}\")\nself._LOGGER.info(f\"Queue Filter {queue}\")\nself._LOGGER.info(f\"Cadence Filter {cadence}\")\ngab_path = self.spec.gab_base_path\nself._LOGGER.info(f\"Gab Base Path {gab_path}\")\nlookup_query_builder_df = lookup_query_builder_df.filter(\n(\n(lookup_query_builder_df.query_label.isin(query_label))\n&amp; (lookup_query_builder_df.queue.isin(queue))\n&amp; (lookup_query_builder_df.is_active != lit(\"N\"))\n)\n)\nlookup_query_builder_df.cache()\nfor use_case in lookup_query_builder_df.collect():\nself._process_use_case(\nuse_case=use_case,\nlookup_query_builder=lookup_query_builder_df,\nselected_cadences=cadence,\ngab_path=gab_path,\n)\nlookup_query_builder_df.unpersist()\ndef _process_use_case(\nself,\nuse_case: Row,\nlookup_query_builder: DataFrame,\nselected_cadences: list[str],\ngab_path: str,\n) -&gt; None:\n\"\"\"Process each gab use case.\n        Args:\n            use_case: gab use case to process.\n            lookup_query_builder: gab configuration data.\n            selected_cadences: selected cadences to process.\n            gab_path: gab base path used to get the use case stages sql files.\n        \"\"\"\nself._LOGGER.info(f\"Executing use case: {use_case['query_label']}\")\nreconciliation = GABUtils.get_json_column_as_dict(\nlookup_query_builder=lookup_query_builder,\nquery_id=use_case[\"query_id\"],\nquery_column=\"recon_window\",\n)\nself._LOGGER.info(f\"reconcilation window - {reconciliation}\")\nconfigured_cadences = list(reconciliation.keys())\nstages = GABUtils.get_json_column_as_dict(\nlookup_query_builder=lookup_query_builder,\nquery_id=use_case[\"query_id\"],\nquery_column=\"intermediate_stages\",\n)\nself._LOGGER.info(f\"intermediate stages - {stages}\")\nself._LOGGER.info(f\"selected_cadences: {selected_cadences}\")\nself._LOGGER.info(f\"configured_cadences: {configured_cadences}\")\ncadences = self._get_filtered_cadences(selected_cadences, configured_cadences)\nself._LOGGER.info(f\"filtered cadences - {cadences}\")\nlatest_run_date, latest_config_date = self._get_latest_usecase_data(\nuse_case[\"query_id\"]\n)\nself._LOGGER.info(f\"latest_config_date: {latest_config_date}\")\nself._LOGGER.info(f\"latest_run_date: - {latest_run_date}\")\nself._set_use_case_stage_template_file(stages, gab_path, use_case)\nprocessed_cadences = []\nfor cadence in cadences:\nis_cadence_processed = self._process_use_case_query_cadence(\ncadence,\nreconciliation,\nuse_case,\nstages,\nlookup_query_builder,\n)\nif is_cadence_processed:\nprocessed_cadences.append(is_cadence_processed)\nif processed_cadences:\nself._generate_ddl(\nlatest_config_date=latest_config_date,\nlatest_run_date=latest_run_date,\nquery_id=use_case[\"query_id\"],\nlookup_query_builder=lookup_query_builder,\n)\nelse:\nself._LOGGER.info(\nf\"Skipping use case {use_case['query_label']}. No cadence processed \"\n\"for the use case.\"\n)\n@classmethod\ndef _set_use_case_stage_template_file(\ncls, stages: dict, gab_path: str, use_case: Row\n) -&gt; None:\n\"\"\"Set templated file for each stage.\n        Args:\n            stages: use case stages with their configuration.\n            gab_path: gab base path used to get the use case stages SQL files.\n            use_case: gab use case to process.\n        \"\"\"\ncls._LOGGER.info(\"Reading templated file for each stage...\")\nfor i in range(1, len(stages) + 1):\nstage = stages[str(i)]\nstage_file_path = stage[\"file_path\"]\nfull_path = gab_path + stage_file_path\ncls._LOGGER.info(f\"Stage file path is: {full_path}\")\nfile_read = open(full_path, \"r\").read()\ntemplated_file = file_read.replace(\n\"replace_offset_value\", str(use_case[\"timezone_offset\"])\n)\nstage[\"templated_file\"] = templated_file\nstage[\"full_file_path\"] = full_path\ndef _process_use_case_query_cadence(\nself,\ncadence: str,\nreconciliation: dict,\nuse_case: Row,\nstages: dict,\nlookup_query_builder: DataFrame,\n) -&gt; bool:\n\"\"\"Identify use case reconciliation window and cadence.\n        Args:\n            cadence:  cadence to process.\n            reconciliation: configured use case reconciliation window.\n            use_case: gab use case to process.\n            stages: use case stages with their configuration.\n            lookup_query_builder: gab configuration data.\n        \"\"\"\nselected_reconciliation_window = {}\nselected_cadence = reconciliation.get(cadence)\nself._LOGGER.info(f\"Processing cadence: {cadence}\")\nself._LOGGER.info(f\"Reconciliation Window - {selected_cadence}\")\nif selected_cadence:\nselected_reconciliation_window = selected_cadence.get(\"recon_window\")\nself._LOGGER.info(f\"{cadence}: {self.spec.start_date} - {self.spec.end_date}\")\nstart_of_week = use_case[\"start_of_the_week\"]\nself._set_week_configuration_by_uc_start_of_week(start_of_week)\ncadence_configuration_at_end_date = (\nGABUtils.get_cadence_configuration_at_end_date(self.spec.end_date)\n)\nreconciliation_cadences = GABUtils().get_reconciliation_cadences(\ncadence=cadence,\nselected_reconciliation_window=selected_reconciliation_window,\ncadence_configuration_at_end_date=cadence_configuration_at_end_date,\nrerun_flag=self.spec.rerun_flag,\n)\nstart_date_str = GABUtils.format_datetime_to_default(self.spec.start_date)\nend_date_str = GABUtils.format_datetime_to_default(self.spec.end_date)\nfor reconciliation_cadence, snapshot_flag in reconciliation_cadences.items():\nself._process_reconciliation_cadence(\nreconciliation_cadence=reconciliation_cadence,\nsnapshot_flag=snapshot_flag,\ncadence=cadence,\nstart_date_str=start_date_str,\nend_date_str=end_date_str,\nuse_case=use_case,\nlookup_query_builder=lookup_query_builder,\nstages=stages,\n)\nreturn (cadence in reconciliation.keys()) or (\nreconciliation_cadences is not None\n)\ndef _process_reconciliation_cadence(\nself,\nreconciliation_cadence: str,\nsnapshot_flag: str,\ncadence: str,\nstart_date_str: str,\nend_date_str: str,\nuse_case: Row,\nlookup_query_builder: DataFrame,\nstages: dict,\n) -&gt; None:\n\"\"\"Process use case reconciliation window.\n        Reconcile the pre-aggregated data to cover the late events.\n        Args:\n            reconciliation_cadence: reconciliation to process.\n            snapshot_flag: flag indicating if for this cadence the snapshot is enabled.\n            cadence: cadence to process.\n            start_date_str: start date of the period to process.\n            end_date_str: end date of the period to process.\n            use_case: gab use case to process.\n            lookup_query_builder: gab configuration data.\n            stages: use case stages with their configuration.\n        Example:\n            Cadence: week;\n            Reconciliation: monthly;\n            This means every weekend previous week aggregations will be calculated and\n                on month end we will reconcile the numbers calculated for last 4 weeks\n                to readjust the number for late events.\n        \"\"\"\n(\nwindow_start_date,\nwindow_end_date,\nfilter_start_date,\nfilter_end_date,\n) = GABCadenceManager().extended_window_calculator(\ncadence,\nreconciliation_cadence,\nself.spec.current_date,\nstart_date_str,\nend_date_str,\nuse_case[\"query_type\"],\nself.spec.rerun_flag,\nsnapshot_flag,\n)\nif use_case[\"timezone_offset\"]:\nfilter_start_date = filter_start_date + timedelta(\nhours=use_case[\"timezone_offset\"]\n)\nfilter_end_date = filter_end_date + timedelta(\nhours=use_case[\"timezone_offset\"]\n)\nfilter_start_date_str = GABUtils.format_datetime_to_default(filter_start_date)\nfilter_end_date_str = GABUtils.format_datetime_to_default(filter_end_date)\npartition_end = GABUtils.format_datetime_to_default(\n(window_end_date - timedelta(days=1))\n)\nwindow_start_date_str = GABUtils.format_datetime_to_default(window_start_date)\nwindow_end_date_str = GABUtils.format_datetime_to_default(window_end_date)\npartition_filter = GABPartitionUtils.get_partition_condition(\nfilter_start_date_str, partition_end\n)\nself._LOGGER.info(\n\"extended window for start and end dates are: \"\nf\"{filter_start_date_str} - {filter_end_date_str}\"\n)\nunpersist_list = []\nfor i in range(1, len(stages) + 1):\nstage = stages[str(i)]\ntemplated_file = stage[\"templated_file\"]\nstage_file_path = stage[\"full_file_path\"]\ntemplated = self._process_use_case_query_step(\nstage=stages[str(i)],\ntemplated_file=templated_file,\nuse_case=use_case,\nreconciliation_cadence=reconciliation_cadence,\ncadence=cadence,\nsnapshot_flag=snapshot_flag,\nwindow_start_date=window_start_date_str,\npartition_end=partition_end,\nfilter_start_date=filter_start_date_str,\nfilter_end_date=filter_end_date_str,\npartition_filter=partition_filter,\n)\ntemp_stage_view_name = self._create_stage_view(\ntemplated,\nstages[str(i)],\nwindow_start_date_str,\nwindow_end_date_str,\nuse_case[\"query_id\"],\nuse_case[\"query_label\"],\ncadence,\nstage_file_path,\n)\nunpersist_list.append(temp_stage_view_name)\ninsert_success = self._generate_view_statement(\nquery_id=use_case[\"query_id\"],\ncadence=cadence,\ntemp_stage_view_name=temp_stage_view_name,\nlookup_query_builder=lookup_query_builder,\nwindow_start_date=window_start_date_str,\nwindow_end_date=window_end_date_str,\nquery_label=use_case[\"query_label\"],\n)\nself._LOGGER.info(f\"Inserted data to generate the view: {insert_success}\")\nself._unpersist_cached_views(unpersist_list)\ndef _process_use_case_query_step(\nself,\nstage: dict,\ntemplated_file: str,\nuse_case: Row,\nreconciliation_cadence: str,\ncadence: str,\nsnapshot_flag: str,\nwindow_start_date: str,\npartition_end: str,\nfilter_start_date: str,\nfilter_end_date: str,\npartition_filter: str,\n) -&gt; str:\n\"\"\"Process each use case step.\n        Process any intermediate view defined in the gab configuration table as step for\n            the use case.\n        Args:\n            stage: stage to process.\n            templated_file: sql file to process at this stage.\n            use_case: gab use case to process.\n            reconciliation_cadence: configured use case reconciliation window.\n            cadence: cadence to process.\n            snapshot_flag: flag indicating if for this cadence the snapshot is enabled.\n            window_start_date: start date for the configured stage.\n            partition_end: end date for the configured stage.\n            filter_start_date: filter start date to replace in the stage query.\n            filter_end_date: filter end date to replace in the stage query.\n            partition_filter: partition condition.\n        \"\"\"\nfilter_col = stage[\"project_date_column\"]\nif stage[\"filter_date_column\"]:\nfilter_col = stage[\"filter_date_column\"]\n# dummy value to avoid empty error if empty on the configuration\nproject_col = stage.get(\"project_date_column\", \"X\")\ngab_base_configuration_copy = copy.deepcopy(\nGABCombinedConfiguration.COMBINED_CONFIGURATION.value\n)\nfor item in gab_base_configuration_copy.values():\nself._update_rendered_item_cadence(\nreconciliation_cadence, cadence, project_col, item  # type: ignore\n)\n(\nrendered_date,\nrendered_to_date,\njoin_condition,\n) = self._get_cadence_configuration(\ngab_base_configuration_copy,\ncadence,\nreconciliation_cadence,\nsnapshot_flag,\nuse_case[\"start_of_the_week\"],\nproject_col,\nwindow_start_date,\npartition_end,\n)\nrendered_file = self._render_template_query(\ntemplated=templated_file,\ncadence=cadence,\nstart_of_the_week=use_case[\"start_of_the_week\"],\nquery_id=use_case[\"query_id\"],\nrendered_date=rendered_date,\nfilter_start_date=filter_start_date,\nfilter_end_date=filter_end_date,\nfilter_col=filter_col,\ntimezone_offset=use_case[\"timezone_offset\"],\njoin_condition=join_condition,\npartition_filter=partition_filter,\nrendered_to_date=rendered_to_date,\n)\nreturn rendered_file\n@classmethod\ndef _get_filtered_cadences(\ncls, selected_cadences: list[str], configured_cadences: list[str]\n) -&gt; list[str]:\n\"\"\"Get filtered cadences.\n        Get the intersection of user selected cadences and use case configured cadences.\n        Args:\n            selected_cadences: user selected cadences.\n            configured_cadences: use case configured cadences.\n        \"\"\"\nreturn (\nconfigured_cadences\nif \"All\" in selected_cadences\nelse GABCadence.order_cadences(\nlist(set(selected_cadences).intersection(configured_cadences))\n)\n)\ndef _get_latest_usecase_data(self, query_id: str) -&gt; tuple[datetime, datetime]:\n\"\"\"Get latest use case data.\n        Args:\n            query_id: use case query id.\n        \"\"\"\nreturn (\nself._get_latest_run_date(query_id),\nself._get_latest_use_case_date(query_id),\n)\ndef _get_latest_run_date(self, query_id: str) -&gt; datetime:\n\"\"\"Get latest use case run date.\n        Args:\n            query_id: use case query id.\n        \"\"\"\nlast_success_run_sql = \"\"\"\n            SELECT run_start_time\n            FROM {database}.gab_log_events\n            WHERE query_id = {query_id}\n            AND stage_name = 'Final Insert'\n            AND status = 'Success'\n            ORDER BY 1 DESC\n            LIMIT 1\n            \"\"\".format(  # nosec: B608\ndatabase=self.spec.target_database, query_id=query_id\n)\ntry:\nlatest_run_date: datetime = ExecEnv.SESSION.sql(\nlast_success_run_sql\n).collect()[0][0]\nexcept Exception:\nlatest_run_date = datetime.strptime(\n\"2020-01-01\", GABDefaults.DATE_FORMAT.value\n)\nreturn latest_run_date\ndef _get_latest_use_case_date(self, query_id: str) -&gt; datetime:\n\"\"\"Get latest use case configured date.\n        Args:\n            query_id: use case query id.\n        \"\"\"\nquery_config_sql = \"\"\"\n            SELECT lh_created_on\n            FROM {lkp_query_builder}\n            WHERE query_id = {query_id}\n        \"\"\".format(  # nosec: B608\nlkp_query_builder=self.spec.lookup_table,\nquery_id=query_id,\n)\nlatest_config_date: datetime = ExecEnv.SESSION.sql(query_config_sql).collect()[\n0\n][0]\nreturn latest_config_date\n@classmethod\ndef _set_week_configuration_by_uc_start_of_week(cls, start_of_week: str) -&gt; None:\n\"\"\"Set week configuration by use case start of week.\n        Args:\n            start_of_week: use case start of week (MONDAY or SUNDAY).\n        \"\"\"\nif start_of_week.upper() == \"MONDAY\":\npendulum.week_starts_at(pendulum.MONDAY)\npendulum.week_ends_at(pendulum.SUNDAY)\nelif start_of_week.upper() == \"SUNDAY\":\npendulum.week_starts_at(pendulum.SUNDAY)\npendulum.week_ends_at(pendulum.SATURDAY)\nelse:\nraise NotImplementedError(\nf\"The requested {start_of_week} is not implemented.\"\n\"Supported `start_of_week` values: [MONDAY, SUNDAY]\"\n)\n@classmethod\ndef _update_rendered_item_cadence(\ncls, reconciliation_cadence: str, cadence: str, project_col: str, item: dict\n) -&gt; None:\n\"\"\"Override item properties based in the rendered item cadence.\n        Args:\n            reconciliation_cadence: configured use case reconciliation window.\n            cadence: cadence to process.\n            project_col: use case projection date column name.\n            item: predefined use case combination.\n        \"\"\"\nrendered_item = cls._get_rendered_item_cadence(\nreconciliation_cadence, cadence, project_col, item\n)\nitem[\"join_select\"] = rendered_item[\"join_select\"]\nitem[\"project_start\"] = rendered_item[\"project_start\"]\nitem[\"project_end\"] = rendered_item[\"project_end\"]\n@classmethod\ndef _get_rendered_item_cadence(\ncls, reconciliation_cadence: str, cadence: str, project_col: str, item: dict\n) -&gt; dict:\n\"\"\"Update pre-configured gab parameters with use case data.\n        Args:\n            reconciliation_cadence: configured use case reconciliation window.\n            cadence: cadence to process.\n            project_col: use case projection date column name.\n            item: predefined use case combination.\n        \"\"\"\nreturn {\nGABKeys.JOIN_SELECT: (\nitem[GABKeys.JOIN_SELECT]\n.replace(GABReplaceableKeys.CONFIG_WEEK_START, \"Monday\")\n.replace(\nGABReplaceableKeys.RECONCILIATION_CADENCE,\nreconciliation_cadence,\n)\n.replace(GABReplaceableKeys.CADENCE, cadence)\n),\nGABKeys.PROJECT_START: (\nitem[GABKeys.PROJECT_START]\n.replace(GABReplaceableKeys.CADENCE, cadence)\n.replace(GABReplaceableKeys.DATE_COLUMN, project_col)\n),\nGABKeys.PROJECT_END: (\nitem[GABKeys.PROJECT_END]\n.replace(GABReplaceableKeys.CADENCE, cadence)\n.replace(GABReplaceableKeys.DATE_COLUMN, project_col)\n),\n}\n@classmethod\ndef _get_cadence_configuration(\ncls,\nuse_case_configuration: dict,\ncadence: str,\nreconciliation_cadence: str,\nsnapshot_flag: str,\nstart_of_week: str,\nproject_col: str,\nwindow_start_date: str,\npartition_end: str,\n) -&gt; tuple[str, str, str]:\n\"\"\"Get use case configuration fields to replace pre-configured parameters.\n        Args:\n            use_case_configuration: use case configuration.\n            cadence: cadence to process.\n            reconciliation_cadence: cadence to be reconciliated.\n            snapshot_flag: flag indicating if for this cadence the snapshot is enabled.\n            start_of_week: use case start of week (MONDAY or SUNDAY).\n            project_col: use case projection date column name.\n            window_start_date: start date for the configured stage.\n            partition_end: end date for the configured stage.\n        Returns:\n            rendered_from_date: projection start date.\n            rendered_to_date: projection end date.\n            join_condition: string containing the join condition to replace in the\n                templated query by jinja substitution.\n        \"\"\"\ncadence_dict = next(\n(\ndict(configuration)\nfor configuration in use_case_configuration.values()\nif (\n(cadence in configuration[\"cadence\"])\nand (reconciliation_cadence in configuration[\"recon\"])\nand (snapshot_flag in configuration[\"snap_flag\"])\nand (\nGABStartOfWeek.get_start_of_week()[start_of_week.upper()]\nin configuration[\"week_start\"]\n)\n)\n),\nNone,\n)\nrendered_from_date = None\nrendered_to_date = None\njoin_condition = None\nif cadence_dict:\nrendered_from_date = (\ncadence_dict[GABKeys.PROJECT_START]\n.replace(GABReplaceableKeys.CADENCE, cadence)\n.replace(GABReplaceableKeys.DATE_COLUMN, project_col)\n)\nrendered_to_date = (\ncadence_dict[GABKeys.PROJECT_END]\n.replace(GABReplaceableKeys.CADENCE, cadence)\n.replace(GABReplaceableKeys.DATE_COLUMN, project_col)\n)\nif cadence_dict[GABKeys.JOIN_SELECT]:\njoin_condition = \"\"\"\n                 inner join (\n{join_select} from df_cal\n                     where calendar_date\n                     between '{bucket_start}' and '{bucket_end}'\n                 )\n                 df_cal on date({date_column})\n                     between df_cal.cadence_start_date and df_cal.cadence_end_date\n                 \"\"\".format(\njoin_select=cadence_dict[GABKeys.JOIN_SELECT],\nbucket_start=window_start_date,\nbucket_end=partition_end,\ndate_column=project_col,\n)\nreturn rendered_from_date, rendered_to_date, join_condition\ndef _render_template_query(\nself,\ntemplated: str,\ncadence: str,\nstart_of_the_week: str,\nquery_id: str,\nrendered_date: str,\nfilter_start_date: str,\nfilter_end_date: str,\nfilter_col: str,\ntimezone_offset: str,\njoin_condition: str,\npartition_filter: str,\nrendered_to_date: str,\n) -&gt; str:\n\"\"\"Replace jinja templated parameters in the SQL with the actual data.\n        Args:\n            templated: templated sql file to process at this stage.\n            cadence: cadence to process.\n            start_of_the_week: use case start of week (MONDAY or SUNDAY).\n            query_id: gab configuration table use case identifier.\n            rendered_date: projection start date.\n            filter_start_date: filter start date to replace in the stage query.\n            filter_end_date: filter end date to replace in the stage query.\n            filter_col: use case projection date column name.\n            timezone_offset: timezone offset configured in the use case.\n            join_condition: string containing the join condition.\n            partition_filter: partition condition.\n            rendered_to_date: projection end date.\n        \"\"\"\nreturn Template(templated).render(\ncadence=\"'{cadence}' as cadence\".format(cadence=cadence),\ncadence_run=cadence,\nweek_start=start_of_the_week,\nquery_id=\"'{query_id}' as query_id\".format(query_id=query_id),\nproject_date_column=rendered_date,\ntarget_table=self.spec.target_table,\ndatabase=self.spec.source_database,\nstart_date=filter_start_date,\nend_date=filter_end_date,\nfilter_date_column=filter_col,\noffset_value=timezone_offset,\njoins=join_condition if join_condition else \"\",\npartition_filter=partition_filter,\nto_date=rendered_to_date,\n)\ndef _create_stage_view(\nself,\nrendered_template: str,\nstage: dict,\nwindow_start_date: str,\nwindow_end_date: str,\nquery_id: str,\nquery_label: str,\ncadence: str,\nstage_file_path: str,\n) -&gt; str:\n\"\"\"Create each use case stage view.\n        Each stage has a specific order and refer to a specific SQL to be executed.\n        Args:\n            rendered_template: rendered stage SQL file.\n            stage: stage to process.\n            window_start_date: start date for the configured stage.\n            window_end_date: end date for the configured stage.\n            query_id: gab configuration table use case identifier.\n            query_label: gab configuration table use case name.\n            cadence: cadence to process.\n            stage_file_path: full stage file path (gab path + stage path).\n        \"\"\"\nrun_start_time = datetime.now()\ncreation_status: str\nerror_message: Union[Exception, str]\ntry:\ntmp = ExecEnv.SESSION.sql(rendered_template)\nnum_partitions = ExecEnv.SESSION.conf.get(\nself._SPARK_DEFAULT_PARALLELISM_CONFIG,\nself._SPARK_DEFAULT_PARALLELISM_VALUE,\n)\nif stage[\"repartition\"]:\nif stage[\"repartition\"].get(\"numPartitions\"):\nnum_partitions = stage[\"repartition\"][\"numPartitions\"]\nif stage[\"repartition\"].get(\"keys\"):\ntmp = tmp.repartition(\nint(num_partitions), *stage[\"repartition\"][\"keys\"]\n)\nself._LOGGER.info(\"Repartitioned on given Key(s)\")\nelse:\ntmp = tmp.repartition(int(num_partitions))\nself._LOGGER.info(\"Repartitioned on given partition count\")\ntemp_step_view_name: str = stage[\"table_alias\"]\ntmp.createOrReplaceTempView(temp_step_view_name)\nif stage[\"storage_level\"]:\nExecEnv.SESSION.sql(\n\"CACHE TABLE {tbl} \"\n\"OPTIONS ('storageLevel' '{type}')\".format(\ntbl=temp_step_view_name,\ntype=stage[\"storage_level\"],\n)\n)\nExecEnv.SESSION.sql(\n\"SELECT COUNT(*) FROM {tbl}\".format(  # nosec: B608\ntbl=temp_step_view_name\n)\n)\nself._LOGGER.info(f\"Cached stage view - {temp_step_view_name} \")\ncreation_status = \"Success\"\nerror_message = \"NA\"\nexcept Exception as err:\ncreation_status = \"Failed\"\nerror_message = err\nraise err\nfinally:\nrun_end_time = datetime.now()\nGABUtils().logger(\nrun_start_time,\nrun_end_time,\nwindow_start_date,\nwindow_end_date,\nquery_id,\nquery_label,\ncadence,\nstage_file_path,\nrendered_template,\ncreation_status,\nerror_message,\nself.spec.target_database,\n)\nreturn temp_step_view_name\ndef _generate_view_statement(\nself,\nquery_id: str,\ncadence: str,\ntemp_stage_view_name: str,\nlookup_query_builder: DataFrame,\nwindow_start_date: str,\nwindow_end_date: str,\nquery_label: str,\n) -&gt; bool:\n\"\"\"Feed use case data to the insights table (default: unified use case table).\n        Args:\n            query_id: gab configuration table use case identifier.\n            cadence: cadence to process.\n            temp_stage_view_name: name of the temp view generated by the stage.\n            lookup_query_builder: gab configuration data.\n            window_start_date: start date for the configured stage.\n            window_end_date: end date for the configured stage.\n            query_label: gab configuration table use case name.\n        \"\"\"\nrun_start_time = datetime.now()\ncreation_status: str\nerror_message: Union[Exception, str]\nGABDeleteGenerator(\nquery_id=query_id,\ncadence=cadence,\ntemp_stage_view_name=temp_stage_view_name,\nlookup_query_builder=lookup_query_builder,\ntarget_database=self.spec.target_database,\ntarget_table=self.spec.target_table,\n).generate_sql()\ngen_ins = GABInsertGenerator(\nquery_id=query_id,\ncadence=cadence,\nfinal_stage_table=temp_stage_view_name,\nlookup_query_builder=lookup_query_builder,\ntarget_database=self.spec.target_database,\ntarget_table=self.spec.target_table,\n).generate_sql()\ntry:\nExecEnv.SESSION.sql(gen_ins)\ncreation_status = \"Success\"\nerror_message = \"NA\"\ninserted = True\nexcept Exception as err:\ncreation_status = \"Failed\"\nerror_message = err\nraise\nfinally:\nrun_end_time = datetime.now()\nGABUtils().logger(\nrun_start_time,\nrun_end_time,\nwindow_start_date,\nwindow_end_date,\nquery_id,\nquery_label,\ncadence,\n\"Final Insert\",\ngen_ins,\ncreation_status,\nerror_message,\nself.spec.target_database,\n)\nreturn inserted\n@classmethod\ndef _unpersist_cached_views(cls, unpersist_list: list[str]) -&gt; None:\n\"\"\"Unpersist cached views.\n        Args:\n            unpersist_list: list containing the view names to unpersist.\n        \"\"\"\n[\nExecEnv.SESSION.sql(\"UNCACHE TABLE {tbl}\".format(tbl=i))\nfor i in unpersist_list\n]\ndef _generate_ddl(\nself,\nlatest_config_date: datetime,\nlatest_run_date: datetime,\nquery_id: str,\nlookup_query_builder: DataFrame,\n) -&gt; None:\n\"\"\"Generate the actual gold asset.\n        It will create and return the view containing all specified dimensions, metrics\n            and computed metric for each cadence/reconciliation window.\n        Args:\n            latest_config_date: latest use case configuration date.\n            latest_run_date: latest use case run date.\n            query_id: gab configuration table use case identifier.\n            lookup_query_builder: gab configuration data.\n        \"\"\"\nif str(latest_config_date) &gt; str(latest_run_date):\nGABViewManager(\nquery_id=query_id,\nlookup_query_builder=lookup_query_builder,\ntarget_database=self.spec.target_database,\ntarget_table=self.spec.target_table,\n).generate_use_case_views()\nelse:\nself._LOGGER.info(\n\"View is not being re-created as there are no changes in the \"\n\"configuration after the latest run\"\n)\n</code></pre>"},{"location":"reference/packages/algorithms/gab.html#packages.algorithms.gab.GAB.__init__","title":"<code>__init__(acon)</code>","text":"<p>Construct GAB instances.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>algorithm configuration.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/gab.py</code> <pre><code>def __init__(self, acon: dict):\n\"\"\"Construct GAB instances.\n    Args:\n        acon: algorithm configuration.\n    \"\"\"\nself.spec: GABSpec = GABSpec.create_from_acon(acon=acon)\n</code></pre>"},{"location":"reference/packages/algorithms/gab.html#packages.algorithms.gab.GAB.execute","title":"<code>execute()</code>","text":"<p>Execute the Gold Asset Builder.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/gab.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the Gold Asset Builder.\"\"\"\nself._LOGGER.info(f\"Reading {self.spec.lookup_table} as lkp_query_builder\")\nlookup_query_builder_df = ExecEnv.SESSION.read.table(self.spec.lookup_table)\nExecEnv.SESSION.read.table(self.spec.calendar_table).createOrReplaceTempView(\n\"df_cal\"\n)\nself._LOGGER.info(f\"Generating calendar from {self.spec.calendar_table}\")\nquery_label = self.spec.query_label_filter\nqueue = self.spec.queue_filter\ncadence = self.spec.cadence_filter\nself._LOGGER.info(f\"Query Label Filter {query_label}\")\nself._LOGGER.info(f\"Queue Filter {queue}\")\nself._LOGGER.info(f\"Cadence Filter {cadence}\")\ngab_path = self.spec.gab_base_path\nself._LOGGER.info(f\"Gab Base Path {gab_path}\")\nlookup_query_builder_df = lookup_query_builder_df.filter(\n(\n(lookup_query_builder_df.query_label.isin(query_label))\n&amp; (lookup_query_builder_df.queue.isin(queue))\n&amp; (lookup_query_builder_df.is_active != lit(\"N\"))\n)\n)\nlookup_query_builder_df.cache()\nfor use_case in lookup_query_builder_df.collect():\nself._process_use_case(\nuse_case=use_case,\nlookup_query_builder=lookup_query_builder_df,\nselected_cadences=cadence,\ngab_path=gab_path,\n)\nlookup_query_builder_df.unpersist()\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html","title":"Reconciliator","text":"<p>Module containing the Reconciliator class.</p>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.ReconciliationTransformers","title":"<code>ReconciliationTransformers</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Transformers Available for the Reconciliation Algorithm.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>class ReconciliationTransformers(Enum):\n\"\"\"Transformers Available for the Reconciliation Algorithm.\"\"\"\nAVAILABLE_TRANSFORMERS: dict = {\n\"cache\": Optimizers.cache,\n\"persist\": Optimizers.persist,\n}\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.ReconciliationType","title":"<code>ReconciliationType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Type of Reconciliation.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>class ReconciliationType(Enum):\n\"\"\"Type of Reconciliation.\"\"\"\nPCT = \"percentage\"\nABS = \"absolute\"\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.Reconciliator","title":"<code>Reconciliator</code>","text":"<p>         Bases: <code>Executable</code></p> <p>Class to define the behavior of an algorithm that checks if data reconciles.</p> <p>Checking if data reconciles, using this algorithm, is a matter of reading the 'truth' data and the 'current' data. You can use any input specification compatible with the lakehouse engine to read 'truth' or 'current' data. On top of that, you can pass a 'truth_preprocess_query' and a 'current_preprocess_query' so you can preprocess the data before it goes into the actual reconciliation process. Moreover, you can use the 'truth_preprocess_query_args' and 'current_preprocess_query_args' to pass additional arguments to be used to apply additional operations on top of the dataframe, resulting from the previous steps. With these arguments you can apply additional operations like caching or persisting the Dataframe. The way to pass the additional arguments for the operations is similar to the TransformSpec, but only a few operations are allowed. Those are defined in ReconciliationTransformers.AVAILABLE_TRANSFORMERS.</p> <p>The reconciliation process is focused on joining 'truth' with 'current' by all provided columns except the ones passed as 'metrics'. After that it calculates the differences in the metrics attributes (either percentage or absolute difference). Finally, it aggregates the differences, using the supplied aggregation function (e.g., sum, avg, min, max, etc).</p> <p>All of these configurations are passed via the ACON to instantiate a ReconciliatorSpec object.</p> <p>Note</p> <p>It is crucial that both the current and truth datasets have exactly the same structure.</p> <p>Note</p> <p>You should not use 0 as yellow or red threshold, as the algorithm will verify if the difference between the truth and current values is bigger or equal than those thresholds.</p> <p>Note</p> <p>The reconciliation does not produce any negative values or percentages, as we use the absolute value of the differences. This means that the recon result will not indicate if it was the current values that were bigger or smaller than the truth values, or vice versa.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>class Reconciliator(Executable):\n\"\"\"Class to define the behavior of an algorithm that checks if data reconciles.\n    Checking if data reconciles, using this algorithm, is a matter of reading the\n    'truth' data and the 'current' data. You can use any input specification compatible\n    with the lakehouse engine to read 'truth' or 'current' data. On top of that, you\n    can pass a 'truth_preprocess_query' and a 'current_preprocess_query' so you can\n    preprocess the data before it goes into the actual reconciliation process.\n    Moreover, you can use the 'truth_preprocess_query_args' and\n    'current_preprocess_query_args' to pass additional arguments to be used to apply\n    additional operations on top of the dataframe, resulting from the previous steps.\n    With these arguments you can apply additional operations like caching or persisting\n    the Dataframe. The way to pass the additional arguments for the operations is\n    similar to the TransformSpec, but only a few operations are allowed. Those are\n    defined in ReconciliationTransformers.AVAILABLE_TRANSFORMERS.\n    The reconciliation process is focused on joining 'truth' with 'current' by all\n    provided columns except the ones passed as 'metrics'. After that it calculates the\n    differences in the metrics attributes (either percentage or absolute difference).\n    Finally, it aggregates the differences, using the supplied aggregation function\n    (e.g., sum, avg, min, max, etc).\n    All of these configurations are passed via the ACON to instantiate a\n    ReconciliatorSpec object.\n    !!! note\n        It is crucial that both the current and truth datasets have exactly the same\n        structure.\n    !!! note\n        You should not use 0 as yellow or red threshold, as the algorithm will verify\n        if the difference between the truth and current values is bigger\n        or equal than those thresholds.\n    !!! note\n        The reconciliation does not produce any negative values or percentages, as we\n        use the absolute value of the differences. This means that the recon result\n        will not indicate if it was the current values that were bigger or smaller\n        than the truth values, or vice versa.\n    \"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, acon: dict):\n\"\"\"Construct Algorithm instances.\n        Args:\n            acon: algorithm configuration.\n        \"\"\"\nself.spec: ReconciliatorSpec = ReconciliatorSpec(\nmetrics=acon[\"metrics\"],\ntruth_input_spec=InputSpec(**acon[\"truth_input_spec\"]),\ncurrent_input_spec=InputSpec(**acon[\"current_input_spec\"]),\ntruth_preprocess_query=acon.get(\"truth_preprocess_query\", None),\ntruth_preprocess_query_args=acon.get(\"truth_preprocess_query_args\", None),\ncurrent_preprocess_query=acon.get(\"current_preprocess_query\", None),\ncurrent_preprocess_query_args=acon.get(\n\"current_preprocess_query_args\", None\n),\nignore_empty_df=acon.get(\"ignore_empty_df\", False),\n)\ndef get_source_of_truth(self) -&gt; DataFrame:\n\"\"\"Get the source of truth (expected result) for the reconciliation process.\n        Returns:\n            DataFrame containing the source of truth.\n        \"\"\"\ntruth_df = ReaderFactory.get_data(self.spec.truth_input_spec)\nif self.spec.truth_preprocess_query:\ntruth_df.createOrReplaceTempView(\"truth\")\ntruth_df = ExecEnv.SESSION.sql(self.spec.truth_preprocess_query)\nreturn truth_df\ndef get_current_results(self) -&gt; DataFrame:\n\"\"\"Get the current results from the table that we are checking if it reconciles.\n        Returns:\n            DataFrame containing the current results.\n        \"\"\"\ncurrent_df = ReaderFactory.get_data(self.spec.current_input_spec)\nif self.spec.current_preprocess_query:\ncurrent_df.createOrReplaceTempView(\"current\")\ncurrent_df = ExecEnv.SESSION.sql(self.spec.current_preprocess_query)\nreturn current_df\ndef execute(self) -&gt; None:\n\"\"\"Reconcile the current results against the truth dataset.\"\"\"\ntruth_df = self.get_source_of_truth()\nself._apply_preprocess_query_args(\ntruth_df, self.spec.truth_preprocess_query_args\n)\nself._logger.info(\"Source of truth:\")\ntruth_df.show(1000, truncate=False)\ncurrent_results_df = self.get_current_results()\nself._apply_preprocess_query_args(\ncurrent_results_df, self.spec.current_preprocess_query_args\n)\nself._logger.info(\"Current results:\")\ncurrent_results_df.show(1000, truncate=False)\nstatus = \"green\"\n# if ignore_empty_df is true, run empty check on truth_df and current_results_df\n# if both the dataframes are empty then exit with green\nif (\nself.spec.ignore_empty_df\nand truth_df.isEmpty()\nand current_results_df.isEmpty()\n):\nself._logger.info(\nf\"ignore_empty_df is {self.spec.ignore_empty_df}, \"\nf\"truth_df and current_results_df are empty, \"\nf\"hence ignoring reconciliation\"\n)\nself._logger.info(\"The Reconciliation process has succeeded.\")\nreturn\nrecon_results = self._get_recon_results(\ntruth_df, current_results_df, self.spec.metrics\n)\nself._logger.info(f\"Reconciliation result: {recon_results}\")\nfor m in self.spec.metrics:\nmetric_name = f\"{m['metric']}_{m['type']}_diff_{m['aggregation']}\"\nif m[\"yellow\"] &lt;= recon_results[metric_name] &lt; m[\"red\"]:\nif status == \"green\":\n# only switch to yellow if it was green before, otherwise we want\n# to preserve 'red' as the final status.\nstatus = \"yellow\"\nelif m[\"red\"] &lt;= recon_results[metric_name]:\nstatus = \"red\"\nif status != \"green\":\nraise ReconciliationFailedException(\nf\"The Reconciliation process has failed with status: {status}.\"\n)\nelse:\nself._logger.info(\"The Reconciliation process has succeeded.\")\n@staticmethod\ndef _apply_preprocess_query_args(\ndf: DataFrame, preprocess_query_args: List[dict]\n) -&gt; DataFrame:\n\"\"\"Apply transformers on top of the preprocessed query.\n        Args:\n            df: dataframe being transformed.\n            preprocess_query_args: dict having the functions/transformations to\n                apply and respective arguments.\n        Returns: the transformed Dataframe.\n        \"\"\"\ntransformed_df = df\nif preprocess_query_args is None:\ntransformed_df = df.transform(Optimizers.cache())\nelif len(preprocess_query_args) &gt; 0:\nfor transformation in preprocess_query_args:\nrec_func = ReconciliationTransformers.AVAILABLE_TRANSFORMERS.value[\ntransformation[\"function\"]\n](**transformation.get(\"args\", {}))\ntransformed_df = df.transform(rec_func)\nelse:\ntransformed_df = df\nreturn transformed_df\ndef _get_recon_results(\nself, truth_df: DataFrame, current_results_df: DataFrame, metrics: List[dict]\n) -&gt; dict:\n\"\"\"Get the reconciliation results by comparing truth_df with current_results_df.\n        Args:\n            truth_df: dataframe with the truth data to reconcile against. It is\n                typically an aggregated dataset to use as baseline and then we match the\n                current_results_df (Aggregated at the same level) against this truth.\n            current_results_df: dataframe with the current results of the dataset we\n                are trying to reconcile.\n            metrics: list of dicts containing metric, aggregation, yellow threshold and\n                red threshold.\n        Return:\n            dictionary with the results (difference between truth and current results)\n        \"\"\"\nif len(truth_df.head(1)) == 0 or len(current_results_df.head(1)) == 0:\nraise ReconciliationFailedException(\n\"The reconciliation has failed because either the truth dataset or the \"\n\"current results dataset was empty.\"\n)\n# truth and current are joined on all columns except the metrics\njoined_df = truth_df.alias(\"truth\").join(\ncurrent_results_df.alias(\"current\"),\n[\ntruth_df[c] == current_results_df[c]\nfor c in current_results_df.columns\nif c not in [m[\"metric\"] for m in metrics]\n],\nhow=\"full\",\n)\nfor m in metrics:\nif m[\"type\"] == ReconciliationType.PCT.value:\njoined_df = joined_df.withColumn(\nf\"{m['metric']}_{m['type']}_diff\",\ncoalesce(\n(\n# we need to make sure we don't produce negative values\n# because our thresholds only accept &gt; or &gt;= comparisons.\nabs(\n(\ncol(f\"current.{m['metric']}\")\n- col(f\"truth.{m['metric']}\")\n)\n/ abs(col(f\"truth.{m['metric']}\"))\n)\n),\n# if the formula above produces null, we need to consider where\n# it came from: we check below if the values were the same,\n# and if so the diff is 0, if not the diff is 1 (e.g., the null\n# result might have come from a division by 0).\nwhen(\ncol(f\"current.{m['metric']}\").eqNullSafe(\ncol(f\"truth.{m['metric']}\")\n),\nlit(0),\n).otherwise(lit(1)),\n),\n)\nelif m[\"type\"] == ReconciliationType.ABS.value:\njoined_df = joined_df.withColumn(\nf\"{m['metric']}_{m['type']}_diff\",\nabs(\ncoalesce(col(f\"current.{m['metric']}\"), lit(0))\n- coalesce(col(f\"truth.{m['metric']}\"), lit(0))\n),\n)\nelse:\nraise NotImplementedError(\n\"The requested reconciliation type is not yet implemented.\"\n)\njoined_df = joined_df.withColumn(\nf\"{m['metric']}_{m['type']}_diff\",\ncol(f\"{m['metric']}_{m['type']}_diff\").cast(FloatType()),\n)\nresults_df = joined_df.agg(\n*[\ngetattr(spark_fns, m[\"aggregation\"])(\nf\"{m['metric']}_{m['type']}_diff\"\n).alias(f\"{m['metric']}_{m['type']}_diff_{m['aggregation']}\")\nfor m in metrics\n]\n)\nreturn results_df.collect()[0].asDict()\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.Reconciliator.__init__","title":"<code>__init__(acon)</code>","text":"<p>Construct Algorithm instances.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>algorithm configuration.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>def __init__(self, acon: dict):\n\"\"\"Construct Algorithm instances.\n    Args:\n        acon: algorithm configuration.\n    \"\"\"\nself.spec: ReconciliatorSpec = ReconciliatorSpec(\nmetrics=acon[\"metrics\"],\ntruth_input_spec=InputSpec(**acon[\"truth_input_spec\"]),\ncurrent_input_spec=InputSpec(**acon[\"current_input_spec\"]),\ntruth_preprocess_query=acon.get(\"truth_preprocess_query\", None),\ntruth_preprocess_query_args=acon.get(\"truth_preprocess_query_args\", None),\ncurrent_preprocess_query=acon.get(\"current_preprocess_query\", None),\ncurrent_preprocess_query_args=acon.get(\n\"current_preprocess_query_args\", None\n),\nignore_empty_df=acon.get(\"ignore_empty_df\", False),\n)\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.Reconciliator.execute","title":"<code>execute()</code>","text":"<p>Reconcile the current results against the truth dataset.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Reconcile the current results against the truth dataset.\"\"\"\ntruth_df = self.get_source_of_truth()\nself._apply_preprocess_query_args(\ntruth_df, self.spec.truth_preprocess_query_args\n)\nself._logger.info(\"Source of truth:\")\ntruth_df.show(1000, truncate=False)\ncurrent_results_df = self.get_current_results()\nself._apply_preprocess_query_args(\ncurrent_results_df, self.spec.current_preprocess_query_args\n)\nself._logger.info(\"Current results:\")\ncurrent_results_df.show(1000, truncate=False)\nstatus = \"green\"\n# if ignore_empty_df is true, run empty check on truth_df and current_results_df\n# if both the dataframes are empty then exit with green\nif (\nself.spec.ignore_empty_df\nand truth_df.isEmpty()\nand current_results_df.isEmpty()\n):\nself._logger.info(\nf\"ignore_empty_df is {self.spec.ignore_empty_df}, \"\nf\"truth_df and current_results_df are empty, \"\nf\"hence ignoring reconciliation\"\n)\nself._logger.info(\"The Reconciliation process has succeeded.\")\nreturn\nrecon_results = self._get_recon_results(\ntruth_df, current_results_df, self.spec.metrics\n)\nself._logger.info(f\"Reconciliation result: {recon_results}\")\nfor m in self.spec.metrics:\nmetric_name = f\"{m['metric']}_{m['type']}_diff_{m['aggregation']}\"\nif m[\"yellow\"] &lt;= recon_results[metric_name] &lt; m[\"red\"]:\nif status == \"green\":\n# only switch to yellow if it was green before, otherwise we want\n# to preserve 'red' as the final status.\nstatus = \"yellow\"\nelif m[\"red\"] &lt;= recon_results[metric_name]:\nstatus = \"red\"\nif status != \"green\":\nraise ReconciliationFailedException(\nf\"The Reconciliation process has failed with status: {status}.\"\n)\nelse:\nself._logger.info(\"The Reconciliation process has succeeded.\")\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.Reconciliator.get_current_results","title":"<code>get_current_results()</code>","text":"<p>Get the current results from the table that we are checking if it reconciles.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the current results.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>def get_current_results(self) -&gt; DataFrame:\n\"\"\"Get the current results from the table that we are checking if it reconciles.\n    Returns:\n        DataFrame containing the current results.\n    \"\"\"\ncurrent_df = ReaderFactory.get_data(self.spec.current_input_spec)\nif self.spec.current_preprocess_query:\ncurrent_df.createOrReplaceTempView(\"current\")\ncurrent_df = ExecEnv.SESSION.sql(self.spec.current_preprocess_query)\nreturn current_df\n</code></pre>"},{"location":"reference/packages/algorithms/reconciliator.html#packages.algorithms.reconciliator.Reconciliator.get_source_of_truth","title":"<code>get_source_of_truth()</code>","text":"<p>Get the source of truth (expected result) for the reconciliation process.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the source of truth.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/reconciliator.py</code> <pre><code>def get_source_of_truth(self) -&gt; DataFrame:\n\"\"\"Get the source of truth (expected result) for the reconciliation process.\n    Returns:\n        DataFrame containing the source of truth.\n    \"\"\"\ntruth_df = ReaderFactory.get_data(self.spec.truth_input_spec)\nif self.spec.truth_preprocess_query:\ntruth_df.createOrReplaceTempView(\"truth\")\ntruth_df = ExecEnv.SESSION.sql(self.spec.truth_preprocess_query)\nreturn truth_df\n</code></pre>"},{"location":"reference/packages/algorithms/sensor.html","title":"Sensor","text":"<p>Module to define Sensor algorithm behavior.</p>"},{"location":"reference/packages/algorithms/sensor.html#packages.algorithms.sensor.Sensor","title":"<code>Sensor</code>","text":"<p>         Bases: <code>Algorithm</code></p> <p>Class representing a sensor to check if the upstream has new data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/sensor.py</code> <pre><code>class Sensor(Algorithm):\n\"\"\"Class representing a sensor to check if the upstream has new data.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef __init__(self, acon: dict):\n\"\"\"Construct Sensor instances.\n        Args:\n            acon: algorithm configuration.\n        \"\"\"\nself.spec: SensorSpec = SensorSpec.create_from_acon(acon=acon)\nself._validate_sensor_spec()\nif self._check_if_sensor_already_exists():\nraise SensorAlreadyExistsException(\n\"There's already a sensor registered with same id or assets!\"\n)\ndef execute(self) -&gt; bool:\n\"\"\"Execute the sensor.\"\"\"\nself._LOGGER.info(f\"Starting {self.spec.input_spec.data_format} sensor...\")\nnew_data_df = SensorUpstreamManager.read_new_data(sensor_spec=self.spec)\nif self.spec.input_spec.read_type == ReadType.STREAMING.value:\nSensor._run_streaming_sensor(sensor_spec=self.spec, new_data_df=new_data_df)\nelif self.spec.input_spec.read_type == ReadType.BATCH.value:\nSensor._run_batch_sensor(\nsensor_spec=self.spec,\nnew_data_df=new_data_df,\n)\nhas_new_data = SensorControlTableManager.check_if_sensor_has_acquired_data(\nself.spec.sensor_id,\nself.spec.control_db_table_name,\n)\nself._LOGGER.info(\nf\"Sensor {self.spec.sensor_id} has previously \"\nf\"acquired data? {has_new_data}\"\n)\nif self.spec.fail_on_empty_result and not has_new_data:\nraise NoNewDataException(\nf\"No data was acquired by {self.spec.sensor_id} sensor.\"\n)\nreturn has_new_data\ndef _check_if_sensor_already_exists(self) -&gt; bool:\n\"\"\"Check if sensor already exists in the table to avoid duplicates.\"\"\"\nrow = SensorControlTableManager.read_sensor_table_data(\nsensor_id=self.spec.sensor_id,\ncontrol_db_table_name=self.spec.control_db_table_name,\n)\nif row and row.assets != self.spec.assets:\nreturn True\nelse:\nrow = SensorControlTableManager.read_sensor_table_data(\nassets=self.spec.assets,\ncontrol_db_table_name=self.spec.control_db_table_name,\n)\nreturn row is not None and row.sensor_id != self.spec.sensor_id\n@classmethod\ndef _run_streaming_sensor(\ncls, sensor_spec: SensorSpec, new_data_df: DataFrame\n) -&gt; None:\n\"\"\"Run sensor in streaming mode (internally runs in batch mode).\"\"\"\ndef foreach_batch_check_new_data(df: DataFrame, batch_id: int) -&gt; None:\n# forcing session to be available inside forEachBatch on\n# Spark Connect\nExecEnv.get_or_create()\nSensor._run_batch_sensor(\nsensor_spec=sensor_spec,\nnew_data_df=df,\n)\nnew_data_df.writeStream.trigger(availableNow=True).option(\n\"checkpointLocation\", sensor_spec.checkpoint_location\n).foreachBatch(foreach_batch_check_new_data).start().awaitTermination()\n@classmethod\ndef _run_batch_sensor(\ncls,\nsensor_spec: SensorSpec,\nnew_data_df: DataFrame,\n) -&gt; None:\n\"\"\"Run sensor in batch mode.\n        Args:\n            sensor_spec: sensor spec containing all sensor information.\n            new_data_df: DataFrame possibly containing new data.\n        \"\"\"\nnew_data_first_row = SensorUpstreamManager.get_new_data(new_data_df)\ncls._LOGGER.info(\nf\"Sensor {sensor_spec.sensor_id} has new data from upstream? \"\nf\"{new_data_first_row is not None}\"\n)\nif new_data_first_row:\nSensorControlTableManager.update_sensor_status(\nsensor_spec=sensor_spec,\nstatus=SensorStatus.ACQUIRED_NEW_DATA.value,\nupstream_key=(\nnew_data_first_row.UPSTREAM_KEY\nif \"UPSTREAM_KEY\" in new_data_df.columns\nelse None\n),\nupstream_value=(\nnew_data_first_row.UPSTREAM_VALUE\nif \"UPSTREAM_VALUE\" in new_data_df.columns\nelse None\n),\n)\ncls._LOGGER.info(\nf\"Successfully updated sensor status for sensor \"\nf\"{sensor_spec.sensor_id}...\"\n)\ndef _validate_sensor_spec(self) -&gt; None:\n\"\"\"Validate if sensor spec Read Type is allowed for the selected Data Format.\"\"\"\nif InputFormat.exists(self.spec.input_spec.data_format):\nif (\nself.spec.input_spec.data_format\nnot in SENSOR_ALLOWED_DATA_FORMATS[self.spec.input_spec.read_type]\n):\nraise NotImplementedError(\nf\"A sensor has not been implemented yet for this data format or, \"\nf\"this data format is not available for the read_type\"\nf\" {self.spec.input_spec.read_type}. \"\nf\"Check the allowed combinations of read_type and data_formats:\"\nf\" {SENSOR_ALLOWED_DATA_FORMATS}\"\n)\nelse:\nraise NotImplementedError(\nf\"Data format {self.spec.input_spec.data_format} isn't implemented yet.\"\n)\n</code></pre>"},{"location":"reference/packages/algorithms/sensor.html#packages.algorithms.sensor.Sensor.__init__","title":"<code>__init__(acon)</code>","text":"<p>Construct Sensor instances.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>algorithm configuration.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/sensor.py</code> <pre><code>def __init__(self, acon: dict):\n\"\"\"Construct Sensor instances.\n    Args:\n        acon: algorithm configuration.\n    \"\"\"\nself.spec: SensorSpec = SensorSpec.create_from_acon(acon=acon)\nself._validate_sensor_spec()\nif self._check_if_sensor_already_exists():\nraise SensorAlreadyExistsException(\n\"There's already a sensor registered with same id or assets!\"\n)\n</code></pre>"},{"location":"reference/packages/algorithms/sensor.html#packages.algorithms.sensor.Sensor.execute","title":"<code>execute()</code>","text":"<p>Execute the sensor.</p> Source code in <code>mkdocs/lakehouse_engine/packages/algorithms/sensor.py</code> <pre><code>def execute(self) -&gt; bool:\n\"\"\"Execute the sensor.\"\"\"\nself._LOGGER.info(f\"Starting {self.spec.input_spec.data_format} sensor...\")\nnew_data_df = SensorUpstreamManager.read_new_data(sensor_spec=self.spec)\nif self.spec.input_spec.read_type == ReadType.STREAMING.value:\nSensor._run_streaming_sensor(sensor_spec=self.spec, new_data_df=new_data_df)\nelif self.spec.input_spec.read_type == ReadType.BATCH.value:\nSensor._run_batch_sensor(\nsensor_spec=self.spec,\nnew_data_df=new_data_df,\n)\nhas_new_data = SensorControlTableManager.check_if_sensor_has_acquired_data(\nself.spec.sensor_id,\nself.spec.control_db_table_name,\n)\nself._LOGGER.info(\nf\"Sensor {self.spec.sensor_id} has previously \"\nf\"acquired data? {has_new_data}\"\n)\nif self.spec.fail_on_empty_result and not has_new_data:\nraise NoNewDataException(\nf\"No data was acquired by {self.spec.sensor_id} sensor.\"\n)\nreturn has_new_data\n</code></pre>"},{"location":"reference/packages/configs/index.html","title":"Configs","text":"<p>This module receives a config file which is included in the wheel.</p>"},{"location":"reference/packages/core/index.html","title":"Core","text":"<p>Package with the core behaviour of the lakehouse engine.</p>"},{"location":"reference/packages/core/dbfs_file_manager.html","title":"Dbfs file manager","text":"<p>File manager module using dbfs.</p>"},{"location":"reference/packages/core/dbfs_file_manager.html#packages.core.dbfs_file_manager.DBFSFileManager","title":"<code>DBFSFileManager</code>","text":"<p>         Bases: <code>FileManager</code></p> <p>Set of actions to manipulate dbfs files in several ways.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/dbfs_file_manager.py</code> <pre><code>class DBFSFileManager(FileManager):\n\"\"\"Set of actions to manipulate dbfs files in several ways.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef get_function(self) -&gt; None:\n\"\"\"Get a specific function to execute.\"\"\"\navailable_functions = {\n\"delete_objects\": self.delete_objects,\n\"copy_objects\": self.copy_objects,\n\"move_objects\": self.move_objects,\n}\nself._logger.info(\"Function being executed: {}\".format(self.function))\nif self.function in available_functions.keys():\nfunc = available_functions[self.function]\nfunc()\nelse:\nraise NotImplementedError(\nf\"The requested function {self.function} is not implemented.\"\n)\n@staticmethod\ndef _delete_objects(bucket: str, objects_paths: list) -&gt; None:\n\"\"\"Delete objects recursively.\n        Params:\n            bucket: name of bucket to perform the delete operation.\n            objects_paths: objects to be deleted.\n        \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\nfor path in objects_paths:\npath = _get_path(bucket, path)\nDBFSFileManager._logger.info(f\"Deleting: {path}\")\ntry:\ndelete_operation = DatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.rm(\npath, True\n)\nif delete_operation:\nDBFSFileManager._logger.info(f\"Deleted: {path}\")\nelse:\nDBFSFileManager._logger.info(f\"Not able to delete: {path}\")\nexcept Exception as e:\nDBFSFileManager._logger.error(f\"Error deleting {path} - {e}\")\nraise e\ndef delete_objects(self) -&gt; None:\n\"\"\"Delete objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be deleted based on the given keys.\n        \"\"\"\nbucket = self.configs[\"bucket\"]\nobjects_paths = self.configs[\"object_paths\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=bucket, object_paths=objects_paths)\nself._logger.info(\"Paths that would be deleted:\")\nself._logger.info(response)\nelse:\nself._delete_objects(bucket, objects_paths)\ndef copy_objects(self) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be copied based on the given keys.\n        \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nself._logger.info(\"Paths that would be copied:\")\nself._logger.info(response)\nelse:\nself._copy_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\n)\n@staticmethod\ndef _copy_objects(\nsource_bucket: str,\nsource_object: str,\ndestination_bucket: str,\ndestination_object: str,\n) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n        Args:\n            source_bucket: name of bucket to perform the copy.\n            source_object: object/folder to be copied.\n            destination_bucket: name of the target bucket to copy.\n            destination_object: target object/folder to copy.\n        \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\ncopy_from = _get_path(source_bucket, source_object)\ncopy_to = _get_path(destination_bucket, destination_object)\nDBFSFileManager._logger.info(f\"Copying: {copy_from} to {copy_to}\")\ntry:\nDatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.cp(\ncopy_from, copy_to, True\n)\nDBFSFileManager._logger.info(f\"Copied: {copy_from} to {copy_to}\")\nexcept Exception as e:\nDBFSFileManager._logger.error(\nf\"Error copying file {copy_from} to {copy_to} - {e}\"\n)\nraise e\ndef move_objects(self) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be moved based on the given keys.\n        \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nself._logger.info(\"Paths that would be moved:\")\nself._logger.info(response)\nelse:\nself._move_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\n)\n@staticmethod\ndef _move_objects(\nsource_bucket: str,\nsource_object: str,\ndestination_bucket: str,\ndestination_object: str,\n) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n        Args:\n            source_bucket: name of bucket to perform the move.\n            source_object: object/folder to be moved.\n            destination_bucket: name of the target bucket to move.\n            destination_object: target object/folder to move.\n        \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\nmove_from = _get_path(source_bucket, source_object)\nmove_to = _get_path(destination_bucket, destination_object)\nDBFSFileManager._logger.info(f\"Moving: {move_from} to {move_to}\")\ntry:\nDatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.mv(\nmove_from, move_to, True\n)\nDBFSFileManager._logger.info(f\"Moved: {move_from} to {move_to}\")\nexcept Exception as e:\nDBFSFileManager._logger.error(\nf\"Error moving file {move_from} to {move_to} - {e}\"\n)\nraise e\n</code></pre>"},{"location":"reference/packages/core/dbfs_file_manager.html#packages.core.dbfs_file_manager.DBFSFileManager.copy_objects","title":"<code>copy_objects()</code>","text":"<p>Copies objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be copied based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/dbfs_file_manager.py</code> <pre><code>def copy_objects(self) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be copied based on the given keys.\n    \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nself._logger.info(\"Paths that would be copied:\")\nself._logger.info(response)\nelse:\nself._copy_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\n)\n</code></pre>"},{"location":"reference/packages/core/dbfs_file_manager.html#packages.core.dbfs_file_manager.DBFSFileManager.delete_objects","title":"<code>delete_objects()</code>","text":"<p>Delete objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be deleted based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/dbfs_file_manager.py</code> <pre><code>def delete_objects(self) -&gt; None:\n\"\"\"Delete objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be deleted based on the given keys.\n    \"\"\"\nbucket = self.configs[\"bucket\"]\nobjects_paths = self.configs[\"object_paths\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=bucket, object_paths=objects_paths)\nself._logger.info(\"Paths that would be deleted:\")\nself._logger.info(response)\nelse:\nself._delete_objects(bucket, objects_paths)\n</code></pre>"},{"location":"reference/packages/core/dbfs_file_manager.html#packages.core.dbfs_file_manager.DBFSFileManager.get_function","title":"<code>get_function()</code>","text":"<p>Get a specific function to execute.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/dbfs_file_manager.py</code> <pre><code>def get_function(self) -&gt; None:\n\"\"\"Get a specific function to execute.\"\"\"\navailable_functions = {\n\"delete_objects\": self.delete_objects,\n\"copy_objects\": self.copy_objects,\n\"move_objects\": self.move_objects,\n}\nself._logger.info(\"Function being executed: {}\".format(self.function))\nif self.function in available_functions.keys():\nfunc = available_functions[self.function]\nfunc()\nelse:\nraise NotImplementedError(\nf\"The requested function {self.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/core/dbfs_file_manager.html#packages.core.dbfs_file_manager.DBFSFileManager.move_objects","title":"<code>move_objects()</code>","text":"<p>Moves objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be moved based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/dbfs_file_manager.py</code> <pre><code>def move_objects(self) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be moved based on the given keys.\n    \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nself._logger.info(\"Paths that would be moved:\")\nself._logger.info(response)\nelse:\nself._move_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html","title":"Definitions","text":"<p>Definitions of standard values and structures for core components.</p>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.CollectEngineUsage","title":"<code>CollectEngineUsage</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Options for collecting engine usage stats.</p> <ul> <li>enabled, enables the collection and storage of Lakehouse Engine usage statistics for any environment.</li> <li>prod_only, enables the collection and storage of Lakehouse Engine usage statistics for production environment only.</li> <li>disabled, disables the collection and storage of Lakehouse Engine usage statistics, for all environments.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class CollectEngineUsage(Enum):\n\"\"\"Options for collecting engine usage stats.\n    - enabled, enables the collection and storage of Lakehouse Engine\n    usage statistics for any environment.\n    - prod_only, enables the collection and storage of Lakehouse Engine\n    usage statistics for production environment only.\n    - disabled, disables the collection and storage of Lakehouse Engine\n    usage statistics, for all environments.\n    \"\"\"\nENABLED = \"enabled\"\nPROD_ONLY = \"prod_only\"\nDISABLED = \"disabled\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQDefaults","title":"<code>DQDefaults</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Defaults used on the data quality process.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class DQDefaults(Enum):\n\"\"\"Defaults used on the data quality process.\"\"\"\nFILE_SYSTEM_STORE = \"file_system\"\nFILE_SYSTEM_S3_STORE = \"s3\"\nDQ_BATCH_IDENTIFIERS = [\"spec_id\", \"input_id\", \"timestamp\"]\nDATASOURCE_CLASS_NAME = \"Datasource\"\nDATASOURCE_EXECUTION_ENGINE = \"SparkDFExecutionEngine\"\nDATA_CONNECTORS_CLASS_NAME = \"RuntimeDataConnector\"\nDATA_CONNECTORS_MODULE_NAME = \"great_expectations.datasource.data_connector\"\nDATA_CHECKPOINTS_CLASS_NAME = \"SimpleCheckpoint\"\nDATA_CHECKPOINTS_CONFIG_VERSION = 1.0\nSTORE_BACKEND = \"s3\"\nEXPECTATIONS_STORE_PREFIX = \"dq/expectations/\"\nVALIDATIONS_STORE_PREFIX = \"dq/validations/\"\nDATA_DOCS_PREFIX = \"dq/data_docs/site/\"\nCHECKPOINT_STORE_PREFIX = \"dq/checkpoints/\"\nVALIDATION_COLUMN_IDENTIFIER = \"validationresultidentifier\"\nCUSTOM_EXPECTATION_LIST = [\n\"expect_column_values_to_be_date_not_older_than\",\n\"expect_column_pair_a_to_be_smaller_or_equal_than_b\",\n\"expect_multicolumn_column_a_must_equal_b_or_c\",\n\"expect_queried_column_agg_value_to_be\",\n\"expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b\",\n\"expect_column_pair_a_to_be_not_equal_to_b\",\n\"expect_column_values_to_not_be_null_or_empty_string\",\n]\nDQ_VALIDATIONS_SCHEMA = StructType(\n[\nStructField(\n\"dq_validations\",\nStructType(\n[\nStructField(\"run_name\", StringType()),\nStructField(\"run_success\", BooleanType()),\nStructField(\"raised_exceptions\", BooleanType()),\nStructField(\"run_row_success\", BooleanType()),\nStructField(\n\"dq_failure_details\",\nArrayType(\nStructType(\n[\nStructField(\"expectation_type\", StringType()),\nStructField(\"kwargs\", StringType()),\n]\n),\n),\n),\n]\n),\n)\n]\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQExecutionPoint","title":"<code>DQExecutionPoint</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Available data quality execution points.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class DQExecutionPoint(Enum):\n\"\"\"Available data quality execution points.\"\"\"\nIN_MOTION = \"in_motion\"\nAT_REST = \"at_rest\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQFunctionSpec","title":"<code>DQFunctionSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Defines a data quality function specification.</p> <ul> <li>function - name of the data quality function (expectation) to execute. It follows the great_expectations api https://greatexpectations.io/expectations/.</li> <li>args - args of the function (expectation). Follow the same api as above.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass DQFunctionSpec(object):\n\"\"\"Defines a data quality function specification.\n    - function - name of the data quality function (expectation) to execute.\n    It follows the great_expectations api https://greatexpectations.io/expectations/.\n    - args - args of the function (expectation). Follow the same api as above.\n    \"\"\"\nfunction: str\nargs: Optional[dict] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQSpec","title":"<code>DQSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Data quality overall specification.</p> <ul> <li>spec_id - id of the specification.</li> <li>input_id - id of the input specification.</li> <li>dq_type - type of DQ process to execute (e.g. validator).</li> <li>dq_functions - list of function specifications to execute.</li> <li>dq_db_table - name of table to derive the dq functions from.</li> <li>dq_table_table_filter - name of the table which rules are to be applied in the     validations (Only used when deriving dq functions).</li> <li>dq_table_extra_filters - extra filters to be used when deriving dq functions.     This is a sql expression to be applied to the dq_db_table.</li> <li>execution_point - execution point of the dq functions. [at_rest, in_motion].     This is set during the load_data or dq_validator functions.</li> <li>unexpected_rows_pk - the list of columns composing the primary key of the     source data to identify the rows failing the DQ validations. Note: only one     of tbl_to_derive_pk or unexpected_rows_pk arguments need to be provided. It     is mandatory to provide one of these arguments when using tag_source_data     as True. When tag_source_data is False, this is not mandatory, but still     recommended.</li> <li>tbl_to_derive_pk - db.table to automatically derive the unexpected_rows_pk from.     Note: only one of tbl_to_derive_pk or unexpected_rows_pk arguments need to     be provided. It is mandatory to provide one of these arguments when using     tag_source_data as True. hen tag_source_data is False, this is not     mandatory, but still recommended.</li> <li>sort_processed_keys - when using the <code>prisma</code> <code>dq_type</code>, a column <code>processed_keys</code>     is automatically added to give observability over the PK values that were     processed during a run. This parameter (<code>sort_processed_keys</code>) controls whether     the processed keys column value should be sorted or not. Default: False.</li> <li>gx_result_format - great expectations result format. Default: \"COMPLETE\".</li> <li>tag_source_data - when set to true, this will ensure that the DQ process ends by     tagging the source data with an additional column with information about the     DQ results. This column makes it possible to identify if the DQ run was     succeeded in general and, if not, it unlocks the insights to know what     specific rows have made the DQ validations fail and why. Default: False.     Note: it only works if result_sink_explode is True, gx_result_format is     COMPLETE, fail_on_error is False (which is done automatically when     you specify tag_source_data as True) and tbl_to_derive_pk or     unexpected_rows_pk is configured.</li> <li>store_backend - which store_backend to use (e.g. s3 or file_system).</li> <li>local_fs_root_dir - path of the root directory. Note: only applicable for     store_backend file_system.</li> <li>data_docs_local_fs - the path for data docs only for store_backend     file_system.</li> <li>bucket - the bucket name to consider for the store_backend (store DQ artefacts).     Note: only applicable for store_backend s3.</li> <li>data_docs_bucket - the bucket name for data docs only. When defined, it will     supersede bucket parameter. Note: only applicable for store_backend s3.</li> <li>expectations_store_prefix - prefix where to store expectations' data. Note: only     applicable for store_backend s3.</li> <li>validations_store_prefix - prefix where to store validations' data. Note: only     applicable for store_backend s3.</li> <li>data_docs_prefix - prefix where to store data_docs' data.</li> <li>checkpoint_store_prefix - prefix where to store checkpoints' data. Note: only     applicable for store_backend s3.</li> <li>data_asset_name - name of the data asset to consider when configuring the great     expectations' data source.</li> <li>expectation_suite_name - name to consider for great expectations' suite.</li> <li>result_sink_db_table - db.table_name indicating the database and table in which     to save the results of the DQ process.</li> <li>result_sink_location - file system location in which to save the results of the     DQ process.</li> <li>data_product_name - name of the data product.</li> <li>result_sink_partitions - the list of partitions to consider.</li> <li>result_sink_format - format of the result table (e.g. delta, parquet, kafka...).</li> <li>result_sink_options - extra spark options for configuring the result sink.     E.g: can be used to configure a Kafka sink if result_sink_format is kafka.</li> <li>result_sink_explode - flag to determine if the output table/location should have     the columns exploded (as True) or not (as False). Default: True.</li> <li>result_sink_extra_columns - list of extra columns to be exploded (following     the pattern \".*\") or columns to be selected. It is only used when     result_sink_explode is set to True. <li>source - name of data source, to be easier to identify in analysis. If not     specified, it is set as default . This will be only used     when result_sink_explode is set to True. <li>fail_on_error - whether to fail the algorithm if the validations of your data in     the DQ process failed.</li> <li>cache_df - whether to cache the dataframe before running the DQ process or not.</li> <li>critical_functions - functions that should not fail. When this argument is     defined, fail_on_error is nullified.</li> <li>max_percentage_failure - percentage of failure that should be allowed.     This argument has priority over both fail_on_error and critical_functions.</li> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass DQSpec(object):\n\"\"\"Data quality overall specification.\n    - spec_id - id of the specification.\n    - input_id - id of the input specification.\n    - dq_type - type of DQ process to execute (e.g. validator).\n    - dq_functions - list of function specifications to execute.\n    - dq_db_table - name of table to derive the dq functions from.\n    - dq_table_table_filter - name of the table which rules are to be applied in the\n        validations (Only used when deriving dq functions).\n    - dq_table_extra_filters - extra filters to be used when deriving dq functions.\n        This is a sql expression to be applied to the dq_db_table.\n    - execution_point - execution point of the dq functions. [at_rest, in_motion].\n        This is set during the load_data or dq_validator functions.\n    - unexpected_rows_pk - the list of columns composing the primary key of the\n        source data to identify the rows failing the DQ validations. Note: only one\n        of tbl_to_derive_pk or unexpected_rows_pk arguments need to be provided. It\n        is mandatory to provide one of these arguments when using tag_source_data\n        as True. When tag_source_data is False, this is not mandatory, but still\n        recommended.\n    - tbl_to_derive_pk - db.table to automatically derive the unexpected_rows_pk from.\n        Note: only one of tbl_to_derive_pk or unexpected_rows_pk arguments need to\n        be provided. It is mandatory to provide one of these arguments when using\n        tag_source_data as True. hen tag_source_data is False, this is not\n        mandatory, but still recommended.\n    - sort_processed_keys - when using the `prisma` `dq_type`, a column `processed_keys`\n        is automatically added to give observability over the PK values that were\n        processed during a run. This parameter (`sort_processed_keys`) controls whether\n        the processed keys column value should be sorted or not. Default: False.\n    - gx_result_format - great expectations result format. Default: \"COMPLETE\".\n    - tag_source_data - when set to true, this will ensure that the DQ process ends by\n        tagging the source data with an additional column with information about the\n        DQ results. This column makes it possible to identify if the DQ run was\n        succeeded in general and, if not, it unlocks the insights to know what\n        specific rows have made the DQ validations fail and why. Default: False.\n        Note: it only works if result_sink_explode is True, gx_result_format is\n        COMPLETE, fail_on_error is False (which is done automatically when\n        you specify tag_source_data as True) and tbl_to_derive_pk or\n        unexpected_rows_pk is configured.\n    - store_backend - which store_backend to use (e.g. s3 or file_system).\n    - local_fs_root_dir - path of the root directory. Note: only applicable for\n        store_backend file_system.\n    - data_docs_local_fs - the path for data docs only for store_backend\n        file_system.\n    - bucket - the bucket name to consider for the store_backend (store DQ artefacts).\n        Note: only applicable for store_backend s3.\n    - data_docs_bucket - the bucket name for data docs only. When defined, it will\n        supersede bucket parameter. Note: only applicable for store_backend s3.\n    - expectations_store_prefix - prefix where to store expectations' data. Note: only\n        applicable for store_backend s3.\n    - validations_store_prefix - prefix where to store validations' data. Note: only\n        applicable for store_backend s3.\n    - data_docs_prefix - prefix where to store data_docs' data.\n    - checkpoint_store_prefix - prefix where to store checkpoints' data. Note: only\n        applicable for store_backend s3.\n    - data_asset_name - name of the data asset to consider when configuring the great\n        expectations' data source.\n    - expectation_suite_name - name to consider for great expectations' suite.\n    - result_sink_db_table - db.table_name indicating the database and table in which\n        to save the results of the DQ process.\n    - result_sink_location - file system location in which to save the results of the\n        DQ process.\n    - data_product_name - name of the data product.\n    - result_sink_partitions - the list of partitions to consider.\n    - result_sink_format - format of the result table (e.g. delta, parquet, kafka...).\n    - result_sink_options - extra spark options for configuring the result sink.\n        E.g: can be used to configure a Kafka sink if result_sink_format is kafka.\n    - result_sink_explode - flag to determine if the output table/location should have\n        the columns exploded (as True) or not (as False). Default: True.\n    - result_sink_extra_columns - list of extra columns to be exploded (following\n        the pattern \"&lt;name&gt;.*\") or columns to be selected. It is only used when\n        result_sink_explode is set to True.\n    - source - name of data source, to be easier to identify in analysis. If not\n        specified, it is set as default &lt;input_id&gt;. This will be only used\n        when result_sink_explode is set to True.\n    - fail_on_error - whether to fail the algorithm if the validations of your data in\n        the DQ process failed.\n    - cache_df - whether to cache the dataframe before running the DQ process or not.\n    - critical_functions - functions that should not fail. When this argument is\n        defined, fail_on_error is nullified.\n    - max_percentage_failure - percentage of failure that should be allowed.\n        This argument has priority over both fail_on_error and critical_functions.\n    \"\"\"\nspec_id: str\ninput_id: str\ndq_type: str\ndq_functions: Optional[List[DQFunctionSpec]] = None\ndq_db_table: Optional[str] = None\ndq_table_table_filter: Optional[str] = None\ndq_table_extra_filters: Optional[str] = None\nexecution_point: Optional[str] = None\nunexpected_rows_pk: Optional[List[str]] = None\ntbl_to_derive_pk: Optional[str] = None\nsort_processed_keys: Optional[bool] = False\ngx_result_format: Optional[str] = \"COMPLETE\"\ntag_source_data: Optional[bool] = False\nstore_backend: str = DQDefaults.STORE_BACKEND.value\nlocal_fs_root_dir: Optional[str] = None\ndata_docs_local_fs: Optional[str] = None\nbucket: Optional[str] = None\ndata_docs_bucket: Optional[str] = None\nexpectations_store_prefix: str = DQDefaults.EXPECTATIONS_STORE_PREFIX.value\nvalidations_store_prefix: str = DQDefaults.VALIDATIONS_STORE_PREFIX.value\ndata_docs_prefix: str = DQDefaults.DATA_DOCS_PREFIX.value\ncheckpoint_store_prefix: str = DQDefaults.CHECKPOINT_STORE_PREFIX.value\ndata_asset_name: Optional[str] = None\nexpectation_suite_name: Optional[str] = None\nresult_sink_db_table: Optional[str] = None\nresult_sink_location: Optional[str] = None\ndata_product_name: Optional[str] = None\nresult_sink_partitions: Optional[List[str]] = None\nresult_sink_format: str = OutputFormat.DELTAFILES.value\nresult_sink_options: Optional[dict] = None\nresult_sink_explode: bool = True\nresult_sink_extra_columns: Optional[List[str]] = None\nsource: Optional[str] = None\nfail_on_error: bool = True\ncache_df: bool = False\ncritical_functions: Optional[List[DQFunctionSpec]] = None\nmax_percentage_failure: Optional[float] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQTableBaseParameters","title":"<code>DQTableBaseParameters</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Base parameters for importing DQ rules from a table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class DQTableBaseParameters(Enum):\n\"\"\"Base parameters for importing DQ rules from a table.\"\"\"\nPRISMA_BASE_PARAMETERS = [\"arguments\", \"dq_tech_function\"]\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQType","title":"<code>DQType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Available data quality tasks.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class DQType(Enum):\n\"\"\"Available data quality tasks.\"\"\"\nVALIDATOR = \"validator\"\nPRISMA = \"prisma\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.DQValidatorSpec","title":"<code>DQValidatorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Data Quality Validator Specification.</p> <ul> <li>input_spec: input specification of the data to be checked/validated.</li> <li>dq_spec: data quality specification.</li> <li>restore_prev_version: specify if, having     delta table/files as input, they should be restored to the     previous version if the data quality process fails. Note: this     is only considered if fail_on_error is kept as True.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass DQValidatorSpec(object):\n\"\"\"Data Quality Validator Specification.\n    - input_spec: input specification of the data to be checked/validated.\n    - dq_spec: data quality specification.\n    - restore_prev_version: specify if, having\n        delta table/files as input, they should be restored to the\n        previous version if the data quality process fails. Note: this\n        is only considered if fail_on_error is kept as True.\n    \"\"\"\ninput_spec: InputSpec\ndq_spec: DQSpec\nrestore_prev_version: Optional[bool] = False\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.EngineConfig","title":"<code>EngineConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Definitions that can come from the Engine Config file.</p> <ul> <li>dq_bucket: S3 prod bucket used to store data quality related artifacts.</li> <li>dq_dev_bucket: S3 dev bucket used to store data quality related artifacts.</li> <li>notif_disallowed_email_servers: email servers not allowed to be used     for sending notifications.</li> <li>engine_usage_path: path where the engine prod usage stats are stored.</li> <li>engine_dev_usage_path: path where the engine dev usage stats are stored.</li> <li>collect_engine_usage: whether to enable the collection of lakehouse     engine usage stats or not.</li> <li>dq_functions_column_list: list of columns to be added to the meta argument     of GX when using PRISMA.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass EngineConfig(object):\n\"\"\"Definitions that can come from the Engine Config file.\n    - dq_bucket: S3 prod bucket used to store data quality related artifacts.\n    - dq_dev_bucket: S3 dev bucket used to store data quality related artifacts.\n    - notif_disallowed_email_servers: email servers not allowed to be used\n        for sending notifications.\n    - engine_usage_path: path where the engine prod usage stats are stored.\n    - engine_dev_usage_path: path where the engine dev usage stats are stored.\n    - collect_engine_usage: whether to enable the collection of lakehouse\n        engine usage stats or not.\n    - dq_functions_column_list: list of columns to be added to the meta argument\n        of GX when using PRISMA.\n    \"\"\"\ndq_bucket: Optional[str] = None\ndq_dev_bucket: Optional[str] = None\nnotif_disallowed_email_servers: Optional[list] = None\nengine_usage_path: Optional[str] = None\nengine_dev_usage_path: Optional[str] = None\ncollect_engine_usage: str = CollectEngineUsage.ENABLED.value\ndq_functions_column_list: Optional[list] = None\nsharepoint_authority: Optional[str] = None\nsharepoint_company_domain: Optional[str] = None\nsharepoint_api_domain: Optional[str] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.EngineStats","title":"<code>EngineStats</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Definitions for collection of Lakehouse Engine Stats.</p> <p>Note</p> <p>whenever the value comes from a key inside a Spark Config that returns an array, it can be specified with a '#' so that it is adequately processed.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class EngineStats(Enum):\n\"\"\"Definitions for collection of Lakehouse Engine Stats.\n    !!! note\n        whenever the value comes from a key inside a Spark Config\n        that returns an array, it can be specified with a '#' so that it\n        is adequately processed.\n    \"\"\"\nCLUSTER_USAGE_TAGS = \"spark.databricks.clusterUsageTags\"\nDEF_SPARK_CONFS = {\n\"dp_name\": f\"{CLUSTER_USAGE_TAGS}.clusterAllTags#accountName\",\n\"environment\": f\"{CLUSTER_USAGE_TAGS}.clusterAllTags#environment\",\n\"workspace_id\": f\"{CLUSTER_USAGE_TAGS}.orgId\",\n\"job_id\": f\"{CLUSTER_USAGE_TAGS}.clusterAllTags#JobId\",\n\"job_name\": f\"{CLUSTER_USAGE_TAGS}.clusterAllTags#RunName\",\n\"run_id\": f\"{CLUSTER_USAGE_TAGS}.clusterAllTags#ClusterName\",\n}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.FileManagerAPIKeys","title":"<code>FileManagerAPIKeys</code>","text":"<p>         Bases: <code>Enum</code></p> <p>File Manager s3 api keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class FileManagerAPIKeys(Enum):\n\"\"\"File Manager s3 api keys.\"\"\"\nCONTENTS = \"Contents\"\nKEY = \"Key\"\nCONTINUATION = \"NextContinuationToken\"\nBUCKET = \"Bucket\"\nOBJECTS = \"Objects\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABCadence","title":"<code>GABCadence</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Representation of the supported cadences on GAB.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class GABCadence(Enum):\n\"\"\"Representation of the supported cadences on GAB.\"\"\"\nDAY = 1\nWEEK = 2\nMONTH = 3\nQUARTER = 4\nYEAR = 5\n@classmethod\ndef get_ordered_cadences(cls) -&gt; dict:\n\"\"\"Get the cadences ordered by the value.\n        Returns:\n            dict containing ordered cadences as `{name:value}`.\n        \"\"\"\ncadences = list(GABCadence)\nreturn {\ncadence.name: cadence.value\nfor cadence in sorted(cadences, key=lambda gab_cadence: gab_cadence.value)\n}\n@classmethod\ndef get_cadences(cls) -&gt; set[str]:\n\"\"\"Get the cadences values as set.\n        Returns:\n            set containing all possible cadence values as `{value}`.\n        \"\"\"\nreturn {cadence.name for cadence in list(GABCadence)}\n@classmethod\ndef order_cadences(cls, cadences_to_order: list[str]) -&gt; list[str]:\n\"\"\"Order a list of cadences by value.\n        Returns:\n            ordered set containing the received cadences.\n        \"\"\"\nreturn sorted(\ncadences_to_order,\nkey=lambda item: cls.get_ordered_cadences().get(item),  # type: ignore\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABCadence.get_cadences","title":"<code>get_cadences()</code>  <code>classmethod</code>","text":"<p>Get the cadences values as set.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set containing all possible cadence values as <code>{value}</code>.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef get_cadences(cls) -&gt; set[str]:\n\"\"\"Get the cadences values as set.\n    Returns:\n        set containing all possible cadence values as `{value}`.\n    \"\"\"\nreturn {cadence.name for cadence in list(GABCadence)}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABCadence.get_ordered_cadences","title":"<code>get_ordered_cadences()</code>  <code>classmethod</code>","text":"<p>Get the cadences ordered by the value.</p> <p>Returns:</p> Type Description <code>dict</code> <p>dict containing ordered cadences as <code>{name:value}</code>.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef get_ordered_cadences(cls) -&gt; dict:\n\"\"\"Get the cadences ordered by the value.\n    Returns:\n        dict containing ordered cadences as `{name:value}`.\n    \"\"\"\ncadences = list(GABCadence)\nreturn {\ncadence.name: cadence.value\nfor cadence in sorted(cadences, key=lambda gab_cadence: gab_cadence.value)\n}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABCadence.order_cadences","title":"<code>order_cadences(cadences_to_order)</code>  <code>classmethod</code>","text":"<p>Order a list of cadences by value.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>ordered set containing the received cadences.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef order_cadences(cls, cadences_to_order: list[str]) -&gt; list[str]:\n\"\"\"Order a list of cadences by value.\n    Returns:\n        ordered set containing the received cadences.\n    \"\"\"\nreturn sorted(\ncadences_to_order,\nkey=lambda item: cls.get_ordered_cadences().get(item),  # type: ignore\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABCombinedConfiguration","title":"<code>GABCombinedConfiguration</code>","text":"<p>         Bases: <code>Enum</code></p> <p>GAB combined configuration.</p> <p>Based on the use case configuration return the values to override in the SQL file. This enum aims to exhaustively map each combination of <code>cadence</code>, <code>reconciliation</code>,     <code>week_start</code> and <code>snap_flag</code> return the corresponding values <code>join_select</code>,     <code>project_start</code> and <code>project_end</code> to replace this values in the stages SQL file.</p> <p>Return corresponding configuration (join_select, project_start, project_end) for     each combination (cadence x recon x week_start x snap_flag).</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class GABCombinedConfiguration(Enum):\n\"\"\"GAB combined configuration.\n    Based on the use case configuration return the values to override in the SQL file.\n    This enum aims to exhaustively map each combination of `cadence`, `reconciliation`,\n        `week_start` and `snap_flag` return the corresponding values `join_select`,\n        `project_start` and `project_end` to replace this values in the stages SQL file.\n    Return corresponding configuration (join_select, project_start, project_end) for\n        each combination (cadence x recon x week_start x snap_flag).\n    \"\"\"\n_PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE = (\n\"date(date_trunc('${cad}',${date_column}))\"\n)\n_DEFAULT_PROJECT_START = \"df_cal.cadence_start_date\"\n_DEFAULT_PROJECT_END = \"df_cal.cadence_end_date\"\nCOMBINED_CONFIGURATION = {\n# Combination of:\n# - cadence: `DAY`\n# - reconciliation_window: `DAY`, `WEEK`, `MONTH`, `QUARTER`, `YEAR`\n# - week_start: `S`, `M`\n# - snapshot_flag: `Y`, `N`\n1: {\n\"cadence\": GABCadence.DAY.name,\n\"recon\": GABCadence.get_cadences(),\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": {\"Y\", \"N\"},\n\"join_select\": \"\",\n\"project_start\": _PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE,\n\"project_end\": _PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE,\n},\n# Combination of:\n# - cadence: `WEEK`\n# - reconciliation_window: `DAY`\n# - week_start: `S`, `M`\n# - snapshot_flag: `Y`\n2: {\n\"cadence\": GABCadence.WEEK.name,\n\"recon\": GABCadence.DAY.name,\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct case\n                when '${config_week_start}' = 'Monday' then weekstart_mon\n                when '${config_week_start}' = 'Sunday' then weekstart_sun\n            end as cadence_start_date,\n            calendar_date as cadence_end_date\n        \"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n# Combination of:\n# - cadence: `WEEK`\n# - reconciliation_window: `DAY, `MONTH`, `QUARTER`, `YEAR`\n# - week_start: `M`\n# - snapshot_flag: `Y`, `N`\n3: {\n\"cadence\": GABCadence.WEEK.name,\n\"recon\": {\nGABCadence.DAY.name,\nGABCadence.MONTH.name,\nGABCadence.QUARTER.name,\nGABCadence.YEAR.name,\n},\n\"week_start\": \"M\",\n\"snap_flag\": {\"Y\", \"N\"},\n\"join_select\": \"\"\"\n            select distinct case\n                when '${config_week_start}'  = 'Monday' then weekstart_mon\n                when '${config_week_start}' = 'Sunday' then weekstart_sun\n            end as cadence_start_date,\n            case\n                when '${config_week_start}' = 'Monday' then weekend_mon\n                when '${config_week_start}' = 'Sunday' then weekend_sun\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n4: {\n\"cadence\": GABCadence.MONTH.name,\n\"recon\": GABCadence.DAY.name,\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct month_start as cadence_start_date,\n            calendar_date as cadence_end_date\n        \"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n5: {\n\"cadence\": GABCadence.MONTH.name,\n\"recon\": GABCadence.WEEK.name,\n\"week_start\": GABStartOfWeek.MONDAY.value,\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct month_start as cadence_start_date,\n            case\n                when date(\n                    date_trunc('MONTH',add_months(calendar_date, 1))\n                )-1 &lt; weekend_mon\n                    then date(date_trunc('MONTH',add_months(calendar_date, 1)))-1\n                else weekend_mon\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n6: {\n\"cadence\": GABCadence.MONTH.name,\n\"recon\": GABCadence.WEEK.name,\n\"week_start\": GABStartOfWeek.SUNDAY.value,\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct month_start as cadence_start_date,\n            case\n                when date(\n                    date_trunc('MONTH',add_months(calendar_date, 1))\n                )-1 &lt; weekend_sun\n                    then date(date_trunc('MONTH',add_months(calendar_date, 1)))-1\n                else weekend_sun\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n7: {\n\"cadence\": GABCadence.MONTH.name,\n\"recon\": GABCadence.get_cadences(),\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": {\"Y\", \"N\"},\n\"join_select\": \"\",\n\"project_start\": _PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE,\n\"project_end\": \"date(date_trunc('MONTH',add_months(${date_column}, 1)))-1\",\n},\n8: {\n\"cadence\": GABCadence.QUARTER.name,\n\"recon\": GABCadence.DAY.name,\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct quarter_start as cadence_start_date,\n            calendar_date as cadence_end_date\n        \"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n9: {\n\"cadence\": GABCadence.QUARTER.name,\n\"recon\": GABCadence.WEEK.name,\n\"week_start\": GABStartOfWeek.MONDAY.value,\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct quarter_start as cadence_start_date,\n            case\n                when weekend_mon &gt; date(\n                    date_trunc('QUARTER',add_months(calendar_date, 3))\n                )-1\n                    then date(date_trunc('QUARTER',add_months(calendar_date, 3)))-1\n                else weekend_mon\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n10: {\n\"cadence\": GABCadence.QUARTER.name,\n\"recon\": GABCadence.WEEK.name,\n\"week_start\": GABStartOfWeek.SUNDAY.value,\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct quarter_start as cadence_start_date,\n            case\n                when weekend_sun &gt; date(\n                    date_trunc('QUARTER',add_months(calendar_date, 3))\n                )-1\n                    then date(date_trunc('QUARTER',add_months(calendar_date, 3)))-1\n                else weekend_sun\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n11: {\n\"cadence\": GABCadence.QUARTER.name,\n\"recon\": GABCadence.MONTH.name,\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct quarter_start as cadence_start_date,\n            month_end as cadence_end_date\n        \"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n12: {\n\"cadence\": GABCadence.QUARTER.name,\n\"recon\": GABCadence.YEAR.name,\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"N\",\n\"join_select\": \"\",\n\"project_start\": _PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE,\n\"project_end\": \"\"\"\n            date(\n                date_trunc(\n                    '${cad}',add_months(date(date_trunc('${cad}',${date_column})), 3)\n                )\n            )-1\n        \"\"\",\n},\n13: {\n\"cadence\": GABCadence.QUARTER.name,\n\"recon\": GABCadence.get_cadences(),\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"N\",\n\"join_select\": \"\",\n\"project_start\": _PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE,\n\"project_end\": \"\"\"\n            date(\n                date_trunc(\n                    '${cad}',add_months( date(date_trunc('${cad}',${date_column})), 3)\n                )\n            )-1\n        \"\"\",\n},\n14: {\n\"cadence\": GABCadence.YEAR.name,\n\"recon\": GABCadence.WEEK.name,\n\"week_start\": GABStartOfWeek.MONDAY.value,\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct year_start as cadence_start_date,\n            case\n                when weekend_mon &gt; date(\n                    date_trunc('YEAR',add_months(calendar_date, 12))\n                )-1\n                    then date(date_trunc('YEAR',add_months(calendar_date, 12)))-1\n                else weekend_mon\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n15: {\n\"cadence\": GABCadence.YEAR.name,\n\"recon\": GABCadence.WEEK.name,\n\"week_start\": GABStartOfWeek.SUNDAY.value,\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct year_start as cadence_start_date,\n            case\n                when weekend_sun &gt; date(\n                    date_trunc('YEAR',add_months(calendar_date, 12))\n                )-1\n                    then date(date_trunc('YEAR',add_months(calendar_date, 12)))-1\n                else weekend_sun\n            end as cadence_end_date\"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n16: {\n\"cadence\": GABCadence.YEAR.name,\n\"recon\": GABCadence.get_cadences(),\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"N\",\n\"inverse_flag\": \"Y\",\n\"join_select\": \"\",\n\"project_start\": _PROJECT_DATE_COLUMN_TRUNCATED_BY_CADENCE,\n\"project_end\": \"\"\"\n            date(\n                date_trunc(\n                    '${cad}',add_months(date(date_trunc('${cad}',${date_column})), 12)\n                )\n            )-1\n        \"\"\",\n},\n17: {\n\"cadence\": GABCadence.YEAR.name,\n\"recon\": {\nGABCadence.DAY.name,\nGABCadence.MONTH.name,\nGABCadence.QUARTER.name,\n},\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": \"Y\",\n\"join_select\": \"\"\"\n            select distinct year_start as cadence_start_date,\n            case\n                when '${rec_cadence}' = 'DAY' then calendar_date\n                when '${rec_cadence}' = 'MONTH' then month_end\n                when '${rec_cadence}' = 'QUARTER' then quarter_end\n            end as cadence_end_date\n        \"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n18: {\n\"cadence\": GABCadence.get_cadences(),\n\"recon\": GABCadence.get_cadences(),\n\"week_start\": GABStartOfWeek.get_values(),\n\"snap_flag\": {\"Y\", \"N\"},\n\"join_select\": \"\"\"\n            select distinct\n            case\n                when '${cad}' = 'WEEK' and '${config_week_start}' = 'Monday'\n                    then weekstart_mon\n                when  '${cad}' = 'WEEK' and '${config_week_start}' = 'Sunday'\n                    then weekstart_sun\n                else\n                    date(date_trunc('${cad}',calendar_date))\n            end as cadence_start_date,\n            case\n                when '${cad}' = 'WEEK' and '${config_week_start}' = 'Monday'\n                    then weekend_mon\n                when  '${cad}' = 'WEEK' and '${config_week_start}' = 'Sunday'\n                    then weekend_sun\n                when '${cad}' = 'DAY'\n                    then date(date_trunc('${cad}',calendar_date))\n                when '${cad}' = 'MONTH'\n                    then date(\n                        date_trunc(\n                            'MONTH',\n                            add_months(date(date_trunc('${cad}',calendar_date)), 1)\n                        )\n                    )-1\n                when '${cad}' = 'QUARTER'\n                    then date(\n                        date_trunc(\n                            'QUARTER',\n                            add_months(date(date_trunc('${cad}',calendar_date)) , 3)\n                        )\n                    )-1\n                when '${cad}' = 'YEAR'\n                    then date(\n                        date_trunc(\n                            'YEAR',\n                            add_months(date(date_trunc('${cad}',calendar_date)), 12)\n                        )\n                    )-1\n            end as cadence_end_date\n        \"\"\",\n\"project_start\": _DEFAULT_PROJECT_START,\n\"project_end\": _DEFAULT_PROJECT_END,\n},\n}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABDefaults","title":"<code>GABDefaults</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Defaults used on the GAB process.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class GABDefaults(Enum):\n\"\"\"Defaults used on the GAB process.\"\"\"\nDATE_FORMAT = \"%Y-%m-%d\"\nDIMENSIONS_DEFAULT_COLUMNS = [\"from_date\", \"to_date\"]\nDEFAULT_DIMENSION_CALENDAR_TABLE = \"dim_calendar\"\nDEFAULT_LOOKUP_QUERY_BUILDER_TABLE = \"lkp_query_builder\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABKeys","title":"<code>GABKeys</code>","text":"<p>Constants used to update pre-configured gab dict key.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class GABKeys:\n\"\"\"Constants used to update pre-configured gab dict key.\"\"\"\nJOIN_SELECT = \"join_select\"\nPROJECT_START = \"project_start\"\nPROJECT_END = \"project_end\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABReplaceableKeys","title":"<code>GABReplaceableKeys</code>","text":"<p>Constants used to replace pre-configured gab dict values.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class GABReplaceableKeys:\n\"\"\"Constants used to replace pre-configured gab dict values.\"\"\"\nCADENCE = \"${cad}\"\nDATE_COLUMN = \"${date_column}\"\nCONFIG_WEEK_START = \"${config_week_start}\"\nRECONCILIATION_CADENCE = \"${rec_cadence}\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABSpec","title":"<code>GABSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Gab Specification.</p> <ul> <li>query_label_filter: query use-case label to execute.</li> <li>queue_filter: queue to execute the job.</li> <li>cadence_filter: selected cadences to build the asset.</li> <li>target_database: target database to write.</li> <li>curr_date: current date.</li> <li>start_date: period start date.</li> <li>end_date: period end date.</li> <li>rerun_flag: rerun flag.</li> <li>target_table: target table to write.</li> <li>source_database: source database.</li> <li>gab_base_path: base path to read the use cases.</li> <li>lookup_table: gab configuration table.</li> <li>calendar_table: gab calendar table.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass GABSpec(object):\n\"\"\"Gab Specification.\n    - query_label_filter: query use-case label to execute.\n    - queue_filter: queue to execute the job.\n    - cadence_filter: selected cadences to build the asset.\n    - target_database: target database to write.\n    - curr_date: current date.\n    - start_date: period start date.\n    - end_date: period end date.\n    - rerun_flag: rerun flag.\n    - target_table: target table to write.\n    - source_database: source database.\n    - gab_base_path: base path to read the use cases.\n    - lookup_table: gab configuration table.\n    - calendar_table: gab calendar table.\n    \"\"\"\nquery_label_filter: list[str]\nqueue_filter: list[str]\ncadence_filter: list[str]\ntarget_database: str\ncurrent_date: datetime\nstart_date: datetime\nend_date: datetime\nrerun_flag: str\ntarget_table: str\nsource_database: str\ngab_base_path: str\nlookup_table: str\ncalendar_table: str\n@classmethod\ndef create_from_acon(cls, acon: dict):  # type: ignore\n\"\"\"Create GabSpec from acon.\n        Args:\n            acon: gab ACON.\n        \"\"\"\nlookup_table = f\"{acon['source_database']}.\" + (\nacon.get(\n\"lookup_table\", GABDefaults.DEFAULT_LOOKUP_QUERY_BUILDER_TABLE.value\n)\n)\ncalendar_table = f\"{acon['source_database']}.\" + (\nacon.get(\n\"calendar_table\", GABDefaults.DEFAULT_DIMENSION_CALENDAR_TABLE.value\n)\n)\ndef format_date(date_to_format: Union[datetime, str]) -&gt; datetime:\nif isinstance(date_to_format, str):\nreturn datetime.strptime(date_to_format, GABDefaults.DATE_FORMAT.value)\nelse:\nreturn date_to_format\nreturn cls(\nquery_label_filter=acon[\"query_label_filter\"],\nqueue_filter=acon[\"queue_filter\"],\ncadence_filter=acon[\"cadence_filter\"],\ntarget_database=acon[\"target_database\"],\ncurrent_date=datetime.now(),\nstart_date=format_date(acon[\"start_date\"]),\nend_date=format_date(acon[\"end_date\"]),\nrerun_flag=acon[\"rerun_flag\"],\ntarget_table=acon[\"target_table\"],\nsource_database=acon[\"source_database\"],\ngab_base_path=acon[\"gab_base_path\"],\nlookup_table=lookup_table,\ncalendar_table=calendar_table,\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABSpec.create_from_acon","title":"<code>create_from_acon(acon)</code>  <code>classmethod</code>","text":"<p>Create GabSpec from acon.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>gab ACON.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef create_from_acon(cls, acon: dict):  # type: ignore\n\"\"\"Create GabSpec from acon.\n    Args:\n        acon: gab ACON.\n    \"\"\"\nlookup_table = f\"{acon['source_database']}.\" + (\nacon.get(\n\"lookup_table\", GABDefaults.DEFAULT_LOOKUP_QUERY_BUILDER_TABLE.value\n)\n)\ncalendar_table = f\"{acon['source_database']}.\" + (\nacon.get(\n\"calendar_table\", GABDefaults.DEFAULT_DIMENSION_CALENDAR_TABLE.value\n)\n)\ndef format_date(date_to_format: Union[datetime, str]) -&gt; datetime:\nif isinstance(date_to_format, str):\nreturn datetime.strptime(date_to_format, GABDefaults.DATE_FORMAT.value)\nelse:\nreturn date_to_format\nreturn cls(\nquery_label_filter=acon[\"query_label_filter\"],\nqueue_filter=acon[\"queue_filter\"],\ncadence_filter=acon[\"cadence_filter\"],\ntarget_database=acon[\"target_database\"],\ncurrent_date=datetime.now(),\nstart_date=format_date(acon[\"start_date\"]),\nend_date=format_date(acon[\"end_date\"]),\nrerun_flag=acon[\"rerun_flag\"],\ntarget_table=acon[\"target_table\"],\nsource_database=acon[\"source_database\"],\ngab_base_path=acon[\"gab_base_path\"],\nlookup_table=lookup_table,\ncalendar_table=calendar_table,\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABStartOfWeek","title":"<code>GABStartOfWeek</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Representation of start of week values on GAB.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class GABStartOfWeek(Enum):\n\"\"\"Representation of start of week values on GAB.\"\"\"\nSUNDAY = \"S\"\nMONDAY = \"M\"\n@classmethod\ndef get_start_of_week(cls) -&gt; dict:\n\"\"\"Get the start of week enum as a dict.\n        Returns:\n            dict containing all enum entries as `{name:value}`.\n        \"\"\"\nreturn {\nstart_of_week.name: start_of_week.value\nfor start_of_week in list(GABStartOfWeek)\n}\n@classmethod\ndef get_values(cls) -&gt; set[str]:\n\"\"\"Get the start of week enum values as set.\n        Returns:\n            set containing all possible values `{value}`.\n        \"\"\"\nreturn {start_of_week.value for start_of_week in list(GABStartOfWeek)}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABStartOfWeek.get_start_of_week","title":"<code>get_start_of_week()</code>  <code>classmethod</code>","text":"<p>Get the start of week enum as a dict.</p> <p>Returns:</p> Type Description <code>dict</code> <p>dict containing all enum entries as <code>{name:value}</code>.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef get_start_of_week(cls) -&gt; dict:\n\"\"\"Get the start of week enum as a dict.\n    Returns:\n        dict containing all enum entries as `{name:value}`.\n    \"\"\"\nreturn {\nstart_of_week.name: start_of_week.value\nfor start_of_week in list(GABStartOfWeek)\n}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.GABStartOfWeek.get_values","title":"<code>get_values()</code>  <code>classmethod</code>","text":"<p>Get the start of week enum values as set.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set containing all possible values <code>{value}</code>.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef get_values(cls) -&gt; set[str]:\n\"\"\"Get the start of week enum values as set.\n    Returns:\n        set containing all possible values `{value}`.\n    \"\"\"\nreturn {start_of_week.value for start_of_week in list(GABStartOfWeek)}\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.InputFormat","title":"<code>InputFormat</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Formats of algorithm input.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class InputFormat(Enum):\n\"\"\"Formats of algorithm input.\"\"\"\nJDBC = \"jdbc\"\nAVRO = \"avro\"\nJSON = \"json\"\nCSV = \"csv\"\nPARQUET = \"parquet\"\nDELTAFILES = \"delta\"\nCLOUDFILES = \"cloudfiles\"\nKAFKA = \"kafka\"\nSQL = \"sql\"\nSAP_BW = \"sap_bw\"\nSAP_B4 = \"sap_b4\"\nDATAFRAME = \"dataframe\"\nSFTP = \"sftp\"\n@classmethod\ndef values(cls):  # type: ignore\n\"\"\"Generates a list containing all enum values.\n        Returns:\n            A list with all enum values.\n        \"\"\"\nreturn (c.value for c in cls)\n@classmethod\ndef exists(cls, input_format: str) -&gt; bool:\n\"\"\"Checks if the input format exists in the enum values.\n        Args:\n            input_format: format to check if exists.\n        Returns:\n            If the input format exists in our enum.\n        \"\"\"\nreturn input_format in cls.values()\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.InputFormat.exists","title":"<code>exists(input_format)</code>  <code>classmethod</code>","text":"<p>Checks if the input format exists in the enum values.</p> <p>Parameters:</p> Name Type Description Default <code>input_format</code> <code>str</code> <p>format to check if exists.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the input format exists in our enum.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef exists(cls, input_format: str) -&gt; bool:\n\"\"\"Checks if the input format exists in the enum values.\n    Args:\n        input_format: format to check if exists.\n    Returns:\n        If the input format exists in our enum.\n    \"\"\"\nreturn input_format in cls.values()\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.InputFormat.values","title":"<code>values()</code>  <code>classmethod</code>","text":"<p>Generates a list containing all enum values.</p> <p>Returns:</p> Type Description <p>A list with all enum values.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef values(cls):  # type: ignore\n\"\"\"Generates a list containing all enum values.\n    Returns:\n        A list with all enum values.\n    \"\"\"\nreturn (c.value for c in cls)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.InputSpec","title":"<code>InputSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Specification of an algorithm input.</p> <p>This is very aligned with the way the execution environment connects to the sources (e.g., spark sources).</p> <ul> <li>spec_id: spec_id of the input specification read_type: ReadType type of read     operation.</li> <li>data_format: format of the input.</li> <li>sftp_files_format: format of the files (csv, fwf, json, xml...) in a sftp     directory.</li> <li>df_name: dataframe name.</li> <li>db_table: table name in the form of <code>&lt;db&gt;.&lt;table&gt;</code>.</li> <li>location: uri that identifies from where to read data in the specified format.</li> <li>enforce_schema_from_table: if we want to enforce the table schema or not, by     providing a table name in the form of <code>&lt;db&gt;.&lt;table&gt;</code>.</li> <li>query: sql query to execute and return the dataframe. Use it if you do not want to     read from a file system nor from a table, but rather from a sql query instead.</li> <li>schema: dict representation of a schema of the input (e.g., Spark struct type     schema).</li> <li>schema_path: path to a file with a representation of a schema of the input (e.g.,     Spark struct type schema).</li> <li>disable_dbfs_retry: optional flag to disable file storage dbfs.</li> <li>with_filepath: if we want to include the path of the file that is being read. Only     works with the file reader (batch and streaming modes are supported).</li> <li>options: dict with other relevant options according to the execution     environment (e.g., spark) possible sources.</li> <li>calculate_upper_bound: when to calculate upper bound to extract from SAP BW     or not.</li> <li>calc_upper_bound_schema: specific schema for the calculated upper_bound.</li> <li>generate_predicates: when to generate predicates to extract from SAP BW or not.</li> <li>predicates_add_null: if we want to include is null on partition by predicates.</li> <li>temp_view: optional name of a view to point to the input dataframe to be used     to create or replace a temp view on top of the dataframe.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass InputSpec(object):\n\"\"\"Specification of an algorithm input.\n    This is very aligned with the way the execution environment connects to the sources\n    (e.g., spark sources).\n    - spec_id: spec_id of the input specification read_type: ReadType type of read\n        operation.\n    - data_format: format of the input.\n    - sftp_files_format: format of the files (csv, fwf, json, xml...) in a sftp\n        directory.\n    - df_name: dataframe name.\n    - db_table: table name in the form of `&lt;db&gt;.&lt;table&gt;`.\n    - location: uri that identifies from where to read data in the specified format.\n    - enforce_schema_from_table: if we want to enforce the table schema or not, by\n        providing a table name in the form of `&lt;db&gt;.&lt;table&gt;`.\n    - query: sql query to execute and return the dataframe. Use it if you do not want to\n        read from a file system nor from a table, but rather from a sql query instead.\n    - schema: dict representation of a schema of the input (e.g., Spark struct type\n        schema).\n    - schema_path: path to a file with a representation of a schema of the input (e.g.,\n        Spark struct type schema).\n    - disable_dbfs_retry: optional flag to disable file storage dbfs.\n    - with_filepath: if we want to include the path of the file that is being read. Only\n        works with the file reader (batch and streaming modes are supported).\n    - options: dict with other relevant options according to the execution\n        environment (e.g., spark) possible sources.\n    - calculate_upper_bound: when to calculate upper bound to extract from SAP BW\n        or not.\n    - calc_upper_bound_schema: specific schema for the calculated upper_bound.\n    - generate_predicates: when to generate predicates to extract from SAP BW or not.\n    - predicates_add_null: if we want to include is null on partition by predicates.\n    - temp_view: optional name of a view to point to the input dataframe to be used\n        to create or replace a temp view on top of the dataframe.\n    \"\"\"\nspec_id: str\nread_type: str\ndata_format: Optional[str] = None\nsftp_files_format: Optional[str] = None\ndf_name: Optional[DataFrame] = None\ndb_table: Optional[str] = None\nlocation: Optional[str] = None\nquery: Optional[str] = None\nenforce_schema_from_table: Optional[str] = None\nschema: Optional[dict] = None\nschema_path: Optional[str] = None\ndisable_dbfs_retry: bool = False\nwith_filepath: bool = False\noptions: Optional[dict] = None\njdbc_args: Optional[dict] = None\ncalculate_upper_bound: bool = False\ncalc_upper_bound_schema: Optional[str] = None\ngenerate_predicates: bool = False\npredicates_add_null: bool = True\ntemp_view: Optional[str] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.MergeOptions","title":"<code>MergeOptions</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Options for a merge operation.</p> <ul> <li>merge_predicate: predicate to apply to the merge operation so that we can     check if a new record corresponds to a record already included in the     historical data.</li> <li>insert_only: indicates if the merge should only insert data (e.g., deduplicate     scenarios).</li> <li>delete_predicate: predicate to apply to the delete operation.</li> <li>update_predicate: predicate to apply to the update operation.</li> <li>insert_predicate: predicate to apply to the insert operation.</li> <li>update_column_set: rules to apply to the update operation which allows to     set the value for each column to be updated.     (e.g. {\"data\": \"new.data\", \"count\": \"current.count + 1\"} )</li> <li>insert_column_set: rules to apply to the insert operation which allows to     set the value for each column to be inserted.     (e.g. {\"date\": \"updates.date\", \"count\": \"1\"} )</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass MergeOptions(object):\n\"\"\"Options for a merge operation.\n    - merge_predicate: predicate to apply to the merge operation so that we can\n        check if a new record corresponds to a record already included in the\n        historical data.\n    - insert_only: indicates if the merge should only insert data (e.g., deduplicate\n        scenarios).\n    - delete_predicate: predicate to apply to the delete operation.\n    - update_predicate: predicate to apply to the update operation.\n    - insert_predicate: predicate to apply to the insert operation.\n    - update_column_set: rules to apply to the update operation which allows to\n        set the value for each column to be updated.\n        (e.g. {\"data\": \"new.data\", \"count\": \"current.count + 1\"} )\n    - insert_column_set: rules to apply to the insert operation which allows to\n        set the value for each column to be inserted.\n        (e.g. {\"date\": \"updates.date\", \"count\": \"1\"} )\n    \"\"\"\nmerge_predicate: str\ninsert_only: bool = False\ndelete_predicate: Optional[str] = None\nupdate_predicate: Optional[str] = None\ninsert_predicate: Optional[str] = None\nupdate_column_set: Optional[dict] = None\ninsert_column_set: Optional[dict] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.NotificationRuntimeParameters","title":"<code>NotificationRuntimeParameters</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Parameters to be replaced in runtime.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class NotificationRuntimeParameters(Enum):\n\"\"\"Parameters to be replaced in runtime.\"\"\"\nDATABRICKS_JOB_NAME = \"databricks_job_name\"\nDATABRICKS_WORKSPACE_ID = \"databricks_workspace_id\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.NotifierType","title":"<code>NotifierType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Type of notifier available.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class NotifierType(Enum):\n\"\"\"Type of notifier available.\"\"\"\nEMAIL = \"email\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.OutputFormat","title":"<code>OutputFormat</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Formats of algorithm output.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class OutputFormat(Enum):\n\"\"\"Formats of algorithm output.\"\"\"\nJDBC = \"jdbc\"\nAVRO = \"avro\"\nJSON = \"json\"\nCSV = \"csv\"\nPARQUET = \"parquet\"\nDELTAFILES = \"delta\"\nKAFKA = \"kafka\"\nCONSOLE = \"console\"\nNOOP = \"noop\"\nDATAFRAME = \"dataframe\"\nREST_API = \"rest_api\"\nFILE = \"file\"  # Internal use only\nTABLE = \"table\"  # Internal use only\nSHAREPOINT = \"sharepoint\"\n@classmethod\ndef values(cls):  # type: ignore\n\"\"\"Generates a list containing all enum values.\n        Returns:\n            A list with all enum values.\n        \"\"\"\nreturn (c.value for c in cls)\n@classmethod\ndef exists(cls, output_format: str) -&gt; bool:\n\"\"\"Checks if the output format exists in the enum values.\n        Args:\n            output_format: format to check if exists.\n        Returns:\n            If the output format exists in our enum.\n        \"\"\"\nreturn output_format in cls.values()\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.OutputFormat.exists","title":"<code>exists(output_format)</code>  <code>classmethod</code>","text":"<p>Checks if the output format exists in the enum values.</p> <p>Parameters:</p> Name Type Description Default <code>output_format</code> <code>str</code> <p>format to check if exists.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the output format exists in our enum.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef exists(cls, output_format: str) -&gt; bool:\n\"\"\"Checks if the output format exists in the enum values.\n    Args:\n        output_format: format to check if exists.\n    Returns:\n        If the output format exists in our enum.\n    \"\"\"\nreturn output_format in cls.values()\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.OutputFormat.values","title":"<code>values()</code>  <code>classmethod</code>","text":"<p>Generates a list containing all enum values.</p> <p>Returns:</p> Type Description <p>A list with all enum values.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef values(cls):  # type: ignore\n\"\"\"Generates a list containing all enum values.\n    Returns:\n        A list with all enum values.\n    \"\"\"\nreturn (c.value for c in cls)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.OutputSpec","title":"<code>OutputSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Specification of an algorithm output.</p> <p>This is very aligned with the way the execution environment connects to the output systems (e.g., spark outputs).</p> <ul> <li>spec_id: id of the output specification.</li> <li>input_id: id of the corresponding input specification.</li> <li>write_type: type of write operation.</li> <li>data_format: format of the output. Defaults to DELTA.</li> <li>db_table: table name in the form of <code>&lt;db&gt;.&lt;table&gt;</code>.</li> <li>location: uri that identifies from where to write data in the specified format.</li> <li>sharepoint_opts: options to apply on writing on sharepoint operations.</li> <li>partitions: list of partition input_col names.</li> <li>merge_opts: options to apply to the merge operation.</li> <li>streaming_micro_batch_transformers: transformers to invoke for each streaming     micro batch, before writing (i.e., in Spark's foreachBatch structured     streaming function). Note: the lakehouse engine manages this for you, so     you don't have to manually specify streaming transformations here, so we don't     advise you to manually specify transformations through this parameter. Supply     them as regular transformers in the transform_specs sections of an ACON.</li> <li>streaming_once: if the streaming query is to be executed just once, or not,     generating just one micro batch.</li> <li>streaming_processing_time: if streaming query is to be kept alive, this indicates     the processing time of each micro batch.</li> <li>streaming_available_now: if set to True, set a trigger that processes all     available data in multiple batches then terminates the query.     When using streaming, this is the default trigger that the lakehouse-engine will     use, unless you configure a different one.</li> <li>streaming_continuous: set a trigger that runs a continuous query with a given     checkpoint interval.</li> <li>streaming_await_termination: whether to wait (True) for the termination of the     streaming query (e.g. timeout or exception) or not (False). Default: True.</li> <li>streaming_await_termination_timeout: a timeout to set to the     streaming_await_termination. Default: None.</li> <li>with_batch_id: whether to include the streaming batch id in the final data,     or not. It only takes effect in streaming mode.</li> <li>options: dict with other relevant options according to the execution environment     (e.g., spark) possible outputs.  E.g.,: JDBC options, checkpoint location for     streaming, etc.</li> <li>streaming_micro_batch_dq_processors: similar to streaming_micro_batch_transformers     but for the DQ functions to be executed. Used internally by the lakehouse     engine, so you don't have to supply DQ functions through this parameter. Use the     dq_specs of the acon instead.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass OutputSpec(object):\n\"\"\"Specification of an algorithm output.\n    This is very aligned with the way the execution environment connects to the output\n    systems (e.g., spark outputs).\n    - spec_id: id of the output specification.\n    - input_id: id of the corresponding input specification.\n    - write_type: type of write operation.\n    - data_format: format of the output. Defaults to DELTA.\n    - db_table: table name in the form of `&lt;db&gt;.&lt;table&gt;`.\n    - location: uri that identifies from where to write data in the specified format.\n    - sharepoint_opts: options to apply on writing on sharepoint operations.\n    - partitions: list of partition input_col names.\n    - merge_opts: options to apply to the merge operation.\n    - streaming_micro_batch_transformers: transformers to invoke for each streaming\n        micro batch, before writing (i.e., in Spark's foreachBatch structured\n        streaming function). Note: the lakehouse engine manages this for you, so\n        you don't have to manually specify streaming transformations here, so we don't\n        advise you to manually specify transformations through this parameter. Supply\n        them as regular transformers in the transform_specs sections of an ACON.\n    - streaming_once: if the streaming query is to be executed just once, or not,\n        generating just one micro batch.\n    - streaming_processing_time: if streaming query is to be kept alive, this indicates\n        the processing time of each micro batch.\n    - streaming_available_now: if set to True, set a trigger that processes all\n        available data in multiple batches then terminates the query.\n        When using streaming, this is the default trigger that the lakehouse-engine will\n        use, unless you configure a different one.\n    - streaming_continuous: set a trigger that runs a continuous query with a given\n        checkpoint interval.\n    - streaming_await_termination: whether to wait (True) for the termination of the\n        streaming query (e.g. timeout or exception) or not (False). Default: True.\n    - streaming_await_termination_timeout: a timeout to set to the\n        streaming_await_termination. Default: None.\n    - with_batch_id: whether to include the streaming batch id in the final data,\n        or not. It only takes effect in streaming mode.\n    - options: dict with other relevant options according to the execution environment\n        (e.g., spark) possible outputs.  E.g.,: JDBC options, checkpoint location for\n        streaming, etc.\n    - streaming_micro_batch_dq_processors: similar to streaming_micro_batch_transformers\n        but for the DQ functions to be executed. Used internally by the lakehouse\n        engine, so you don't have to supply DQ functions through this parameter. Use the\n        dq_specs of the acon instead.\n    \"\"\"\nspec_id: str\ninput_id: str\nwrite_type: str\ndata_format: str = OutputFormat.DELTAFILES.value\ndb_table: Optional[str] = None\nlocation: Optional[str] = None\nsharepoint_opts: Optional[SharepointOptions] = None\nmerge_opts: Optional[MergeOptions] = None\npartitions: Optional[List[str]] = None\nstreaming_micro_batch_transformers: Optional[List[TransformerSpec]] = None\nstreaming_once: Optional[bool] = None\nstreaming_processing_time: Optional[str] = None\nstreaming_available_now: bool = True\nstreaming_continuous: Optional[str] = None\nstreaming_await_termination: bool = True\nstreaming_await_termination_timeout: Optional[int] = None\nwith_batch_id: bool = False\noptions: Optional[dict] = None\nstreaming_micro_batch_dq_processors: Optional[List[DQSpec]] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.ReadMode","title":"<code>ReadMode</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Different modes that control how we handle compliance to the provided schema.</p> <p>These read modes map to Spark's read modes at the moment.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class ReadMode(Enum):\n\"\"\"Different modes that control how we handle compliance to the provided schema.\n    These read modes map to Spark's read modes at the moment.\n    \"\"\"\nPERMISSIVE = \"PERMISSIVE\"\nFAILFAST = \"FAILFAST\"\nDROPMALFORMED = \"DROPMALFORMED\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.ReadType","title":"<code>ReadType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Define the types of read operations.</p> <ul> <li>BATCH - read the data in batch mode (e.g., Spark batch).</li> <li>STREAMING - read the data in streaming mode (e.g., Spark streaming).</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class ReadType(Enum):\n\"\"\"Define the types of read operations.\n    - BATCH - read the data in batch mode (e.g., Spark batch).\n    - STREAMING - read the data in streaming mode (e.g., Spark streaming).\n    \"\"\"\nBATCH = \"batch\"\nSTREAMING = \"streaming\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.ReconciliatorSpec","title":"<code>ReconciliatorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Reconciliator Specification.</p> <ul> <li>metrics: list of metrics in the form of:     [{         metric: name of the column present in both truth and current datasets,         aggregation: sum, avg, max, min, ...,         type: percentage or absolute,         yellow: value,         red: value     }].</li> <li>recon_type: reconciliation type (percentage or absolute). Percentage calculates     the difference between truth and current results as a percentage (x-y/x), and     absolute calculates the raw difference (x - y).</li> <li>truth_input_spec: input specification of the truth data.</li> <li>current_input_spec: input specification of the current results data</li> <li>truth_preprocess_query: additional query on top of the truth input data to     preprocess the truth data before it gets fueled into the reconciliation process.     Important note: you need to assume that the data out of     the truth_input_spec is referencable by a table called 'truth'.</li> <li>truth_preprocess_query_args: optional dict having the functions/transformations to     apply on top of the truth_preprocess_query and respective arguments. Note: cache     is being applied on the Dataframe, by default. For turning the default behavior     off, pass <code>\"truth_preprocess_query_args\": []</code>.</li> <li>current_preprocess_query: additional query on top of the current results input     data to preprocess the current results data before it gets fueled into the     reconciliation process. Important note: you need to assume that the data out of     the current_results_input_spec is referencable by a table called 'current'.</li> <li>current_preprocess_query_args: optional dict having the     functions/transformations to apply on top of the current_preprocess_query     and respective arguments. Note: cache is being applied on the Dataframe,     by default. For turning the default behavior off, pass     <code>\"current_preprocess_query_args\": []</code>.</li> <li>ignore_empty_df: optional boolean, to ignore the recon process if source &amp; target    dataframes are empty, recon will exit success code (passed)</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass ReconciliatorSpec(object):\n\"\"\"Reconciliator Specification.\n    - metrics: list of metrics in the form of:\n        [{\n            metric: name of the column present in both truth and current datasets,\n            aggregation: sum, avg, max, min, ...,\n            type: percentage or absolute,\n            yellow: value,\n            red: value\n        }].\n    - recon_type: reconciliation type (percentage or absolute). Percentage calculates\n        the difference between truth and current results as a percentage (x-y/x), and\n        absolute calculates the raw difference (x - y).\n    - truth_input_spec: input specification of the truth data.\n    - current_input_spec: input specification of the current results data\n    - truth_preprocess_query: additional query on top of the truth input data to\n        preprocess the truth data before it gets fueled into the reconciliation process.\n        Important note: you need to assume that the data out of\n        the truth_input_spec is referencable by a table called 'truth'.\n    - truth_preprocess_query_args: optional dict having the functions/transformations to\n        apply on top of the truth_preprocess_query and respective arguments. Note: cache\n        is being applied on the Dataframe, by default. For turning the default behavior\n        off, pass `\"truth_preprocess_query_args\": []`.\n    - current_preprocess_query: additional query on top of the current results input\n        data to preprocess the current results data before it gets fueled into the\n        reconciliation process. Important note: you need to assume that the data out of\n        the current_results_input_spec is referencable by a table called 'current'.\n    - current_preprocess_query_args: optional dict having the\n        functions/transformations to apply on top of the current_preprocess_query\n        and respective arguments. Note: cache is being applied on the Dataframe,\n        by default. For turning the default behavior off, pass\n        `\"current_preprocess_query_args\": []`.\n    - ignore_empty_df: optional boolean, to ignore the recon process if source &amp; target\n       dataframes are empty, recon will exit success code (passed)\n    \"\"\"\nmetrics: List[dict]\ntruth_input_spec: InputSpec\ncurrent_input_spec: InputSpec\ntruth_preprocess_query: Optional[str] = None\ntruth_preprocess_query_args: Optional[List[dict]] = None\ncurrent_preprocess_query: Optional[str] = None\ncurrent_preprocess_query_args: Optional[List[dict]] = None\nignore_empty_df: Optional[bool] = False\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.RestoreStatus","title":"<code>RestoreStatus</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Archive types.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class RestoreStatus(Enum):\n\"\"\"Archive types.\"\"\"\nNOT_STARTED = \"not_started\"\nONGOING = \"ongoing\"\nRESTORED = \"restored\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.RestoreType","title":"<code>RestoreType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Archive types.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class RestoreType(Enum):\n\"\"\"Archive types.\"\"\"\nBULK = \"Bulk\"\nSTANDARD = \"Standard\"\nEXPEDITED = \"Expedited\"\n@classmethod\ndef values(cls):  # type: ignore\n\"\"\"Generates a list containing all enum values.\n        Returns:\n            A list with all enum values.\n        \"\"\"\nreturn (c.value for c in cls)\n@classmethod\ndef exists(cls, restore_type: str) -&gt; bool:\n\"\"\"Checks if the restore type exists in the enum values.\n        Args:\n            restore_type: restore type to check if exists.\n        Returns:\n            If the restore type exists in our enum.\n        \"\"\"\nreturn restore_type in cls.values()\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.RestoreType.exists","title":"<code>exists(restore_type)</code>  <code>classmethod</code>","text":"<p>Checks if the restore type exists in the enum values.</p> <p>Parameters:</p> Name Type Description Default <code>restore_type</code> <code>str</code> <p>restore type to check if exists.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the restore type exists in our enum.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef exists(cls, restore_type: str) -&gt; bool:\n\"\"\"Checks if the restore type exists in the enum values.\n    Args:\n        restore_type: restore type to check if exists.\n    Returns:\n        If the restore type exists in our enum.\n    \"\"\"\nreturn restore_type in cls.values()\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.RestoreType.values","title":"<code>values()</code>  <code>classmethod</code>","text":"<p>Generates a list containing all enum values.</p> <p>Returns:</p> Type Description <p>A list with all enum values.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef values(cls):  # type: ignore\n\"\"\"Generates a list containing all enum values.\n    Returns:\n        A list with all enum values.\n    \"\"\"\nreturn (c.value for c in cls)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SAPLogchain","title":"<code>SAPLogchain</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Defaults used on consuming data from SAP Logchain.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class SAPLogchain(Enum):\n\"\"\"Defaults used on consuming data from SAP Logchain.\"\"\"\nDBTABLE = \"SAPPHA.RSPCLOGCHAIN\"\nGREEN_STATUS = \"G\"\nENGINE_TABLE = \"sensor_new_data\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SQLDefinitions","title":"<code>SQLDefinitions</code>","text":"<p>         Bases: <code>Enum</code></p> <p>SQL definitions statements.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class SQLDefinitions(Enum):\n\"\"\"SQL definitions statements.\"\"\"\ncompute_table_stats = \"ANALYZE TABLE {} COMPUTE STATISTICS\"\ndrop_table_stmt = \"DROP TABLE IF EXISTS\"\ndrop_view_stmt = \"DROP VIEW IF EXISTS\"\ntruncate_stmt = \"TRUNCATE TABLE\"\ndescribe_stmt = \"DESCRIBE TABLE\"\noptimize_stmt = \"OPTIMIZE\"\nshow_tbl_props_stmt = \"SHOW TBLPROPERTIES\"\ndelete_where_stmt = \"DELETE FROM {} WHERE {}\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SQLParser","title":"<code>SQLParser</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Defaults to use for parsing.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class SQLParser(Enum):\n\"\"\"Defaults to use for parsing.\"\"\"\nDOUBLE_QUOTES = '\"'\nSINGLE_QUOTES = \"'\"\nBACKSLASH = \"\\\\\"\nSINGLE_TRACE = \"-\"\nDOUBLE_TRACES = \"--\"\nSLASH = \"/\"\nOPENING_MULTIPLE_LINE_COMMENT = \"/*\"\nCLOSING_MULTIPLE_LINE_COMMENT = \"*/\"\nPARAGRAPH = \"\\n\"\nSTAR = \"*\"\nMULTIPLE_LINE_COMMENT = [\nOPENING_MULTIPLE_LINE_COMMENT,\nCLOSING_MULTIPLE_LINE_COMMENT,\n]\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SensorSpec","title":"<code>SensorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Sensor Specification.</p> <ul> <li>sensor_id: sensor id.</li> <li>assets: a list of assets that are considered as available to     consume downstream after this sensor has status     PROCESSED_NEW_DATA.</li> <li>control_db_table_name: db.table to store sensor metadata.</li> <li>input_spec: input specification of the source to be checked for new data.</li> <li>preprocess_query: SQL query to transform/filter the result from the     upstream. Consider that we should refer to 'new_data' whenever     we are referring to the input of the sensor. E.g.:         \"SELECT dummy_col FROM new_data WHERE ...\"</li> <li>checkpoint_location: optional location to store checkpoints to resume     from. These checkpoints use the same as Spark checkpoint strategy.     For Spark readers that do not support checkpoints, use the     preprocess_query parameter to form a SQL query to filter the result     from the upstream accordingly.</li> <li>fail_on_empty_result: if the sensor should throw an error if there is no new     data in the upstream. Default: True.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass SensorSpec(object):\n\"\"\"Sensor Specification.\n    - sensor_id: sensor id.\n    - assets: a list of assets that are considered as available to\n        consume downstream after this sensor has status\n        PROCESSED_NEW_DATA.\n    - control_db_table_name: db.table to store sensor metadata.\n    - input_spec: input specification of the source to be checked for new data.\n    - preprocess_query: SQL query to transform/filter the result from the\n        upstream. Consider that we should refer to 'new_data' whenever\n        we are referring to the input of the sensor. E.g.:\n            \"SELECT dummy_col FROM new_data WHERE ...\"\n    - checkpoint_location: optional location to store checkpoints to resume\n        from. These checkpoints use the same as Spark checkpoint strategy.\n        For Spark readers that do not support checkpoints, use the\n        preprocess_query parameter to form a SQL query to filter the result\n        from the upstream accordingly.\n    - fail_on_empty_result: if the sensor should throw an error if there is no new\n        data in the upstream. Default: True.\n    \"\"\"\nsensor_id: str\nassets: List[str]\ncontrol_db_table_name: str\ninput_spec: InputSpec\npreprocess_query: Optional[str]\ncheckpoint_location: Optional[str]\nfail_on_empty_result: bool = True\n@classmethod\ndef create_from_acon(cls, acon: dict):  # type: ignore\n\"\"\"Create SensorSpec from acon.\n        Args:\n            acon: sensor ACON.\n        \"\"\"\ncheckpoint_location = acon.get(\"base_checkpoint_location\")\nif checkpoint_location:\ncheckpoint_location = (\nf\"{checkpoint_location.rstrip('/')}/lakehouse_engine/\"\nf\"sensors/{acon['sensor_id']}\"\n)\nreturn cls(\nsensor_id=acon[\"sensor_id\"],\nassets=acon[\"assets\"],\ncontrol_db_table_name=acon[\"control_db_table_name\"],\ninput_spec=InputSpec(**acon[\"input_spec\"]),\npreprocess_query=acon.get(\"preprocess_query\"),\ncheckpoint_location=checkpoint_location,\nfail_on_empty_result=acon.get(\"fail_on_empty_result\", True),\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SensorSpec.create_from_acon","title":"<code>create_from_acon(acon)</code>  <code>classmethod</code>","text":"<p>Create SensorSpec from acon.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>sensor ACON.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@classmethod\ndef create_from_acon(cls, acon: dict):  # type: ignore\n\"\"\"Create SensorSpec from acon.\n    Args:\n        acon: sensor ACON.\n    \"\"\"\ncheckpoint_location = acon.get(\"base_checkpoint_location\")\nif checkpoint_location:\ncheckpoint_location = (\nf\"{checkpoint_location.rstrip('/')}/lakehouse_engine/\"\nf\"sensors/{acon['sensor_id']}\"\n)\nreturn cls(\nsensor_id=acon[\"sensor_id\"],\nassets=acon[\"assets\"],\ncontrol_db_table_name=acon[\"control_db_table_name\"],\ninput_spec=InputSpec(**acon[\"input_spec\"]),\npreprocess_query=acon.get(\"preprocess_query\"),\ncheckpoint_location=checkpoint_location,\nfail_on_empty_result=acon.get(\"fail_on_empty_result\", True),\n)\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SensorStatus","title":"<code>SensorStatus</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Status for a sensor.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class SensorStatus(Enum):\n\"\"\"Status for a sensor.\"\"\"\nACQUIRED_NEW_DATA = \"ACQUIRED_NEW_DATA\"\nPROCESSED_NEW_DATA = \"PROCESSED_NEW_DATA\"\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.SharepointOptions","title":"<code>SharepointOptions</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>options for a sharepoint write operation.</p> <ul> <li>client_id (str): azure client ID application.</li> <li>tenant_id (str): tenant ID associated with the SharePoint site.</li> <li>site_name (str): name of the SharePoint site where the document library resides.</li> <li>drive_name (str): name of the document library where the file will be uploaded.</li> <li>file_name (str): name of the file to be uploaded to local path and to SharePoint.</li> <li>secret (str): client secret for authentication.</li> <li>local_path (str): local path (or similar, e.g. mounted file systems like     databricks volumes or dbfs) where files will be temporarily stored during     the SharePoint write operation.</li> <li>api_version (str): version of the Graph SharePoint API to be used for operations.</li> <li>folder_relative_path (Optional[str]): relative folder path within the document     library to upload the file.</li> <li>chunk_size (Optional[int]): Optional; size (in Bytes) of the file chunks for     uploading to SharePoint. Default is 100 Mb.</li> <li>local_options (Optional[dict]): Optional; additional options for customizing     write to csv action to local path. You can check the available options     here: https://spark.apache.org/docs/3.5.3/sql-data-sources-csv.html</li> <li>conflict_behaviour (Optional[str]): Optional; behavior to adopt in case     of a conflict (e.g., 'replace', 'fail').</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass SharepointOptions(object):\n\"\"\"options for a sharepoint write operation.\n    - client_id (str): azure client ID application.\n    - tenant_id (str): tenant ID associated with the SharePoint site.\n    - site_name (str): name of the SharePoint site where the document library resides.\n    - drive_name (str): name of the document library where the file will be uploaded.\n    - file_name (str): name of the file to be uploaded to local path and to SharePoint.\n    - secret (str): client secret for authentication.\n    - local_path (str): local path (or similar, e.g. mounted file systems like\n        databricks volumes or dbfs) where files will be temporarily stored during\n        the SharePoint write operation.\n    - api_version (str): version of the Graph SharePoint API to be used for operations.\n    - folder_relative_path (Optional[str]): relative folder path within the document\n        library to upload the file.\n    - chunk_size (Optional[int]): Optional; size (in Bytes) of the file chunks for\n        uploading to SharePoint. Default is 100 Mb.\n    - local_options (Optional[dict]): Optional; additional options for customizing\n        write to csv action to local path. You can check the available options\n        here: https://spark.apache.org/docs/3.5.3/sql-data-sources-csv.html\n    - conflict_behaviour (Optional[str]): Optional; behavior to adopt in case\n        of a conflict (e.g., 'replace', 'fail').\n    \"\"\"\nclient_id: str\ntenant_id: str\nsite_name: str\ndrive_name: str\nfile_name: str\nsecret: str\nlocal_path: str\napi_version: str = \"v1.0\"\nfolder_relative_path: Optional[str] = None\nchunk_size: Optional[int] = 100 * 1024 * 1024  # 100 MB\nlocal_options: Optional[dict] = None\nconflict_behaviour: Optional[str] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.TerminatorSpec","title":"<code>TerminatorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Terminator Specification.</p> <p>I.e., the specification that defines a terminator operation to be executed. Examples are compute statistics, vacuum, optimize, etc.</p> <ul> <li>function: terminator function to execute.</li> <li>args: arguments of the terminator function.</li> <li>input_id: id of the corresponding output specification (Optional).</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass TerminatorSpec(object):\n\"\"\"Terminator Specification.\n    I.e., the specification that defines a terminator operation to be executed. Examples\n    are compute statistics, vacuum, optimize, etc.\n    - function: terminator function to execute.\n    - args: arguments of the terminator function.\n    - input_id: id of the corresponding output specification (Optional).\n    \"\"\"\nfunction: str\nargs: Optional[dict] = None\ninput_id: Optional[str] = None\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.TransformSpec","title":"<code>TransformSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Transformation Specification.</p> <p>I.e., the specification that defines the many transformations to be done to the data that was read.</p> <ul> <li>spec_id: id of the terminate specification</li> <li>input_id: id of the corresponding input specification.</li> <li>transformers: list of transformers to execute.</li> <li>force_streaming_foreach_batch_processing: sometimes, when using streaming, we want     to force the transform to be executed in the foreachBatch function to ensure     non-supported streaming operations can be properly executed.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass TransformSpec(object):\n\"\"\"Transformation Specification.\n    I.e., the specification that defines the many transformations to be done to the data\n    that was read.\n    - spec_id: id of the terminate specification\n    - input_id: id of the corresponding input\n    specification.\n    - transformers: list of transformers to execute.\n    - force_streaming_foreach_batch_processing: sometimes, when using streaming, we want\n        to force the transform to be executed in the foreachBatch function to ensure\n        non-supported streaming operations can be properly executed.\n    \"\"\"\nspec_id: str\ninput_id: str\ntransformers: List[TransformerSpec]\nforce_streaming_foreach_batch_processing: bool = False\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.TransformerSpec","title":"<code>TransformerSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Transformer Specification, i.e., a single transformation amongst many.</p> <ul> <li>function: name of the function (or callable function) to be executed.</li> <li>args: (not applicable if using a callable function) dict with the arguments     to pass to the function <code>&lt;k,v&gt;</code> pairs with the name of the parameter of     the function and the respective value.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>@dataclass\nclass TransformerSpec(object):\n\"\"\"Transformer Specification, i.e., a single transformation amongst many.\n    - function: name of the function (or callable function) to be executed.\n    - args: (not applicable if using a callable function) dict with the arguments\n        to pass to the function `&lt;k,v&gt;` pairs with the name of the parameter of\n        the function and the respective value.\n    \"\"\"\nfunction: str\nargs: dict\n</code></pre>"},{"location":"reference/packages/core/definitions.html#packages.core.definitions.WriteType","title":"<code>WriteType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Types of write operations.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/definitions.py</code> <pre><code>class WriteType(Enum):\n\"\"\"Types of write operations.\"\"\"\nOVERWRITE = \"overwrite\"\nCOMPLETE = \"complete\"\nAPPEND = \"append\"\nUPDATE = \"update\"\nMERGE = \"merge\"\nERROR_IF_EXISTS = \"error\"\nIGNORE_IF_EXISTS = \"ignore\"\n</code></pre>"},{"location":"reference/packages/core/exec_env.html","title":"Exec env","text":"<p>Module to take care of creating a singleton of the execution environment class.</p>"},{"location":"reference/packages/core/exec_env.html#packages.core.exec_env.ExecEnv","title":"<code>ExecEnv</code>","text":"<p>         Bases: <code>object</code></p> <p>Represents the basic resources regarding the engine execution environment.</p> <p>Currently, it is used to encapsulate both the logic to get the Spark session and the engine configurations.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/exec_env.py</code> <pre><code>class ExecEnv(object):\n\"\"\"Represents the basic resources regarding the engine execution environment.\n    Currently, it is used to encapsulate both the logic to get the Spark\n    session and the engine configurations.\n    \"\"\"\nSESSION: SparkSession\n_LOGGER = LoggingHandler(__name__).get_logger()\nDEFAULT_AWS_REGION = \"eu-west-1\"\nENGINE_CONFIG: EngineConfig = EngineConfig(**ConfigUtils.get_config())\n@classmethod\ndef set_default_engine_config(\ncls,\npackage: str = \"lakehouse_engine.configs\",\ncustom_configs_dict: dict = None,\ncustom_configs_file_path: str = None,\n) -&gt; None:\n\"\"\"Set default engine configurations.\n        The function set the default engine configurations by reading\n        them from a specified package and overwrite them if the user\n        pass a dictionary or a file path with new configurations.\n        Args:\n            package: package where the engine default configurations can be found.\n            custom_configs_dict: a dictionary with custom configurations\n            to overwrite the default ones.\n            custom_configs_file_path: path for the file with custom\n            configurations to overwrite the default ones.\n        \"\"\"\ncls.ENGINE_CONFIG = EngineConfig(**ConfigUtils.get_config(package))\nif custom_configs_dict:\ncls.ENGINE_CONFIG = replace(cls.ENGINE_CONFIG, **custom_configs_dict)\nif custom_configs_file_path:\ncls.ENGINE_CONFIG = replace(\ncls.ENGINE_CONFIG,\n**ConfigUtils.get_config_from_file(custom_configs_file_path),\n)\n@classmethod\ndef get_or_create(\ncls,\nsession: SparkSession = None,\nenable_hive_support: bool = True,\napp_name: str = None,\nconfig: dict = None,\n) -&gt; None:\n\"\"\"Get or create an execution environment session (currently Spark).\n        It instantiates a singleton session that can be accessed anywhere from the\n        lakehouse engine. By default, if there is an existing Spark Session in\n        the environment (getActiveSession()), this function re-uses it. It can\n        be further extended in the future to support forcing the creation of new\n        isolated sessions even when a Spark Session is already active.\n        Args:\n            session: spark session.\n            enable_hive_support: whether to enable hive support or not.\n            app_name: application name.\n            config: extra spark configs to supply to the spark session.\n        \"\"\"\ndefault_config = {\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.sql.adaptive.enabled\": True,\n\"spark.databricks.delta.merge.enableLowShuffle\": True,\n}\ncls._LOGGER.info(\nf\"Using the following default configs you may want to override them for \"\nf\"your job: {default_config}\"\n)\nfinal_config: dict = {**default_config, **(config if config else {})}\ncls._LOGGER.info(f\"Final config is: {final_config}\")\nif session:\ncls.SESSION = session\nelif SparkSession.getActiveSession():\ncls.SESSION = SparkSession.getActiveSession()\nfor key, value in final_config.items():\ncls.SESSION.conf.set(key, value)\nelse:\ncls._LOGGER.info(\"Creating a new Spark Session\")\nsession_builder = SparkSession.builder.appName(app_name)\nfor k, v in final_config.items():\nsession_builder.config(k, v)\nif enable_hive_support:\nsession_builder = session_builder.enableHiveSupport()\ncls.SESSION = session_builder.getOrCreate()\nif not session:\ncls._set_environment_variables(final_config.get(\"os_env_vars\"))\n@classmethod\ndef _set_environment_variables(cls, os_env_vars: dict = None) -&gt; None:\n\"\"\"Set environment variables at OS level.\n        By default, we are setting the AWS_DEFAULT_REGION as we have identified this is\n        beneficial to avoid getBucketLocation permission problems.\n        Args:\n            os_env_vars: this parameter can be used to pass the environment variables to\n                be defined.\n        \"\"\"\nif os_env_vars is None:\nos_env_vars = {}\nfor env_var in os_env_vars.items():\nos.environ[env_var[0]] = env_var[1]\nif \"AWS_DEFAULT_REGION\" not in os_env_vars:\nos.environ[\"AWS_DEFAULT_REGION\"] = cls.SESSION.conf.get(\n\"spark.databricks.clusterUsageTags.region\", cls.DEFAULT_AWS_REGION\n)\n@classmethod\ndef get_environment(cls) -&gt; str:\n\"\"\"Get the environment where the process is running.\n        Returns:\n            Name of the environment.\n        \"\"\"\ntag_array = ast.literal_eval(\ncls.SESSION.conf.get(\n\"spark.databricks.clusterUsageTags.clusterAllTags\", \"[]\"\n)\n)\nfor key_val in tag_array:\nif key_val[\"key\"] == \"environment\":\nreturn str(key_val[\"value\"])\nreturn \"prod\"\n</code></pre>"},{"location":"reference/packages/core/exec_env.html#packages.core.exec_env.ExecEnv.get_environment","title":"<code>get_environment()</code>  <code>classmethod</code>","text":"<p>Get the environment where the process is running.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the environment.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/exec_env.py</code> <pre><code>@classmethod\ndef get_environment(cls) -&gt; str:\n\"\"\"Get the environment where the process is running.\n    Returns:\n        Name of the environment.\n    \"\"\"\ntag_array = ast.literal_eval(\ncls.SESSION.conf.get(\n\"spark.databricks.clusterUsageTags.clusterAllTags\", \"[]\"\n)\n)\nfor key_val in tag_array:\nif key_val[\"key\"] == \"environment\":\nreturn str(key_val[\"value\"])\nreturn \"prod\"\n</code></pre>"},{"location":"reference/packages/core/exec_env.html#packages.core.exec_env.ExecEnv.get_or_create","title":"<code>get_or_create(session=None, enable_hive_support=True, app_name=None, config=None)</code>  <code>classmethod</code>","text":"<p>Get or create an execution environment session (currently Spark).</p> <p>It instantiates a singleton session that can be accessed anywhere from the lakehouse engine. By default, if there is an existing Spark Session in the environment (getActiveSession()), this function re-uses it. It can be further extended in the future to support forcing the creation of new isolated sessions even when a Spark Session is already active.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SparkSession</code> <p>spark session.</p> <code>None</code> <code>enable_hive_support</code> <code>bool</code> <p>whether to enable hive support or not.</p> <code>True</code> <code>app_name</code> <code>str</code> <p>application name.</p> <code>None</code> <code>config</code> <code>dict</code> <p>extra spark configs to supply to the spark session.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/core/exec_env.py</code> <pre><code>@classmethod\ndef get_or_create(\ncls,\nsession: SparkSession = None,\nenable_hive_support: bool = True,\napp_name: str = None,\nconfig: dict = None,\n) -&gt; None:\n\"\"\"Get or create an execution environment session (currently Spark).\n    It instantiates a singleton session that can be accessed anywhere from the\n    lakehouse engine. By default, if there is an existing Spark Session in\n    the environment (getActiveSession()), this function re-uses it. It can\n    be further extended in the future to support forcing the creation of new\n    isolated sessions even when a Spark Session is already active.\n    Args:\n        session: spark session.\n        enable_hive_support: whether to enable hive support or not.\n        app_name: application name.\n        config: extra spark configs to supply to the spark session.\n    \"\"\"\ndefault_config = {\n\"spark.databricks.delta.optimizeWrite.enabled\": True,\n\"spark.sql.adaptive.enabled\": True,\n\"spark.databricks.delta.merge.enableLowShuffle\": True,\n}\ncls._LOGGER.info(\nf\"Using the following default configs you may want to override them for \"\nf\"your job: {default_config}\"\n)\nfinal_config: dict = {**default_config, **(config if config else {})}\ncls._LOGGER.info(f\"Final config is: {final_config}\")\nif session:\ncls.SESSION = session\nelif SparkSession.getActiveSession():\ncls.SESSION = SparkSession.getActiveSession()\nfor key, value in final_config.items():\ncls.SESSION.conf.set(key, value)\nelse:\ncls._LOGGER.info(\"Creating a new Spark Session\")\nsession_builder = SparkSession.builder.appName(app_name)\nfor k, v in final_config.items():\nsession_builder.config(k, v)\nif enable_hive_support:\nsession_builder = session_builder.enableHiveSupport()\ncls.SESSION = session_builder.getOrCreate()\nif not session:\ncls._set_environment_variables(final_config.get(\"os_env_vars\"))\n</code></pre>"},{"location":"reference/packages/core/exec_env.html#packages.core.exec_env.ExecEnv.set_default_engine_config","title":"<code>set_default_engine_config(package='lakehouse_engine.configs', custom_configs_dict=None, custom_configs_file_path=None)</code>  <code>classmethod</code>","text":"<p>Set default engine configurations.</p> <p>The function set the default engine configurations by reading them from a specified package and overwrite them if the user pass a dictionary or a file path with new configurations.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>package where the engine default configurations can be found.</p> <code>'lakehouse_engine.configs'</code> <code>custom_configs_dict</code> <code>dict</code> <p>a dictionary with custom configurations</p> <code>None</code> <code>custom_configs_file_path</code> <code>str</code> <p>path for the file with custom</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/core/exec_env.py</code> <pre><code>@classmethod\ndef set_default_engine_config(\ncls,\npackage: str = \"lakehouse_engine.configs\",\ncustom_configs_dict: dict = None,\ncustom_configs_file_path: str = None,\n) -&gt; None:\n\"\"\"Set default engine configurations.\n    The function set the default engine configurations by reading\n    them from a specified package and overwrite them if the user\n    pass a dictionary or a file path with new configurations.\n    Args:\n        package: package where the engine default configurations can be found.\n        custom_configs_dict: a dictionary with custom configurations\n        to overwrite the default ones.\n        custom_configs_file_path: path for the file with custom\n        configurations to overwrite the default ones.\n    \"\"\"\ncls.ENGINE_CONFIG = EngineConfig(**ConfigUtils.get_config(package))\nif custom_configs_dict:\ncls.ENGINE_CONFIG = replace(cls.ENGINE_CONFIG, **custom_configs_dict)\nif custom_configs_file_path:\ncls.ENGINE_CONFIG = replace(\ncls.ENGINE_CONFIG,\n**ConfigUtils.get_config_from_file(custom_configs_file_path),\n)\n</code></pre>"},{"location":"reference/packages/core/executable.html","title":"Executable","text":"<p>Module representing an executable lakehouse engine component.</p>"},{"location":"reference/packages/core/executable.html#packages.core.executable.Executable","title":"<code>Executable</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract class defining the behaviour of an executable component.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/executable.py</code> <pre><code>class Executable(ABC):\n\"\"\"Abstract class defining the behaviour of an executable component.\"\"\"\n@abstractmethod\ndef execute(self) -&gt; Optional[Any]:\n\"\"\"Define the executable component behaviour.\n        E.g., the behaviour of an algorithm inheriting from this.\n        \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/executable.html#packages.core.executable.Executable.execute","title":"<code>execute()</code>  <code>abstractmethod</code>","text":"<p>Define the executable component behaviour.</p> <p>E.g., the behaviour of an algorithm inheriting from this.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/executable.py</code> <pre><code>@abstractmethod\ndef execute(self) -&gt; Optional[Any]:\n\"\"\"Define the executable component behaviour.\n    E.g., the behaviour of an algorithm inheriting from this.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/file_manager.html","title":"File manager","text":"<p>Module for abstract representation of a file manager system.</p>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManager","title":"<code>FileManager</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract file manager class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>class FileManager(ABC):  # noqa: B024\n\"\"\"Abstract file manager class.\"\"\"\ndef __init__(self, configs: dict):\n\"\"\"Construct FileManager algorithm instances.\n        Args:\n            configs: configurations for the FileManager algorithm.\n        \"\"\"\nself.configs = configs\nself.function = self.configs[\"function\"]\n@abstractmethod\ndef delete_objects(self) -&gt; None:\n\"\"\"Delete objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be deleted based on the given keys.\n        \"\"\"\npass\n@abstractmethod\ndef copy_objects(self) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be copied based on the given keys.\n        \"\"\"\npass\n@abstractmethod\ndef move_objects(self) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be moved based on the given keys.\n        \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManager.__init__","title":"<code>__init__(configs)</code>","text":"<p>Construct FileManager algorithm instances.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>dict</code> <p>configurations for the FileManager algorithm.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>def __init__(self, configs: dict):\n\"\"\"Construct FileManager algorithm instances.\n    Args:\n        configs: configurations for the FileManager algorithm.\n    \"\"\"\nself.configs = configs\nself.function = self.configs[\"function\"]\n</code></pre>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManager.copy_objects","title":"<code>copy_objects()</code>  <code>abstractmethod</code>","text":"<p>Copies objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be copied based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>@abstractmethod\ndef copy_objects(self) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be copied based on the given keys.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManager.delete_objects","title":"<code>delete_objects()</code>  <code>abstractmethod</code>","text":"<p>Delete objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be deleted based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>@abstractmethod\ndef delete_objects(self) -&gt; None:\n\"\"\"Delete objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be deleted based on the given keys.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManager.move_objects","title":"<code>move_objects()</code>  <code>abstractmethod</code>","text":"<p>Moves objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be moved based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>@abstractmethod\ndef move_objects(self) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be moved based on the given keys.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManagerFactory","title":"<code>FileManagerFactory</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Class for file manager factory.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>class FileManagerFactory(ABC):  # noqa: B024\n\"\"\"Class for file manager factory.\"\"\"\n@staticmethod\ndef execute_function(configs: dict) -&gt; Any:\n\"\"\"Get a specific File Manager and function to execute.\"\"\"\nfrom lakehouse_engine.core.dbfs_file_manager import DBFSFileManager\nfrom lakehouse_engine.core.s3_file_manager import S3FileManager\ndisable_dbfs_retry = (\nconfigs[\"disable_dbfs_retry\"]\nif \"disable_dbfs_retry\" in configs.keys()\nelse False\n)\nif disable_dbfs_retry:\nS3FileManager(configs).get_function()\nelif FileStorageFunctions.is_boto3_configured():\ntry:\nS3FileManager(configs).get_function()\nexcept (ValueError, NotImplementedError, RestoreTypeNotFoundException):\nraise\nexcept Exception:\nDBFSFileManager(configs).get_function()\nelse:\nDBFSFileManager(configs).get_function()\n</code></pre>"},{"location":"reference/packages/core/file_manager.html#packages.core.file_manager.FileManagerFactory.execute_function","title":"<code>execute_function(configs)</code>  <code>staticmethod</code>","text":"<p>Get a specific File Manager and function to execute.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/file_manager.py</code> <pre><code>@staticmethod\ndef execute_function(configs: dict) -&gt; Any:\n\"\"\"Get a specific File Manager and function to execute.\"\"\"\nfrom lakehouse_engine.core.dbfs_file_manager import DBFSFileManager\nfrom lakehouse_engine.core.s3_file_manager import S3FileManager\ndisable_dbfs_retry = (\nconfigs[\"disable_dbfs_retry\"]\nif \"disable_dbfs_retry\" in configs.keys()\nelse False\n)\nif disable_dbfs_retry:\nS3FileManager(configs).get_function()\nelif FileStorageFunctions.is_boto3_configured():\ntry:\nS3FileManager(configs).get_function()\nexcept (ValueError, NotImplementedError, RestoreTypeNotFoundException):\nraise\nexcept Exception:\nDBFSFileManager(configs).get_function()\nelse:\nDBFSFileManager(configs).get_function()\n</code></pre>"},{"location":"reference/packages/core/gab_manager.html","title":"Gab manager","text":"<p>Module to define GAB Manager classes.</p>"},{"location":"reference/packages/core/gab_manager.html#packages.core.gab_manager.GABCadenceManager","title":"<code>GABCadenceManager</code>","text":"<p>         Bases: <code>object</code></p> <p>Class to control the GAB Cadence Window.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_manager.py</code> <pre><code>class GABCadenceManager(object):\n\"\"\"Class to control the GAB Cadence Window.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef extended_window_calculator(\nself,\ncadence: str,\nreconciliation_cadence: str,\ncurrent_date: datetime,\nstart_date_str: str,\nend_date_str: str,\nquery_type: str,\nrerun_flag: str,\nsnapshot_flag: str,\n) -&gt; tuple[datetime, datetime, datetime, datetime]:\n\"\"\"extended_window_calculator function.\n        Calculates the extended window of any cadence despite the user providing\n        custom dates which are not the exact start and end dates of a cadence.\n        Args:\n            cadence: cadence to process\n            reconciliation_cadence: reconciliation to process.\n            current_date: current date.\n            start_date_str: start date of the period to process.\n            end_date_str: end date of the period to process.\n            query_type: use case query type.\n            rerun_flag: flag indicating if it's a rerun or a normal run.\n            snapshot_flag: flag indicating if for this cadence the snapshot is enabled.\n        \"\"\"\ncad_order = GABCadence.get_ordered_cadences()\nderived_cadence = self._get_reconciliation_cadence(\ncad_order, rerun_flag, cadence, reconciliation_cadence, snapshot_flag\n)\nself._LOGGER.info(f\"cadence passed to extended window: {derived_cadence}\")\nstart_date = datetime.strptime(start_date_str, GABDefaults.DATE_FORMAT.value)\nend_date = datetime.strptime(end_date_str, GABDefaults.DATE_FORMAT.value)\nbucket_start_date, bucket_end_date = self.get_cadence_start_end_dates(\ncadence, derived_cadence, start_date, end_date, query_type, current_date\n)\nself._LOGGER.info(f\"bucket dates: {bucket_start_date} - {bucket_end_date}\")\nfilter_start_date, filter_end_date = self.get_cadence_start_end_dates(\ncadence,\n(\nreconciliation_cadence\nif cad_order[cadence] &lt; cad_order[reconciliation_cadence]\nelse cadence\n),\nstart_date,\nend_date,\nquery_type,\ncurrent_date,\n)\nself._LOGGER.info(f\"filter dates: {filter_start_date} - {filter_end_date}\")\nreturn bucket_start_date, bucket_end_date, filter_start_date, filter_end_date\n@classmethod\ndef _get_reconciliation_cadence(\ncls,\ncadence_order: dict,\nrerun_flag: str,\ncadence: str,\nreconciliation_cadence: str,\nsnapshot_flag: str,\n) -&gt; str:\n\"\"\"Get bigger cadence when rerun_flag or snapshot.\n        Args:\n            cadence_order: ordered cadences.\n            rerun_flag: flag indicating if it's a rerun or a normal run.\n            cadence: cadence to process.\n            reconciliation_cadence: reconciliation to process.\n            snapshot_flag: flag indicating if for this cadence the snapshot is enabled.\n        \"\"\"\nderived_cadence = reconciliation_cadence\nif rerun_flag == \"Y\":\nif cadence_order[cadence] &gt; cadence_order[reconciliation_cadence]:\nderived_cadence = cadence\nelif cadence_order[cadence] &lt; cadence_order[reconciliation_cadence]:\nderived_cadence = reconciliation_cadence\nelse:\nif (\ncadence_order[cadence] &gt; cadence_order[reconciliation_cadence]\nand snapshot_flag == \"Y\"\n) or (cadence_order[cadence] &lt; cadence_order[reconciliation_cadence]):\nderived_cadence = reconciliation_cadence\nelif (\ncadence_order[cadence] &gt; cadence_order[reconciliation_cadence]\nand snapshot_flag == \"N\"\n):\nderived_cadence = cadence\nreturn derived_cadence\ndef get_cadence_start_end_dates(\nself,\ncadence: str,\nderived_cadence: str,\nstart_date: datetime,\nend_date: datetime,\nquery_type: str,\ncurrent_date: datetime,\n) -&gt; tuple[datetime, datetime]:\n\"\"\"Generate the new set of extended start and end dates based on the cadence.\n        Running week cadence again to extend to correct week start and end date in case\n            of recon window for Week cadence is present.\n        For end_date 2012-12-31,in case of Quarter Recon window present for Week\n            cadence, start and end dates are recalculated to 2022-10-01 to 2022-12-31.\n        But these are not start and end dates of week. Hence, to correct this, new dates\n            are passed again to get the correct dates.\n        Args:\n            cadence: cadence to process.\n            derived_cadence: cadence reconciliation to process.\n            start_date: start date of the period to process.\n            end_date: end date of the period to process.\n            query_type: use case query type.\n            current_date: current date to be used in the end date, in case the end date\n                is greater than current date so the end date should be the current date.\n        \"\"\"\nnew_start_date = self._get_cadence_calculated_date(\nderived_cadence=derived_cadence, base_date=start_date, is_start=True\n)\nnew_end_date = self._get_cadence_calculated_date(\nderived_cadence=derived_cadence, base_date=end_date, is_start=False\n)\nif cadence.upper() == \"WEEK\":\nnew_start_date = (\npendulum.datetime(\nint(new_start_date.strftime(\"%Y\")),\nint(new_start_date.strftime(\"%m\")),\nint(new_start_date.strftime(\"%d\")),\n)\n.start_of(\"week\")\n.replace(tzinfo=None)\n)\nnew_end_date = (\npendulum.datetime(\nint(new_end_date.strftime(\"%Y\")),\nint(new_end_date.strftime(\"%m\")),\nint(new_end_date.strftime(\"%d\")),\n)\n.end_of(\"week\")\n.replace(hour=0, minute=0, second=0, microsecond=0)\n.replace(tzinfo=None)\n)\nnew_end_date = new_end_date + timedelta(days=1)\nif new_end_date &gt;= current_date:\nnew_end_date = current_date\nif query_type == \"NAM\":\nnew_end_date = new_end_date + timedelta(days=1)\nreturn new_start_date, new_end_date\n@classmethod\ndef _get_cadence_calculated_date(\ncls, derived_cadence: str, base_date: datetime, is_start: bool\n) -&gt; Union[datetime, DateTime]:  # type: ignore\ncadence_base_date = cls._get_cadence_base_date(derived_cadence, base_date)\ncadence_date_calculated: Union[DateTime, datetime]\nif derived_cadence.upper() == \"WEEK\":\ncadence_date_calculated = cls._get_calculated_week_date(\ncast(DateTime, cadence_base_date), is_start\n)\nelif derived_cadence.upper() == \"MONTH\":\ncadence_date_calculated = cls._get_calculated_month_date(\ncast(datetime, cadence_base_date), is_start\n)\nelif derived_cadence.upper() in [\"QUARTER\", \"YEAR\"]:\ncadence_date_calculated = cls._get_calculated_quarter_or_year_date(\ncast(DateTime, cadence_base_date), is_start, derived_cadence\n)\nelse:\ncadence_date_calculated = cadence_base_date  # type: ignore\nreturn cadence_date_calculated  # type: ignore\n@classmethod\ndef _get_cadence_base_date(\ncls, derived_cadence: str, base_date: datetime\n) -&gt; Union[datetime, DateTime, str]:  # type: ignore\n\"\"\"Get start date for the selected cadence.\n        Args:\n            derived_cadence: cadence reconciliation to process.\n            base_date: base date used to compute the start date of the cadence.\n        \"\"\"\nif derived_cadence.upper() in [\"DAY\", \"MONTH\"]:\ncadence_date_calculated = base_date\nelif derived_cadence.upper() in [\"WEEK\", \"QUARTER\", \"YEAR\"]:\ncadence_date_calculated = pendulum.datetime(\nint(base_date.strftime(\"%Y\")),\nint(base_date.strftime(\"%m\")),\nint(base_date.strftime(\"%d\")),\n)\nelse:\ncadence_date_calculated = \"0\"  # type: ignore\nreturn cadence_date_calculated\n@classmethod\ndef _get_calculated_week_date(\ncls, cadence_date_calculated: DateTime, is_start: bool\n) -&gt; DateTime:\n\"\"\"Get WEEK start/end date.\n        Args:\n            cadence_date_calculated: base date to compute the week date.\n            is_start: flag indicating if we should get the start or end for the cadence.\n        \"\"\"\nif is_start:\ncadence_date_calculated = cadence_date_calculated.start_of(\"week\").replace(\ntzinfo=None\n)\nelse:\ncadence_date_calculated = (\ncadence_date_calculated.end_of(\"week\")\n.replace(hour=0, minute=0, second=0, microsecond=0)\n.replace(tzinfo=None)\n)\nreturn cadence_date_calculated\n@classmethod\ndef _get_calculated_month_date(\ncls, cadence_date_calculated: datetime, is_start: bool\n) -&gt; datetime:\n\"\"\"Get MONTH start/end date.\n        Args:\n            cadence_date_calculated: base date to compute the month date.\n            is_start: flag indicating if we should get the start or end for the cadence.\n        \"\"\"\nif is_start:\ncadence_date_calculated = cadence_date_calculated - timedelta(\ndays=(int(cadence_date_calculated.strftime(\"%d\")) - 1)\n)\nelse:\ncadence_date_calculated = datetime(\nint(cadence_date_calculated.strftime(\"%Y\")),\nint(cadence_date_calculated.strftime(\"%m\")),\ncalendar.monthrange(\nint(cadence_date_calculated.strftime(\"%Y\")),\nint(cadence_date_calculated.strftime(\"%m\")),\n)[1],\n)\nreturn cadence_date_calculated\n@classmethod\ndef _get_calculated_quarter_or_year_date(\ncls, cadence_date_calculated: DateTime, is_start: bool, cadence: str\n) -&gt; DateTime:\n\"\"\"Get QUARTER/YEAR start/end date.\n        Args:\n            cadence_date_calculated: base date to compute the quarter/year date.\n            is_start: flag indicating if we should get the start or end for the cadence.\n            cadence: selected cadence (possible values: QUARTER or YEAR).\n        \"\"\"\nif is_start:\ncadence_date_calculated = cadence_date_calculated.first_of(\ncadence.lower()\n).replace(tzinfo=None)\nelse:\ncadence_date_calculated = cadence_date_calculated.last_of(\ncadence.lower()\n).replace(tzinfo=None)\nreturn cadence_date_calculated\n</code></pre>"},{"location":"reference/packages/core/gab_manager.html#packages.core.gab_manager.GABCadenceManager.extended_window_calculator","title":"<code>extended_window_calculator(cadence, reconciliation_cadence, current_date, start_date_str, end_date_str, query_type, rerun_flag, snapshot_flag)</code>","text":"<p>extended_window_calculator function.</p> <p>Calculates the extended window of any cadence despite the user providing custom dates which are not the exact start and end dates of a cadence.</p> <p>Parameters:</p> Name Type Description Default <code>cadence</code> <code>str</code> <p>cadence to process</p> required <code>reconciliation_cadence</code> <code>str</code> <p>reconciliation to process.</p> required <code>current_date</code> <code>datetime</code> <p>current date.</p> required <code>start_date_str</code> <code>str</code> <p>start date of the period to process.</p> required <code>end_date_str</code> <code>str</code> <p>end date of the period to process.</p> required <code>query_type</code> <code>str</code> <p>use case query type.</p> required <code>rerun_flag</code> <code>str</code> <p>flag indicating if it's a rerun or a normal run.</p> required <code>snapshot_flag</code> <code>str</code> <p>flag indicating if for this cadence the snapshot is enabled.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_manager.py</code> <pre><code>def extended_window_calculator(\nself,\ncadence: str,\nreconciliation_cadence: str,\ncurrent_date: datetime,\nstart_date_str: str,\nend_date_str: str,\nquery_type: str,\nrerun_flag: str,\nsnapshot_flag: str,\n) -&gt; tuple[datetime, datetime, datetime, datetime]:\n\"\"\"extended_window_calculator function.\n    Calculates the extended window of any cadence despite the user providing\n    custom dates which are not the exact start and end dates of a cadence.\n    Args:\n        cadence: cadence to process\n        reconciliation_cadence: reconciliation to process.\n        current_date: current date.\n        start_date_str: start date of the period to process.\n        end_date_str: end date of the period to process.\n        query_type: use case query type.\n        rerun_flag: flag indicating if it's a rerun or a normal run.\n        snapshot_flag: flag indicating if for this cadence the snapshot is enabled.\n    \"\"\"\ncad_order = GABCadence.get_ordered_cadences()\nderived_cadence = self._get_reconciliation_cadence(\ncad_order, rerun_flag, cadence, reconciliation_cadence, snapshot_flag\n)\nself._LOGGER.info(f\"cadence passed to extended window: {derived_cadence}\")\nstart_date = datetime.strptime(start_date_str, GABDefaults.DATE_FORMAT.value)\nend_date = datetime.strptime(end_date_str, GABDefaults.DATE_FORMAT.value)\nbucket_start_date, bucket_end_date = self.get_cadence_start_end_dates(\ncadence, derived_cadence, start_date, end_date, query_type, current_date\n)\nself._LOGGER.info(f\"bucket dates: {bucket_start_date} - {bucket_end_date}\")\nfilter_start_date, filter_end_date = self.get_cadence_start_end_dates(\ncadence,\n(\nreconciliation_cadence\nif cad_order[cadence] &lt; cad_order[reconciliation_cadence]\nelse cadence\n),\nstart_date,\nend_date,\nquery_type,\ncurrent_date,\n)\nself._LOGGER.info(f\"filter dates: {filter_start_date} - {filter_end_date}\")\nreturn bucket_start_date, bucket_end_date, filter_start_date, filter_end_date\n</code></pre>"},{"location":"reference/packages/core/gab_manager.html#packages.core.gab_manager.GABCadenceManager.get_cadence_start_end_dates","title":"<code>get_cadence_start_end_dates(cadence, derived_cadence, start_date, end_date, query_type, current_date)</code>","text":"<p>Generate the new set of extended start and end dates based on the cadence.</p> <p>Running week cadence again to extend to correct week start and end date in case     of recon window for Week cadence is present. For end_date 2012-12-31,in case of Quarter Recon window present for Week     cadence, start and end dates are recalculated to 2022-10-01 to 2022-12-31. But these are not start and end dates of week. Hence, to correct this, new dates     are passed again to get the correct dates.</p> <p>Parameters:</p> Name Type Description Default <code>cadence</code> <code>str</code> <p>cadence to process.</p> required <code>derived_cadence</code> <code>str</code> <p>cadence reconciliation to process.</p> required <code>start_date</code> <code>datetime</code> <p>start date of the period to process.</p> required <code>end_date</code> <code>datetime</code> <p>end date of the period to process.</p> required <code>query_type</code> <code>str</code> <p>use case query type.</p> required <code>current_date</code> <code>datetime</code> <p>current date to be used in the end date, in case the end date is greater than current date so the end date should be the current date.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_manager.py</code> <pre><code>def get_cadence_start_end_dates(\nself,\ncadence: str,\nderived_cadence: str,\nstart_date: datetime,\nend_date: datetime,\nquery_type: str,\ncurrent_date: datetime,\n) -&gt; tuple[datetime, datetime]:\n\"\"\"Generate the new set of extended start and end dates based on the cadence.\n    Running week cadence again to extend to correct week start and end date in case\n        of recon window for Week cadence is present.\n    For end_date 2012-12-31,in case of Quarter Recon window present for Week\n        cadence, start and end dates are recalculated to 2022-10-01 to 2022-12-31.\n    But these are not start and end dates of week. Hence, to correct this, new dates\n        are passed again to get the correct dates.\n    Args:\n        cadence: cadence to process.\n        derived_cadence: cadence reconciliation to process.\n        start_date: start date of the period to process.\n        end_date: end date of the period to process.\n        query_type: use case query type.\n        current_date: current date to be used in the end date, in case the end date\n            is greater than current date so the end date should be the current date.\n    \"\"\"\nnew_start_date = self._get_cadence_calculated_date(\nderived_cadence=derived_cadence, base_date=start_date, is_start=True\n)\nnew_end_date = self._get_cadence_calculated_date(\nderived_cadence=derived_cadence, base_date=end_date, is_start=False\n)\nif cadence.upper() == \"WEEK\":\nnew_start_date = (\npendulum.datetime(\nint(new_start_date.strftime(\"%Y\")),\nint(new_start_date.strftime(\"%m\")),\nint(new_start_date.strftime(\"%d\")),\n)\n.start_of(\"week\")\n.replace(tzinfo=None)\n)\nnew_end_date = (\npendulum.datetime(\nint(new_end_date.strftime(\"%Y\")),\nint(new_end_date.strftime(\"%m\")),\nint(new_end_date.strftime(\"%d\")),\n)\n.end_of(\"week\")\n.replace(hour=0, minute=0, second=0, microsecond=0)\n.replace(tzinfo=None)\n)\nnew_end_date = new_end_date + timedelta(days=1)\nif new_end_date &gt;= current_date:\nnew_end_date = current_date\nif query_type == \"NAM\":\nnew_end_date = new_end_date + timedelta(days=1)\nreturn new_start_date, new_end_date\n</code></pre>"},{"location":"reference/packages/core/gab_manager.html#packages.core.gab_manager.GABViewManager","title":"<code>GABViewManager</code>","text":"<p>         Bases: <code>object</code></p> <p>Class to control the GAB View creation.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_manager.py</code> <pre><code>class GABViewManager(object):\n\"\"\"Class to control the GAB View creation.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef __init__(\nself,\nquery_id: str,\nlookup_query_builder: DataFrame,\ntarget_database: str,\ntarget_table: str,\n):\n\"\"\"Construct GABViewManager instances.\n        Args:\n            query_id: gab configuration table use case identifier.\n            lookup_query_builder: gab configuration data.\n            target_database: target database to write.\n            target_table: target table to write.\n        \"\"\"\nself.query_id = query_id\nself.lookup_query_builder = lookup_query_builder\nself.target_database = target_database\nself.target_table = target_table\ndef generate_use_case_views(self) -&gt; None:\n\"\"\"Generate all the use case views.\n        Generates the DDLs for each of the views. This DDL is dynamically built based on\n        the mappings provided in the config table.\n        \"\"\"\nreconciliation_window = GABUtils.get_json_column_as_dict(\nself.lookup_query_builder, self.query_id, \"recon_window\"\n)\ncadence_snapshot_status = self._get_cadence_snapshot_status(\nreconciliation_window\n)\n(\ncadences_with_snapshot,\ncadences_without_snapshot,\n) = self._split_cadence_by_snapshot(cadence_snapshot_status)\nmappings = GABUtils.get_json_column_as_dict(\nself.lookup_query_builder, self.query_id, \"mappings\"\n)\nfor view_name in mappings.keys():\nself._generate_use_case_view(\nmappings,\nview_name,\ncadence_snapshot_status,\ncadences_with_snapshot,\ncadences_without_snapshot,\nself.target_database,\nself.target_table,\nself.query_id,\n)\n@classmethod\ndef _generate_use_case_view(\ncls,\nmappings: dict,\nview_name: str,\ncadence_snapshot_status: dict,\ncadences_with_snapshot: list[str],\ncadences_without_snapshot: list[str],\ntarget_database: str,\ntarget_table: str,\nquery_id: str,\n) -&gt; None:\n\"\"\"Generate the selected use case views.\n        Args:\n            mappings: use case mappings configuration.\n            view_name: name of the view to be generated.\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n            cadences_with_snapshot: cadences to execute with snapshot.\n            cadences_without_snapshot: cadences to execute without snapshot.\n            target_database: target database to write.\n            target_table: target table to write.\n            query_id: gab configuration table use case identifier.\n        \"\"\"\nview_configuration = mappings[view_name]\nview_dimensions = view_configuration[\"dimensions\"]\nview_metrics = view_configuration[\"metric\"]\ncustom_filter = view_configuration[\"filter\"]\nview_filter = \" \"\nif custom_filter:\nview_filter = \" AND \" + custom_filter\n(\ndimensions,\ndimensions_and_metrics,\ndimensions_and_metrics_with_alias,\n) = cls._get_dimensions_and_metrics_from_use_case_view(\nview_dimensions, view_metrics\n)\n(\nfinal_cols,\nfinal_calculated_script,\nfinal_calculated_script_snapshot,\n) = cls._get_calculated_and_derived_metrics_from_use_case_view(\nview_metrics, view_dimensions, cadence_snapshot_status\n)\nGABViewGenerator(\ncadence_snapshot_status=cadence_snapshot_status,\ntarget_database=target_database,\nview_name=view_name,\nfinal_cols=final_cols,\ntarget_table=target_table,\ndimensions_and_metrics_with_alias=dimensions_and_metrics_with_alias,\ndimensions=dimensions,\ndimensions_and_metrics=dimensions_and_metrics,\nfinal_calculated_script=final_calculated_script,\nquery_id=query_id,\nview_filter=view_filter,\nfinal_calculated_script_snapshot=final_calculated_script_snapshot,\nwithout_snapshot_cadences=cadences_without_snapshot,\nwith_snapshot_cadences=cadences_with_snapshot,\n).generate_sql()\n@classmethod\ndef _get_dimensions_and_metrics_from_use_case_view(\ncls, view_dimensions: dict, view_metrics: dict\n) -&gt; Tuple[str, str, str]:\n\"\"\"Get dimensions and metrics from use case.\n        Args:\n            view_dimensions: use case configured dimensions.\n            view_metrics: use case configured metrics.\n        \"\"\"\n(\nextracted_dimensions_with_alias,\nextracted_dimensions_without_alias,\n) = GABUtils.extract_columns_from_mapping(\ncolumns=view_dimensions,\nis_dimension=True,\nextract_column_without_alias=True,\ntable_alias=\"a\",\nis_extracted_value_as_name=False,\n)\ndimensions_without_default_columns = [\nextracted_dimension\nfor extracted_dimension in extracted_dimensions_without_alias\nif extracted_dimension not in GABDefaults.DIMENSIONS_DEFAULT_COLUMNS.value\n]\ndimensions = \",\".join(dimensions_without_default_columns)\ndimensions_with_alias = \",\".join(extracted_dimensions_with_alias)\n(\nextracted_metrics_with_alias,\nextracted_metrics_without_alias,\n) = GABUtils.extract_columns_from_mapping(\ncolumns=view_metrics,\nis_dimension=False,\nextract_column_without_alias=True,\ntable_alias=\"a\",\nis_extracted_value_as_name=False,\n)\nmetrics = \",\".join(extracted_metrics_without_alias)\nmetrics_with_alias = \",\".join(extracted_metrics_with_alias)\ndimensions_and_metrics_with_alias = (\ndimensions_with_alias + \",\" + metrics_with_alias\n)\ndimensions_and_metrics = dimensions + \",\" + metrics\nreturn dimensions, dimensions_and_metrics, dimensions_and_metrics_with_alias\n@classmethod\ndef _get_calculated_and_derived_metrics_from_use_case_view(\ncls, view_metrics: dict, view_dimensions: dict, cadence_snapshot_status: dict\n) -&gt; Tuple[str, str, str]:\n\"\"\"Get calculated and derived metrics from use case.\n        Args:\n            view_dimensions: use case configured dimensions.\n            view_metrics: use case configured metrics.\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n        \"\"\"\ncalculated_script = []\ncalculated_script_snapshot = []\nderived_script = []\nfor metric_key, metric_value in view_metrics.items():\n(\ncalculated_metrics_script,\ncalculated_metrics_script_snapshot,\nderived_metrics_script,\n) = cls._get_calculated_metrics(\nmetric_key, metric_value, view_dimensions, cadence_snapshot_status\n)\ncalculated_script += [*calculated_metrics_script]\ncalculated_script_snapshot += [*calculated_metrics_script_snapshot]\nderived_script += [*derived_metrics_script]\njoined_calculated_script = cls._join_list_to_string_when_present(\ncalculated_script\n)\njoined_calculated_script_snapshot = cls._join_list_to_string_when_present(\ncalculated_script_snapshot\n)\njoined_derived = cls._join_list_to_string_when_present(\nto_join=derived_script, starting_value=\"*,\", default_value=\"*\"\n)\nreturn (\njoined_derived,\njoined_calculated_script,\njoined_calculated_script_snapshot,\n)\n@classmethod\ndef _join_list_to_string_when_present(\ncls,\nto_join: list[str],\nseparator: str = \",\",\nstarting_value: str = \",\",\ndefault_value: str = \"\",\n) -&gt; str:\n\"\"\"Join list to string when has values, otherwise return the default value.\n        Args:\n            to_join: values to join.\n            separator: separator to be used in the join.\n            starting_value: value to be started before the join.\n            default_value: value to be returned if the list is empty.\n        \"\"\"\nreturn starting_value + separator.join(to_join) if to_join else default_value\n@classmethod\ndef _get_cadence_snapshot_status(cls, result: dict) -&gt; dict:\ncadence_snapshot_status = {}\nfor k, v in result.items():\ncadence_snapshot_status[k] = next(\n(\nnext(\n(\nsnap_list[\"snapshot\"]\nfor snap_list in loop_outer_cad.values()\nif snap_list[\"snapshot\"] == \"Y\"\n),\n\"N\",\n)\nfor loop_outer_cad in v.values()\nif v\n),\n\"N\",\n)\nreturn cadence_snapshot_status\n@classmethod\ndef _split_cadence_by_snapshot(\ncls, cadence_snapshot_status: dict\n) -&gt; tuple[list[str], list[str]]:\n\"\"\"Split cadences by the snapshot value.\n        Args:\n            cadence_snapshot_status: cadences to be split by snapshot status.\n        \"\"\"\nwith_snapshot_cadences = []\nwithout_snapshot_cadences = []\nfor key_snap_status, value_snap_status in cadence_snapshot_status.items():\nif value_snap_status == \"Y\":\nwith_snapshot_cadences.append(key_snap_status)\nelse:\nwithout_snapshot_cadences.append(key_snap_status)\nreturn with_snapshot_cadences, without_snapshot_cadences\n@classmethod\ndef _get_calculated_metrics(\ncls,\nmetric_key: str,\nmetric_value: dict,\nview_dimensions: dict,\ncadence_snapshot_status: dict,\n) -&gt; tuple[list[str], list[str], list[str]]:\n\"\"\"Get calculated metrics from use case.\n        Args:\n            metric_key: use case metric name.\n            metric_value: use case metric value.\n            view_dimensions: use case configured dimensions.\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n        \"\"\"\ndim_partition = \",\".join([str(i) for i in view_dimensions.keys()][2:])\ndim_partition = \"cadence,\" + dim_partition\ncalculated_metrics = metric_value[\"calculated_metric\"]\nderived_metrics = metric_value[\"derived_metric\"]\ncalculated_metrics_script: list[str] = []\ncalculated_metrics_script_snapshot: list[str] = []\nderived_metrics_script: list[str] = []\nif calculated_metrics:\n(\ncalculated_metrics_script,\ncalculated_metrics_script_snapshot,\n) = cls._get_calculated_metric(\nmetric_key, calculated_metrics, dim_partition, cadence_snapshot_status\n)\nif derived_metrics:\nderived_metrics_script = cls._get_derived_metrics(derived_metrics)\nreturn (\ncalculated_metrics_script,\ncalculated_metrics_script_snapshot,\nderived_metrics_script,\n)\n@classmethod\ndef _get_derived_metrics(cls, derived_metric: dict) -&gt; list[str]:\n\"\"\"Get derived metrics from use case.\n        Args:\n            derived_metric: use case derived metrics.\n        \"\"\"\nderived_metric_script = []\nfor i in range(0, len(derived_metric)):\nderived_formula = str(derived_metric[i][\"formula\"])\nderived_label = derived_metric[i][\"label\"]\nderived_metric_script.append(derived_formula + \" AS \" + derived_label)\nreturn derived_metric_script\n@classmethod\ndef _get_calculated_metric(\ncls,\nmetric_key: str,\ncalculated_metric: dict,\ndimension_partition: str,\ncadence_snapshot_status: dict,\n) -&gt; tuple[list[str], list[str]]:\n\"\"\"Get calculated metrics from use case.\n        Args:\n            metric_key: use case metric name.\n            calculated_metric: use case calculated metrics.\n            dimension_partition: dimension partition.\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n        \"\"\"\nlast_cadence_script: list[str] = []\nlast_year_cadence_script: list[str] = []\nwindow_script: list[str] = []\nlast_cadence_script_snapshot: list[str] = []\nlast_year_cadence_script_snapshot: list[str] = []\nwindow_script_snapshot: list[str] = []\nif \"last_cadence\" in calculated_metric:\n(\nlast_cadence_script,\nlast_cadence_script_snapshot,\n) = cls._get_cadence_calculated_metric(\nmetric_key,\ndimension_partition,\ncalculated_metric,\ncadence_snapshot_status,\n\"last_cadence\",\n)\nif \"last_year_cadence\" in calculated_metric:\n(\nlast_year_cadence_script,\nlast_year_cadence_script_snapshot,\n) = cls._get_cadence_calculated_metric(\nmetric_key,\ndimension_partition,\ncalculated_metric,\ncadence_snapshot_status,\n\"last_year_cadence\",\n)\nif \"window_function\" in calculated_metric:\nwindow_script, window_script_snapshot = cls._get_window_calculated_metric(\nmetric_key,\ndimension_partition,\ncalculated_metric,\ncadence_snapshot_status,\n)\ncalculated_script = [\n*last_cadence_script,\n*last_year_cadence_script,\n*window_script,\n]\ncalculated_script_snapshot = [\n*last_cadence_script_snapshot,\n*last_year_cadence_script_snapshot,\n*window_script_snapshot,\n]\nreturn calculated_script, calculated_script_snapshot\n@classmethod\ndef _get_window_calculated_metric(\ncls,\nmetric_key: str,\ndimension_partition: str,\ncalculated_metric: dict,\ncadence_snapshot_status: dict,\n) -&gt; tuple[list, list]:\n\"\"\"Get window calculated metrics from use case.\n        Args:\n            metric_key: use case metric name.\n            dimension_partition: dimension partition.\n            calculated_metric: use case calculated metrics.\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n        \"\"\"\ncalculated_script = []\ncalculated_script_snapshot = []\nfor i in range(0, len(calculated_metric[\"window_function\"])):\nwindow_function = calculated_metric[\"window_function\"][i][\"agg_func\"]\nwindow_function_start = calculated_metric[\"window_function\"][i][\"window\"][0]\nwindow_function_end = calculated_metric[\"window_function\"][i][\"window\"][1]\nwindow_label = calculated_metric[\"window_function\"][i][\"label\"]\ncalculated_script.append(\nf\"\"\"\n                NVL(\n{window_function}({metric_key}) OVER\n                    (\n                        PARTITION BY {dimension_partition}\n                        order by from_date ROWS BETWEEN\n{str(window_function_start)} PRECEDING\n                            AND {str(window_function_end)} PRECEDING\n                    ),\n                    0\n                ) AS\n{window_label}\n                \"\"\"\n)\nif \"Y\" in cadence_snapshot_status.values():\ncalculated_script_snapshot.append(\nf\"\"\"\n                    NVL(\n{window_function}({metric_key}) OVER\n                        (\n                            PARTITION BY {dimension_partition} ,rn\n                            order by from_date ROWS BETWEEN\n{str(window_function_start)} PRECEDING\n                                AND {str(window_function_end)} PRECEDING\n                        ),\n                        0\n                    ) AS\n{window_label}\n                    \"\"\"\n)\nreturn calculated_script, calculated_script_snapshot\n@classmethod\ndef _get_cadence_calculated_metric(\ncls,\nmetric_key: str,\ndimension_partition: str,\ncalculated_metric: dict,\ncadence_snapshot_status: dict,\ncadence: str,\n) -&gt; tuple[list, list]:\n\"\"\"Get cadence calculated metrics from use case.\n        Args:\n            metric_key: use case metric name.\n            calculated_metric: use case calculated metrics.\n            dimension_partition: dimension partition.\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n            cadence: cadence to process.\n        \"\"\"\ncalculated_script = []\ncalculated_script_snapshot = []\nfor i in range(0, len(calculated_metric[cadence])):\ncadence_lag = cls._get_cadence_item_lag(calculated_metric, cadence, i)\ncadence_label = calculated_metric[cadence][i][\"label\"]\ncalculated_script.append(\ncls._get_cadence_lag_statement(\nmetric_key,\ncadence_lag,\ndimension_partition,\ncadence_label,\nsnapshot=False,\ncadence=cadence,\n)\n)\nif \"Y\" in cadence_snapshot_status.values():\ncalculated_script_snapshot.append(\ncls._get_cadence_lag_statement(\nmetric_key,\ncadence_lag,\ndimension_partition,\ncadence_label,\nsnapshot=True,\ncadence=cadence,\n)\n)\nreturn calculated_script, calculated_script_snapshot\n@classmethod\ndef _get_cadence_item_lag(\ncls, calculated_metric: dict, cadence: str, item: int\n) -&gt; str:\n\"\"\"Get calculated metric item lag.\n        Args:\n            calculated_metric: use case calculated metrics.\n            cadence: cadence to process.\n            item: metric item.\n        \"\"\"\nreturn str(calculated_metric[cadence][item][\"window\"])\n@classmethod\ndef _get_cadence_lag_statement(\ncls,\nmetric_key: str,\ncadence_lag: str,\ndimension_partition: str,\ncadence_label: str,\nsnapshot: bool,\ncadence: str,\n) -&gt; str:\n\"\"\"Get cadence lag statement.\n        Args:\n            metric_key: use case metric name.\n            cadence_lag: cadence window lag.\n            dimension_partition: dimension partition.\n            cadence_label: cadence name.\n            snapshot: indicate if the snapshot is enabled.\n            cadence: cadence to process.\n        \"\"\"\ncadence_lag_statement = \"\"\nif cadence == \"last_cadence\":\ncadence_lag_statement = (\n\"NVL(LAG(\"\n+ metric_key\n+ \",\"\n+ cadence_lag\n+ \") OVER(PARTITION BY \"\n+ dimension_partition\n+ (\",rn\" if snapshot else \"\")\n+ \" order by from_date),0) AS \"\n+ cadence_label\n)\nelif cadence == \"last_year_cadence\":\ncadence_lag_statement = (\n\"NVL(LAG(\"\n+ metric_key\n+ \",\"\n+ cadence_lag\n+ \") OVER(PARTITION BY \"\n+ dimension_partition\n+ (\",rn\" if snapshot else \"\")\n+ \"\"\",\n                    case\n                        when cadence in ('DAY','MONTH','QUARTER')\n                            then struct(month(from_date), day(from_date))\n                        when cadence in('WEEK')\n                            then struct(weekofyear(from_date+1),1)\n                    end order by from_date),0) AS \"\"\"\n+ cadence_label\n)\nelse:\ncls._LOGGER.error(f\"Cadence {cadence} not implemented yet\")\nreturn cadence_lag_statement\n</code></pre>"},{"location":"reference/packages/core/gab_manager.html#packages.core.gab_manager.GABViewManager.__init__","title":"<code>__init__(query_id, lookup_query_builder, target_database, target_table)</code>","text":"<p>Construct GABViewManager instances.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>str</code> <p>gab configuration table use case identifier.</p> required <code>lookup_query_builder</code> <code>DataFrame</code> <p>gab configuration data.</p> required <code>target_database</code> <code>str</code> <p>target database to write.</p> required <code>target_table</code> <code>str</code> <p>target table to write.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_manager.py</code> <pre><code>def __init__(\nself,\nquery_id: str,\nlookup_query_builder: DataFrame,\ntarget_database: str,\ntarget_table: str,\n):\n\"\"\"Construct GABViewManager instances.\n    Args:\n        query_id: gab configuration table use case identifier.\n        lookup_query_builder: gab configuration data.\n        target_database: target database to write.\n        target_table: target table to write.\n    \"\"\"\nself.query_id = query_id\nself.lookup_query_builder = lookup_query_builder\nself.target_database = target_database\nself.target_table = target_table\n</code></pre>"},{"location":"reference/packages/core/gab_manager.html#packages.core.gab_manager.GABViewManager.generate_use_case_views","title":"<code>generate_use_case_views()</code>","text":"<p>Generate all the use case views.</p> <p>Generates the DDLs for each of the views. This DDL is dynamically built based on the mappings provided in the config table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_manager.py</code> <pre><code>def generate_use_case_views(self) -&gt; None:\n\"\"\"Generate all the use case views.\n    Generates the DDLs for each of the views. This DDL is dynamically built based on\n    the mappings provided in the config table.\n    \"\"\"\nreconciliation_window = GABUtils.get_json_column_as_dict(\nself.lookup_query_builder, self.query_id, \"recon_window\"\n)\ncadence_snapshot_status = self._get_cadence_snapshot_status(\nreconciliation_window\n)\n(\ncadences_with_snapshot,\ncadences_without_snapshot,\n) = self._split_cadence_by_snapshot(cadence_snapshot_status)\nmappings = GABUtils.get_json_column_as_dict(\nself.lookup_query_builder, self.query_id, \"mappings\"\n)\nfor view_name in mappings.keys():\nself._generate_use_case_view(\nmappings,\nview_name,\ncadence_snapshot_status,\ncadences_with_snapshot,\ncadences_without_snapshot,\nself.target_database,\nself.target_table,\nself.query_id,\n)\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html","title":"Gab sql generator","text":"<p>Module to define GAB SQL classes.</p>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABDeleteGenerator","title":"<code>GABDeleteGenerator</code>","text":"<p>         Bases: <code>GABSQLGenerator</code></p> <p>GAB delete generator.</p> <p>Creates the delete statement to clean the use case base data on the insights table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>class GABDeleteGenerator(GABSQLGenerator):\n\"\"\"GAB delete generator.\n    Creates the delete statement to clean the use case base data on the insights table.\n    \"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef __init__(\nself,\nquery_id: str,\ncadence: str,\ntemp_stage_view_name: str,\nlookup_query_builder: DataFrame,\ntarget_database: str,\ntarget_table: str,\n):\n\"\"\"Construct GABViewGenerator instances.\n        Args:\n            query_id: gab configuration table use case identifier.\n            cadence:  inputted cadence to process.\n            temp_stage_view_name: stage view name.\n            lookup_query_builder: gab configuration data.\n            target_database: target database to write.\n            target_table: target table to write.\n        \"\"\"\nself.query_id = query_id\nself.cadence = cadence\nself.temp_stage_view_name = temp_stage_view_name\nself.lookup_query_builder = lookup_query_builder\nself.target_database = target_database\nself.target_table = target_table\n@_execute_sql\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Generate delete sql statement.\n        This statement is to clean the insights table for the corresponding use case.\n        \"\"\"\ndelete_sql_statement = self._delete_statement_generator()\nreturn delete_sql_statement\ndef _delete_statement_generator(self) -&gt; str:\ndf_filtered = self.lookup_query_builder.filter(\ncol(\"query_id\") == lit(self.query_id)\n)\ndf_map = df_filtered.select(col(\"mappings\"))\nview_df = df_map.select(\nto_json(struct([df_map[x] for x in df_map.columns]))\n).collect()[0][0]\nline = json.loads(view_df)\nfor line_v in line.values():\nresult = ast.literal_eval(line_v)\nfor result_key in result.keys():\nresult_new = result[result_key]\ndim_from_date = result_new[\"dimensions\"][\"from_date\"]\ndim_to_date = result_new[\"dimensions\"][\"to_date\"]\nself._LOGGER.info(f\"temp stage view name: {self.temp_stage_view_name}\")\nmin_from_date = ExecEnv.SESSION.sql(\n\"\"\"\n            SELECT\n                MIN({from_date}) as min_from_date\n            FROM {iter_stages}\"\"\".format(  # nosec: B608\niter_stages=self.temp_stage_view_name, from_date=dim_from_date\n)\n).collect()[0][0]\nmax_from_date = ExecEnv.SESSION.sql(\n\"\"\"\n            SELECT\n                MAX({from_date}) as max_from_date\n            FROM {iter_stages}\"\"\".format(  # nosec: B608\niter_stages=self.temp_stage_view_name, from_date=dim_from_date\n)\n).collect()[0][0]\nmin_to_date = ExecEnv.SESSION.sql(\n\"\"\"\n            SELECT\n                MIN({to_date}) as min_to_date\n            FROM {iter_stages}\"\"\".format(  # nosec: B608\niter_stages=self.temp_stage_view_name, to_date=dim_to_date\n)\n).collect()[0][0]\nmax_to_date = ExecEnv.SESSION.sql(\n\"\"\"\n            SELECT\n                MAX({to_date}) as max_to_date\n            FROM {iter_stages}\"\"\".format(  # nosec: B608\niter_stages=self.temp_stage_view_name, to_date=dim_to_date\n)\n).collect()[0][0]\ngen_del = \"\"\"\n        DELETE FROM {target_database}.{target_table} a\n            WHERE query_id = {query_id}\n            AND cadence = '{cadence}'\n            AND from_date BETWEEN '{min_from_date}' AND '{max_from_date}'\n            AND to_date BETWEEN '{min_to_date}' AND '{max_to_date}'\n        \"\"\".format(  # nosec: B608\ntarget_database=self.target_database,\ntarget_table=self.target_table,\nquery_id=self.query_id,\ncadence=self.cadence,\nmin_from_date=min_from_date,\nmax_from_date=max_from_date,\nmin_to_date=min_to_date,\nmax_to_date=max_to_date,\n)\nreturn gen_del\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABDeleteGenerator.__init__","title":"<code>__init__(query_id, cadence, temp_stage_view_name, lookup_query_builder, target_database, target_table)</code>","text":"<p>Construct GABViewGenerator instances.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>str</code> <p>gab configuration table use case identifier.</p> required <code>cadence</code> <code>str</code> <p>inputted cadence to process.</p> required <code>temp_stage_view_name</code> <code>str</code> <p>stage view name.</p> required <code>lookup_query_builder</code> <code>DataFrame</code> <p>gab configuration data.</p> required <code>target_database</code> <code>str</code> <p>target database to write.</p> required <code>target_table</code> <code>str</code> <p>target table to write.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>def __init__(\nself,\nquery_id: str,\ncadence: str,\ntemp_stage_view_name: str,\nlookup_query_builder: DataFrame,\ntarget_database: str,\ntarget_table: str,\n):\n\"\"\"Construct GABViewGenerator instances.\n    Args:\n        query_id: gab configuration table use case identifier.\n        cadence:  inputted cadence to process.\n        temp_stage_view_name: stage view name.\n        lookup_query_builder: gab configuration data.\n        target_database: target database to write.\n        target_table: target table to write.\n    \"\"\"\nself.query_id = query_id\nself.cadence = cadence\nself.temp_stage_view_name = temp_stage_view_name\nself.lookup_query_builder = lookup_query_builder\nself.target_database = target_database\nself.target_table = target_table\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABDeleteGenerator.generate_sql","title":"<code>generate_sql()</code>","text":"<p>Generate delete sql statement.</p> <p>This statement is to clean the insights table for the corresponding use case.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>@_execute_sql\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Generate delete sql statement.\n    This statement is to clean the insights table for the corresponding use case.\n    \"\"\"\ndelete_sql_statement = self._delete_statement_generator()\nreturn delete_sql_statement\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABInsertGenerator","title":"<code>GABInsertGenerator</code>","text":"<p>         Bases: <code>GABSQLGenerator</code></p> <p>GAB insert generator.</p> <p>Creates the insert statement based on the dimensions and metrics provided in the configuration table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>class GABInsertGenerator(GABSQLGenerator):\n\"\"\"GAB insert generator.\n    Creates the insert statement based on the dimensions and metrics provided in\n    the configuration table.\n    \"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef __init__(\nself,\nquery_id: str,\ncadence: str,\nfinal_stage_table: str,\nlookup_query_builder: DataFrame,\ntarget_database: str,\ntarget_table: str,\n):\n\"\"\"Construct GABInsertGenerator instances.\n        Args:\n            query_id: gab configuration table use case identifier.\n            cadence:  inputted cadence to process.\n            final_stage_table: stage view name.\n            lookup_query_builder: gab configuration data.\n            target_database: target database to write.\n            target_table: target table to write.\n        \"\"\"\nself.query_id = query_id\nself.cadence = cadence\nself.final_stage_table = final_stage_table\nself.lookup_query_builder = lookup_query_builder\nself.target_database = target_database\nself.target_table = target_table\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Generate insert sql statement to the insights table.\"\"\"\ninsert_sql_statement = self._insert_statement_generator()\nreturn insert_sql_statement\ndef _insert_statement_generator(self) -&gt; str:\n\"\"\"Generate GAB insert statement.\n        Creates the insert statement based on the dimensions and metrics provided in\n        the configuration table.\n        \"\"\"\nresult = GABUtils.get_json_column_as_dict(\nself.lookup_query_builder, self.query_id, \"mappings\"\n)\nfor result_key in result.keys():\njoined_dimensions, joined_metrics = self._get_mapping_columns(\nmapping=result[result_key]\n)\ngen_ins = f\"\"\"\n                INSERT INTO {self.target_database}.{self.target_table}\n                SELECT\n{self.query_id} as query_id,\n                    '{self.cadence}' as cadence,\n{joined_dimensions},\n{joined_metrics},\n                    current_timestamp() as lh_created_on\n                FROM {self.final_stage_table}\n                \"\"\"  # nosec: B608\nreturn gen_ins\n@classmethod\ndef _get_mapping_columns(cls, mapping: dict) -&gt; tuple[str, str]:\n\"\"\"Get mapping columns(dimensions and metrics) as joined string.\n        Args:\n            mapping: use case mappings configuration.\n        \"\"\"\ndimensions_mapping = mapping[\"dimensions\"]\nmetrics_mapping = mapping[\"metric\"]\njoined_dimensions = cls._join_extracted_column_with_filled_columns(\ncolumns=dimensions_mapping, is_dimension=True\n)\njoined_metrics = cls._join_extracted_column_with_filled_columns(\ncolumns=metrics_mapping, is_dimension=False\n)\nreturn joined_dimensions, joined_metrics\n@classmethod\ndef _join_extracted_column_with_filled_columns(\ncls, columns: dict, is_dimension: bool\n) -&gt; str:\n\"\"\"Join extracted columns with empty filled columns.\n        Args:\n            columns: use case columns and values.\n            is_dimension: flag identifying if is a dimension or a metric.\n        \"\"\"\nextracted_columns_with_alias = (\nGABUtils.extract_columns_from_mapping(  # type: ignore\ncolumns=columns, is_dimension=is_dimension\n)\n)\nfilled_columns = cls._fill_empty_columns(\nextracted_columns=extracted_columns_with_alias,  # type: ignore\nis_dimension=is_dimension,\n)\njoined_columns = [*extracted_columns_with_alias, *filled_columns]\nreturn \",\".join(joined_columns)\n@classmethod\ndef _fill_empty_columns(\ncls, extracted_columns: list[str], is_dimension: bool\n) -&gt; list[str]:\n\"\"\"Fill empty columns as null.\n        As the data is expected to have 40 columns we have to fill the unused columns.\n        Args:\n            extracted_columns: use case extracted columns.\n            is_dimension: flag identifying if is a dimension or a metric.\n        \"\"\"\nfilled_columns = []\nfor ins in range(\n(\nlen(extracted_columns) - 1\nif is_dimension\nelse len(extracted_columns) + 1\n),\n41,\n):\nfilled_columns.append(\n\" null as {}{}\".format(\"d\" if is_dimension else \"m\", ins)\n)\nreturn filled_columns\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABInsertGenerator.__init__","title":"<code>__init__(query_id, cadence, final_stage_table, lookup_query_builder, target_database, target_table)</code>","text":"<p>Construct GABInsertGenerator instances.</p> <p>Parameters:</p> Name Type Description Default <code>query_id</code> <code>str</code> <p>gab configuration table use case identifier.</p> required <code>cadence</code> <code>str</code> <p>inputted cadence to process.</p> required <code>final_stage_table</code> <code>str</code> <p>stage view name.</p> required <code>lookup_query_builder</code> <code>DataFrame</code> <p>gab configuration data.</p> required <code>target_database</code> <code>str</code> <p>target database to write.</p> required <code>target_table</code> <code>str</code> <p>target table to write.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>def __init__(\nself,\nquery_id: str,\ncadence: str,\nfinal_stage_table: str,\nlookup_query_builder: DataFrame,\ntarget_database: str,\ntarget_table: str,\n):\n\"\"\"Construct GABInsertGenerator instances.\n    Args:\n        query_id: gab configuration table use case identifier.\n        cadence:  inputted cadence to process.\n        final_stage_table: stage view name.\n        lookup_query_builder: gab configuration data.\n        target_database: target database to write.\n        target_table: target table to write.\n    \"\"\"\nself.query_id = query_id\nself.cadence = cadence\nself.final_stage_table = final_stage_table\nself.lookup_query_builder = lookup_query_builder\nself.target_database = target_database\nself.target_table = target_table\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABInsertGenerator.generate_sql","title":"<code>generate_sql()</code>","text":"<p>Generate insert sql statement to the insights table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>def generate_sql(self) -&gt; Optional[str]:\n\"\"\"Generate insert sql statement to the insights table.\"\"\"\ninsert_sql_statement = self._insert_statement_generator()\nreturn insert_sql_statement\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABSQLGenerator","title":"<code>GABSQLGenerator</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract class defining the behaviour of a GAB SQL Generator.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>class GABSQLGenerator(ABC):\n\"\"\"Abstract class defining the behaviour of a GAB SQL Generator.\"\"\"\n@abstractmethod\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Define the generate sql command.\n        E.g., the behaviour of gab generate sql inheriting from this.\n        \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABSQLGenerator.generate_sql","title":"<code>generate_sql()</code>  <code>abstractmethod</code>","text":"<p>Define the generate sql command.</p> <p>E.g., the behaviour of gab generate sql inheriting from this.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>@abstractmethod\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Define the generate sql command.\n    E.g., the behaviour of gab generate sql inheriting from this.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABViewGenerator","title":"<code>GABViewGenerator</code>","text":"<p>         Bases: <code>GABSQLGenerator</code></p> <p>GAB view generator.</p> <p>Creates the use case view statement to be consumed.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>class GABViewGenerator(GABSQLGenerator):\n\"\"\"GAB view generator.\n    Creates the use case view statement to be consumed.\n    \"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef __init__(\nself,\ncadence_snapshot_status: dict,\ntarget_database: str,\nview_name: str,\nfinal_cols: str,\ntarget_table: str,\ndimensions_and_metrics_with_alias: str,\ndimensions: str,\ndimensions_and_metrics: str,\nfinal_calculated_script: str,\nquery_id: str,\nview_filter: str,\nfinal_calculated_script_snapshot: str,\nwithout_snapshot_cadences: list[str],\nwith_snapshot_cadences: list[str],\n):\n\"\"\"Construct GABViewGenerator instances.\n        Args:\n            cadence_snapshot_status: each cadence with the corresponding snapshot\n                status.\n            target_database: target database to write.\n            view_name: name of the view to be generated.\n            final_cols: columns to return in the view.\n            target_table: target table to write.\n            dimensions_and_metrics_with_alias: configured dimensions and metrics with\n                alias to compute in the view.\n            dimensions: use case configured dimensions.\n            dimensions_and_metrics: use case configured dimensions and metrics.\n            final_calculated_script: use case calculated metrics.\n            query_id: gab configuration table use case identifier.\n            view_filter: filter to add in the view.\n            final_calculated_script_snapshot: use case calculated metrics with snapshot.\n            without_snapshot_cadences: cadences without snapshot.\n            with_snapshot_cadences: cadences with snapshot.\n        \"\"\"\nself.cadence_snapshot_status = cadence_snapshot_status\nself.target_database = target_database\nself.result_key = view_name\nself.final_cols = final_cols\nself.target_table = target_table\nself.dimensions_and_metrics_with_alias = dimensions_and_metrics_with_alias\nself.dimensions = dimensions\nself.dimensions_and_metrics = dimensions_and_metrics\nself.final_calculated_script = final_calculated_script\nself.query_id = query_id\nself.view_filter = view_filter\nself.final_calculated_script_snapshot = final_calculated_script_snapshot\nself.without_snapshot_cadences = without_snapshot_cadences\nself.with_snapshot_cadences = with_snapshot_cadences\n@_execute_sql\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Generate use case view sql statement.\"\"\"\nconsumption_view_sql = self._create_consumption_view()\nreturn consumption_view_sql\ndef _create_consumption_view(self) -&gt; str:\n\"\"\"Create consumption view.\"\"\"\nfinal_view_query = self._generate_consumption_view_statement(\nself.cadence_snapshot_status,\nself.target_database,\nself.final_cols,\nself.target_table,\nself.dimensions_and_metrics_with_alias,\nself.dimensions,\nself.dimensions_and_metrics,\nself.final_calculated_script,\nself.query_id,\nself.view_filter,\nself.final_calculated_script_snapshot,\nwithout_snapshot_cadences=\",\".join(\nf'\"{w}\"' for w in self.without_snapshot_cadences\n),\nwith_snapshot_cadences=\",\".join(\nf'\"{w}\"' for w in self.with_snapshot_cadences\n),\n)\nrendered_query = \"\"\"\n            CREATE OR REPLACE VIEW {database}.{view_name} AS {final_view_query}\n            \"\"\".format(\ndatabase=self.target_database,\nview_name=self.result_key,\nfinal_view_query=final_view_query,\n)\nself._LOGGER.info(f\"Consumption view statement: {rendered_query}\")\nreturn rendered_query\n@classmethod\ndef _generate_consumption_view_statement(\ncls,\ncadence_snapshot_status: dict,\ntarget_database: str,\nfinal_cols: str,\ntarget_table: str,\ndimensions_and_metrics_with_alias: str,\ndimensions: str,\ndimensions_and_metrics: str,\nfinal_calculated_script: str,\nquery_id: str,\nview_filter: str,\nfinal_calculated_script_snapshot: str,\nwithout_snapshot_cadences: str,\nwith_snapshot_cadences: str,\n) -&gt; str:\n\"\"\"Generate consumption view.\n        Args:\n            cadence_snapshot_status: cadences to execute with the information if it has\n                snapshot.\n            target_database: target database to write.\n            final_cols: use case columns exposed in the consumption view.\n            target_table: target table to write.\n            dimensions_and_metrics_with_alias: dimensions and metrics as string columns\n                with alias.\n            dimensions: dimensions as string columns.\n            dimensions_and_metrics: dimensions and metrics as string columns\n                without alias.\n            final_calculated_script: final calculated metrics script.\n            query_id: gab configuration table use case identifier.\n            view_filter: filter to execute on the view.\n            final_calculated_script_snapshot: final calculated metrics with snapshot\n                script.\n            without_snapshot_cadences: cadences without snapshot.\n            with_snapshot_cadences: cadences with snapshot.\n        \"\"\"\ncls._LOGGER.info(\"Generating consumption view statement...\")\ncls._LOGGER.info(\nf\"\"\"\n{{\n                target_database: {target_database},\n                target_table: {target_table},\n                query_id: {query_id},\n                cadence_and_snapshot_status: {cadence_snapshot_status},\n                cadences_without_snapshot: [{without_snapshot_cadences}],\n                cadences_with_snapshot: [{with_snapshot_cadences}],\n                final_cols: {final_cols},\n                dimensions_and_metrics_with_alias: {dimensions_and_metrics_with_alias},\n                dimensions: {dimensions},\n                dimensions_with_metrics: {dimensions_and_metrics},\n                final_calculated_script: {final_calculated_script},\n                final_calculated_script_snapshot: {final_calculated_script_snapshot},\n                view_filter: {view_filter}\n}}\"\"\"\n)\nif (\n\"Y\" in cadence_snapshot_status.values()\nand \"N\" in cadence_snapshot_status.values()\n):\nconsumption_view_query = f\"\"\"\n                WITH TEMP1 AS (\n                    SELECT\n                        a.cadence,\n{dimensions_and_metrics_with_alias}{final_calculated_script}\n                    FROM {target_database}.{target_table} a\n                    WHERE a.query_id = {query_id}\n                    AND cadence IN ({without_snapshot_cadences})\n{view_filter}\n                ),\n                TEMP_RN AS (\n                    SELECT\n                        a.cadence,\n                        a.from_date,\n                        a.to_date,\n{dimensions_and_metrics},\n                        row_number() over(\n                            PARTITION BY\n                                a.cadence,\n{dimensions},\n                                a.from_date\n                            order by to_date\n                        ) as rn\n                    FROM {target_database}.{target_table} a\n                    WHERE a.query_id = {query_id}\n                    AND cadence IN ({with_snapshot_cadences})\n{view_filter}\n                ),\n                TEMP2 AS (\n                    SELECT\n                        a.cadence,\n{dimensions_and_metrics_with_alias}{final_calculated_script_snapshot}\n                    FROM TEMP_RN a\n                ),\n                TEMP3 AS (SELECT * FROM TEMP1 UNION SELECT * from TEMP2)\n                SELECT {final_cols} FROM TEMP3\n            \"\"\"  # nosec: B608\nelif \"N\" in cadence_snapshot_status.values():\nconsumption_view_query = f\"\"\"\n                WITH TEMP1 AS (\n                    SELECT\n                        a.cadence,\n{dimensions_and_metrics_with_alias}{final_calculated_script}\n                    FROM {target_database}.{target_table} a\n                    WHERE a.query_id = {query_id}\n                    AND cadence IN ({without_snapshot_cadences})  {view_filter}\n                )\n                SELECT {final_cols} FROM TEMP1\n            \"\"\"  # nosec: B608\nelse:\nconsumption_view_query = f\"\"\"\n                WITH TEMP_RN AS (\n                    SELECT\n                        a.cadence,\n                        a.from_date,\n                        a.to_date,\n{dimensions_and_metrics},\n                        row_number() over(\n                            PARTITION BY\n                                a.cadence,\n                                a.from_date,\n                                a.to_date,\n{dimensions},\n                                a.from_date\n                        order by to_date) as rn\n                    FROM {target_database}.{target_table} a\n                    WHERE a.query_id = {query_id}\n                    AND cadence IN ({with_snapshot_cadences})\n{view_filter}\n                ),\n                TEMP2 AS (\n                    SELECT\n                        a.cadence,\n{dimensions_and_metrics_with_alias}{final_calculated_script_snapshot}\n                    FROM TEMP_RN a\n                )\n                SELECT {final_cols} FROM TEMP2\n            \"\"\"  # nosec: B608\nreturn consumption_view_query\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABViewGenerator.__init__","title":"<code>__init__(cadence_snapshot_status, target_database, view_name, final_cols, target_table, dimensions_and_metrics_with_alias, dimensions, dimensions_and_metrics, final_calculated_script, query_id, view_filter, final_calculated_script_snapshot, without_snapshot_cadences, with_snapshot_cadences)</code>","text":"<p>Construct GABViewGenerator instances.</p> <p>Parameters:</p> Name Type Description Default <code>cadence_snapshot_status</code> <code>dict</code> <p>each cadence with the corresponding snapshot status.</p> required <code>target_database</code> <code>str</code> <p>target database to write.</p> required <code>view_name</code> <code>str</code> <p>name of the view to be generated.</p> required <code>final_cols</code> <code>str</code> <p>columns to return in the view.</p> required <code>target_table</code> <code>str</code> <p>target table to write.</p> required <code>dimensions_and_metrics_with_alias</code> <code>str</code> <p>configured dimensions and metrics with alias to compute in the view.</p> required <code>dimensions</code> <code>str</code> <p>use case configured dimensions.</p> required <code>dimensions_and_metrics</code> <code>str</code> <p>use case configured dimensions and metrics.</p> required <code>final_calculated_script</code> <code>str</code> <p>use case calculated metrics.</p> required <code>query_id</code> <code>str</code> <p>gab configuration table use case identifier.</p> required <code>view_filter</code> <code>str</code> <p>filter to add in the view.</p> required <code>final_calculated_script_snapshot</code> <code>str</code> <p>use case calculated metrics with snapshot.</p> required <code>without_snapshot_cadences</code> <code>list[str]</code> <p>cadences without snapshot.</p> required <code>with_snapshot_cadences</code> <code>list[str]</code> <p>cadences with snapshot.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>def __init__(\nself,\ncadence_snapshot_status: dict,\ntarget_database: str,\nview_name: str,\nfinal_cols: str,\ntarget_table: str,\ndimensions_and_metrics_with_alias: str,\ndimensions: str,\ndimensions_and_metrics: str,\nfinal_calculated_script: str,\nquery_id: str,\nview_filter: str,\nfinal_calculated_script_snapshot: str,\nwithout_snapshot_cadences: list[str],\nwith_snapshot_cadences: list[str],\n):\n\"\"\"Construct GABViewGenerator instances.\n    Args:\n        cadence_snapshot_status: each cadence with the corresponding snapshot\n            status.\n        target_database: target database to write.\n        view_name: name of the view to be generated.\n        final_cols: columns to return in the view.\n        target_table: target table to write.\n        dimensions_and_metrics_with_alias: configured dimensions and metrics with\n            alias to compute in the view.\n        dimensions: use case configured dimensions.\n        dimensions_and_metrics: use case configured dimensions and metrics.\n        final_calculated_script: use case calculated metrics.\n        query_id: gab configuration table use case identifier.\n        view_filter: filter to add in the view.\n        final_calculated_script_snapshot: use case calculated metrics with snapshot.\n        without_snapshot_cadences: cadences without snapshot.\n        with_snapshot_cadences: cadences with snapshot.\n    \"\"\"\nself.cadence_snapshot_status = cadence_snapshot_status\nself.target_database = target_database\nself.result_key = view_name\nself.final_cols = final_cols\nself.target_table = target_table\nself.dimensions_and_metrics_with_alias = dimensions_and_metrics_with_alias\nself.dimensions = dimensions\nself.dimensions_and_metrics = dimensions_and_metrics\nself.final_calculated_script = final_calculated_script\nself.query_id = query_id\nself.view_filter = view_filter\nself.final_calculated_script_snapshot = final_calculated_script_snapshot\nself.without_snapshot_cadences = without_snapshot_cadences\nself.with_snapshot_cadences = with_snapshot_cadences\n</code></pre>"},{"location":"reference/packages/core/gab_sql_generator.html#packages.core.gab_sql_generator.GABViewGenerator.generate_sql","title":"<code>generate_sql()</code>","text":"<p>Generate use case view sql statement.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/gab_sql_generator.py</code> <pre><code>@_execute_sql\ndef generate_sql(self) -&gt; Optional[str]:\n\"\"\"Generate use case view sql statement.\"\"\"\nconsumption_view_sql = self._create_consumption_view()\nreturn consumption_view_sql\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html","title":"S3 file manager","text":"<p>File manager module using boto3.</p>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.ArchiveFileManager","title":"<code>ArchiveFileManager</code>","text":"<p>         Bases: <code>object</code></p> <p>Set of actions to restore archives.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>class ArchiveFileManager(object):\n\"\"\"Set of actions to restore archives.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef _get_archived_object(bucket: str, object_key: str) -&gt; Optional[Any]:\n\"\"\"Get the archived object if it's an object.\n        Args:\n            bucket: name of bucket to check get the object.\n            object_key: object to get.\n        Returns:\n            S3 Object if it's an archived object, otherwise None.\n        \"\"\"\ns3 = boto3.resource(\"s3\")\nobject_to_restore = s3.Object(bucket, object_key)\nif (\nobject_to_restore.storage_class is not None\nand object_to_restore.storage_class in ARCHIVE_STORAGE_CLASS\n):\nreturn object_to_restore\nelse:\nreturn None\n@staticmethod\ndef _check_object_restore_status(\nbucket: str, object_key: str\n) -&gt; Optional[RestoreStatus]:\n\"\"\"Check the restore status of the archive.\n        Args:\n            bucket: name of bucket to check the restore status.\n            object_key: object to check the restore status.\n        Returns:\n            The restore status represented by an enum, possible values are:\n                NOT_STARTED, ONGOING or RESTORED\n        \"\"\"\narchived_object = ArchiveFileManager._get_archived_object(bucket, object_key)\nif archived_object is None:\nstatus = None\nelif archived_object.restore is None:\nstatus = RestoreStatus.NOT_STARTED\nelif 'ongoing-request=\"true\"' in archived_object.restore:\nstatus = RestoreStatus.ONGOING\nelse:\nstatus = RestoreStatus.RESTORED\nreturn status\n@staticmethod\ndef check_restore_status(source_bucket: str, source_object: str) -&gt; dict:\n\"\"\"Check the restore status of archived data.\n        Args:\n            source_bucket: name of bucket to check the restore status.\n            source_object: object to check the restore status.\n        Returns:\n            A dict containing the amount of objects in each status.\n        \"\"\"\nnot_started_objects = 0\nongoing_objects = 0\nrestored_objects = 0\ntotal_objects = 0\nif _check_directory(source_bucket, source_object):\nsource_object = get_directory_path(source_object)\nobjects_to_restore = _list_objects_recursively(\nbucket=source_bucket, path=source_object\n)\nfor obj in objects_to_restore:\nArchiveFileManager._logger.info(f\"Checking restore status for: {obj}\")\nrestore_status = ArchiveFileManager._check_object_restore_status(\nsource_bucket, obj\n)\nif not restore_status:\nArchiveFileManager._logger.warning(\nf\"Restore status not found for {source_bucket}/{obj}\"\n)\nelse:\ntotal_objects += 1\nif RestoreStatus.NOT_STARTED == restore_status:\nnot_started_objects += 1\nelif RestoreStatus.ONGOING == restore_status:\nongoing_objects += 1\nelse:\nrestored_objects += 1\nArchiveFileManager._logger.info(\nf\"{obj} restore status is {restore_status.value}\"\n)\nreturn {\n\"total_objects\": total_objects,\n\"not_started_objects\": not_started_objects,\n\"ongoing_objects\": ongoing_objects,\n\"restored_objects\": restored_objects,\n}\n@staticmethod\ndef _request_restore_object(\nbucket: str, object_key: str, expiration: int, retrieval_tier: str\n) -&gt; None:\n\"\"\"Request a restore of the archive.\n        Args:\n            bucket: name of bucket to perform the restore.\n            object_key: object to be restored.\n            expiration: restore expiration.\n            retrieval_tier: type of restore, possible values are:\n                Bulk, Standard or Expedited.\n        \"\"\"\nif not RestoreType.exists(retrieval_tier):\nraise RestoreTypeNotFoundException(\nf\"Restore type {retrieval_tier} not supported.\"\n)\nif _check_directory(bucket, object_key):\nobject_key = get_directory_path(object_key)\narchived_object = ArchiveFileManager._get_archived_object(bucket, object_key)\nif archived_object and archived_object.restore is None:\nArchiveFileManager._logger.info(f\"Restoring archive {bucket}/{object_key}.\")\narchived_object.restore_object(\nRestoreRequest={\n\"Days\": expiration,\n\"GlacierJobParameters\": {\"Tier\": retrieval_tier},\n}\n)\nelse:\nArchiveFileManager._logger.info(\nf\"Restore request for {bucket}/{object_key} not performed.\"\n)\n@staticmethod\ndef request_restore(\nsource_bucket: str,\nsource_object: str,\nrestore_expiration: int,\nretrieval_tier: str,\ndry_run: bool,\n) -&gt; None:\n\"\"\"Request the restore of archived data.\n        Args:\n            source_bucket: name of bucket to perform the restore.\n            source_object: object to be restored.\n            restore_expiration: restore expiration in days.\n            retrieval_tier: type of restore, possible values are:\n                Bulk, Standard or Expedited.\n            dry_run: if dry_run is set to True the function will print a dict with\n                all the paths that would be deleted based on the given keys.\n        \"\"\"\nif _check_directory(source_bucket, source_object):\nsource_object = get_directory_path(source_object)\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nArchiveFileManager._logger.info(\"Paths that would be restored:\")\nArchiveFileManager._logger.info(response)\nelse:\nobjects_to_restore = _list_objects_recursively(\nbucket=source_bucket, path=source_object\n)\nfor obj in objects_to_restore:\nArchiveFileManager._request_restore_object(\nsource_bucket,\nobj,\nrestore_expiration,\nretrieval_tier,\n)\n@staticmethod\ndef request_restore_and_wait(\nsource_bucket: str,\nsource_object: str,\nrestore_expiration: int,\nretrieval_tier: str,\ndry_run: bool,\n) -&gt; None:\n\"\"\"Request and wait for the restore to complete, polling the restore status.\n        Args:\n            source_bucket: name of bucket to perform the restore.\n            source_object: object to be restored.\n            restore_expiration: restore expiration in days.\n            retrieval_tier: type of restore, possible values are:\n                Bulk, Standard or Expedited.\n            dry_run: if dry_run is set to True the function will print a dict with\n                all the paths that would be deleted based on the given keys.\n        \"\"\"\nif retrieval_tier != RestoreType.EXPEDITED.value:\nArchiveFileManager._logger.error(\nf\"Retrieval Tier {retrieval_tier} not allowed on this operation! This \"\n\"kind of restore should be used just with `Expedited` retrieval tier \"\n\"to save cluster costs.\"\n)\nraise ValueError(\nf\"Retrieval Tier {retrieval_tier} not allowed on this operation! This \"\n\"kind of restore should be used just with `Expedited` retrieval tier \"\n\"to save cluster costs.\"\n)\nArchiveFileManager.request_restore(\nsource_bucket=source_bucket,\nsource_object=source_object,\nrestore_expiration=restore_expiration,\nretrieval_tier=retrieval_tier,\ndry_run=dry_run,\n)\nrestore_status = ArchiveFileManager.check_restore_status(\nsource_bucket, source_object\n)\nArchiveFileManager._logger.info(f\"Restore status: {restore_status}\")\nif not dry_run:\nArchiveFileManager._logger.info(\"Checking the restore status in 5 minutes.\")\nwait_time = 300\nwhile restore_status.get(\"total_objects\") &gt; restore_status.get(\n\"restored_objects\"\n):\nArchiveFileManager._logger.info(\n\"Not all objects have been restored yet, checking the status again \"\nf\"in {wait_time} seconds.\"\n)\ntime.sleep(wait_time)\nwait_time = 30\nrestore_status = ArchiveFileManager.check_restore_status(\nsource_bucket, source_object\n)\nArchiveFileManager._logger.info(f\"Restore status: {restore_status}\")\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.ArchiveFileManager.check_restore_status","title":"<code>check_restore_status(source_bucket, source_object)</code>  <code>staticmethod</code>","text":"<p>Check the restore status of archived data.</p> <p>Parameters:</p> Name Type Description Default <code>source_bucket</code> <code>str</code> <p>name of bucket to check the restore status.</p> required <code>source_object</code> <code>str</code> <p>object to check the restore status.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the amount of objects in each status.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>@staticmethod\ndef check_restore_status(source_bucket: str, source_object: str) -&gt; dict:\n\"\"\"Check the restore status of archived data.\n    Args:\n        source_bucket: name of bucket to check the restore status.\n        source_object: object to check the restore status.\n    Returns:\n        A dict containing the amount of objects in each status.\n    \"\"\"\nnot_started_objects = 0\nongoing_objects = 0\nrestored_objects = 0\ntotal_objects = 0\nif _check_directory(source_bucket, source_object):\nsource_object = get_directory_path(source_object)\nobjects_to_restore = _list_objects_recursively(\nbucket=source_bucket, path=source_object\n)\nfor obj in objects_to_restore:\nArchiveFileManager._logger.info(f\"Checking restore status for: {obj}\")\nrestore_status = ArchiveFileManager._check_object_restore_status(\nsource_bucket, obj\n)\nif not restore_status:\nArchiveFileManager._logger.warning(\nf\"Restore status not found for {source_bucket}/{obj}\"\n)\nelse:\ntotal_objects += 1\nif RestoreStatus.NOT_STARTED == restore_status:\nnot_started_objects += 1\nelif RestoreStatus.ONGOING == restore_status:\nongoing_objects += 1\nelse:\nrestored_objects += 1\nArchiveFileManager._logger.info(\nf\"{obj} restore status is {restore_status.value}\"\n)\nreturn {\n\"total_objects\": total_objects,\n\"not_started_objects\": not_started_objects,\n\"ongoing_objects\": ongoing_objects,\n\"restored_objects\": restored_objects,\n}\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.ArchiveFileManager.request_restore","title":"<code>request_restore(source_bucket, source_object, restore_expiration, retrieval_tier, dry_run)</code>  <code>staticmethod</code>","text":"<p>Request the restore of archived data.</p> <p>Parameters:</p> Name Type Description Default <code>source_bucket</code> <code>str</code> <p>name of bucket to perform the restore.</p> required <code>source_object</code> <code>str</code> <p>object to be restored.</p> required <code>restore_expiration</code> <code>int</code> <p>restore expiration in days.</p> required <code>retrieval_tier</code> <code>str</code> <p>type of restore, possible values are: Bulk, Standard or Expedited.</p> required <code>dry_run</code> <code>bool</code> <p>if dry_run is set to True the function will print a dict with all the paths that would be deleted based on the given keys.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>@staticmethod\ndef request_restore(\nsource_bucket: str,\nsource_object: str,\nrestore_expiration: int,\nretrieval_tier: str,\ndry_run: bool,\n) -&gt; None:\n\"\"\"Request the restore of archived data.\n    Args:\n        source_bucket: name of bucket to perform the restore.\n        source_object: object to be restored.\n        restore_expiration: restore expiration in days.\n        retrieval_tier: type of restore, possible values are:\n            Bulk, Standard or Expedited.\n        dry_run: if dry_run is set to True the function will print a dict with\n            all the paths that would be deleted based on the given keys.\n    \"\"\"\nif _check_directory(source_bucket, source_object):\nsource_object = get_directory_path(source_object)\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nArchiveFileManager._logger.info(\"Paths that would be restored:\")\nArchiveFileManager._logger.info(response)\nelse:\nobjects_to_restore = _list_objects_recursively(\nbucket=source_bucket, path=source_object\n)\nfor obj in objects_to_restore:\nArchiveFileManager._request_restore_object(\nsource_bucket,\nobj,\nrestore_expiration,\nretrieval_tier,\n)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.ArchiveFileManager.request_restore_and_wait","title":"<code>request_restore_and_wait(source_bucket, source_object, restore_expiration, retrieval_tier, dry_run)</code>  <code>staticmethod</code>","text":"<p>Request and wait for the restore to complete, polling the restore status.</p> <p>Parameters:</p> Name Type Description Default <code>source_bucket</code> <code>str</code> <p>name of bucket to perform the restore.</p> required <code>source_object</code> <code>str</code> <p>object to be restored.</p> required <code>restore_expiration</code> <code>int</code> <p>restore expiration in days.</p> required <code>retrieval_tier</code> <code>str</code> <p>type of restore, possible values are: Bulk, Standard or Expedited.</p> required <code>dry_run</code> <code>bool</code> <p>if dry_run is set to True the function will print a dict with all the paths that would be deleted based on the given keys.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>@staticmethod\ndef request_restore_and_wait(\nsource_bucket: str,\nsource_object: str,\nrestore_expiration: int,\nretrieval_tier: str,\ndry_run: bool,\n) -&gt; None:\n\"\"\"Request and wait for the restore to complete, polling the restore status.\n    Args:\n        source_bucket: name of bucket to perform the restore.\n        source_object: object to be restored.\n        restore_expiration: restore expiration in days.\n        retrieval_tier: type of restore, possible values are:\n            Bulk, Standard or Expedited.\n        dry_run: if dry_run is set to True the function will print a dict with\n            all the paths that would be deleted based on the given keys.\n    \"\"\"\nif retrieval_tier != RestoreType.EXPEDITED.value:\nArchiveFileManager._logger.error(\nf\"Retrieval Tier {retrieval_tier} not allowed on this operation! This \"\n\"kind of restore should be used just with `Expedited` retrieval tier \"\n\"to save cluster costs.\"\n)\nraise ValueError(\nf\"Retrieval Tier {retrieval_tier} not allowed on this operation! This \"\n\"kind of restore should be used just with `Expedited` retrieval tier \"\n\"to save cluster costs.\"\n)\nArchiveFileManager.request_restore(\nsource_bucket=source_bucket,\nsource_object=source_object,\nrestore_expiration=restore_expiration,\nretrieval_tier=retrieval_tier,\ndry_run=dry_run,\n)\nrestore_status = ArchiveFileManager.check_restore_status(\nsource_bucket, source_object\n)\nArchiveFileManager._logger.info(f\"Restore status: {restore_status}\")\nif not dry_run:\nArchiveFileManager._logger.info(\"Checking the restore status in 5 minutes.\")\nwait_time = 300\nwhile restore_status.get(\"total_objects\") &gt; restore_status.get(\n\"restored_objects\"\n):\nArchiveFileManager._logger.info(\n\"Not all objects have been restored yet, checking the status again \"\nf\"in {wait_time} seconds.\"\n)\ntime.sleep(wait_time)\nwait_time = 30\nrestore_status = ArchiveFileManager.check_restore_status(\nsource_bucket, source_object\n)\nArchiveFileManager._logger.info(f\"Restore status: {restore_status}\")\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager","title":"<code>S3FileManager</code>","text":"<p>         Bases: <code>FileManager</code></p> <p>Set of actions to manipulate s3 files in several ways.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>class S3FileManager(FileManager):\n\"\"\"Set of actions to manipulate s3 files in several ways.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef get_function(self) -&gt; None:\n\"\"\"Get a specific function to execute.\"\"\"\navailable_functions = {\n\"delete_objects\": self.delete_objects,\n\"copy_objects\": self.copy_objects,\n\"request_restore\": self.request_restore,\n\"check_restore_status\": self.check_restore_status,\n\"request_restore_to_destination_and_wait\": (\nself.request_restore_to_destination_and_wait\n),\n}\nself._logger.info(\"Function being executed: {}\".format(self.function))\nif self.function in available_functions.keys():\nfunc = available_functions[self.function]\nfunc()\nelse:\nraise NotImplementedError(\nf\"The requested function {self.function} is not implemented.\"\n)\ndef _delete_objects(self, bucket: str, objects_paths: list) -&gt; None:\n\"\"\"Delete objects recursively in s3.\n        Params:\n            bucket: name of bucket to perform the delete operation.\n            objects_paths: objects to be deleted.\n        \"\"\"\ns3 = boto3.client(\"s3\")\nfor path in objects_paths:\nif _check_directory(bucket, path):\npath = get_directory_path(path)\nelse:\npath = path.strip()\nmore_objects = True\npaginator = \"\"\nobjects_to_delete = []\nwhile more_objects:\nobjects_found, paginator = _list_objects(\ns3_client=s3, bucket=bucket, path=path, paginator=paginator\n)\nfor obj in objects_found:\nobjects_to_delete.append({FileManagerAPIKeys.KEY.value: obj})\nif not paginator:\nmore_objects = False\nresponse = s3.delete_objects(\nBucket=bucket,\nDelete={FileManagerAPIKeys.OBJECTS.value: objects_to_delete},\n)\nself._logger.info(response)\nobjects_to_delete = []\ndef delete_objects(self) -&gt; None:\n\"\"\"Delete objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be deleted based on the given keys.\n        \"\"\"\nbucket = self.configs[\"bucket\"]\nobjects_paths = self.configs[\"object_paths\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=bucket, object_paths=objects_paths)\nself._logger.info(\"Paths that would be deleted:\")\nself._logger.info(response)\nelse:\nself._delete_objects(bucket, objects_paths)\ndef copy_objects(self) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be copied based on the given keys.\n        \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\ndry_run = self.configs[\"dry_run\"]\nS3FileManager._copy_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\ndry_run=dry_run,\n)\ndef move_objects(self) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n        If dry_run is set to True the function will print a dict with all the\n        paths that would be moved based on the given keys.\n        \"\"\"\npass\ndef request_restore(self) -&gt; None:\n\"\"\"Request the restore of archived data.\"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\nrestore_expiration = self.configs[\"restore_expiration\"]\nretrieval_tier = self.configs[\"retrieval_tier\"]\ndry_run = self.configs[\"dry_run\"]\nArchiveFileManager.request_restore(\nsource_bucket,\nsource_object,\nrestore_expiration,\nretrieval_tier,\ndry_run,\n)\ndef check_restore_status(self) -&gt; None:\n\"\"\"Check the restore status of archived data.\"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\nrestore_status = ArchiveFileManager.check_restore_status(\nsource_bucket, source_object\n)\nself._logger.info(\nf\"\"\"\n            Restore status:\n            - Not Started: {restore_status.get('not_started_objects')}\n            - Ongoing: {restore_status.get('ongoing_objects')}\n            - Restored: {restore_status.get('restored_objects')}\n            Total objects in this restore process: {restore_status.get('total_objects')}\n            \"\"\"\n)\ndef request_restore_to_destination_and_wait(self) -&gt; None:\n\"\"\"Request and wait for the restore to complete, polling the restore status.\n        After the restore is done, copy the restored files to destination\n        \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\nrestore_expiration = self.configs[\"restore_expiration\"]\nretrieval_tier = self.configs[\"retrieval_tier\"]\ndry_run = self.configs[\"dry_run\"]\nArchiveFileManager.request_restore_and_wait(\nsource_bucket=source_bucket,\nsource_object=source_object,\nrestore_expiration=restore_expiration,\nretrieval_tier=retrieval_tier,\ndry_run=dry_run,\n)\nS3FileManager._logger.info(\nf\"Restoration complete for {source_bucket} and {source_object}\"\n)\nS3FileManager._logger.info(\nf\"Starting to copy data from {source_bucket}/{source_object} to \"\nf\"{destination_bucket}/{destination_object}\"\n)\nS3FileManager._copy_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\ndry_run=dry_run,\n)\nS3FileManager._logger.info(\nf\"Finished copying data, data should be available on {destination_bucket}/\"\nf\"{destination_object}\"\n)\n@staticmethod\ndef _copy_objects(\nsource_bucket: str,\nsource_object: str,\ndestination_bucket: str,\ndestination_object: str,\ndry_run: bool,\n) -&gt; None:\n\"\"\"Copies objects and 'directories' in s3.\n        Args:\n            source_bucket: name of bucket to perform the copy.\n            source_object: object/folder to be copied.\n            destination_bucket: name of the target bucket to copy.\n            destination_object: target object/folder to copy.\n            dry_run: if dry_run is set to True the function will print a dict with\n                all the paths that would be deleted based on the given keys.\n        \"\"\"\ns3 = boto3.client(\"s3\")\nif dry_run:\nresponse = _dry_run(bucket=source_bucket, object_paths=[source_object])\nS3FileManager._logger.info(\"Paths that would be copied:\")\nS3FileManager._logger.info(response)\nelse:\noriginal_object_name = source_object.split(\"/\")[-1]\nif _check_directory(source_bucket, source_object):\nsource_object = get_directory_path(source_object)\ncopy_object = _list_objects_recursively(\nbucket=source_bucket, path=source_object\n)\nfor obj in copy_object:\nS3FileManager._logger.info(f\"Copying obj: {obj}\")\nfinal_path = obj.replace(source_object, \"\")\nresponse = s3.copy_object(\nBucket=destination_bucket,\nCopySource={\nFileManagerAPIKeys.BUCKET.value: source_bucket,\nFileManagerAPIKeys.KEY.value: obj,\n},\nKey=f\"{destination_object}/{original_object_name}/{final_path}\",\n)\nS3FileManager._logger.info(response)\nelse:\nS3FileManager._logger.info(f\"Copying obj: {source_object}\")\nresponse = s3.copy_object(\nBucket=destination_bucket,\nCopySource={\nFileManagerAPIKeys.BUCKET.value: source_bucket,\nFileManagerAPIKeys.KEY.value: source_object,\n},\nKey=f\"\"\"{destination_object}/{original_object_name}\"\"\",\n)\nS3FileManager._logger.info(response)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.check_restore_status","title":"<code>check_restore_status()</code>","text":"<p>Check the restore status of archived data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def check_restore_status(self) -&gt; None:\n\"\"\"Check the restore status of archived data.\"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\nrestore_status = ArchiveFileManager.check_restore_status(\nsource_bucket, source_object\n)\nself._logger.info(\nf\"\"\"\n        Restore status:\n        - Not Started: {restore_status.get('not_started_objects')}\n        - Ongoing: {restore_status.get('ongoing_objects')}\n        - Restored: {restore_status.get('restored_objects')}\n        Total objects in this restore process: {restore_status.get('total_objects')}\n        \"\"\"\n)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.copy_objects","title":"<code>copy_objects()</code>","text":"<p>Copies objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be copied based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def copy_objects(self) -&gt; None:\n\"\"\"Copies objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be copied based on the given keys.\n    \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\ndry_run = self.configs[\"dry_run\"]\nS3FileManager._copy_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\ndry_run=dry_run,\n)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.delete_objects","title":"<code>delete_objects()</code>","text":"<p>Delete objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be deleted based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def delete_objects(self) -&gt; None:\n\"\"\"Delete objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be deleted based on the given keys.\n    \"\"\"\nbucket = self.configs[\"bucket\"]\nobjects_paths = self.configs[\"object_paths\"]\ndry_run = self.configs[\"dry_run\"]\nif dry_run:\nresponse = _dry_run(bucket=bucket, object_paths=objects_paths)\nself._logger.info(\"Paths that would be deleted:\")\nself._logger.info(response)\nelse:\nself._delete_objects(bucket, objects_paths)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.get_function","title":"<code>get_function()</code>","text":"<p>Get a specific function to execute.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def get_function(self) -&gt; None:\n\"\"\"Get a specific function to execute.\"\"\"\navailable_functions = {\n\"delete_objects\": self.delete_objects,\n\"copy_objects\": self.copy_objects,\n\"request_restore\": self.request_restore,\n\"check_restore_status\": self.check_restore_status,\n\"request_restore_to_destination_and_wait\": (\nself.request_restore_to_destination_and_wait\n),\n}\nself._logger.info(\"Function being executed: {}\".format(self.function))\nif self.function in available_functions.keys():\nfunc = available_functions[self.function]\nfunc()\nelse:\nraise NotImplementedError(\nf\"The requested function {self.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.move_objects","title":"<code>move_objects()</code>","text":"<p>Moves objects and 'directories'.</p> <p>If dry_run is set to True the function will print a dict with all the paths that would be moved based on the given keys.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def move_objects(self) -&gt; None:\n\"\"\"Moves objects and 'directories'.\n    If dry_run is set to True the function will print a dict with all the\n    paths that would be moved based on the given keys.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.request_restore","title":"<code>request_restore()</code>","text":"<p>Request the restore of archived data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def request_restore(self) -&gt; None:\n\"\"\"Request the restore of archived data.\"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\nrestore_expiration = self.configs[\"restore_expiration\"]\nretrieval_tier = self.configs[\"retrieval_tier\"]\ndry_run = self.configs[\"dry_run\"]\nArchiveFileManager.request_restore(\nsource_bucket,\nsource_object,\nrestore_expiration,\nretrieval_tier,\ndry_run,\n)\n</code></pre>"},{"location":"reference/packages/core/s3_file_manager.html#packages.core.s3_file_manager.S3FileManager.request_restore_to_destination_and_wait","title":"<code>request_restore_to_destination_and_wait()</code>","text":"<p>Request and wait for the restore to complete, polling the restore status.</p> <p>After the restore is done, copy the restored files to destination</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/s3_file_manager.py</code> <pre><code>def request_restore_to_destination_and_wait(self) -&gt; None:\n\"\"\"Request and wait for the restore to complete, polling the restore status.\n    After the restore is done, copy the restored files to destination\n    \"\"\"\nsource_bucket = self.configs[\"bucket\"]\nsource_object = self.configs[\"source_object\"]\ndestination_bucket = self.configs[\"destination_bucket\"]\ndestination_object = self.configs[\"destination_object\"]\nrestore_expiration = self.configs[\"restore_expiration\"]\nretrieval_tier = self.configs[\"retrieval_tier\"]\ndry_run = self.configs[\"dry_run\"]\nArchiveFileManager.request_restore_and_wait(\nsource_bucket=source_bucket,\nsource_object=source_object,\nrestore_expiration=restore_expiration,\nretrieval_tier=retrieval_tier,\ndry_run=dry_run,\n)\nS3FileManager._logger.info(\nf\"Restoration complete for {source_bucket} and {source_object}\"\n)\nS3FileManager._logger.info(\nf\"Starting to copy data from {source_bucket}/{source_object} to \"\nf\"{destination_bucket}/{destination_object}\"\n)\nS3FileManager._copy_objects(\nsource_bucket=source_bucket,\nsource_object=source_object,\ndestination_bucket=destination_bucket,\ndestination_object=destination_object,\ndry_run=dry_run,\n)\nS3FileManager._logger.info(\nf\"Finished copying data, data should be available on {destination_bucket}/\"\nf\"{destination_object}\"\n)\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html","title":"Sensor manager","text":"<p>Module to define Sensor Manager classes.</p>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorControlTableManager","title":"<code>SensorControlTableManager</code>","text":"<p>         Bases: <code>object</code></p> <p>Class to control the Sensor execution.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>class SensorControlTableManager(object):\n\"\"\"Class to control the Sensor execution.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef check_if_sensor_has_acquired_data(\ncls,\nsensor_id: str,\ncontrol_db_table_name: str,\n) -&gt; bool:\n\"\"\"Check if sensor has acquired new data.\n        Args:\n            sensor_id: sensor id.\n            control_db_table_name: `db.table` to control sensor runs.\n        Returns:\n            True if acquired new data, otherwise False\n        \"\"\"\nsensor_table_data = cls.read_sensor_table_data(\nsensor_id=sensor_id, control_db_table_name=control_db_table_name\n)\ncls._LOGGER.info(f\"sensor_table_data = {sensor_table_data}\")\nreturn (\nsensor_table_data is not None\nand sensor_table_data.status == SensorStatus.ACQUIRED_NEW_DATA.value\n)\n@classmethod\ndef update_sensor_status(\ncls,\nsensor_spec: SensorSpec,\nstatus: str,\nupstream_key: str = None,\nupstream_value: str = None,\n) -&gt; None:\n\"\"\"Control sensor execution storing the execution data in a delta table.\n        Args:\n            sensor_spec: sensor spec containing all sensor\n                information we need to update the control status.\n            status: status of the sensor.\n            upstream_key: upstream key (e.g., used to store an attribute\n                name from the upstream so that new data can be detected\n                automatically).\n            upstream_value: upstream value (e.g., used to store the max\n                attribute value from the upstream so that new data can be\n                detected automatically).\n        \"\"\"\ncls._LOGGER.info(\nf\"Updating sensor status for sensor {sensor_spec.sensor_id}...\"\n)\ndata = cls._convert_sensor_to_data(\nspec=sensor_spec,\nstatus=status,\nupstream_key=upstream_key,\nupstream_value=upstream_value,\n)\nsensor_update_set = cls._get_sensor_update_set(\nassets=sensor_spec.assets,\ncheckpoint_location=sensor_spec.checkpoint_location,\nupstream_key=upstream_key,\nupstream_value=upstream_value,\n)\ncls._update_sensor_control(\ndata=data,\nsensor_update_set=sensor_update_set,\nsensor_control_table=sensor_spec.control_db_table_name,\nsensor_id=sensor_spec.sensor_id,\n)\n@classmethod\ndef _update_sensor_control(\ncls,\ndata: List[dict],\nsensor_update_set: dict,\nsensor_control_table: str,\nsensor_id: str,\n) -&gt; None:\n\"\"\"Update sensor control delta table.\n        Args:\n            data: to be updated.\n            sensor_update_set: columns which we had update.\n            sensor_control_table: control table name.\n            sensor_id: sensor_id to be updated.\n        \"\"\"\nsensors_delta_table = DeltaTable.forName(\nExecEnv.SESSION,\nsensor_control_table,\n)\nsensors_updates = ExecEnv.SESSION.createDataFrame(data, SENSOR_SCHEMA)\nsensors_delta_table.alias(\"sensors\").merge(\nsensors_updates.alias(\"updates\"),\nf\"sensors.sensor_id = '{sensor_id}' AND \"\n\"sensors.sensor_id = updates.sensor_id\",\n).whenMatchedUpdate(set=sensor_update_set).whenNotMatchedInsertAll().execute()\n@classmethod\ndef _convert_sensor_to_data(\ncls,\nspec: SensorSpec,\nstatus: str,\nupstream_key: str,\nupstream_value: str,\nstatus_change_timestamp: Optional[datetime] = None,\n) -&gt; List[dict]:\n\"\"\"Convert sensor data to dataframe input data.\n        Args:\n            spec: sensor spec containing sensor identifier data.\n            status: new sensor data status.\n            upstream_key: key used to acquired data from the upstream.\n            upstream_value: max value from the upstream_key\n                acquired from the upstream.\n            status_change_timestamp: timestamp we commit\n                this change in the sensor control table.\n        Returns:\n            Sensor data as list[dict], used to create a\n                dataframe to store the data into the sensor_control_table.\n        \"\"\"\nstatus_change_timestamp = (\ndatetime.now()\nif status_change_timestamp is None\nelse status_change_timestamp\n)\nreturn [\n{\n\"sensor_id\": spec.sensor_id,\n\"assets\": spec.assets,\n\"status\": status,\n\"status_change_timestamp\": status_change_timestamp,\n\"checkpoint_location\": spec.checkpoint_location,\n\"upstream_key\": str(upstream_key),\n\"upstream_value\": str(upstream_value),\n}\n]\n@classmethod\ndef _get_sensor_update_set(cls, **kwargs: Union[Optional[str], List[str]]) -&gt; dict:\n\"\"\"Get the sensor update set.\n        Args:\n            kwargs: Containing the following keys:\n            - assets\n            - checkpoint_location\n            - upstream_key\n            - upstream_value\n        Returns:\n            A set containing the fields to update in the control_table.\n        \"\"\"\nsensor_update_set = dict(SENSOR_UPDATE_SET)\nfor key, value in kwargs.items():\nif value:\nsensor_update_set[f\"sensors.{key}\"] = f\"updates.{key}\"\nreturn sensor_update_set\n@classmethod\ndef read_sensor_table_data(\ncls,\ncontrol_db_table_name: str,\nsensor_id: str = None,\nassets: list = None,\n) -&gt; Optional[Row]:\n\"\"\"Read data from delta table containing sensor status info.\n        Args:\n            sensor_id: sensor id. If this parameter is defined search occurs\n                only considering this parameter. Otherwise, it considers sensor\n                assets and checkpoint location.\n            control_db_table_name: db.table to control sensor runs.\n            assets: list of assets that are fueled by the pipeline\n                where this sensor is.\n        Returns:\n            Row containing the data for the provided sensor_id.\n        \"\"\"\ndf = DeltaTable.forName(\nExecEnv.SESSION,\ncontrol_db_table_name,\n).toDF()\nif sensor_id:\ndf = df.where(col(\"sensor_id\") == sensor_id)\nelif assets:\ndf = df.where(col(\"assets\") == array(*[lit(asset) for asset in assets]))\nelse:\nraise ValueError(\n\"Either sensor_id or assets need to be provided as arguments.\"\n)\nreturn df.first()\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorControlTableManager.check_if_sensor_has_acquired_data","title":"<code>check_if_sensor_has_acquired_data(sensor_id, control_db_table_name)</code>  <code>classmethod</code>","text":"<p>Check if sensor has acquired new data.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id.</p> required <code>control_db_table_name</code> <code>str</code> <p><code>db.table</code> to control sensor runs.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if acquired new data, otherwise False</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef check_if_sensor_has_acquired_data(\ncls,\nsensor_id: str,\ncontrol_db_table_name: str,\n) -&gt; bool:\n\"\"\"Check if sensor has acquired new data.\n    Args:\n        sensor_id: sensor id.\n        control_db_table_name: `db.table` to control sensor runs.\n    Returns:\n        True if acquired new data, otherwise False\n    \"\"\"\nsensor_table_data = cls.read_sensor_table_data(\nsensor_id=sensor_id, control_db_table_name=control_db_table_name\n)\ncls._LOGGER.info(f\"sensor_table_data = {sensor_table_data}\")\nreturn (\nsensor_table_data is not None\nand sensor_table_data.status == SensorStatus.ACQUIRED_NEW_DATA.value\n)\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorControlTableManager.read_sensor_table_data","title":"<code>read_sensor_table_data(control_db_table_name, sensor_id=None, assets=None)</code>  <code>classmethod</code>","text":"<p>Read data from delta table containing sensor status info.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id. If this parameter is defined search occurs only considering this parameter. Otherwise, it considers sensor assets and checkpoint location.</p> <code>None</code> <code>control_db_table_name</code> <code>str</code> <p>db.table to control sensor runs.</p> required <code>assets</code> <code>list</code> <p>list of assets that are fueled by the pipeline where this sensor is.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Row]</code> <p>Row containing the data for the provided sensor_id.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef read_sensor_table_data(\ncls,\ncontrol_db_table_name: str,\nsensor_id: str = None,\nassets: list = None,\n) -&gt; Optional[Row]:\n\"\"\"Read data from delta table containing sensor status info.\n    Args:\n        sensor_id: sensor id. If this parameter is defined search occurs\n            only considering this parameter. Otherwise, it considers sensor\n            assets and checkpoint location.\n        control_db_table_name: db.table to control sensor runs.\n        assets: list of assets that are fueled by the pipeline\n            where this sensor is.\n    Returns:\n        Row containing the data for the provided sensor_id.\n    \"\"\"\ndf = DeltaTable.forName(\nExecEnv.SESSION,\ncontrol_db_table_name,\n).toDF()\nif sensor_id:\ndf = df.where(col(\"sensor_id\") == sensor_id)\nelif assets:\ndf = df.where(col(\"assets\") == array(*[lit(asset) for asset in assets]))\nelse:\nraise ValueError(\n\"Either sensor_id or assets need to be provided as arguments.\"\n)\nreturn df.first()\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorControlTableManager.update_sensor_status","title":"<code>update_sensor_status(sensor_spec, status, upstream_key=None, upstream_value=None)</code>  <code>classmethod</code>","text":"<p>Control sensor execution storing the execution data in a delta table.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_spec</code> <code>SensorSpec</code> <p>sensor spec containing all sensor information we need to update the control status.</p> required <code>status</code> <code>str</code> <p>status of the sensor.</p> required <code>upstream_key</code> <code>str</code> <p>upstream key (e.g., used to store an attribute name from the upstream so that new data can be detected automatically).</p> <code>None</code> <code>upstream_value</code> <code>str</code> <p>upstream value (e.g., used to store the max attribute value from the upstream so that new data can be detected automatically).</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef update_sensor_status(\ncls,\nsensor_spec: SensorSpec,\nstatus: str,\nupstream_key: str = None,\nupstream_value: str = None,\n) -&gt; None:\n\"\"\"Control sensor execution storing the execution data in a delta table.\n    Args:\n        sensor_spec: sensor spec containing all sensor\n            information we need to update the control status.\n        status: status of the sensor.\n        upstream_key: upstream key (e.g., used to store an attribute\n            name from the upstream so that new data can be detected\n            automatically).\n        upstream_value: upstream value (e.g., used to store the max\n            attribute value from the upstream so that new data can be\n            detected automatically).\n    \"\"\"\ncls._LOGGER.info(\nf\"Updating sensor status for sensor {sensor_spec.sensor_id}...\"\n)\ndata = cls._convert_sensor_to_data(\nspec=sensor_spec,\nstatus=status,\nupstream_key=upstream_key,\nupstream_value=upstream_value,\n)\nsensor_update_set = cls._get_sensor_update_set(\nassets=sensor_spec.assets,\ncheckpoint_location=sensor_spec.checkpoint_location,\nupstream_key=upstream_key,\nupstream_value=upstream_value,\n)\ncls._update_sensor_control(\ndata=data,\nsensor_update_set=sensor_update_set,\nsensor_control_table=sensor_spec.control_db_table_name,\nsensor_id=sensor_spec.sensor_id,\n)\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorUpstreamManager","title":"<code>SensorUpstreamManager</code>","text":"<p>         Bases: <code>object</code></p> <p>Class to deal with Sensor Upstream data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>class SensorUpstreamManager(object):\n\"\"\"Class to deal with Sensor Upstream data.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef generate_filter_exp_query(\ncls,\nsensor_id: str,\nfilter_exp: str,\ncontrol_db_table_name: str = None,\nupstream_key: str = None,\nupstream_value: str = None,\nupstream_table_name: str = None,\n) -&gt; str:\n\"\"\"Generates a sensor preprocess query based on timestamp logic.\n        Args:\n            sensor_id: sensor id.\n            filter_exp: expression to filter incoming new data.\n                You can use the placeholder `?upstream_value` so that\n                it can be replaced by the upstream_value in the\n                control_db_table_name for this specific sensor_id.\n            control_db_table_name: db.table to retrieve the last status change\n                timestamp. This is only relevant for the jdbc sensor.\n            upstream_key: the key of custom sensor information\n                to control how to identify new data from the\n                upstream (e.g., a time column in the upstream).\n            upstream_value: value for custom sensor\n                to identify new data from the upstream\n                (e.g., the value of a time present in the upstream)\n                If none we will set the default value.\n                Note: This parameter is used just to override the\n                default value `-2147483647`.\n            upstream_table_name: value for custom sensor\n                to query new data from the upstream.\n                If none we will set the default value,\n                our `sensor_new_data` view.\n        Returns:\n            The query string.\n        \"\"\"\nsource_table = upstream_table_name if upstream_table_name else \"sensor_new_data\"\nselect_exp = \"SELECT COUNT(1) as count\"\nif control_db_table_name:\nif not upstream_key:\nraise ValueError(\n\"If control_db_table_name is defined, upstream_key should \"\n\"also be defined!\"\n)\ndefault_upstream_value: str = \"-2147483647\"\ntrigger_name = upstream_key\ntrigger_value = (\ndefault_upstream_value if upstream_value is None else upstream_value\n)\nsensor_table_data = SensorControlTableManager.read_sensor_table_data(\nsensor_id=sensor_id, control_db_table_name=control_db_table_name\n)\nif sensor_table_data and sensor_table_data.upstream_value:\ntrigger_value = sensor_table_data.upstream_value\nfilter_exp = filter_exp.replace(\"?upstream_key\", trigger_name).replace(\n\"?upstream_value\", trigger_value\n)\nselect_exp = (\nf\"SELECT COUNT(1) as count, '{trigger_name}' as UPSTREAM_KEY, \"\nf\"max({trigger_name}) as UPSTREAM_VALUE\"\n)\nquery = (\nf\"{select_exp} \"\nf\"FROM {source_table} \"\nf\"WHERE {filter_exp} \"\nf\"HAVING COUNT(1) &gt; 0\"\n)\nreturn query\n@classmethod\ndef generate_sensor_table_preprocess_query(\ncls,\nsensor_id: str,\n) -&gt; str:\n\"\"\"Generates a query to be used for a sensor having other sensor as upstream.\n        Args:\n            sensor_id: sensor id.\n        Returns:\n            The query string.\n        \"\"\"\nquery = (\nf\"SELECT * \"  # nosec\nf\"FROM sensor_new_data \"\nf\"WHERE\"\nf\" _change_type in ('insert', 'update_postimage')\"\nf\" and sensor_id = '{sensor_id}'\"\nf\" and status = '{SensorStatus.PROCESSED_NEW_DATA.value}'\"\n)\nreturn query\n@classmethod\ndef read_new_data(cls, sensor_spec: SensorSpec) -&gt; DataFrame:\n\"\"\"Read new data from the upstream into the sensor 'new_data_df'.\n        Args:\n            sensor_spec: sensor spec containing all sensor information.\n        Returns:\n            An empty dataframe if it doesn't have new data otherwise the new data\n        \"\"\"\nnew_data_df = ReaderFactory.get_data(sensor_spec.input_spec)\nif sensor_spec.preprocess_query:\nnew_data_df.createOrReplaceTempView(\"sensor_new_data\")\nnew_data_df = ExecEnv.SESSION.sql(sensor_spec.preprocess_query)\nreturn new_data_df\n@classmethod\ndef get_new_data(\ncls,\nnew_data_df: DataFrame,\n) -&gt; Optional[Row]:\n\"\"\"Get new data from upstream df if it's present.\n        Args:\n            new_data_df: DataFrame possibly containing new data.\n        Returns:\n            Optional row, present if there is new data in the upstream,\n            absent otherwise.\n        \"\"\"\nreturn new_data_df.first()\n@classmethod\ndef generate_sensor_sap_logchain_query(\ncls,\nchain_id: str,\ndbtable: str = SAPLogchain.DBTABLE.value,\nstatus: str = SAPLogchain.GREEN_STATUS.value,\nengine_table_name: str = SAPLogchain.ENGINE_TABLE.value,\n) -&gt; str:\n\"\"\"Generates a sensor query based in the SAP Logchain table.\n        Args:\n            chain_id: chain id to query the status on SAP.\n            dbtable: db.table to retrieve the data to\n                check if the sap chain is already finished.\n            status: db.table to retrieve the last status change\n                timestamp.\n            engine_table_name: table name exposed with the SAP LOGCHAIN data.\n                This table will be used in the jdbc query.\n        Returns:\n            The query string.\n        \"\"\"\nif not chain_id:\nraise ValueError(\n\"To query on log chain SAP table the chain id should be defined!\"\n)\nselect_exp = (\n\"SELECT CHAIN_ID, CONCAT(DATUM, ZEIT) AS LOAD_DATE, ANALYZED_STATUS\"\n)\nfilter_exp = (\nf\"UPPER(CHAIN_ID) = UPPER('{chain_id}') \"\nf\"AND UPPER(ANALYZED_STATUS) = UPPER('{status}')\"\n)\nquery = (\nf\"WITH {engine_table_name} AS (\"\nf\"{select_exp} \"\nf\"FROM {dbtable} \"\nf\"WHERE {filter_exp}\"\n\")\"\n)\nreturn query\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorUpstreamManager.generate_filter_exp_query","title":"<code>generate_filter_exp_query(sensor_id, filter_exp, control_db_table_name=None, upstream_key=None, upstream_value=None, upstream_table_name=None)</code>  <code>classmethod</code>","text":"<p>Generates a sensor preprocess query based on timestamp logic.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id.</p> required <code>filter_exp</code> <code>str</code> <p>expression to filter incoming new data. You can use the placeholder <code>?upstream_value</code> so that it can be replaced by the upstream_value in the control_db_table_name for this specific sensor_id.</p> required <code>control_db_table_name</code> <code>str</code> <p>db.table to retrieve the last status change timestamp. This is only relevant for the jdbc sensor.</p> <code>None</code> <code>upstream_key</code> <code>str</code> <p>the key of custom sensor information to control how to identify new data from the upstream (e.g., a time column in the upstream).</p> <code>None</code> <code>upstream_value</code> <code>str</code> <p>value for custom sensor to identify new data from the upstream (e.g., the value of a time present in the upstream) If none we will set the default value. Note: This parameter is used just to override the default value <code>-2147483647</code>.</p> <code>None</code> <code>upstream_table_name</code> <code>str</code> <p>value for custom sensor to query new data from the upstream. If none we will set the default value, our <code>sensor_new_data</code> view.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The query string.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef generate_filter_exp_query(\ncls,\nsensor_id: str,\nfilter_exp: str,\ncontrol_db_table_name: str = None,\nupstream_key: str = None,\nupstream_value: str = None,\nupstream_table_name: str = None,\n) -&gt; str:\n\"\"\"Generates a sensor preprocess query based on timestamp logic.\n    Args:\n        sensor_id: sensor id.\n        filter_exp: expression to filter incoming new data.\n            You can use the placeholder `?upstream_value` so that\n            it can be replaced by the upstream_value in the\n            control_db_table_name for this specific sensor_id.\n        control_db_table_name: db.table to retrieve the last status change\n            timestamp. This is only relevant for the jdbc sensor.\n        upstream_key: the key of custom sensor information\n            to control how to identify new data from the\n            upstream (e.g., a time column in the upstream).\n        upstream_value: value for custom sensor\n            to identify new data from the upstream\n            (e.g., the value of a time present in the upstream)\n            If none we will set the default value.\n            Note: This parameter is used just to override the\n            default value `-2147483647`.\n        upstream_table_name: value for custom sensor\n            to query new data from the upstream.\n            If none we will set the default value,\n            our `sensor_new_data` view.\n    Returns:\n        The query string.\n    \"\"\"\nsource_table = upstream_table_name if upstream_table_name else \"sensor_new_data\"\nselect_exp = \"SELECT COUNT(1) as count\"\nif control_db_table_name:\nif not upstream_key:\nraise ValueError(\n\"If control_db_table_name is defined, upstream_key should \"\n\"also be defined!\"\n)\ndefault_upstream_value: str = \"-2147483647\"\ntrigger_name = upstream_key\ntrigger_value = (\ndefault_upstream_value if upstream_value is None else upstream_value\n)\nsensor_table_data = SensorControlTableManager.read_sensor_table_data(\nsensor_id=sensor_id, control_db_table_name=control_db_table_name\n)\nif sensor_table_data and sensor_table_data.upstream_value:\ntrigger_value = sensor_table_data.upstream_value\nfilter_exp = filter_exp.replace(\"?upstream_key\", trigger_name).replace(\n\"?upstream_value\", trigger_value\n)\nselect_exp = (\nf\"SELECT COUNT(1) as count, '{trigger_name}' as UPSTREAM_KEY, \"\nf\"max({trigger_name}) as UPSTREAM_VALUE\"\n)\nquery = (\nf\"{select_exp} \"\nf\"FROM {source_table} \"\nf\"WHERE {filter_exp} \"\nf\"HAVING COUNT(1) &gt; 0\"\n)\nreturn query\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorUpstreamManager.generate_sensor_sap_logchain_query","title":"<code>generate_sensor_sap_logchain_query(chain_id, dbtable=SAPLogchain.DBTABLE.value, status=SAPLogchain.GREEN_STATUS.value, engine_table_name=SAPLogchain.ENGINE_TABLE.value)</code>  <code>classmethod</code>","text":"<p>Generates a sensor query based in the SAP Logchain table.</p> <p>Parameters:</p> Name Type Description Default <code>chain_id</code> <code>str</code> <p>chain id to query the status on SAP.</p> required <code>dbtable</code> <code>str</code> <p>db.table to retrieve the data to check if the sap chain is already finished.</p> <code>SAPLogchain.DBTABLE.value</code> <code>status</code> <code>str</code> <p>db.table to retrieve the last status change timestamp.</p> <code>SAPLogchain.GREEN_STATUS.value</code> <code>engine_table_name</code> <code>str</code> <p>table name exposed with the SAP LOGCHAIN data. This table will be used in the jdbc query.</p> <code>SAPLogchain.ENGINE_TABLE.value</code> <p>Returns:</p> Type Description <code>str</code> <p>The query string.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef generate_sensor_sap_logchain_query(\ncls,\nchain_id: str,\ndbtable: str = SAPLogchain.DBTABLE.value,\nstatus: str = SAPLogchain.GREEN_STATUS.value,\nengine_table_name: str = SAPLogchain.ENGINE_TABLE.value,\n) -&gt; str:\n\"\"\"Generates a sensor query based in the SAP Logchain table.\n    Args:\n        chain_id: chain id to query the status on SAP.\n        dbtable: db.table to retrieve the data to\n            check if the sap chain is already finished.\n        status: db.table to retrieve the last status change\n            timestamp.\n        engine_table_name: table name exposed with the SAP LOGCHAIN data.\n            This table will be used in the jdbc query.\n    Returns:\n        The query string.\n    \"\"\"\nif not chain_id:\nraise ValueError(\n\"To query on log chain SAP table the chain id should be defined!\"\n)\nselect_exp = (\n\"SELECT CHAIN_ID, CONCAT(DATUM, ZEIT) AS LOAD_DATE, ANALYZED_STATUS\"\n)\nfilter_exp = (\nf\"UPPER(CHAIN_ID) = UPPER('{chain_id}') \"\nf\"AND UPPER(ANALYZED_STATUS) = UPPER('{status}')\"\n)\nquery = (\nf\"WITH {engine_table_name} AS (\"\nf\"{select_exp} \"\nf\"FROM {dbtable} \"\nf\"WHERE {filter_exp}\"\n\")\"\n)\nreturn query\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorUpstreamManager.generate_sensor_table_preprocess_query","title":"<code>generate_sensor_table_preprocess_query(sensor_id)</code>  <code>classmethod</code>","text":"<p>Generates a query to be used for a sensor having other sensor as upstream.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The query string.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef generate_sensor_table_preprocess_query(\ncls,\nsensor_id: str,\n) -&gt; str:\n\"\"\"Generates a query to be used for a sensor having other sensor as upstream.\n    Args:\n        sensor_id: sensor id.\n    Returns:\n        The query string.\n    \"\"\"\nquery = (\nf\"SELECT * \"  # nosec\nf\"FROM sensor_new_data \"\nf\"WHERE\"\nf\" _change_type in ('insert', 'update_postimage')\"\nf\" and sensor_id = '{sensor_id}'\"\nf\" and status = '{SensorStatus.PROCESSED_NEW_DATA.value}'\"\n)\nreturn query\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorUpstreamManager.get_new_data","title":"<code>get_new_data(new_data_df)</code>  <code>classmethod</code>","text":"<p>Get new data from upstream df if it's present.</p> <p>Parameters:</p> Name Type Description Default <code>new_data_df</code> <code>DataFrame</code> <p>DataFrame possibly containing new data.</p> required <p>Returns:</p> Type Description <code>Optional[Row]</code> <p>Optional row, present if there is new data in the upstream,</p> <code>Optional[Row]</code> <p>absent otherwise.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef get_new_data(\ncls,\nnew_data_df: DataFrame,\n) -&gt; Optional[Row]:\n\"\"\"Get new data from upstream df if it's present.\n    Args:\n        new_data_df: DataFrame possibly containing new data.\n    Returns:\n        Optional row, present if there is new data in the upstream,\n        absent otherwise.\n    \"\"\"\nreturn new_data_df.first()\n</code></pre>"},{"location":"reference/packages/core/sensor_manager.html#packages.core.sensor_manager.SensorUpstreamManager.read_new_data","title":"<code>read_new_data(sensor_spec)</code>  <code>classmethod</code>","text":"<p>Read new data from the upstream into the sensor 'new_data_df'.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_spec</code> <code>SensorSpec</code> <p>sensor spec containing all sensor information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>An empty dataframe if it doesn't have new data otherwise the new data</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/sensor_manager.py</code> <pre><code>@classmethod\ndef read_new_data(cls, sensor_spec: SensorSpec) -&gt; DataFrame:\n\"\"\"Read new data from the upstream into the sensor 'new_data_df'.\n    Args:\n        sensor_spec: sensor spec containing all sensor information.\n    Returns:\n        An empty dataframe if it doesn't have new data otherwise the new data\n    \"\"\"\nnew_data_df = ReaderFactory.get_data(sensor_spec.input_spec)\nif sensor_spec.preprocess_query:\nnew_data_df.createOrReplaceTempView(\"sensor_new_data\")\nnew_data_df = ExecEnv.SESSION.sql(sensor_spec.preprocess_query)\nreturn new_data_df\n</code></pre>"},{"location":"reference/packages/core/table_manager.html","title":"Table manager","text":"<p>Table manager module.</p>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager","title":"<code>TableManager</code>","text":"<p>         Bases: <code>object</code></p> <p>Set of actions to manipulate tables/views in several ways.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>class TableManager(object):\n\"\"\"Set of actions to manipulate tables/views in several ways.\"\"\"\ndef __init__(self, configs: dict):\n\"\"\"Construct TableManager algorithm instances.\n        Args:\n            configs: configurations for the TableManager algorithm.\n        \"\"\"\nself._logger = LoggingHandler(__name__).get_logger()\nself.configs = configs\nself.function = self.configs[\"function\"]\ndef get_function(self) -&gt; None:\n\"\"\"Get a specific function to execute.\"\"\"\navailable_functions = {\n\"compute_table_statistics\": self.compute_table_statistics,\n\"create_table\": self.create,\n\"create_tables\": self.create_many,\n\"create_view\": self.create,\n\"drop_table\": self.drop_table,\n\"drop_view\": self.drop_view,\n\"execute_sql\": self.execute_sql,\n\"truncate\": self.truncate,\n\"vacuum\": self.vacuum,\n\"describe\": self.describe,\n\"optimize\": self.optimize,\n\"show_tbl_properties\": self.show_tbl_properties,\n\"get_tbl_pk\": self.get_tbl_pk,\n\"repair_table\": self.repair_table,\n\"delete_where\": self.delete_where,\n}\nself._logger.info(\"Function being executed: {}\".format(self.function))\nif self.function in available_functions.keys():\nfunc = available_functions[self.function]\nfunc()\nelse:\nraise NotImplementedError(\nf\"The requested function {self.function} is not implemented.\"\n)\ndef create(self) -&gt; None:\n\"\"\"Create a new table or view on metastore.\"\"\"\ndisable_dbfs_retry = (\nself.configs[\"disable_dbfs_retry\"]\nif \"disable_dbfs_retry\" in self.configs.keys()\nelse False\n)\nsql = ConfigUtils.read_sql(self.configs[\"path\"], disable_dbfs_retry)\ntry:\nsql_commands = SQLParserUtils().split_sql_commands(\nsql_commands=sql,\ndelimiter=self.configs.get(\"delimiter\", \";\"),\nadvanced_parser=self.configs.get(\"advanced_parser\", False),\n)\nfor command in sql_commands:\nif command.strip():\nself._logger.info(f\"sql command: {command}\")\nExecEnv.SESSION.sql(command)\nself._logger.info(f\"{self.function} successfully executed!\")\nexcept Exception as e:\nself._logger.error(e)\nraise\ndef create_many(self) -&gt; None:\n\"\"\"Create multiple tables or views on metastore.\n        In this function the path to the ddl files can be separated by comma.\n        \"\"\"\nself.execute_multiple_sql_files()\ndef compute_table_statistics(self) -&gt; None:\n\"\"\"Compute table statistics.\"\"\"\nsql = SQLDefinitions.compute_table_stats.value.format(\nself.configs[\"table_or_view\"]\n)\ntry:\nself._logger.info(f\"sql command: {sql}\")\nExecEnv.SESSION.sql(sql)\nself._logger.info(f\"{self.function} successfully executed!\")\nexcept Exception as e:\nself._logger.error(e)\nraise\ndef drop_table(self) -&gt; None:\n\"\"\"Delete table function deletes table from metastore and erases all data.\"\"\"\ndrop_stmt = \"{} {}\".format(\nSQLDefinitions.drop_table_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {drop_stmt}\")\nExecEnv.SESSION.sql(drop_stmt)\nself._logger.info(\"Table successfully dropped!\")\ndef drop_view(self) -&gt; None:\n\"\"\"Delete view function deletes view from metastore and erases all data.\"\"\"\ndrop_stmt = \"{} {}\".format(\nSQLDefinitions.drop_view_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {drop_stmt}\")\nExecEnv.SESSION.sql(drop_stmt)\nself._logger.info(\"View successfully dropped!\")\ndef truncate(self) -&gt; None:\n\"\"\"Truncate function erases all data but keeps metadata.\"\"\"\ntruncate_stmt = \"{} {}\".format(\nSQLDefinitions.truncate_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {truncate_stmt}\")\nExecEnv.SESSION.sql(truncate_stmt)\nself._logger.info(\"Table successfully truncated!\")\ndef vacuum(self) -&gt; None:\n\"\"\"Vacuum function erases older versions from Delta Lake tables or locations.\"\"\"\nif not self.configs.get(\"table_or_view\", None):\ndelta_table = DeltaTable.forPath(ExecEnv.SESSION, self.configs[\"path\"])\nself._logger.info(f\"Vacuuming location: {self.configs['path']}\")\ndelta_table.vacuum(self.configs.get(\"vacuum_hours\", 168))\nelse:\ndelta_table = DeltaTable.forName(\nExecEnv.SESSION, self.configs[\"table_or_view\"]\n)\nself._logger.info(f\"Vacuuming table: {self.configs['table_or_view']}\")\ndelta_table.vacuum(self.configs.get(\"vacuum_hours\", 168))\ndef describe(self) -&gt; None:\n\"\"\"Describe function describes metadata from some table or view.\"\"\"\ndescribe_stmt = \"{} {}\".format(\nSQLDefinitions.describe_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {describe_stmt}\")\noutput = ExecEnv.SESSION.sql(describe_stmt)\nself._logger.info(output)\ndef optimize(self) -&gt; None:\n\"\"\"Optimize function optimizes the layout of Delta Lake data.\"\"\"\nif self.configs.get(\"where_clause\", None):\nwhere_exp = \"WHERE {}\".format(self.configs[\"where_clause\"].strip())\nelse:\nwhere_exp = \"\"\nif self.configs.get(\"optimize_zorder_col_list\", None):\nzorder_exp = \"ZORDER BY ({})\".format(\nself.configs[\"optimize_zorder_col_list\"].strip()\n)\nelse:\nzorder_exp = \"\"\noptimize_stmt = \"{} {} {} {}\".format(\nSQLDefinitions.optimize_stmt.value,\n(\nf\"delta.`{self.configs.get('path', None)}`\"\nif not self.configs.get(\"table_or_view\", None)\nelse self.configs.get(\"table_or_view\", None)\n),\nwhere_exp,\nzorder_exp,\n)\nself._logger.info(f\"sql command: {optimize_stmt}\")\noutput = ExecEnv.SESSION.sql(optimize_stmt)\nself._logger.info(output)\ndef execute_multiple_sql_files(self) -&gt; None:\n\"\"\"Execute multiple statements in multiple sql files.\n        In this function the path to the files is separated by comma.\n        \"\"\"\nfor table_metadata_file in self.configs[\"path\"].split(\",\"):\ndisable_dbfs_retry = (\nself.configs[\"disable_dbfs_retry\"]\nif \"disable_dbfs_retry\" in self.configs.keys()\nelse False\n)\nsql = ConfigUtils.read_sql(table_metadata_file.strip(), disable_dbfs_retry)\nsql_commands = SQLParserUtils().split_sql_commands(\nsql_commands=sql,\ndelimiter=self.configs.get(\"delimiter\", \";\"),\nadvanced_parser=self.configs.get(\"advanced_parser\", False),\n)\nfor command in sql_commands:\nif command.strip():\nself._logger.info(f\"sql command: {command}\")\nExecEnv.SESSION.sql(command)\nself._logger.info(\"sql file successfully executed!\")\ndef execute_sql(self) -&gt; None:\n\"\"\"Execute sql commands separated by semicolon (;).\"\"\"\nsql_commands = SQLParserUtils().split_sql_commands(\nsql_commands=self.configs.get(\"sql\"),\ndelimiter=self.configs.get(\"delimiter\", \";\"),\nadvanced_parser=self.configs.get(\"advanced_parser\", False),\n)\nfor command in sql_commands:\nif command.strip():\nself._logger.info(f\"sql command: {command}\")\nExecEnv.SESSION.sql(command)\nself._logger.info(\"sql successfully executed!\")\ndef show_tbl_properties(self) -&gt; DataFrame:\n\"\"\"Show Table Properties.\n        Returns:\n            A dataframe with the table properties.\n        \"\"\"\nshow_tbl_props_stmt = \"{} {}\".format(\nSQLDefinitions.show_tbl_props_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {show_tbl_props_stmt}\")\noutput = ExecEnv.SESSION.sql(show_tbl_props_stmt)\nself._logger.info(output)\nreturn output\ndef get_tbl_pk(self) -&gt; List[str]:\n\"\"\"Get the primary key of a particular table.\n        Returns:\n            The list of columns that are part of the primary key.\n        \"\"\"\noutput: List[str] = (\nself.show_tbl_properties()\n.filter(\"key == 'lakehouse.primary_key'\")\n.select(\"value\")\n.withColumn(\"value\", translate(\"value\", \" `\", \"\"))\n.first()[0]\n.split(\",\")\n)\nself._logger.info(output)\nreturn output\ndef repair_table(self) -&gt; None:\n\"\"\"Run the repair table command.\"\"\"\ntable_name = self.configs[\"table_or_view\"]\nsync_metadata = self.configs[\"sync_metadata\"]\nrepair_stmt = (\nf\"MSCK REPAIR TABLE {table_name} \"\nf\"{'SYNC METADATA' if sync_metadata else ''}\"\n)\nself._logger.info(f\"sql command: {repair_stmt}\")\noutput = ExecEnv.SESSION.sql(repair_stmt)\nself._logger.info(output)\ndef delete_where(self) -&gt; None:\n\"\"\"Run the delete where command.\"\"\"\ntable_name = self.configs[\"table_or_view\"]\ndelete_where = self.configs[\"where_clause\"].strip()\ndelete_stmt = SQLDefinitions.delete_where_stmt.value.format(\ntable_name, delete_where\n)\nself._logger.info(f\"sql command: {delete_stmt}\")\noutput = ExecEnv.SESSION.sql(delete_stmt)\nself._logger.info(output)\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.__init__","title":"<code>__init__(configs)</code>","text":"<p>Construct TableManager algorithm instances.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>dict</code> <p>configurations for the TableManager algorithm.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def __init__(self, configs: dict):\n\"\"\"Construct TableManager algorithm instances.\n    Args:\n        configs: configurations for the TableManager algorithm.\n    \"\"\"\nself._logger = LoggingHandler(__name__).get_logger()\nself.configs = configs\nself.function = self.configs[\"function\"]\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.compute_table_statistics","title":"<code>compute_table_statistics()</code>","text":"<p>Compute table statistics.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def compute_table_statistics(self) -&gt; None:\n\"\"\"Compute table statistics.\"\"\"\nsql = SQLDefinitions.compute_table_stats.value.format(\nself.configs[\"table_or_view\"]\n)\ntry:\nself._logger.info(f\"sql command: {sql}\")\nExecEnv.SESSION.sql(sql)\nself._logger.info(f\"{self.function} successfully executed!\")\nexcept Exception as e:\nself._logger.error(e)\nraise\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.create","title":"<code>create()</code>","text":"<p>Create a new table or view on metastore.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def create(self) -&gt; None:\n\"\"\"Create a new table or view on metastore.\"\"\"\ndisable_dbfs_retry = (\nself.configs[\"disable_dbfs_retry\"]\nif \"disable_dbfs_retry\" in self.configs.keys()\nelse False\n)\nsql = ConfigUtils.read_sql(self.configs[\"path\"], disable_dbfs_retry)\ntry:\nsql_commands = SQLParserUtils().split_sql_commands(\nsql_commands=sql,\ndelimiter=self.configs.get(\"delimiter\", \";\"),\nadvanced_parser=self.configs.get(\"advanced_parser\", False),\n)\nfor command in sql_commands:\nif command.strip():\nself._logger.info(f\"sql command: {command}\")\nExecEnv.SESSION.sql(command)\nself._logger.info(f\"{self.function} successfully executed!\")\nexcept Exception as e:\nself._logger.error(e)\nraise\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.create_many","title":"<code>create_many()</code>","text":"<p>Create multiple tables or views on metastore.</p> <p>In this function the path to the ddl files can be separated by comma.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def create_many(self) -&gt; None:\n\"\"\"Create multiple tables or views on metastore.\n    In this function the path to the ddl files can be separated by comma.\n    \"\"\"\nself.execute_multiple_sql_files()\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.delete_where","title":"<code>delete_where()</code>","text":"<p>Run the delete where command.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def delete_where(self) -&gt; None:\n\"\"\"Run the delete where command.\"\"\"\ntable_name = self.configs[\"table_or_view\"]\ndelete_where = self.configs[\"where_clause\"].strip()\ndelete_stmt = SQLDefinitions.delete_where_stmt.value.format(\ntable_name, delete_where\n)\nself._logger.info(f\"sql command: {delete_stmt}\")\noutput = ExecEnv.SESSION.sql(delete_stmt)\nself._logger.info(output)\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.describe","title":"<code>describe()</code>","text":"<p>Describe function describes metadata from some table or view.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def describe(self) -&gt; None:\n\"\"\"Describe function describes metadata from some table or view.\"\"\"\ndescribe_stmt = \"{} {}\".format(\nSQLDefinitions.describe_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {describe_stmt}\")\noutput = ExecEnv.SESSION.sql(describe_stmt)\nself._logger.info(output)\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.drop_table","title":"<code>drop_table()</code>","text":"<p>Delete table function deletes table from metastore and erases all data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def drop_table(self) -&gt; None:\n\"\"\"Delete table function deletes table from metastore and erases all data.\"\"\"\ndrop_stmt = \"{} {}\".format(\nSQLDefinitions.drop_table_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {drop_stmt}\")\nExecEnv.SESSION.sql(drop_stmt)\nself._logger.info(\"Table successfully dropped!\")\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.drop_view","title":"<code>drop_view()</code>","text":"<p>Delete view function deletes view from metastore and erases all data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def drop_view(self) -&gt; None:\n\"\"\"Delete view function deletes view from metastore and erases all data.\"\"\"\ndrop_stmt = \"{} {}\".format(\nSQLDefinitions.drop_view_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {drop_stmt}\")\nExecEnv.SESSION.sql(drop_stmt)\nself._logger.info(\"View successfully dropped!\")\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.execute_multiple_sql_files","title":"<code>execute_multiple_sql_files()</code>","text":"<p>Execute multiple statements in multiple sql files.</p> <p>In this function the path to the files is separated by comma.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def execute_multiple_sql_files(self) -&gt; None:\n\"\"\"Execute multiple statements in multiple sql files.\n    In this function the path to the files is separated by comma.\n    \"\"\"\nfor table_metadata_file in self.configs[\"path\"].split(\",\"):\ndisable_dbfs_retry = (\nself.configs[\"disable_dbfs_retry\"]\nif \"disable_dbfs_retry\" in self.configs.keys()\nelse False\n)\nsql = ConfigUtils.read_sql(table_metadata_file.strip(), disable_dbfs_retry)\nsql_commands = SQLParserUtils().split_sql_commands(\nsql_commands=sql,\ndelimiter=self.configs.get(\"delimiter\", \";\"),\nadvanced_parser=self.configs.get(\"advanced_parser\", False),\n)\nfor command in sql_commands:\nif command.strip():\nself._logger.info(f\"sql command: {command}\")\nExecEnv.SESSION.sql(command)\nself._logger.info(\"sql file successfully executed!\")\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.execute_sql","title":"<code>execute_sql()</code>","text":"<p>Execute sql commands separated by semicolon (;).</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def execute_sql(self) -&gt; None:\n\"\"\"Execute sql commands separated by semicolon (;).\"\"\"\nsql_commands = SQLParserUtils().split_sql_commands(\nsql_commands=self.configs.get(\"sql\"),\ndelimiter=self.configs.get(\"delimiter\", \";\"),\nadvanced_parser=self.configs.get(\"advanced_parser\", False),\n)\nfor command in sql_commands:\nif command.strip():\nself._logger.info(f\"sql command: {command}\")\nExecEnv.SESSION.sql(command)\nself._logger.info(\"sql successfully executed!\")\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.get_function","title":"<code>get_function()</code>","text":"<p>Get a specific function to execute.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def get_function(self) -&gt; None:\n\"\"\"Get a specific function to execute.\"\"\"\navailable_functions = {\n\"compute_table_statistics\": self.compute_table_statistics,\n\"create_table\": self.create,\n\"create_tables\": self.create_many,\n\"create_view\": self.create,\n\"drop_table\": self.drop_table,\n\"drop_view\": self.drop_view,\n\"execute_sql\": self.execute_sql,\n\"truncate\": self.truncate,\n\"vacuum\": self.vacuum,\n\"describe\": self.describe,\n\"optimize\": self.optimize,\n\"show_tbl_properties\": self.show_tbl_properties,\n\"get_tbl_pk\": self.get_tbl_pk,\n\"repair_table\": self.repair_table,\n\"delete_where\": self.delete_where,\n}\nself._logger.info(\"Function being executed: {}\".format(self.function))\nif self.function in available_functions.keys():\nfunc = available_functions[self.function]\nfunc()\nelse:\nraise NotImplementedError(\nf\"The requested function {self.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.get_tbl_pk","title":"<code>get_tbl_pk()</code>","text":"<p>Get the primary key of a particular table.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>The list of columns that are part of the primary key.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def get_tbl_pk(self) -&gt; List[str]:\n\"\"\"Get the primary key of a particular table.\n    Returns:\n        The list of columns that are part of the primary key.\n    \"\"\"\noutput: List[str] = (\nself.show_tbl_properties()\n.filter(\"key == 'lakehouse.primary_key'\")\n.select(\"value\")\n.withColumn(\"value\", translate(\"value\", \" `\", \"\"))\n.first()[0]\n.split(\",\")\n)\nself._logger.info(output)\nreturn output\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.optimize","title":"<code>optimize()</code>","text":"<p>Optimize function optimizes the layout of Delta Lake data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def optimize(self) -&gt; None:\n\"\"\"Optimize function optimizes the layout of Delta Lake data.\"\"\"\nif self.configs.get(\"where_clause\", None):\nwhere_exp = \"WHERE {}\".format(self.configs[\"where_clause\"].strip())\nelse:\nwhere_exp = \"\"\nif self.configs.get(\"optimize_zorder_col_list\", None):\nzorder_exp = \"ZORDER BY ({})\".format(\nself.configs[\"optimize_zorder_col_list\"].strip()\n)\nelse:\nzorder_exp = \"\"\noptimize_stmt = \"{} {} {} {}\".format(\nSQLDefinitions.optimize_stmt.value,\n(\nf\"delta.`{self.configs.get('path', None)}`\"\nif not self.configs.get(\"table_or_view\", None)\nelse self.configs.get(\"table_or_view\", None)\n),\nwhere_exp,\nzorder_exp,\n)\nself._logger.info(f\"sql command: {optimize_stmt}\")\noutput = ExecEnv.SESSION.sql(optimize_stmt)\nself._logger.info(output)\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.repair_table","title":"<code>repair_table()</code>","text":"<p>Run the repair table command.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def repair_table(self) -&gt; None:\n\"\"\"Run the repair table command.\"\"\"\ntable_name = self.configs[\"table_or_view\"]\nsync_metadata = self.configs[\"sync_metadata\"]\nrepair_stmt = (\nf\"MSCK REPAIR TABLE {table_name} \"\nf\"{'SYNC METADATA' if sync_metadata else ''}\"\n)\nself._logger.info(f\"sql command: {repair_stmt}\")\noutput = ExecEnv.SESSION.sql(repair_stmt)\nself._logger.info(output)\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.show_tbl_properties","title":"<code>show_tbl_properties()</code>","text":"<p>Show Table Properties.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the table properties.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def show_tbl_properties(self) -&gt; DataFrame:\n\"\"\"Show Table Properties.\n    Returns:\n        A dataframe with the table properties.\n    \"\"\"\nshow_tbl_props_stmt = \"{} {}\".format(\nSQLDefinitions.show_tbl_props_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {show_tbl_props_stmt}\")\noutput = ExecEnv.SESSION.sql(show_tbl_props_stmt)\nself._logger.info(output)\nreturn output\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.truncate","title":"<code>truncate()</code>","text":"<p>Truncate function erases all data but keeps metadata.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def truncate(self) -&gt; None:\n\"\"\"Truncate function erases all data but keeps metadata.\"\"\"\ntruncate_stmt = \"{} {}\".format(\nSQLDefinitions.truncate_stmt.value,\nself.configs[\"table_or_view\"],\n)\nself._logger.info(f\"sql command: {truncate_stmt}\")\nExecEnv.SESSION.sql(truncate_stmt)\nself._logger.info(\"Table successfully truncated!\")\n</code></pre>"},{"location":"reference/packages/core/table_manager.html#packages.core.table_manager.TableManager.vacuum","title":"<code>vacuum()</code>","text":"<p>Vacuum function erases older versions from Delta Lake tables or locations.</p> Source code in <code>mkdocs/lakehouse_engine/packages/core/table_manager.py</code> <pre><code>def vacuum(self) -&gt; None:\n\"\"\"Vacuum function erases older versions from Delta Lake tables or locations.\"\"\"\nif not self.configs.get(\"table_or_view\", None):\ndelta_table = DeltaTable.forPath(ExecEnv.SESSION, self.configs[\"path\"])\nself._logger.info(f\"Vacuuming location: {self.configs['path']}\")\ndelta_table.vacuum(self.configs.get(\"vacuum_hours\", 168))\nelse:\ndelta_table = DeltaTable.forName(\nExecEnv.SESSION, self.configs[\"table_or_view\"]\n)\nself._logger.info(f\"Vacuuming table: {self.configs['table_or_view']}\")\ndelta_table.vacuum(self.configs.get(\"vacuum_hours\", 168))\n</code></pre>"},{"location":"reference/packages/dq_processors/index.html","title":"Dq processors","text":"<p>Package to define data quality processes available in the lakehouse engine.</p>"},{"location":"reference/packages/dq_processors/dq_factory.html","title":"Dq factory","text":"<p>Module containing the class definition of the Data Quality Factory.</p>"},{"location":"reference/packages/dq_processors/dq_factory.html#packages.dq_processors.dq_factory.DQFactory","title":"<code>DQFactory</code>","text":"<p>         Bases: <code>object</code></p> <p>Class for the Data Quality Factory.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/dq_factory.py</code> <pre><code>class DQFactory(object):\n\"\"\"Class for the Data Quality Factory.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n_TIMESTAMP = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n@classmethod\ndef run_dq_process(cls, dq_spec: DQSpec, data: DataFrame) -&gt; DataFrame:\n\"\"\"Run the specified data quality process on a dataframe.\n        Based on the dq_specs we apply the defined expectations on top of the dataframe\n        in order to apply the necessary validations and then output the result of\n        the data quality process.\n        Args:\n            dq_spec: data quality specification.\n            data: input dataframe to run the dq process on.\n        Returns:\n            The DataFrame containing the results of the DQ process.\n        \"\"\"\n# import custom expectations for them to be available to be used.\nfor expectation in DQDefaults.CUSTOM_EXPECTATION_LIST.value:\nimportlib.__import__(\n\"lakehouse_engine.dq_processors.custom_expectations.\" + expectation\n)\ncontext = get_context(project_config=cls._get_data_context_config(dq_spec))\ncontext.add_datasource(**cls._get_data_source_defaults(dq_spec))\nexpectation_suite_name = (\ndq_spec.expectation_suite_name\nif dq_spec.expectation_suite_name\nelse f\"{dq_spec.spec_id}-{dq_spec.input_id}-{dq_spec.dq_type}\"\n)\ncontext.add_or_update_expectation_suite(\nexpectation_suite_name=expectation_suite_name\n)\nbatch_request = cls._get_batch_request(dq_spec, data)\nif (\ndq_spec.dq_type == DQType.VALIDATOR.value\nor dq_spec.dq_type == DQType.PRISMA.value\n):\nValidator.get_dq_validator(\ncontext,\nbatch_request,\nexpectation_suite_name,\ndq_spec.dq_functions,\ndq_spec.critical_functions,\n)\nsource_pk = cls._get_unexpected_rows_pk(dq_spec)\nresults, results_df = cls._configure_and_run_checkpoint(\ndq_spec, context, batch_request, expectation_suite_name, source_pk\n)\nif dq_spec.dq_type == DQType.PRISMA.value:\nresults_df = results_df.withColumn(\"source_primary_key\", lit(source_pk))\nprocessed_keys_df = data.select(\nconcat_ws(\n\", \", *[coalesce(col(c), lit(\"null\")) for c in source_pk]\n).alias(\"combined_pk\")\n)\ncomb_pk_expr = (\nsort_array(collect_list(\"combined_pk\"))\nif dq_spec.sort_processed_keys\nelse collect_list(\"combined_pk\")\n)\nprocessed_keys_df = processed_keys_df.agg(\nconcat_ws(\"||\", comb_pk_expr).alias(\"processed_keys\")\n)\nresults_df = results_df.join(processed_keys_df, lit(1) == lit(1))\ncls._write_to_result_sink(dq_spec, results_df)\ncls._log_or_fail(results, dq_spec)\nif (\ndq_spec.tag_source_data\nand dq_spec.result_sink_explode\nand dq_spec.fail_on_error is not True\n):\ndata = Validator.tag_source_with_dq(source_pk, data, results_df)\nelse:\nraise TypeError(\nf\"Type of Data Quality '{dq_spec.dq_type}' is not supported.\"\n)\nreturn data\n@classmethod\ndef build_data_docs(\ncls,\nstore_backend: str = DQDefaults.STORE_BACKEND.value,\nlocal_fs_root_dir: str = None,\ndata_docs_local_fs: str = None,\ndata_docs_prefix: str = DQDefaults.DATA_DOCS_PREFIX.value,\nbucket: str = None,\ndata_docs_bucket: str = None,\nexpectations_store_prefix: str = DQDefaults.EXPECTATIONS_STORE_PREFIX.value,\nvalidations_store_prefix: str = DQDefaults.VALIDATIONS_STORE_PREFIX.value,\ncheckpoint_store_prefix: str = DQDefaults.CHECKPOINT_STORE_PREFIX.value,\n) -&gt; None:\n\"\"\"Build Data Docs for the project.\n        This function does a full build of data docs based on all the great expectations\n        checkpoints in the specified location, getting all history of run/validations\n        executed and results.\n        Args:\n            store_backend: which store_backend to use (e.g. s3 or file_system).\n            local_fs_root_dir: path of the root directory. Note: only applicable\n                for store_backend file_system\n            data_docs_local_fs: path of the root directory. Note: only applicable\n                for store_backend file_system.\n            data_docs_prefix: prefix where to store data_docs' data.\n            bucket: the bucket name to consider for the store_backend\n                (store DQ artefacts). Note: only applicable for store_backend s3.\n            data_docs_bucket: the bucket name for data docs only. When defined,\n                it will supersede bucket parameter.\n                Note: only applicable for store_backend s3.\n            expectations_store_prefix: prefix where to store expectations' data.\n                Note: only applicable for store_backend s3.\n            validations_store_prefix: prefix where to store validations' data.\n                Note: only applicable for store_backend s3.\n            checkpoint_store_prefix: prefix where to store checkpoints' data.\n                Note: only applicable for store_backend s3.\n        \"\"\"\nif store_backend == DQDefaults.STORE_BACKEND.value:\ndq_spec = DQSpec(\nspec_id=\"dq_validator\",\ninput_id=\"dq\",\ndq_type=DQType.VALIDATOR.value,\nstore_backend=DQDefaults.STORE_BACKEND.value,\ndata_docs_prefix=data_docs_prefix,\nbucket=bucket,\ndata_docs_bucket=data_docs_bucket,\nexpectations_store_prefix=expectations_store_prefix,\nvalidations_store_prefix=validations_store_prefix,\ncheckpoint_store_prefix=checkpoint_store_prefix,\n)\nelif store_backend == DQDefaults.FILE_SYSTEM_STORE.value:\ndq_spec = DQSpec(\nspec_id=\"dq_validator\",\ninput_id=\"dq\",\ndq_type=DQType.VALIDATOR.value,\nstore_backend=DQDefaults.FILE_SYSTEM_STORE.value,\nlocal_fs_root_dir=local_fs_root_dir,\ndata_docs_local_fs=data_docs_local_fs,\ndata_docs_prefix=data_docs_prefix,\n)\ncontext = get_context(project_config=cls._get_data_context_config(dq_spec))\ncls._LOGGER.info(\"The data docs were rebuilt\")\ncontext.build_data_docs()\n@classmethod\ndef _check_critical_functions_tags(cls, failed_expectations: List[Any]) -&gt; list:\ncritical_failure = []\nfor expectation in failed_expectations:\nmeta = expectation[\"meta\"]\nif meta and (\n(\"notes\" in meta.keys() and \"Critical function\" in meta[\"notes\"])\nor (\n\"content\" in meta[\"notes\"].keys()\nand \"Critical function\" in meta[\"notes\"][\"content\"]\n)\n):\ncritical_failure.append(expectation[\"expectation_type\"])\nreturn critical_failure\n@classmethod\ndef _configure_and_run_checkpoint(\ncls,\ndq_spec: DQSpec,\ncontext: EphemeralDataContext,\nbatch_request: RuntimeBatchRequest,\nexpectation_suite_name: str,\nsource_pk: List[str],\n) -&gt; Tuple[CheckpointResult, DataFrame]:\n\"\"\"Configure, run and return checkpoint results.\n        A checkpoint is what enables us to run the validations of the expectations'\n        suite on the batches of data.\n        Args:\n            dq_spec: data quality specification.\n            context: the EphemeralDataContext containing the configurations for the data\n                source and store backend.\n            batch_request: run time batch request to be able to query underlying data.\n            expectation_suite_name: name of the expectation suite.\n            source_pk: the primary key of the source data.\n        Returns:\n            The checkpoint results in two types: CheckpointResult and Dataframe.\n        \"\"\"\ncheckpoint_name = f\"{dq_spec.spec_id}-{dq_spec.input_id}-checkpoint\"\ncontext.add_or_update_checkpoint(\nname=checkpoint_name,\nclass_name=DQDefaults.DATA_CHECKPOINTS_CLASS_NAME.value,\nconfig_version=DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION.value,\nrun_name_template=f\"%Y%m%d-%H%M%S%f-{checkpoint_name}\",\n)\nresult_format: Dict[str, Any] = {\n\"result_format\": dq_spec.gx_result_format,\n}\nif source_pk:\nresult_format = {\n**result_format,\n\"unexpected_index_column_names\": source_pk,\n}\nresults = context.run_checkpoint(\ncheckpoint_name=checkpoint_name,\nvalidations=[\n{\n\"batch_request\": batch_request,\n\"expectation_suite_name\": expectation_suite_name,\n}\n],\nresult_format=result_format,\n)\nreturn results, cls._transform_checkpoint_results(\nresults.to_json_dict(), dq_spec\n)\n@classmethod\ndef _explode_results(\ncls,\ndf: DataFrame,\ndq_spec: DQSpec,\n) -&gt; DataFrame:\n\"\"\"Transform dq results dataframe exploding a set of columns.\n        Args:\n            df: dataframe with dq results to be exploded.\n            dq_spec: data quality specification.\n        \"\"\"\ndf = df.withColumn(\n\"validation_results\", explode(\"run_results.validation_result.results\")\n).withColumn(\"source\", lit(dq_spec.source))\nnew_columns = [\n\"validation_results.expectation_config.kwargs.*\",\n\"run_results.validation_result.statistics.*\",\n\"validation_results.expectation_config.expectation_type\",\n\"validation_results.success as expectation_success\",\n\"validation_results.exception_info\",\n] + dq_spec.result_sink_extra_columns\ndf_exploded = df.selectExpr(*df.columns, *new_columns).drop(\n*[c.replace(\".*\", \"\").split(\" as\")[0] for c in new_columns]\n)\nschema = df_exploded.schema.simpleString()\nif \"unexpected_index_list\" in schema:\ndf_exploded = (\ndf_exploded.withColumn(\n\"unexpected_index_list\",\narray(struct(lit(True).alias(\"run_success\"))),\n)\nif df.select(\ncol(\"validation_results.result.unexpected_index_list\")\n).dtypes[0][1]\n== \"array&lt;string&gt;\"\nelse df_exploded.withColumn(\n\"unexpected_index_list\",\ntransform(\ncol(\"validation_results.result.unexpected_index_list\"),\nlambda x: x.withField(\"run_success\", lit(False)),\n),\n)\n)\nif \"observed_value\" in schema:\ndf_exploded = df_exploded.withColumn(\n\"observed_value\", col(\"validation_results.result.observed_value\")\n)\nreturn (\ndf_exploded.withColumn(\"run_time_year\", year(to_timestamp(\"run_time\")))\n.withColumn(\"run_time_month\", month(to_timestamp(\"run_time\")))\n.withColumn(\"run_time_day\", dayofmonth(to_timestamp(\"run_time\")))\n.withColumn(\"checkpoint_config\", to_json(col(\"checkpoint_config\")))\n.withColumn(\"run_results\", to_json(col(\"run_results\")))\n.withColumn(\n\"kwargs\", to_json(col(\"validation_results.expectation_config.kwargs\"))\n)\n.withColumn(\"validation_results\", to_json(col(\"validation_results\")))\n)\n@classmethod\ndef _get_batch_request(\ncls, dq_spec: DQSpec, data: DataFrame\n) -&gt; RuntimeBatchRequest:\n\"\"\"Get run time batch request to be able to query underlying data.\n        Args:\n            dq_spec: data quality process specification.\n            data: input dataframe to run the dq process on.\n        Returns:\n            The RuntimeBatchRequest object configuration.\n        \"\"\"\nreturn RuntimeBatchRequest(\ndatasource_name=f\"{dq_spec.spec_id}-{dq_spec.input_id}-datasource\",\ndata_connector_name=f\"{dq_spec.spec_id}-{dq_spec.input_id}-data_connector\",\ndata_asset_name=(\ndq_spec.data_asset_name\nif dq_spec.data_asset_name\nelse f\"{dq_spec.spec_id}-{dq_spec.input_id}\"\n),\nbatch_identifiers={\n\"spec_id\": dq_spec.spec_id,\n\"input_id\": dq_spec.input_id,\n\"timestamp\": cls._TIMESTAMP,\n},\nruntime_parameters={\"batch_data\": data},\n)\n@classmethod\ndef _get_data_context_config(cls, dq_spec: DQSpec) -&gt; DataContextConfig:\n\"\"\"Get the configuration of the data context.\n        Based on the configuration it is possible to define the backend to be\n        the file system (e.g. local file system) or S3, meaning that the DQ artefacts\n        will be stored according to this configuration.\n        Args:\n            dq_spec: data quality process specification.\n        Returns:\n            The DataContextConfig object configuration.\n        \"\"\"\nstore_backend: Union[FilesystemStoreBackendDefaults, S3StoreBackendDefaults]\ndata_docs_site = None\nif dq_spec.store_backend == DQDefaults.FILE_SYSTEM_STORE.value:\nstore_backend = FilesystemStoreBackendDefaults(\nroot_directory=dq_spec.local_fs_root_dir\n)\ndata_docs_site = cls._get_data_docs_sites(\n\"local_site\", store_backend.data_docs_sites, dq_spec\n)\nelif dq_spec.store_backend == DQDefaults.FILE_SYSTEM_S3_STORE.value:\nstore_backend = S3StoreBackendDefaults(\ndefault_bucket_name=dq_spec.bucket,\nvalidations_store_prefix=dq_spec.validations_store_prefix,\ncheckpoint_store_prefix=dq_spec.checkpoint_store_prefix,\nexpectations_store_prefix=dq_spec.expectations_store_prefix,\ndata_docs_prefix=dq_spec.data_docs_prefix,\ndata_docs_bucket_name=(\ndq_spec.data_docs_bucket\nif dq_spec.data_docs_bucket\nelse dq_spec.bucket\n),\n)\ndata_docs_site = cls._get_data_docs_sites(\n\"s3_site\", store_backend.data_docs_sites, dq_spec\n)\nreturn DataContextConfig(\nstore_backend_defaults=store_backend,\ndata_docs_sites=data_docs_site,\nanonymous_usage_statistics=AnonymizedUsageStatisticsConfig(enabled=False),\n)\n@classmethod\ndef _get_data_docs_sites(\ncls, site_name: str, data_docs_site: dict, dq_spec: DQSpec\n) -&gt; dict:\n\"\"\"Get the custom configuration of the data_docs_sites.\n        Args:\n            site_name: the name to give to the site.\n            data_docs_site: the default configuration for the data_docs_site.\n            dq_spec: data quality specification.\n        Returns:\n            Modified data_docs_site.\n        \"\"\"\ndata_docs_site[site_name][\"show_how_to_buttons\"] = False\nif site_name == \"local_site\":\ndata_docs_site[site_name][\"store_backend\"][\n\"base_directory\"\n] = dq_spec.data_docs_prefix\nif dq_spec.data_docs_local_fs:\n# Enable to write data_docs in a separated path\ndata_docs_site[site_name][\"store_backend\"][\n\"root_directory\"\n] = dq_spec.data_docs_local_fs\nreturn data_docs_site\n@classmethod\ndef _get_data_source_defaults(cls, dq_spec: DQSpec) -&gt; dict:\n\"\"\"Get the configuration for a datasource.\n        Args:\n            dq_spec: data quality specification.\n        Returns:\n            The python dictionary with the datasource configuration.\n        \"\"\"\nreturn {\n\"name\": f\"{dq_spec.spec_id}-{dq_spec.input_id}-datasource\",\n\"class_name\": DQDefaults.DATASOURCE_CLASS_NAME.value,\n\"execution_engine\": {\n\"class_name\": DQDefaults.DATASOURCE_EXECUTION_ENGINE.value,\n\"persist\": False,\n},\n\"data_connectors\": {\nf\"{dq_spec.spec_id}-{dq_spec.input_id}-data_connector\": {\n\"module_name\": DQDefaults.DATA_CONNECTORS_MODULE_NAME.value,\n\"class_name\": DQDefaults.DATA_CONNECTORS_CLASS_NAME.value,\n\"assets\": {\n(\ndq_spec.data_asset_name\nif dq_spec.data_asset_name\nelse f\"{dq_spec.spec_id}-{dq_spec.input_id}\"\n): {\"batch_identifiers\": DQDefaults.DQ_BATCH_IDENTIFIERS.value}\n},\n}\n},\n}\n@classmethod\ndef _get_failed_expectations(\ncls, results: CheckpointResult, dq_spec: DQSpec\n) -&gt; List[Any]:\n\"\"\"Get the failed expectations of a Checkpoint result.\n        Args:\n            results: the results of the DQ process.\n            dq_spec: data quality specification.\n        Returns: a list of failed expectations.\n        \"\"\"\nfailed_expectations = []\nfor validation_result in results.list_validation_results():\nexpectations_results = validation_result[\"results\"]\nfor result in expectations_results:\nif not result[\"success\"]:\nfailed_expectations.append(result[\"expectation_config\"])\nif result[\"exception_info\"][\"raised_exception\"]:\ncls._LOGGER.error(\nf\"\"\"The expectation {str(result[\"expectation_config\"])}\n                            raised the following exception:\n{result[\"exception_info\"][\"exception_message\"]}\"\"\"\n)\ncls._LOGGER.error(\nf\"{len(failed_expectations)} out of {len(expectations_results)} \"\nf\"Data Quality Expectation(s) have failed! Failed Expectations: \"\nf\"{failed_expectations}\"\n)\npercentage_failure = 1 - (\nvalidation_result[\"statistics\"][\"success_percent\"] / 100\n)\nif (\ndq_spec.max_percentage_failure is not None\nand dq_spec.max_percentage_failure &lt; percentage_failure\n):\nraise DQValidationsFailedException(\nf\"Max error threshold is being surpassed! \"\nf\"Expected: {dq_spec.max_percentage_failure} \"\nf\"Got: {percentage_failure}\"\n)\nreturn failed_expectations\n@classmethod\ndef _get_unexpected_rows_pk(cls, dq_spec: DQSpec) -&gt; Optional[List[str]]:\n\"\"\"Get primary key for using on rows failing DQ validations.\n        Args:\n            dq_spec: data quality specification.\n        Returns: the list of columns that are part of the primary key.\n        \"\"\"\nif dq_spec.unexpected_rows_pk:\nreturn dq_spec.unexpected_rows_pk\nelif dq_spec.tbl_to_derive_pk:\nreturn TableManager(\n{\"function\": \"get_tbl_pk\", \"table_or_view\": dq_spec.tbl_to_derive_pk}\n).get_tbl_pk()\nelif dq_spec.tag_source_data:\nraise ValueError(\n\"You need to provide either the argument \"\n\"'unexpected_rows_pk' or 'tbl_to_derive_pk'.\"\n)\nelse:\nreturn None\n@classmethod\ndef _log_or_fail(cls, results: CheckpointResult, dq_spec: DQSpec) -&gt; None:\n\"\"\"Log the execution of the Data Quality process.\n        Args:\n            results: the results of the DQ process.\n            dq_spec: data quality specification.\n        \"\"\"\nif results[\"success\"]:\ncls._LOGGER.info(\n\"The data passed all the expectations defined. Everything looks good!\"\n)\nelse:\nfailed_expectations = cls._get_failed_expectations(results, dq_spec)\nif dq_spec.critical_functions:\ncritical_failure = cls._check_critical_functions_tags(\nfailed_expectations\n)\nif critical_failure:\nraise DQValidationsFailedException(\nf\"Data Quality Validations Failed, the following critical \"\nf\"expectations failed: {critical_failure}.\"\n)\nelif dq_spec.fail_on_error:\nraise DQValidationsFailedException(\"Data Quality Validations Failed!\")\n@classmethod\ndef _transform_checkpoint_results(\ncls, checkpoint_results: dict, dq_spec: DQSpec\n) -&gt; DataFrame:\n\"\"\"Transforms the checkpoint results and creates new entries.\n        All the items of the dictionary are cast to a json like format.\n        The validation_result_identifier is extracted from the run_results column\n        into a separated column. All columns are cast to json like format.\n        After that the dictionary is converted into a dataframe.\n        Args:\n            checkpoint_results: dict with results of the checkpoint run.\n            dq_spec: data quality specification.\n        Returns:\n            Transformed results dataframe.\n        \"\"\"\nresults_json_dict = loads(dumps(checkpoint_results))\nresults_dict = {}\nfor key, value in results_json_dict.items():\nif key == \"run_results\":\ncheckpoint_result_identifier = list(value.keys())[0]\n# check if the grabbed identifier is correct\nif (\nstr(checkpoint_result_identifier)\n.lower()\n.startswith(DQDefaults.VALIDATION_COLUMN_IDENTIFIER.value)\n):\nresults_dict[\"validation_result_identifier\"] = (\ncheckpoint_result_identifier\n)\nresults_dict[\"run_results\"] = value[checkpoint_result_identifier]\nelse:\nraise DQCheckpointsResultsException(\n\"The checkpoint result identifier format is not \"\n\"in accordance to what is expected\"\n)\nelse:\nresults_dict[key] = value\ndf = ExecEnv.SESSION.createDataFrame(\n[json.dumps(results_dict)],\nschema=StringType(),\n)\nschema = schema_of_json(df.select(\"value\").head()[0])\ndf = df.withColumn(\"value\", from_json(\"value\", schema)).select(\"value.*\")\ncols_to_expand = [\"run_id\"]\ndf = (\ndf.select(\n[\ncol(c) if c not in cols_to_expand else col(f\"{c}.*\")\nfor c in df.columns\n]\n)\n.drop(*cols_to_expand)\n.withColumn(\"spec_id\", lit(dq_spec.spec_id))\n.withColumn(\"input_id\", lit(dq_spec.input_id))\n)\nreturn (\ncls._explode_results(df, dq_spec)\nif dq_spec.result_sink_explode\nelse df.withColumn(\n\"checkpoint_config\", to_json(col(\"checkpoint_config\"))\n).withColumn(\"run_results\", to_json(col(\"run_results\")))\n)\n@classmethod\ndef _write_to_result_sink(\ncls,\ndq_spec: DQSpec,\ndf: DataFrame,\ndata: OrderedDict = None,\n) -&gt; None:\n\"\"\"Write dq results dataframe to a table or location.\n        It can be written:\n        - a raw output (having result_sink_explode set as False)\n        - an exploded output (having result_sink_explode set as True), which\n        is more prepared for analysis, with some columns exploded, flatten and\n        transformed. It can also be set result_sink_extra_columns with other\n        columns desired to have in the output table or location.\n        Args:\n            dq_spec: data quality specification.\n            df: dataframe with dq results to write.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nif dq_spec.result_sink_db_table or dq_spec.result_sink_location:\noptions = {\"mergeSchema\": \"true\"} if dq_spec.result_sink_explode else {}\nWriterFactory.get_writer(\nspec=OutputSpec(\nspec_id=\"dq_result_sink\",\ninput_id=\"dq_result\",\ndb_table=dq_spec.result_sink_db_table,\nlocation=dq_spec.result_sink_location,\npartitions=(\ndq_spec.result_sink_partitions\nif dq_spec.result_sink_partitions\nelse []\n),\nwrite_type=WriteType.APPEND.value,\ndata_format=dq_spec.result_sink_format,\noptions=(\noptions\nif dq_spec.result_sink_options is None\nelse {**dq_spec.result_sink_options, **options}\n),\n),\ndf=df,\ndata=data,\n).write()\n</code></pre>"},{"location":"reference/packages/dq_processors/dq_factory.html#packages.dq_processors.dq_factory.DQFactory.build_data_docs","title":"<code>build_data_docs(store_backend=DQDefaults.STORE_BACKEND.value, local_fs_root_dir=None, data_docs_local_fs=None, data_docs_prefix=DQDefaults.DATA_DOCS_PREFIX.value, bucket=None, data_docs_bucket=None, expectations_store_prefix=DQDefaults.EXPECTATIONS_STORE_PREFIX.value, validations_store_prefix=DQDefaults.VALIDATIONS_STORE_PREFIX.value, checkpoint_store_prefix=DQDefaults.CHECKPOINT_STORE_PREFIX.value)</code>  <code>classmethod</code>","text":"<p>Build Data Docs for the project.</p> <p>This function does a full build of data docs based on all the great expectations checkpoints in the specified location, getting all history of run/validations executed and results.</p> <p>Parameters:</p> Name Type Description Default <code>store_backend</code> <code>str</code> <p>which store_backend to use (e.g. s3 or file_system).</p> <code>DQDefaults.STORE_BACKEND.value</code> <code>local_fs_root_dir</code> <code>str</code> <p>path of the root directory. Note: only applicable for store_backend file_system</p> <code>None</code> <code>data_docs_local_fs</code> <code>str</code> <p>path of the root directory. Note: only applicable for store_backend file_system.</p> <code>None</code> <code>data_docs_prefix</code> <code>str</code> <p>prefix where to store data_docs' data.</p> <code>DQDefaults.DATA_DOCS_PREFIX.value</code> <code>bucket</code> <code>str</code> <p>the bucket name to consider for the store_backend (store DQ artefacts). Note: only applicable for store_backend s3.</p> <code>None</code> <code>data_docs_bucket</code> <code>str</code> <p>the bucket name for data docs only. When defined, it will supersede bucket parameter. Note: only applicable for store_backend s3.</p> <code>None</code> <code>expectations_store_prefix</code> <code>str</code> <p>prefix where to store expectations' data. Note: only applicable for store_backend s3.</p> <code>DQDefaults.EXPECTATIONS_STORE_PREFIX.value</code> <code>validations_store_prefix</code> <code>str</code> <p>prefix where to store validations' data. Note: only applicable for store_backend s3.</p> <code>DQDefaults.VALIDATIONS_STORE_PREFIX.value</code> <code>checkpoint_store_prefix</code> <code>str</code> <p>prefix where to store checkpoints' data. Note: only applicable for store_backend s3.</p> <code>DQDefaults.CHECKPOINT_STORE_PREFIX.value</code> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/dq_factory.py</code> <pre><code>@classmethod\ndef build_data_docs(\ncls,\nstore_backend: str = DQDefaults.STORE_BACKEND.value,\nlocal_fs_root_dir: str = None,\ndata_docs_local_fs: str = None,\ndata_docs_prefix: str = DQDefaults.DATA_DOCS_PREFIX.value,\nbucket: str = None,\ndata_docs_bucket: str = None,\nexpectations_store_prefix: str = DQDefaults.EXPECTATIONS_STORE_PREFIX.value,\nvalidations_store_prefix: str = DQDefaults.VALIDATIONS_STORE_PREFIX.value,\ncheckpoint_store_prefix: str = DQDefaults.CHECKPOINT_STORE_PREFIX.value,\n) -&gt; None:\n\"\"\"Build Data Docs for the project.\n    This function does a full build of data docs based on all the great expectations\n    checkpoints in the specified location, getting all history of run/validations\n    executed and results.\n    Args:\n        store_backend: which store_backend to use (e.g. s3 or file_system).\n        local_fs_root_dir: path of the root directory. Note: only applicable\n            for store_backend file_system\n        data_docs_local_fs: path of the root directory. Note: only applicable\n            for store_backend file_system.\n        data_docs_prefix: prefix where to store data_docs' data.\n        bucket: the bucket name to consider for the store_backend\n            (store DQ artefacts). Note: only applicable for store_backend s3.\n        data_docs_bucket: the bucket name for data docs only. When defined,\n            it will supersede bucket parameter.\n            Note: only applicable for store_backend s3.\n        expectations_store_prefix: prefix where to store expectations' data.\n            Note: only applicable for store_backend s3.\n        validations_store_prefix: prefix where to store validations' data.\n            Note: only applicable for store_backend s3.\n        checkpoint_store_prefix: prefix where to store checkpoints' data.\n            Note: only applicable for store_backend s3.\n    \"\"\"\nif store_backend == DQDefaults.STORE_BACKEND.value:\ndq_spec = DQSpec(\nspec_id=\"dq_validator\",\ninput_id=\"dq\",\ndq_type=DQType.VALIDATOR.value,\nstore_backend=DQDefaults.STORE_BACKEND.value,\ndata_docs_prefix=data_docs_prefix,\nbucket=bucket,\ndata_docs_bucket=data_docs_bucket,\nexpectations_store_prefix=expectations_store_prefix,\nvalidations_store_prefix=validations_store_prefix,\ncheckpoint_store_prefix=checkpoint_store_prefix,\n)\nelif store_backend == DQDefaults.FILE_SYSTEM_STORE.value:\ndq_spec = DQSpec(\nspec_id=\"dq_validator\",\ninput_id=\"dq\",\ndq_type=DQType.VALIDATOR.value,\nstore_backend=DQDefaults.FILE_SYSTEM_STORE.value,\nlocal_fs_root_dir=local_fs_root_dir,\ndata_docs_local_fs=data_docs_local_fs,\ndata_docs_prefix=data_docs_prefix,\n)\ncontext = get_context(project_config=cls._get_data_context_config(dq_spec))\ncls._LOGGER.info(\"The data docs were rebuilt\")\ncontext.build_data_docs()\n</code></pre>"},{"location":"reference/packages/dq_processors/dq_factory.html#packages.dq_processors.dq_factory.DQFactory.run_dq_process","title":"<code>run_dq_process(dq_spec, data)</code>  <code>classmethod</code>","text":"<p>Run the specified data quality process on a dataframe.</p> <p>Based on the dq_specs we apply the defined expectations on top of the dataframe in order to apply the necessary validations and then output the result of the data quality process.</p> <p>Parameters:</p> Name Type Description Default <code>dq_spec</code> <code>DQSpec</code> <p>data quality specification.</p> required <code>data</code> <code>DataFrame</code> <p>input dataframe to run the dq process on.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame containing the results of the DQ process.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/dq_factory.py</code> <pre><code>@classmethod\ndef run_dq_process(cls, dq_spec: DQSpec, data: DataFrame) -&gt; DataFrame:\n\"\"\"Run the specified data quality process on a dataframe.\n    Based on the dq_specs we apply the defined expectations on top of the dataframe\n    in order to apply the necessary validations and then output the result of\n    the data quality process.\n    Args:\n        dq_spec: data quality specification.\n        data: input dataframe to run the dq process on.\n    Returns:\n        The DataFrame containing the results of the DQ process.\n    \"\"\"\n# import custom expectations for them to be available to be used.\nfor expectation in DQDefaults.CUSTOM_EXPECTATION_LIST.value:\nimportlib.__import__(\n\"lakehouse_engine.dq_processors.custom_expectations.\" + expectation\n)\ncontext = get_context(project_config=cls._get_data_context_config(dq_spec))\ncontext.add_datasource(**cls._get_data_source_defaults(dq_spec))\nexpectation_suite_name = (\ndq_spec.expectation_suite_name\nif dq_spec.expectation_suite_name\nelse f\"{dq_spec.spec_id}-{dq_spec.input_id}-{dq_spec.dq_type}\"\n)\ncontext.add_or_update_expectation_suite(\nexpectation_suite_name=expectation_suite_name\n)\nbatch_request = cls._get_batch_request(dq_spec, data)\nif (\ndq_spec.dq_type == DQType.VALIDATOR.value\nor dq_spec.dq_type == DQType.PRISMA.value\n):\nValidator.get_dq_validator(\ncontext,\nbatch_request,\nexpectation_suite_name,\ndq_spec.dq_functions,\ndq_spec.critical_functions,\n)\nsource_pk = cls._get_unexpected_rows_pk(dq_spec)\nresults, results_df = cls._configure_and_run_checkpoint(\ndq_spec, context, batch_request, expectation_suite_name, source_pk\n)\nif dq_spec.dq_type == DQType.PRISMA.value:\nresults_df = results_df.withColumn(\"source_primary_key\", lit(source_pk))\nprocessed_keys_df = data.select(\nconcat_ws(\n\", \", *[coalesce(col(c), lit(\"null\")) for c in source_pk]\n).alias(\"combined_pk\")\n)\ncomb_pk_expr = (\nsort_array(collect_list(\"combined_pk\"))\nif dq_spec.sort_processed_keys\nelse collect_list(\"combined_pk\")\n)\nprocessed_keys_df = processed_keys_df.agg(\nconcat_ws(\"||\", comb_pk_expr).alias(\"processed_keys\")\n)\nresults_df = results_df.join(processed_keys_df, lit(1) == lit(1))\ncls._write_to_result_sink(dq_spec, results_df)\ncls._log_or_fail(results, dq_spec)\nif (\ndq_spec.tag_source_data\nand dq_spec.result_sink_explode\nand dq_spec.fail_on_error is not True\n):\ndata = Validator.tag_source_with_dq(source_pk, data, results_df)\nelse:\nraise TypeError(\nf\"Type of Data Quality '{dq_spec.dq_type}' is not supported.\"\n)\nreturn data\n</code></pre>"},{"location":"reference/packages/dq_processors/exceptions.html","title":"Exceptions","text":"<p>Package defining all the DQ custom exceptions.</p>"},{"location":"reference/packages/dq_processors/exceptions.html#packages.dq_processors.exceptions.DQCheckpointsResultsException","title":"<code>DQCheckpointsResultsException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the checkpoint results parsing fail.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/exceptions.py</code> <pre><code>class DQCheckpointsResultsException(Exception):\n\"\"\"Exception for when the checkpoint results parsing fail.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/dq_processors/exceptions.html#packages.dq_processors.exceptions.DQDuplicateRuleIdException","title":"<code>DQDuplicateRuleIdException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a duplicated rule id is found.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/exceptions.py</code> <pre><code>class DQDuplicateRuleIdException(Exception):\n\"\"\"Exception for when a duplicated rule id is found.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/dq_processors/exceptions.html#packages.dq_processors.exceptions.DQSpecMalformedException","title":"<code>DQSpecMalformedException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the DQSpec is malformed.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/exceptions.py</code> <pre><code>class DQSpecMalformedException(Exception):\n\"\"\"Exception for when the DQSpec is malformed.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/dq_processors/exceptions.html#packages.dq_processors.exceptions.DQValidationsFailedException","title":"<code>DQValidationsFailedException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the data quality validations fail.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/exceptions.py</code> <pre><code>class DQValidationsFailedException(Exception):\n\"\"\"Exception for when the data quality validations fail.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/dq_processors/validator.html","title":"Validator","text":"<p>Module containing the definition of a data quality validator.</p>"},{"location":"reference/packages/dq_processors/validator.html#packages.dq_processors.validator.Validator","title":"<code>Validator</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing the data quality validator.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/validator.py</code> <pre><code>class Validator(object):\n\"\"\"Class containing the data quality validator.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef get_dq_validator(\ncls,\ncontext: BaseDataContext,\nbatch_request: RuntimeBatchRequest,\nexpectation_suite_name: str,\ndq_functions: List[DQFunctionSpec],\ncritical_functions: List[DQFunctionSpec],\n) -&gt; Any:\n\"\"\"Get a validator according to the specification.\n        We use getattr to dynamically execute any expectation available.\n        getattr(validator, function) is similar to validator.function(). With this\n        approach, we can execute any expectation supported.\n        Args:\n            context: the BaseDataContext containing the configurations for the data\n                source and store backend.\n            batch_request: run time batch request to be able to query underlying data.\n            expectation_suite_name: name of the expectation suite.\n            dq_functions: a list of DQFunctionSpec to consider in the expectation suite.\n            critical_functions: list of critical expectations in the expectation suite.\n        Returns:\n            The validator with the expectation suite stored.\n        \"\"\"\nvalidator = context.get_validator(\nbatch_request=batch_request, expectation_suite_name=expectation_suite_name\n)\nif dq_functions:\nfor dq_function in dq_functions:\ngetattr(validator, dq_function.function)(\n**dq_function.args if dq_function.args else {}\n)\nif critical_functions:\nfor critical_function in critical_functions:\nmeta_args = cls._add_critical_function_tag(critical_function.args)\ngetattr(validator, critical_function.function)(**meta_args)\nreturn validator.save_expectation_suite(discard_failed_expectations=False)\n@classmethod\ndef tag_source_with_dq(\ncls, source_pk: List[str], source_df: DataFrame, results_df: DataFrame\n) -&gt; DataFrame:\n\"\"\"Tags the source dataframe with a new column having the DQ results.\n        Args:\n            source_pk: the primary key of the source data.\n            source_df: the source dataframe to be tagged with DQ results.\n            results_df: dq results dataframe.\n        Returns: a dataframe tagged with the DQ results.\n        \"\"\"\nrun_success = True\nrun_name = results_df.select(\"run_name\").first()[0]\nraised_exceptions = (\nTrue\nif results_df.filter(\"exception_info.raised_exception == True\").count() &gt; 0\nelse False\n)\nfailures_df = (\nresults_df.filter(\n\"expectation_success == False and size(unexpected_index_list) &gt; 0\"\n)\nif \"unexpected_index_list\" in results_df.schema.simpleString()\nelse results_df.filter(\"expectation_success == False\")\n)\nif failures_df.isEmpty() is not True:\nrun_success = False\nsource_df = cls._get_row_tagged_fail_df(\nfailures_df, raised_exceptions, source_df, source_pk\n)\nreturn cls._join_complementary_data(\nrun_name, run_success, raised_exceptions, source_df\n)\n@classmethod\ndef _add_critical_function_tag(cls, args: dict) -&gt; dict:\nif \"meta\" in args.keys():\nmeta = args[\"meta\"]\nif isinstance(meta[\"notes\"], str):\nmeta[\"notes\"] = meta[\"notes\"] + \" **Critical function**.\"\nelse:\nmeta[\"notes\"][\"content\"] = (\nmeta[\"notes\"][\"content\"] + \" **Critical function**.\"\n)\nargs[\"meta\"] = meta\nreturn args\nelse:\nargs[\"meta\"] = {\n\"notes\": {\n\"format\": \"markdown\",\n\"content\": \"**Critical function**.\",\n}\n}\nreturn args\n@staticmethod\ndef _get_row_tagged_fail_df(\nfailures_df: DataFrame,\nraised_exceptions: bool,\nsource_df: DataFrame,\nsource_pk: List[str],\n) -&gt; DataFrame:\n\"\"\"Get the source_df DataFrame tagged with the row level failures.\n        Args:\n            failures_df: dataframe having all failed expectations from the DQ execution.\n            raised_exceptions: whether there was at least one expectation raising\n                exceptions (True) or not (False).\n            source_df: the source dataframe being tagged with DQ results.\n            source_pk: the primary key of the source data.\n        Returns: the source_df tagged with the row level failures.\n        \"\"\"\nif \"unexpected_index_list\" in failures_df.schema.simpleString():\nrow_failures_df = (\nfailures_df.alias(\"a\")\n.withColumn(\"exploded_list\", explode(col(\"unexpected_index_list\")))\n.selectExpr(\"a.*\", \"exploded_list.*\")\n.groupBy(*source_pk)\n.agg(\nstruct(\nfirst(col(\"run_name\")).alias(\"run_name\"),\nfirst(col(\"success\")).alias(\"run_success\"),\nlit(raised_exceptions).alias(\"raised_exceptions\"),\nfirst(col(\"expectation_success\")).alias(\"run_row_success\"),\ncollect_set(\nstruct(\ncol(\"expectation_type\"),\ncol(\"kwargs\"),\n)\n).alias(\"dq_failure_details\"),\n).alias(\"dq_validations\")\n)\n)\nif all(item in row_failures_df.columns for item in source_pk):\njoin_cond = [\ncol(f\"a.{key}\").eqNullSafe(col(f\"b.{key}\")) for key in source_pk\n]\nsource_df = (\nsource_df.alias(\"a\")\n.join(row_failures_df.alias(\"b\"), join_cond, \"left\")\n.select(\"a.*\", \"b.dq_validations\")\n)\nreturn source_df\n@staticmethod\ndef _join_complementary_data(\nrun_name: str, run_success: bool, raised_exceptions: bool, source_df: DataFrame\n) -&gt; DataFrame:\n\"\"\"Join the source_df DataFrame with complementary data.\n        The source_df was already tagged/joined with the row level DQ failures, in case\n        there were any. However, there might be cases for which we don't have any\n        failure (everything succeeded) or cases for which only not row level failures\n        happened (e.g. table level expectations or column level aggregations), and, for\n        those we need to join the source_df with complementary data.\n        Args:\n            run_name: the name of the DQ execution in great expectations.\n            run_success: whether the general execution of the DQ was succeeded (True)\n                or not (False).\n            raised_exceptions: whether there was at least one expectation raising\n                exceptions (True) or not (False).\n            source_df: the source dataframe being tagged with DQ results.\n        Returns: the source_df tagged with complementary data.\n        \"\"\"\ncomplementary_data = [\n{\n\"dq_validations\": {\n\"run_name\": run_name,\n\"run_success\": run_success,\n\"raised_exceptions\": raised_exceptions,\n\"run_row_success\": True,\n}\n}\n]\ncomplementary_df = ExecEnv.SESSION.createDataFrame(\ncomplementary_data, schema=DQDefaults.DQ_VALIDATIONS_SCHEMA.value\n)\nreturn (\nsource_df.crossJoin(\ncomplementary_df.withColumnRenamed(\n\"dq_validations\", \"tmp_dq_validations\"\n)\n)\n.withColumn(\n\"dq_validations\",\n(\nwhen(\ncol(\"dq_validations\").isNotNull(), col(\"dq_validations\")\n).otherwise(col(\"tmp_dq_validations\"))\nif \"dq_validations\" in source_df.columns\nelse col(\"tmp_dq_validations\")\n),\n)\n.drop(\"tmp_dq_validations\")\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/validator.html#packages.dq_processors.validator.Validator.get_dq_validator","title":"<code>get_dq_validator(context, batch_request, expectation_suite_name, dq_functions, critical_functions)</code>  <code>classmethod</code>","text":"<p>Get a validator according to the specification.</p> <p>We use getattr to dynamically execute any expectation available. getattr(validator, function) is similar to validator.function(). With this approach, we can execute any expectation supported.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>BaseDataContext</code> <p>the BaseDataContext containing the configurations for the data source and store backend.</p> required <code>batch_request</code> <code>RuntimeBatchRequest</code> <p>run time batch request to be able to query underlying data.</p> required <code>expectation_suite_name</code> <code>str</code> <p>name of the expectation suite.</p> required <code>dq_functions</code> <code>List[DQFunctionSpec]</code> <p>a list of DQFunctionSpec to consider in the expectation suite.</p> required <code>critical_functions</code> <code>List[DQFunctionSpec]</code> <p>list of critical expectations in the expectation suite.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The validator with the expectation suite stored.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/validator.py</code> <pre><code>@classmethod\ndef get_dq_validator(\ncls,\ncontext: BaseDataContext,\nbatch_request: RuntimeBatchRequest,\nexpectation_suite_name: str,\ndq_functions: List[DQFunctionSpec],\ncritical_functions: List[DQFunctionSpec],\n) -&gt; Any:\n\"\"\"Get a validator according to the specification.\n    We use getattr to dynamically execute any expectation available.\n    getattr(validator, function) is similar to validator.function(). With this\n    approach, we can execute any expectation supported.\n    Args:\n        context: the BaseDataContext containing the configurations for the data\n            source and store backend.\n        batch_request: run time batch request to be able to query underlying data.\n        expectation_suite_name: name of the expectation suite.\n        dq_functions: a list of DQFunctionSpec to consider in the expectation suite.\n        critical_functions: list of critical expectations in the expectation suite.\n    Returns:\n        The validator with the expectation suite stored.\n    \"\"\"\nvalidator = context.get_validator(\nbatch_request=batch_request, expectation_suite_name=expectation_suite_name\n)\nif dq_functions:\nfor dq_function in dq_functions:\ngetattr(validator, dq_function.function)(\n**dq_function.args if dq_function.args else {}\n)\nif critical_functions:\nfor critical_function in critical_functions:\nmeta_args = cls._add_critical_function_tag(critical_function.args)\ngetattr(validator, critical_function.function)(**meta_args)\nreturn validator.save_expectation_suite(discard_failed_expectations=False)\n</code></pre>"},{"location":"reference/packages/dq_processors/validator.html#packages.dq_processors.validator.Validator.tag_source_with_dq","title":"<code>tag_source_with_dq(source_pk, source_df, results_df)</code>  <code>classmethod</code>","text":"<p>Tags the source dataframe with a new column having the DQ results.</p> <p>Parameters:</p> Name Type Description Default <code>source_pk</code> <code>List[str]</code> <p>the primary key of the source data.</p> required <code>source_df</code> <code>DataFrame</code> <p>the source dataframe to be tagged with DQ results.</p> required <code>results_df</code> <code>DataFrame</code> <p>dq results dataframe.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/validator.py</code> <pre><code>@classmethod\ndef tag_source_with_dq(\ncls, source_pk: List[str], source_df: DataFrame, results_df: DataFrame\n) -&gt; DataFrame:\n\"\"\"Tags the source dataframe with a new column having the DQ results.\n    Args:\n        source_pk: the primary key of the source data.\n        source_df: the source dataframe to be tagged with DQ results.\n        results_df: dq results dataframe.\n    Returns: a dataframe tagged with the DQ results.\n    \"\"\"\nrun_success = True\nrun_name = results_df.select(\"run_name\").first()[0]\nraised_exceptions = (\nTrue\nif results_df.filter(\"exception_info.raised_exception == True\").count() &gt; 0\nelse False\n)\nfailures_df = (\nresults_df.filter(\n\"expectation_success == False and size(unexpected_index_list) &gt; 0\"\n)\nif \"unexpected_index_list\" in results_df.schema.simpleString()\nelse results_df.filter(\"expectation_success == False\")\n)\nif failures_df.isEmpty() is not True:\nrun_success = False\nsource_df = cls._get_row_tagged_fail_df(\nfailures_df, raised_exceptions, source_df, source_pk\n)\nreturn cls._join_complementary_data(\nrun_name, run_success, raised_exceptions, source_df\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/index.html","title":"Custom expectations","text":"<p>Package containing custom DQ expectations available in the lakehouse engine.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_not_equal_to_b.html","title":"Expect column pair a to be not equal to b","text":"<p>Expectation to check if column 'a' is not equal to column 'b'.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_not_equal_to_b.html#packages.dq_processors.custom_expectations.expect_column_pair_a_to_be_not_equal_to_b.ColumnPairCustom","title":"<code>ColumnPairCustom</code>","text":"<p>         Bases: <code>ColumnPairMapMetricProvider</code></p> <p>Asserts that column 'A' is not equal to column 'B'.</p> <p>Additionally, It compares Null as well.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_not_equal_to_b.py</code> <pre><code>class ColumnPairCustom(ColumnPairMapMetricProvider):\n\"\"\"Asserts that column 'A' is not equal to column 'B'.\n    Additionally, It compares Null as well.\n    \"\"\"\ncondition_metric_name = \"column_pair_values.a_not_equal_to_b\"\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n)\ncondition_value_keys = ()\n@column_pair_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnPairMapMetricProvider,\ncolumn_A: Any,\ncolumn_B: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Args:\n            column_A: Value of the row of column_A.\n            column_B: Value of the row of column_B.\n            kwargs: dict with additional parameters.\n        Returns:\n            If the condition is met.\n        \"\"\"\nreturn ((column_A.isNotNull()) | (column_B.isNotNull())) &amp; (\ncolumn_A != column_B\n)  # noqa: E501\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_not_equal_to_b.html#packages.dq_processors.custom_expectations.expect_column_pair_a_to_be_not_equal_to_b.ExpectColumnPairAToBeNotEqualToB","title":"<code>ExpectColumnPairAToBeNotEqualToB</code>","text":"<p>         Bases: <code>ColumnPairMapExpectation</code></p> <p>Expect values in column A to be not equal to column B.</p> <p>Parameters:</p> Name Type Description Default <code>column_A</code> <p>The first column name.</p> required <code>column_B</code> <p>The second column name.</p> required <p>Other Parameters:</p> Name Type Description <code>allow_cross_type_comparisons</code> <p>If True, allow comparisons between types (e.g. integer and string). Otherwise, attempting such comparisons will raise an exception.</p> <code>ignore_row_if</code> <p>\"both_values_are_missing\", \"either_value_is_missing\", \"neither\" (default).</p> <code>result_format</code> <p>Which output mode to use: <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</p> <code>include_config</code> <p>If True (default), then include the expectation config as part of the result object.</p> <code>catch_exceptions</code> <p>If True, then catch exceptions and include them as part of the result object. Default: False.</p> <code>meta</code> <p>A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification.</p> <p>Returns:</p> Type Description <p>An ExpectationSuiteValidationResult.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_not_equal_to_b.py</code> <pre><code>class ExpectColumnPairAToBeNotEqualToB(ColumnPairMapExpectation):\n\"\"\"Expect values in column A to be not equal to column B.\n    Args:\n        column_A: The first column name.\n        column_B: The second column name.\n    Keyword Args:\n        allow_cross_type_comparisons: If True, allow\n            comparisons between types (e.g. integer and string).\n            Otherwise, attempting such comparisons will raise an exception.\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the expectation config\n            as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [\"IE4019\", \"IM6092\", \"IE1405\"],\n\"b\": [\"IE4019\", \"IM6092\", \"IE1405\"],\n\"c\": [\"IE1404\", \"IN6192\", \"842075\"],\n},\n\"schemas\": {\n\"spark\": {\n\"a\": \"StringType\",\n\"b\": \"StringType\",\n\"c\": \"StringType\",\n}\n},\n}\n],\n\"tests\": [\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"b\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"b\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\"b\": \"IE4019\", \"a\": \"IE4019\"},\n{\"b\": \"IM6092\", \"a\": \"IM6092\"},\n{\"b\": \"IE1405\", \"a\": \"IE1405\"},\n],\n},\n},\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"c\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\n\"success\": True,\n\"unexpected_index_list\": [],\n},\n},\n],\n},\n]\nmap_metric = \"column_pair_values.a_not_equal_to_b\"\nsuccess_keys = (\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n\"mostly\",\n)\ndefault_kwarg_values = {\n\"mostly\": 1.0,\n\"ignore_row_if\": \"neither\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nColumnPairMapExpectation,\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_smaller_or_equal_than_b.html","title":"Expect column pair a to be smaller or equal than b","text":"<p>Expectation to check if column 'a' is lower or equal than column 'b'.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_smaller_or_equal_than_b.html#packages.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b.ColumnPairCustom","title":"<code>ColumnPairCustom</code>","text":"<p>         Bases: <code>ColumnPairMapMetricProvider</code></p> <p>Asserts that column 'A' is lower or equal than column 'B'.</p> <p>Additionally, the 'margin' parameter can be used to add a margin to the check between column 'A' and 'B': 'A' &lt;= 'B' + 'margin'.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_smaller_or_equal_than_b.py</code> <pre><code>class ColumnPairCustom(ColumnPairMapMetricProvider):\n\"\"\"Asserts that column 'A' is lower or equal than column 'B'.\n    Additionally, the 'margin' parameter can be used to add a margin to the\n    check between column 'A' and 'B': 'A' &lt;= 'B' + 'margin'.\n    \"\"\"\ncondition_metric_name = \"column_pair_values.a_smaller_or_equal_than_b\"\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n)\ncondition_value_keys = (\"margin\",)\n@column_pair_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnPairMapMetricProvider,\ncolumn_A: Any,\ncolumn_B: Any,\nmargin: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Args:\n            column_A: Value of the row of column_A.\n            column_B: Value of the row of column_B.\n            margin: margin value to be added to column_b.\n            kwargs: dict with additional parameters.\n        Returns:\n            If the condition is met.\n        \"\"\"\nif margin is None:\napprox = 0\nelif not isinstance(margin, (int, float, complex)):\nraise TypeError(\nf\"margin must be one of int, float, complex.\"\nf\" Found: {margin} as {type(margin)}\"\n)\nelse:\napprox = margin  # type: ignore\nreturn column_A &lt;= column_B + approx  # type: ignore\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_smaller_or_equal_than_b.html#packages.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b.ExpectColumnPairAToBeSmallerOrEqualThanB","title":"<code>ExpectColumnPairAToBeSmallerOrEqualThanB</code>","text":"<p>         Bases: <code>ColumnPairMapExpectation</code></p> <p>Expect values in column A to be lower or equal than column B.</p> <p>Parameters:</p> Name Type Description Default <code>column_A</code> <p>The first column name.</p> required <code>column_B</code> <p>The second column name.</p> required <code>margin</code> <p>additional approximation to column B value.</p> required <p>Other Parameters:</p> Name Type Description <code>allow_cross_type_comparisons</code> <p>If True, allow comparisons between types (e.g. integer and string). Otherwise, attempting such comparisons will raise an exception.</p> <code>ignore_row_if</code> <p>\"both_values_are_missing\", \"either_value_is_missing\", \"neither\" (default).</p> <code>result_format</code> <p>Which output mode to use: <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</p> <code>include_config</code> <p>If True (default), then include the expectation config as part of the result object.</p> <code>catch_exceptions</code> <p>If True, then catch exceptions and include them as part of the result object. Default: False.</p> <code>meta</code> <p>A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification.</p> <p>Returns:</p> Type Description <p>An ExpectationSuiteValidationResult.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_pair_a_to_be_smaller_or_equal_than_b.py</code> <pre><code>class ExpectColumnPairAToBeSmallerOrEqualThanB(ColumnPairMapExpectation):\n\"\"\"Expect values in column A to be lower or equal than column B.\n    Args:\n        column_A: The first column name.\n        column_B: The second column name.\n        margin: additional approximation to column B value.\n    Keyword Args:\n        allow_cross_type_comparisons: If True, allow\n            comparisons between types (e.g. integer and string).\n            Otherwise, attempting such comparisons will raise an exception.\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the expectation config\n            as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [11, 22, 50],\n\"b\": [10, 21, 100],\n\"c\": [9, 21, 30],\n},\n\"schemas\": {\n\"spark\": {\n\"a\": \"IntegerType\",\n\"b\": \"IntegerType\",\n\"c\": \"IntegerType\",\n}\n},\n}\n],\n\"tests\": [\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"c\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"c\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\"c\": 9, \"a\": 11},\n{\"c\": 21, \"a\": 22},\n{\"c\": 30, \"a\": 50},\n],\n},\n},\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"b\",\n\"margin\": 1,\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\n\"success\": True,\n\"unexpected_index_list\": [],\n},\n},\n],\n},\n]\nmap_metric = \"column_pair_values.a_smaller_or_equal_than_b\"\nsuccess_keys = (\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n\"margin\",\n\"mostly\",\n)\ndefault_kwarg_values = {\n\"mostly\": 1.0,\n\"ignore_row_if\": \"neither\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nColumnPairMapExpectation,\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.html","title":"Expect column pair date a to be greater than or equal to date b","text":"<p>Expectation to check if date column 'a' is greater or equal to date column 'b'.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.html#packages.dq_processors.custom_expectations.expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.ColumnPairDateAToBeGreaterOrEqualToDateB","title":"<code>ColumnPairDateAToBeGreaterOrEqualToDateB</code>","text":"<p>         Bases: <code>ColumnPairMapMetricProvider</code></p> <p>Asserts that date column 'A' is greater or equal to date column 'B'.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.py</code> <pre><code>class ColumnPairDateAToBeGreaterOrEqualToDateB(ColumnPairMapMetricProvider):\n\"\"\"Asserts that date column 'A' is greater or equal to date column 'B'.\"\"\"\n# This is the id string that will be used to refer your metric.\ncondition_metric_name = \"column_pair_values.date_a_greater_or_equal_to_date_b\"\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n)\n@column_pair_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnPairMapMetricProvider,\ncolumn_A: Any,\ncolumn_B: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Args:\n            column_A: Value of the row of column_A.\n            column_B: Value of the row of column_B.\n            kwargs: dict with additional parameters.\n        Returns:\n            Boolean on the basis of condition.\n        \"\"\"\nreturn (\n(column_A.isNotNull()) &amp; (column_B.isNotNull()) &amp; (column_A &gt;= column_B)\n)  # type: ignore\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.html#packages.dq_processors.custom_expectations.expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.ExpectColumnPairDateAToBeGreaterThanOrEqualToDateB","title":"<code>ExpectColumnPairDateAToBeGreaterThanOrEqualToDateB</code>","text":"<p>         Bases: <code>ColumnPairMapExpectation</code></p> <p>Expect values in date column A to be greater than or equal to date column B.</p> <p>Parameters:</p> Name Type Description Default <code>column_A</code> <p>The first date column name.</p> required <code>column_B</code> <p>The second date column name.</p> required <p>Other Parameters:</p> Name Type Description <code>ignore_row_if</code> <p>\"both_values_are_missing\", \"either_value_is_missing\", \"neither\" (default).</p> <code>result_format</code> <p>Which output mode to use: <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</p> <code>include_config</code> <p>If True (default), then include the expectation config as part of the result object.</p> <code>catch_exceptions</code> <p>If True, then catch exceptions and include them as part of the result object. Default: False.</p> <code>meta</code> <p>A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification.</p> <p>Returns:</p> Type Description <p>An ExpectationSuiteValidationResult.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_pair_date_a_to_be_greater_than_or_equal_to_date_b.py</code> <pre><code>class ExpectColumnPairDateAToBeGreaterThanOrEqualToDateB(ColumnPairMapExpectation):\n\"\"\"Expect values in date column A to be greater than or equal to date column B.\n    Args:\n        column_A: The first date column name.\n        column_B: The second date column name.\n    Keyword Args:\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the\n            expectation config as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [\n\"2029-01-12\",\n\"2024-11-21\",\n\"2022-01-01\",\n],\n\"b\": [\n\"2019-02-11\",\n\"2014-12-22\",\n\"2012-09-09\",\n],\n\"c\": [\n\"2010-02-11\",\n\"2015-12-22\",\n\"2022-09-09\",\n],\n},\n\"schemas\": {\n\"spark\": {\n\"a\": \"DateType\",\n\"b\": \"DateType\",\n\"c\": \"DateType\",\n}\n},\n}\n],\n\"tests\": [\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"a\",\n\"column_B\": \"b\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\", \"b\"],\n},\n},\n\"out\": {\"success\": True, \"unexpected_index_list\": []},\n},\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_A\": \"b\",\n\"column_B\": \"c\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\n\"a\": datetime.date(2024, 11, 21),\n\"b\": datetime.date(2014, 12, 22),\n\"c\": datetime.date(2015, 12, 22),\n},\n{\n\"a\": datetime.date(2022, 1, 1),\n\"b\": datetime.date(2012, 9, 9),\n\"c\": datetime.date(2022, 9, 9),\n},\n],\n},\n},\n],\n}\n]\nmap_metric = \"column_pair_values.date_a_greater_or_equal_to_date_b\"\nsuccess_keys = (\n\"column_A\",\n\"column_B\",\n\"ignore_row_if\",\n\"mostly\",\n)\ndefault_kwarg_values = {\n\"mostly\": 1.0,\n\"ignore_row_if\": \"neither\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": True,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nColumnPairMapExpectation,\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_values_to_be_date_not_older_than.html","title":"Expect column values to be date not older than","text":"<p>Expectation to check if column value is a date within a timeframe.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_values_to_be_date_not_older_than.html#packages.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than.ColumnValuesDateNotOlderThan","title":"<code>ColumnValuesDateNotOlderThan</code>","text":"<p>         Bases: <code>ColumnMapMetricProvider</code></p> <p>Asserts that column values are a date that isn't older than a given date.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_values_to_be_date_not_older_than.py</code> <pre><code>class ColumnValuesDateNotOlderThan(ColumnMapMetricProvider):\n\"\"\"Asserts that column values are a date that isn't older than a given date.\"\"\"\ncondition_metric_name = \"column_values.date_is_not_older_than\"\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column\",\n\"ignore_row_if\",\n)\ncondition_value_keys = (\"timeframe\",)\n@column_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnMapMetricProvider,\ncolumn: Any,\ntimeframe: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Since timedelta can only define an interval up to weeks, a month is defined\n        as 4 weeks and a year is defined as 52 weeks.\n        Args:\n            column: Name of column to validate.\n            timeframe: dict with the definition of the timeframe.\n            kwargs: dict with additional parameters.\n        Returns:\n            If the condition is met.\n        \"\"\"\nweeks = (\ntimeframe.get(\"weeks\", 0)\n+ (timeframe.get(\"months\", 0) * 4)\n+ (timeframe.get(\"years\", 0) * 52)\n)\ndelta = timedelta(\ndays=timeframe.get(\"days\", 0),\nseconds=timeframe.get(\"seconds\", 0),\nmicroseconds=timeframe.get(\"microseconds\", 0),\nmilliseconds=timeframe.get(\"milliseconds\", 0),\nminutes=timeframe.get(\"minutes\", 0),\nhours=timeframe.get(\"hours\", 0),\nweeks=weeks,\n)\nreturn delta &gt; (datetime.datetime.now() - column)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_values_to_be_date_not_older_than.html#packages.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than.ExpectColumnValuesToBeDateNotOlderThan","title":"<code>ExpectColumnValuesToBeDateNotOlderThan</code>","text":"<p>         Bases: <code>ColumnMapExpectation</code></p> <p>Expect value in column to be date that is not older than a given time.</p> <p>Since timedelta can only define an interval up to weeks, a month is defined as 4 weeks and a year is defined as 52 weeks.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <p>Name of column to validate</p> required <code>Note</code> <p>Column must be of type Date, Timestamp or String (with Timestamp format). Format: yyyy-MM-ddTHHss</p> required <code>timeframe</code> <p>dict with the definition of the timeframe.</p> required <code>kwargs</code> <p>dict with additional parameters.</p> required <p>Other Parameters:</p> Name Type Description <code>allow_cross_type_comparisons</code> <p>If True, allow comparisons between types (e.g. integer and string). Otherwise, attempting such comparisons will raise an exception.</p> <code>ignore_row_if</code> <p>\"both_values_are_missing\", \"either_value_is_missing\", \"neither\" (default).</p> <code>result_format</code> <p>Which output mode to use: <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</p> <code>include_config</code> <p>If True (default), then include the expectation config as part of the result object.</p> <code>catch_exceptions</code> <p>If True, then catch exceptions and include them as part of the result object. Default: False.</p> <code>meta</code> <p>A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification.</p> <p>Returns:</p> Type Description <p>An ExpectationSuiteValidationResult.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_values_to_be_date_not_older_than.py</code> <pre><code>class ExpectColumnValuesToBeDateNotOlderThan(ColumnMapExpectation):\n\"\"\"Expect value in column to be date that is not older than a given time.\n    Since timedelta can only define an interval up to weeks, a month is defined\n    as 4 weeks and a year is defined as 52 weeks.\n    Args:\n        column: Name of column to validate\n        Note: Column must be of type Date, Timestamp or String (with Timestamp format).\n            Format: yyyy-MM-ddTHH:mm:ss\n        timeframe: dict with the definition of the timeframe.\n        kwargs: dict with additional parameters.\n    Keyword Args:\n        allow_cross_type_comparisons: If True, allow\n            comparisons between types (e.g. integer and string).\n            Otherwise, attempting such comparisons will raise an exception.\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the expectation config\n            as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [\ndatetime.datetime(2023, 6, 1, 12, 0, 0),\ndatetime.datetime(2023, 6, 2, 12, 0, 0),\ndatetime.datetime(2023, 6, 3, 12, 0, 0),\n],\n\"b\": [\ndatetime.datetime(1800, 6, 1, 12, 0, 0),\ndatetime.datetime(2023, 6, 2, 12, 0, 0),\ndatetime.datetime(1800, 6, 3, 12, 0, 0),\n],\n}\n}\n],\n\"schemas\": {\"spark\": {\"a\": \"TimestampType\", \"b\": \"TimestampType\"}},\n\"tests\": [\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column\": \"a\",\n\"timeframe\": {\"years\": 100},\n\"result_format\": {\n\"result_format\": \"BASIC\",\n\"unexpected_index_column_names\": [\"b\"],\n},\n},\n\"out\": {\n\"success\": True,\n\"unexpected_index_list\": [],\n},\n},\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column\": \"b\",\n\"timeframe\": {\"years\": 100},\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\n\"a\": datetime.datetime(2023, 6, 1, 12, 0),\n\"b\": datetime.datetime(1800, 6, 1, 12, 0),\n},\n{\n\"a\": datetime.datetime(2023, 6, 3, 12, 0),\n\"b\": datetime.datetime(1800, 6, 3, 12, 0),\n},\n],\n},\n},\n],\n},\n]\nmap_metric = \"column_values.date_is_not_older_than\"\nsuccess_keys = (\n\"column\",\n\"ignore_row_if\",\n\"timeframe\",\n)\ndefault_kwarg_values = {\n\"mostly\": 1.0,\n\"ignore_row_if\": \"neither\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nColumnMapExpectation,\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_values_to_not_be_null_or_empty_string.html","title":"Expect column values to not be null or empty string","text":"<p>Expectation to check if column value is not null or empty string.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_values_to_not_be_null_or_empty_string.html#packages.dq_processors.custom_expectations.expect_column_values_to_not_be_null_or_empty_string.ColumnValuesNotNullOrEpmtyString","title":"<code>ColumnValuesNotNullOrEpmtyString</code>","text":"<p>         Bases: <code>ColumnMapMetricProvider</code></p> <p>Asserts that column values are not null or empty string.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_values_to_not_be_null_or_empty_string.py</code> <pre><code>class ColumnValuesNotNullOrEpmtyString(ColumnMapMetricProvider):\n\"\"\"Asserts that column values are not null or empty string.\"\"\"\ncondition_metric_name = \"column_values.not_null_or_empty_string\"\nfilter_column_isnull = False\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column\",\n\"ignore_row_if\",\n)\ncondition_value_keys = ()\n@column_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: ColumnMapMetricProvider,\ncolumn: Any,\n**kwargs: dict,\n) -&gt; Any:\n\"\"\"Implementation of the expectation's logic.\n        Args:\n            column: Name of column to validate.\n            kwargs: dict with additional parameters.\n        Returns:\n            If the condition is met.\n        \"\"\"\nreturn (column.isNotNull()) &amp; (column != \"\")\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_column_values_to_not_be_null_or_empty_string.html#packages.dq_processors.custom_expectations.expect_column_values_to_not_be_null_or_empty_string.ExpectColumnValuesToNotBeNullOrEmptyString","title":"<code>ExpectColumnValuesToNotBeNullOrEmptyString</code>","text":"<p>         Bases: <code>ColumnMapExpectation</code></p> <p>Expect value in column to be not null or empty string.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <p>Name of column to validate.</p> required <code>kwargs</code> <p>dict with additional parameters.</p> required <p>Other Parameters:</p> Name Type Description <code>allow_cross_type_comparisons</code> <p>If True, allow comparisons between types (e.g. integer and string). Otherwise, attempting such comparisons will raise an exception.</p> <code>ignore_row_if</code> <p>\"both_values_are_missing\", \"either_value_is_missing\", \"neither\" (default).</p> <code>result_format</code> <p>Which output mode to use: <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</p> <code>include_config</code> <p>If True (default), then include the expectation config as part of the result object.</p> <code>catch_exceptions</code> <p>If True, then catch exceptions and include them as part of the result object. Default: False.</p> <code>meta</code> <p>A JSON-serializable dictionary (nesting allowed) that will be included in the output without modification.</p> <p>Returns:</p> Type Description <p>An ExpectationSuiteValidationResult.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_column_values_to_not_be_null_or_empty_string.py</code> <pre><code>class ExpectColumnValuesToNotBeNullOrEmptyString(ColumnMapExpectation):\n\"\"\"Expect value in column to be not null or empty string.\n    Args:\n        column: Name of column to validate.\n        kwargs: dict with additional parameters.\n    Keyword Args:\n        allow_cross_type_comparisons: If True, allow\n            comparisons between types (e.g. integer and string).\n            Otherwise, attempting such comparisons will raise an exception.\n        ignore_row_if: \"both_values_are_missing\",\n            \"either_value_is_missing\", \"neither\" (default).\n        result_format: Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.\n        include_config: If True (default), then include the expectation config\n            as part of the result object.\n        catch_exceptions: If True, then catch exceptions and\n            include them as part of the result object. Default: False.\n        meta: A JSON-serializable dictionary (nesting allowed)\n            that will be included in the output without modification.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [\n\"4061622965678\",\n\"4061622965679\",\n\"4061622965680\",\n],\n\"b\": [\n\"4061622965678\",\n\"\",\n\"4061622965680\",\n],\n}\n}\n],\n\"schemas\": {\"spark\": {\"a\": \"StringType\", \"b\": \"StringType\"}},\n\"tests\": [\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column\": \"a\",\n\"result_format\": {\n\"result_format\": \"BASIC\",\n\"unexpected_index_column_names\": [\"b\"],\n},\n},\n\"out\": {\n\"success\": True,\n\"unexpected_index_list\": [],\n},\n},\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column\": \"b\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\n\"a\": \"4061622965679\",\n\"b\": \"\",\n}\n],\n},\n},\n],\n},\n]\nmap_metric = \"column_values.not_null_or_empty_string\"\nsuccess_keys = (\n\"column\",\n\"ignore_row_if\",\n)\ndefault_kwarg_values = {\n\"mostly\": 1.0,\n\"ignore_row_if\": \"neither\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nColumnMapExpectation,\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_multicolumn_column_a_must_equal_b_or_c.html","title":"Expect multicolumn column a must equal b or c","text":"<p>Expectation to check if column 'a' equals 'b', or 'c'.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_multicolumn_column_a_must_equal_b_or_c.html#packages.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c.ExpectMulticolumnColumnAMustEqualBOrC","title":"<code>ExpectMulticolumnColumnAMustEqualBOrC</code>","text":"<p>         Bases: <code>MulticolumnMapExpectation</code></p> <p>Expect that the column 'a' is equal to 'b' when this is not empty; otherwise 'a' must be equal to 'c'.</p> <p>Parameters:</p> Name Type Description Default <code>column_list</code> <p>The column names to evaluate.</p> required <p>Other Parameters:</p> Name Type Description <code>ignore_row_if</code> <p>default to \"never\".</p> <code>result_format</code> <p>Which output mode to use: <code>BOOLEAN_ONLY</code>, <code>BASIC</code>, <code>COMPLETE</code>, or <code>SUMMARY</code>. Default set to <code>BASIC</code>.</p> <code>include_config</code> <p>If True, then include the expectation config as part of the result object. Default set to True.</p> <code>catch_exceptions</code> <p>If True, then catch exceptions and include them as part of the result object. Default set to False.</p> <p>Returns:</p> Type Description <p>An ExpectationSuiteValidationResult.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_multicolumn_column_a_must_equal_b_or_c.py</code> <pre><code>class ExpectMulticolumnColumnAMustEqualBOrC(MulticolumnMapExpectation):\n\"\"\"Expect that the column 'a' is equal to 'b' when this is not empty; otherwise 'a' must be equal to 'c'.\n    Args:\n        column_list: The column names to evaluate.\n    Keyword Args:\n        ignore_row_if: default to \"never\".\n        result_format:  Which output mode to use:\n            `BOOLEAN_ONLY`, `BASIC`, `COMPLETE`, or `SUMMARY`.\n            Default set to `BASIC`.\n        include_config: If True, then include the expectation\n            config as part of the result object.\n            Default set to True.\n        catch_exceptions: If True, then catch exceptions\n            and include them as part of the result object.\n            Default set to False.\n    Returns:\n        An ExpectationSuiteValidationResult.\n    \"\"\"  # noqa: E501\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"a\": [\"d001\", \"1000\", \"1001\"],\n\"b\": [None, \"1000\", \"1001\"],\n\"c\": [\"d001\", \"d002\", \"d002\"],\n\"d\": [\"d001\", \"d002\", \"1001\"],\n},\n\"schemas\": {\n\"spark\": {\n\"a\": \"StringType\",\n\"b\": \"StringType\",\n\"c\": \"StringType\",\n\"d\": \"StringType\",\n}\n},\n}\n],\n\"tests\": [\n{\n\"title\": \"negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_list\": [\"d\", \"b\", \"c\"],\n\"validation_regex_c\": \"d[0-9]{3}$\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"d\", \"b\", \"c\"],\n},\n},\n\"out\": {\n\"success\": False,\n\"unexpected_index_list\": [\n{\n\"d\": \"d002\",\n\"b\": \"1000\",\n\"c\": \"d002\",\n}\n],\n},\n},\n{\n\"title\": \"positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"column_list\": [\"a\", \"b\", \"c\"],\n\"validation_regex_c\": \"d[0-9]{3}$\",\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n\"unexpected_index_column_names\": [\"a\", \"b\", \"c\"],\n},\n},\n\"out\": {\"success\": True},\n},\n],\n},\n]\nmap_metric = \"multicolumn_values.column_a_must_equal_b_or_c\"\nsuccess_keys = (\n\"validation_regex_b\",\n\"validation_regex_c\",\n\"mostly\",\n)\ndefault_kwarg_values = {\n\"ignore_row_if\": \"never\",\n\"result_format\": \"BASIC\",\n\"include_config\": True,\n\"catch_exceptions\": False,\n\"mostly\": 1,\n}\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: Dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Any:\n\"\"\"Custom implementation of the GE _validate method.\n        This method is used on the tests to validate both the result\n        of the tests themselves and if the unexpected index list\n        is correctly generated.\n        The GE test logic does not do this validation, and thus\n        we need to make it manually.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nreturn validate_result(\nself,\nconfiguration,\nmetrics,\nruntime_configuration,\nexecution_engine,\nMulticolumnMapExpectation,\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_multicolumn_column_a_must_equal_b_or_c.html#packages.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c.MulticolumnCustomMetric","title":"<code>MulticolumnCustomMetric</code>","text":"<p>         Bases: <code>MulticolumnMapMetricProvider</code></p> <p>Expectation metric definition.</p> <p>This expectation asserts that column 'a' must equal to column 'b' or column 'c'. In addition to this it is possible to validate that column 'b' or 'c' match a regex.</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_multicolumn_column_a_must_equal_b_or_c.py</code> <pre><code>class MulticolumnCustomMetric(MulticolumnMapMetricProvider):\n\"\"\"Expectation metric definition.\n    This expectation asserts that column 'a' must equal to column 'b' or column 'c'.\n    In addition to this it is possible to validate that column 'b' or 'c' match a regex.\n    \"\"\"\ncondition_metric_name = \"multicolumn_values.column_a_must_equal_b_or_c\"\ncondition_domain_keys = (\n\"batch_id\",\n\"table\",\n\"column_list\",\n\"ignore_row_if\",\n)\ncondition_value_keys = (\"validation_regex_b\", \"validation_regex_c\")\n@multicolumn_condition_partial(engine=SparkDFExecutionEngine)\ndef _spark(\nself: MulticolumnMapMetricProvider, column_list: list, **kwargs: dict\n) -&gt; Any:\nvalidation_regex_b = (\nkwargs.get(\"validation_regex_b\") if \"validation_regex_b\" in kwargs else \".*\"\n)\nvalidation_regex_c = (\nkwargs.get(\"validation_regex_c\") if \"validation_regex_c\" in kwargs else \".*\"\n)\nreturn (column_list[0].isNotNull()) &amp; (\n(\ncolumn_list[1].isNotNull()\n&amp; (column_list[1].rlike(validation_regex_b))\n&amp; (column_list[0] == column_list[1])\n)\n| (\n(column_list[1].isNull())\n&amp; (column_list[2].rlike(validation_regex_c))\n&amp; (column_list[0] == column_list[2])\n)\n)\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_queried_column_agg_value_to_be.html","title":"Expect queried column agg value to be","text":"<p>Expectation to check if aggregated column satisfy the condition.</p>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_queried_column_agg_value_to_be.html#packages.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be.ExpectQueriedColumnAggValueToBe","title":"<code>ExpectQueriedColumnAggValueToBe</code>","text":"<p>         Bases: <code>QueryExpectation</code></p> <p>Expect agg of column to satisfy the condition specified.</p> <p>Parameters:</p> Name Type Description Default <code>template_dict</code> <p>dict with the following keys: - column (column to check sum). - group_column_list (group by column names to be listed). - condition (how to validate the aggregated value eg: between,     greater, lesser). - max_value (maximum allowed value). - min_value (minimum allowed value). - agg_type (sum/count/max/min).</p> required Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_queried_column_agg_value_to_be.py</code> <pre><code>class ExpectQueriedColumnAggValueToBe(QueryExpectation):\n\"\"\"Expect agg of column to satisfy the condition specified.\n    Args:\n        template_dict: dict with the following keys:\n            - column (column to check sum).\n            - group_column_list (group by column names to be listed).\n            - condition (how to validate the aggregated value eg: between,\n                greater, lesser).\n            - max_value (maximum allowed value).\n            - min_value (minimum allowed value).\n            - agg_type (sum/count/max/min).\n    \"\"\"\nmetric_dependencies = (\"query.template_values\",)\nquery = \"\"\"\n            SELECT {group_column_list}, {agg_type}({column})\n            FROM {active_batch}\n            GROUP BY {group_column_list}\n            \"\"\"\nsuccess_keys = (\"template_dict\", \"query\")\ndomain_keys = (\n\"query\",\n\"template_dict\",\n\"batch_id\",\n\"row_condition\",\n\"condition_parser\",\n)\ndefault_kwarg_values = {\n\"include_config\": True,\n\"mostly\": 1.0,\n\"result_format\": \"BASIC\",\n\"catch_exceptions\": False,\n\"meta\": None,\n\"query\": query,\n}\ndef validate_configuration(\nself, configuration: Optional[ExpectationConfiguration] = None\n) -&gt; None:\n\"\"\"Validates that a configuration has been set.\n        Args:\n            configuration (OPTIONAL[ExpectationConfiguration]):\n                An optional Expectation Configuration entry.\n        Returns:\n            None. Raises InvalidExpectationConfigurationError\n        \"\"\"\nsuper().validate_configuration(configuration)\n@staticmethod\ndef _validate_between(\nx: str, y: int, expected_max_value: int, expected_min_value: int\n) -&gt; dict:\n\"\"\"Method to check whether value satisfy the between condition.\n        Args:\n            x: contains key of dict(query_result).\n            y: contains value of dict(query_result).\n            expected_max_value: max value passed.\n            expected_min_value: min value passed.\n        Returns:\n            dict with the results after being validated.\n        \"\"\"\nif expected_min_value &lt;= y &lt;= expected_max_value:\nreturn {\n\"info\": f\"Value is within range\\\n{expected_min_value} and {expected_max_value}\",\n\"success\": True,\n}\nelse:\nreturn {\n\"success\": False,\n\"result\": {\n\"info\": f\"Value not in range\\\n{expected_min_value} and {expected_max_value}\",\n\"observed_value\": (x, y),\n},\n}\n@staticmethod\ndef _validate_lesser(x: str, y: int, expected_max_value: int) -&gt; dict:\n\"\"\"Method to check whether value satisfy the less condition.\n        Args:\n            x: contains key of dict(query_result).\n            y: contains value of dict(query_result).\n            expected_max_value: max value passed.\n        Returns:\n            dict with the results after being validated.\n        \"\"\"\nif y &lt; expected_max_value:\nreturn {\n\"info\": f\"Value is lesser than {expected_max_value}\",\n\"success\": True,\n}\nelse:\nreturn {\n\"success\": False,\n\"result\": {\n\"info\": f\"Value is greater than {expected_max_value}\",\n\"observed_value\": (x, y),\n},\n}\n@staticmethod\ndef _validate_greater(x: str, y: int, expected_min_value: int) -&gt; dict:\n\"\"\"Method to check whether value satisfy the greater condition.\n        Args:\n            x: contains key of dict(query_result).\n            y: contains value of dict(query_result).\n            expected_min_value: min value passed.\n        Returns:\n            dict with the results after being validated.\n        \"\"\"\nif y &gt; expected_min_value:\nreturn {\n\"info\": f\"Value is greater than {expected_min_value}\",\n\"success\": True,\n}\nelse:\nreturn {\n\"success\": False,\n\"result\": {\n\"info\": f\"Value is less than {expected_min_value}\",\n\"observed_value\": (x, y),\n},\n}\ndef _validate_condition(self, query_result: dict, template_dict: dict) -&gt; dict:\n\"\"\"Method to check whether value satisfy the expected result.\n        Args:\n            query_result: contains dict of key and value.\n            template_dict: contains dict of input provided.\n        Returns:\n            dict with the results after being validated.\n        \"\"\"\nresult: Dict[Any, Any] = {}\nfor x, y in query_result.items():\ncondition_check = template_dict[\"condition\"]\nif condition_check == \"between\":\n_max = template_dict[\"max_value\"]\n_min = template_dict[\"min_value\"]\nresult = self._validate_between(x, y, _max, _min)\nelif condition_check == \"lesser\":\n_max = template_dict[\"max_value\"]\nresult = self._validate_lesser(x, y, _max)\nelse:\n_min = template_dict[\"min_value\"]\nresult = self._validate_greater(x, y, _min)\nreturn result\n@staticmethod\ndef _generate_dict(query_result: list) -&gt; dict:\n\"\"\"Generate a dict from a list of dicts and merge the group by columns values.\n        Args:\n            query_result: contains list of dict values obtained from query.\n        Returns:\n            Dict\n        Example:\n            input: [dict_values(['Male', 25, 3500]), dict_values(['Female', 25, 6200]),\n                dict_values(['Female', 20, 3500]), dict_values(['Male', 20, 6900])].\n            output: {'Male|25': 3500, 'Female|25': 6200,\n                'Female|20': 3500, 'Male|20': 6900}.\n        \"\"\"\nintermediate_list = []\nfinal_list = []\nfor i in range(len(query_result)):\nintermediate_list.append(list(query_result[i]))\nfor element in intermediate_list:\nif type(element) is list:\noutput = \"|\".join(map(str, element))\nkey = \"|\".join(map(str, element[0:-1]))\nvalue = output.replace(key + \"|\", \"\")\nfinal_list.append(key)\nfinal_list.append(value)\nnew_result = {\nfinal_list[i]: int(final_list[i + 1]) for i in range(0, len(final_list), 2)\n}\nreturn new_result\ndef _validate(\nself,\nconfiguration: ExpectationConfiguration,\nmetrics: dict,\nruntime_configuration: Optional[dict] = None,\nexecution_engine: Optional[ExecutionEngine] = None,\n) -&gt; Union[ExpectationValidationResult, dict]:\n\"\"\"Implementation of the GE _validate method.\n        This method is used on the tests to validate the result\n        of the query output.\n        Args:\n            configuration: Configuration used in the test.\n            metrics: Test result metrics.\n            runtime_configuration: Configuration used when running the expectation.\n            execution_engine: Execution Engine where the expectation was run.\n        Returns:\n            Dictionary with the result of the validation.\n        \"\"\"\nquery_result = metrics.get(\"query.template_values\")\nquery_result = [element.values() for element in query_result]\nquery_result = self._generate_dict(query_result)\ntemplate_dict = self._validate_template_dict(configuration)\noutput = self._validate_condition(query_result, template_dict)\nreturn output\n@staticmethod\ndef _validate_template_dict(configuration: ExpectationConfiguration) -&gt; dict:\n\"\"\"Validate the template dict.\n        Args:\n            configuration (ExpectationConfiguration)\n        Returns:\n            Dict. Raises TypeError and KeyError\n        \"\"\"\ntemplate_dict = configuration.kwargs.get(\"template_dict\")\nif not isinstance(template_dict, dict):\nraise TypeError(\"template_dict must be supplied as a dict\")\nif not all(\n[\n\"column\" in template_dict,\n\"group_column_list\" in template_dict,\n\"agg_type\" in template_dict,\n\"condition\" in template_dict,\n]\n):\nraise KeyError(\n\"The following keys have to be in the \\\n                    template dict: column, group_column_list, condition, agg_type\"\n)\nreturn template_dict\nexamples = [\n{\n\"dataset_name\": \"Test Dataset\",\n\"data\": [\n{\n\"data\": {\n\"ID\": [1, 2, 3, 4, 5, 6],\n\"Names\": [\n\"Ramesh\",\n\"Nasser\",\n\"Jessica\",\n\"Komal\",\n\"Jude\",\n\"Muffy\",\n],\n\"Age\": [25, 25, 25, 20, 20, 25],\n\"Gender\": [\n\"Male\",\n\"Male\",\n\"Female\",\n\"Female\",\n\"Male\",\n\"Female\",\n],\n\"Salary\": [1000, 2500, 5000, 3500, 6900, 1200],\n},\n\"schemas\": {\n\"spark\": {\n\"ID\": \"IntegerType\",\n\"Names\": \"StringType\",\n\"Age\": \"IntegerType\",\n\"Gender\": \"StringType\",\n\"Salary\": \"IntegerType\",\n}\n},\n}\n],\n\"tests\": [\n{\n\"title\": \"basic_positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"template_dict\": {\n\"column\": \"Salary\",\n\"group_column_list\": \"Gender\",\n\"agg_type\": \"sum\",\n\"condition\": \"greater\",\n\"min_value\": 2000,\n},\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n},\n},\n\"out\": {\"success\": True},\n\"only_for\": [\"spark\"],\n},\n{\n\"title\": \"basic_positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"template_dict\": {\n\"column\": \"Salary\",\n\"group_column_list\": \"Gender,Age\",\n\"agg_type\": \"sum\",\n\"condition\": \"between\",\n\"max_value\": 7000,\n\"min_value\": 2000,\n},\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n},\n},\n\"out\": {\"success\": True},\n\"only_for\": [\"spark\"],\n},\n{\n\"title\": \"basic_positive_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"template_dict\": {\n\"column\": \"Salary\",\n\"group_column_list\": \"Age\",\n\"agg_type\": \"max\",\n\"condition\": \"lesser\",\n\"max_value\": 10000,\n},\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n},\n},\n\"out\": {\"success\": True},\n\"only_for\": [\"spark\"],\n},\n{\n\"title\": \"basic_negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"template_dict\": {\n\"column\": \"Salary\",\n\"group_column_list\": \"Gender\",\n\"agg_type\": \"count\",\n\"condition\": \"greater\",\n\"min_value\": 4,\n},\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n},\n},\n\"out\": {\"success\": False},\n\"only_for\": [\"sqlite\", \"spark\"],\n},\n{\n\"title\": \"basic_negative_test\",\n\"exact_match_out\": False,\n\"include_in_gallery\": True,\n\"in\": {\n\"template_dict\": {\n\"column\": \"Salary\",\n\"group_column_list\": \"Gender,Age\",\n\"agg_type\": \"sum\",\n\"condition\": \"between\",\n\"max_value\": 2000,\n\"min_value\": 1000,\n},\n\"result_format\": {\n\"result_format\": \"COMPLETE\",\n},\n},\n\"out\": {\"success\": False},\n\"only_for\": [\"spark\"],\n},\n],\n},\n]\nlibrary_metadata = {\n\"tags\": [\"query-based\"],\n}\n</code></pre>"},{"location":"reference/packages/dq_processors/custom_expectations/expect_queried_column_agg_value_to_be.html#packages.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be.ExpectQueriedColumnAggValueToBe.validate_configuration","title":"<code>validate_configuration(configuration=None)</code>","text":"<p>Validates that a configuration has been set.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>OPTIONAL[ExpectationConfiguration]</code> <p>An optional Expectation Configuration entry.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Raises InvalidExpectationConfigurationError</p> Source code in <code>mkdocs/lakehouse_engine/packages/dq_processors/custom_expectations/expect_queried_column_agg_value_to_be.py</code> <pre><code>def validate_configuration(\nself, configuration: Optional[ExpectationConfiguration] = None\n) -&gt; None:\n\"\"\"Validates that a configuration has been set.\n    Args:\n        configuration (OPTIONAL[ExpectationConfiguration]):\n            An optional Expectation Configuration entry.\n    Returns:\n        None. Raises InvalidExpectationConfigurationError\n    \"\"\"\nsuper().validate_configuration(configuration)\n</code></pre>"},{"location":"reference/packages/io/index.html","title":"Io","text":"<p>Input and Output package responsible for the behaviour of reading and writing.</p>"},{"location":"reference/packages/io/exceptions.html","title":"Exceptions","text":"<p>Package defining all the io custom exceptions.</p>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.EndpointNotFoundException","title":"<code>EndpointNotFoundException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the endpoint is not found by the Graph API.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class EndpointNotFoundException(Exception):\n\"\"\"Exception for when the endpoint is not found by the Graph API.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.IncrementalFilterInputNotFoundException","title":"<code>IncrementalFilterInputNotFoundException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when the input of an incremental filter is not found.</p> <p>This may occur when tables are being loaded in incremental way, taking the increment definition out of a specific table, but the table still does not exist, mainly because probably it was not loaded for the first time yet.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class IncrementalFilterInputNotFoundException(Exception):\n\"\"\"Exception for when the input of an incremental filter is not found.\n    This may occur when tables are being loaded in incremental way, taking the increment\n    definition out of a specific table, but the table still does not exist, mainly\n    because probably it was not loaded for the first time yet.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.InputNotFoundException","title":"<code>InputNotFoundException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a user does not provide a mandatory input.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class InputNotFoundException(Exception):\n\"\"\"Exception for when a user does not provide a mandatory input.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.LocalPathNotFoundException","title":"<code>LocalPathNotFoundException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a local path is not found.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class LocalPathNotFoundException(Exception):\n\"\"\"Exception for when a local path is not found.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.NotSupportedException","title":"<code>NotSupportedException</code>","text":"<p>         Bases: <code>RuntimeError</code></p> <p>Exception for when a user provides a not supported operation.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class NotSupportedException(RuntimeError):\n\"\"\"Exception for when a user provides a not supported operation.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.SharePointAPIError","title":"<code>SharePointAPIError</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Custom exception class to handle errors SharePoint API requests.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class SharePointAPIError(Exception):\n\"\"\"Custom exception class to handle errors SharePoint API requests.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.WriteToLocalException","title":"<code>WriteToLocalException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when an error occurs when trying to write to the local path.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class WriteToLocalException(Exception):\n\"\"\"Exception for when an error occurs when trying to write to the local path.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/exceptions.html#packages.io.exceptions.WrongIOFormatException","title":"<code>WrongIOFormatException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a user provides a wrong I/O format.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/exceptions.py</code> <pre><code>class WrongIOFormatException(Exception):\n\"\"\"Exception for when a user provides a wrong I/O format.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/io/reader.html","title":"Reader","text":"<p>Defines abstract reader behaviour.</p>"},{"location":"reference/packages/io/reader.html#packages.io.reader.Reader","title":"<code>Reader</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract Reader class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/reader.py</code> <pre><code>class Reader(ABC):\n\"\"\"Abstract Reader class.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct Reader instances.\n        Args:\n            input_spec: input specification for reading data.\n        \"\"\"\nself._logger = LoggingHandler(self.__class__.__name__).get_logger()\nself._input_spec = input_spec\n@abstractmethod\ndef read(self) -&gt; DataFrame:\n\"\"\"Abstract read method.\n        Returns:\n            A dataframe read according to the input specification.\n        \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/io/reader.html#packages.io.reader.Reader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct Reader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification for reading data.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct Reader instances.\n    Args:\n        input_spec: input specification for reading data.\n    \"\"\"\nself._logger = LoggingHandler(self.__class__.__name__).get_logger()\nself._input_spec = input_spec\n</code></pre>"},{"location":"reference/packages/io/reader.html#packages.io.reader.Reader.read","title":"<code>read()</code>  <code>abstractmethod</code>","text":"<p>Abstract read method.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe read according to the input specification.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/reader.py</code> <pre><code>@abstractmethod\ndef read(self) -&gt; DataFrame:\n\"\"\"Abstract read method.\n    Returns:\n        A dataframe read according to the input specification.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/io/reader_factory.html","title":"Reader factory","text":"<p>Module for reader factory.</p>"},{"location":"reference/packages/io/reader_factory.html#packages.io.reader_factory.ReaderFactory","title":"<code>ReaderFactory</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Class for reader factory.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/reader_factory.py</code> <pre><code>class ReaderFactory(ABC):  # noqa: B024\n\"\"\"Class for reader factory.\"\"\"\n@classmethod\ndef get_data(cls, spec: InputSpec) -&gt; DataFrame:\n\"\"\"Get data according to the input specification following a factory pattern.\n        Args:\n            spec: input specification to get the data.\n        Returns:\n            A dataframe containing the data.\n        \"\"\"\nif spec.db_table:\nread_df = TableReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.JDBC.value:\nread_df = JDBCReader(input_spec=spec).read()\nelif spec.data_format in FILE_INPUT_FORMATS:\nread_df = FileReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.KAFKA.value:\nread_df = KafkaReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SQL.value:\nread_df = QueryReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SAP_BW.value:\nread_df = SAPBWReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SAP_B4.value:\nread_df = SAPB4Reader(input_spec=spec).read()\nelif spec.data_format == InputFormat.DATAFRAME.value:\nread_df = DataFrameReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SFTP.value:\nfrom lakehouse_engine.io.readers.sftp_reader import SFTPReader\nread_df = SFTPReader(input_spec=spec).read()\nelse:\nraise NotImplementedError(\nf\"The requested input spec format {spec.data_format} is not supported.\"\n)\nif spec.temp_view:\nread_df.createOrReplaceTempView(spec.temp_view)\nreturn read_df\n</code></pre>"},{"location":"reference/packages/io/reader_factory.html#packages.io.reader_factory.ReaderFactory.get_data","title":"<code>get_data(spec)</code>  <code>classmethod</code>","text":"<p>Get data according to the input specification following a factory pattern.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>InputSpec</code> <p>input specification to get the data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/reader_factory.py</code> <pre><code>@classmethod\ndef get_data(cls, spec: InputSpec) -&gt; DataFrame:\n\"\"\"Get data according to the input specification following a factory pattern.\n    Args:\n        spec: input specification to get the data.\n    Returns:\n        A dataframe containing the data.\n    \"\"\"\nif spec.db_table:\nread_df = TableReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.JDBC.value:\nread_df = JDBCReader(input_spec=spec).read()\nelif spec.data_format in FILE_INPUT_FORMATS:\nread_df = FileReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.KAFKA.value:\nread_df = KafkaReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SQL.value:\nread_df = QueryReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SAP_BW.value:\nread_df = SAPBWReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SAP_B4.value:\nread_df = SAPB4Reader(input_spec=spec).read()\nelif spec.data_format == InputFormat.DATAFRAME.value:\nread_df = DataFrameReader(input_spec=spec).read()\nelif spec.data_format == InputFormat.SFTP.value:\nfrom lakehouse_engine.io.readers.sftp_reader import SFTPReader\nread_df = SFTPReader(input_spec=spec).read()\nelse:\nraise NotImplementedError(\nf\"The requested input spec format {spec.data_format} is not supported.\"\n)\nif spec.temp_view:\nread_df.createOrReplaceTempView(spec.temp_view)\nreturn read_df\n</code></pre>"},{"location":"reference/packages/io/writer.html","title":"Writer","text":"<p>Defines abstract writer behaviour.</p>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer","title":"<code>Writer</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract Writer class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>class Writer(ABC):\n\"\"\"Abstract Writer class.\"\"\"\ndef __init__(\nself, output_spec: OutputSpec, df: DataFrame, data: OrderedDict = None\n):\n\"\"\"Construct Writer instances.\n        Args:\n            output_spec: output specification to write data.\n            df: dataframe to write.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nself._logger = LoggingHandler(self.__class__.__name__).get_logger()\nself._output_spec = output_spec\nself._df = df\nself._data = data\n@abstractmethod\ndef write(self) -&gt; Optional[OrderedDict]:\n\"\"\"Abstract write method.\"\"\"\nraise NotImplementedError\n@staticmethod\ndef write_transformed_micro_batch(**kwargs: Any) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        This function must define an inner function that manipulates a streaming batch,\n        and then return that function. Look for concrete implementations of this\n        function for more clarity.\n        Args:\n            kwargs: any keyword arguments.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\nlogger = LoggingHandler(__name__).get_logger()\nlogger.warning(\"Skipping transform micro batch... nothing to do.\")\nreturn inner\n@classmethod\ndef get_transformed_micro_batch(\ncls,\noutput_spec: OutputSpec,\nbatch_df: DataFrame,\nbatch_id: int,\ndata: OrderedDict,\n) -&gt; DataFrame:\n\"\"\"Get the result of the transformations applied to a micro batch dataframe.\n        Args:\n            output_spec: output specification associated with the writer.\n            batch_df: batch dataframe (given from streaming foreachBatch).\n            batch_id: if of the batch (given from streaming foreachBatch).\n            data: list of all dfs generated on previous steps before writer\n                to be available on micro batch transforms.\n        Returns:\n            The transformed dataframe.\n        \"\"\"\n# forcing session to be available inside forEachBatch on\n# Spark Connect\nExecEnv.get_or_create()\ntransformed_df = batch_df\nif output_spec.with_batch_id:\ntransformed_df = transformed_df.withColumn(\"lhe_batch_id\", lit(batch_id))\nfor transformer in output_spec.streaming_micro_batch_transformers:\ntransformed_df = transformed_df.transform(\nTransformerFactory.get_transformer(transformer, data)\n)\nreturn transformed_df\n@classmethod\ndef get_streaming_trigger(cls, output_spec: OutputSpec) -&gt; Dict:\n\"\"\"Define which streaming trigger will be used.\n        Args:\n            output_spec: output specification.\n        Returns:\n            A dict containing streaming trigger.\n        \"\"\"\ntrigger: Dict[str, Any] = {}\nif output_spec.streaming_available_now:\ntrigger[\"availableNow\"] = output_spec.streaming_available_now\nelif output_spec.streaming_once:\ntrigger[\"once\"] = output_spec.streaming_once\nelif output_spec.streaming_processing_time:\ntrigger[\"processingTime\"] = output_spec.streaming_processing_time\nelif output_spec.streaming_continuous:\ntrigger[\"continuous\"] = output_spec.streaming_continuous\nelse:\nraise NotImplementedError(\n\"The requested output spec streaming trigger is not supported.\"\n)\nreturn trigger\n@staticmethod\ndef run_micro_batch_dq_process(df: DataFrame, dq_spec: List[DQSpec]) -&gt; DataFrame:\n\"\"\"Run the data quality process in a streaming micro batch dataframe.\n        Iterates over the specs and performs the checks or analysis depending on the\n        data quality specification provided in the configuration.\n        Args:\n            df: the dataframe in which to run the dq process on.\n            dq_spec: data quality specification.\n        Returns: the validated dataframe.\n        \"\"\"\nfrom lakehouse_engine.dq_processors.dq_factory import DQFactory\n# forcing session to be available inside forEachBatch on\n# Spark Connect\nExecEnv.get_or_create()\nvalidated_df = df\nfor spec in dq_spec:\nvalidated_df = DQFactory.run_dq_process(spec, df)\nreturn validated_df\n</code></pre>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer.__init__","title":"<code>__init__(output_spec, df, data=None)</code>","text":"<p>Construct Writer instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification to write data.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to write.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>def __init__(\nself, output_spec: OutputSpec, df: DataFrame, data: OrderedDict = None\n):\n\"\"\"Construct Writer instances.\n    Args:\n        output_spec: output specification to write data.\n        df: dataframe to write.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nself._logger = LoggingHandler(self.__class__.__name__).get_logger()\nself._output_spec = output_spec\nself._df = df\nself._data = data\n</code></pre>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer.get_streaming_trigger","title":"<code>get_streaming_trigger(output_spec)</code>  <code>classmethod</code>","text":"<p>Define which streaming trigger will be used.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>A dict containing streaming trigger.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>@classmethod\ndef get_streaming_trigger(cls, output_spec: OutputSpec) -&gt; Dict:\n\"\"\"Define which streaming trigger will be used.\n    Args:\n        output_spec: output specification.\n    Returns:\n        A dict containing streaming trigger.\n    \"\"\"\ntrigger: Dict[str, Any] = {}\nif output_spec.streaming_available_now:\ntrigger[\"availableNow\"] = output_spec.streaming_available_now\nelif output_spec.streaming_once:\ntrigger[\"once\"] = output_spec.streaming_once\nelif output_spec.streaming_processing_time:\ntrigger[\"processingTime\"] = output_spec.streaming_processing_time\nelif output_spec.streaming_continuous:\ntrigger[\"continuous\"] = output_spec.streaming_continuous\nelse:\nraise NotImplementedError(\n\"The requested output spec streaming trigger is not supported.\"\n)\nreturn trigger\n</code></pre>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer.get_transformed_micro_batch","title":"<code>get_transformed_micro_batch(output_spec, batch_df, batch_id, data)</code>  <code>classmethod</code>","text":"<p>Get the result of the transformations applied to a micro batch dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification associated with the writer.</p> required <code>batch_df</code> <code>DataFrame</code> <p>batch dataframe (given from streaming foreachBatch).</p> required <code>batch_id</code> <code>int</code> <p>if of the batch (given from streaming foreachBatch).</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer to be available on micro batch transforms.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed dataframe.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>@classmethod\ndef get_transformed_micro_batch(\ncls,\noutput_spec: OutputSpec,\nbatch_df: DataFrame,\nbatch_id: int,\ndata: OrderedDict,\n) -&gt; DataFrame:\n\"\"\"Get the result of the transformations applied to a micro batch dataframe.\n    Args:\n        output_spec: output specification associated with the writer.\n        batch_df: batch dataframe (given from streaming foreachBatch).\n        batch_id: if of the batch (given from streaming foreachBatch).\n        data: list of all dfs generated on previous steps before writer\n            to be available on micro batch transforms.\n    Returns:\n        The transformed dataframe.\n    \"\"\"\n# forcing session to be available inside forEachBatch on\n# Spark Connect\nExecEnv.get_or_create()\ntransformed_df = batch_df\nif output_spec.with_batch_id:\ntransformed_df = transformed_df.withColumn(\"lhe_batch_id\", lit(batch_id))\nfor transformer in output_spec.streaming_micro_batch_transformers:\ntransformed_df = transformed_df.transform(\nTransformerFactory.get_transformer(transformer, data)\n)\nreturn transformed_df\n</code></pre>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer.run_micro_batch_dq_process","title":"<code>run_micro_batch_dq_process(df, dq_spec)</code>  <code>staticmethod</code>","text":"<p>Run the data quality process in a streaming micro batch dataframe.</p> <p>Iterates over the specs and performs the checks or analysis depending on the data quality specification provided in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the dataframe in which to run the dq process on.</p> required <code>dq_spec</code> <code>List[DQSpec]</code> <p>data quality specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>@staticmethod\ndef run_micro_batch_dq_process(df: DataFrame, dq_spec: List[DQSpec]) -&gt; DataFrame:\n\"\"\"Run the data quality process in a streaming micro batch dataframe.\n    Iterates over the specs and performs the checks or analysis depending on the\n    data quality specification provided in the configuration.\n    Args:\n        df: the dataframe in which to run the dq process on.\n        dq_spec: data quality specification.\n    Returns: the validated dataframe.\n    \"\"\"\nfrom lakehouse_engine.dq_processors.dq_factory import DQFactory\n# forcing session to be available inside forEachBatch on\n# Spark Connect\nExecEnv.get_or_create()\nvalidated_df = df\nfor spec in dq_spec:\nvalidated_df = DQFactory.run_dq_process(spec, df)\nreturn validated_df\n</code></pre>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer.write","title":"<code>write()</code>  <code>abstractmethod</code>","text":"<p>Abstract write method.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>@abstractmethod\ndef write(self) -&gt; Optional[OrderedDict]:\n\"\"\"Abstract write method.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/io/writer.html#packages.io.writer.Writer.write_transformed_micro_batch","title":"<code>write_transformed_micro_batch(**kwargs)</code>  <code>staticmethod</code>","text":"<p>Define how to write a streaming micro batch after transforming it.</p> <p>This function must define an inner function that manipulates a streaming batch, and then return that function. Look for concrete implementations of this function for more clarity.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>any keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the foreachBatch spark write method.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer.py</code> <pre><code>@staticmethod\ndef write_transformed_micro_batch(**kwargs: Any) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n    This function must define an inner function that manipulates a streaming batch,\n    and then return that function. Look for concrete implementations of this\n    function for more clarity.\n    Args:\n        kwargs: any keyword arguments.\n    Returns:\n        A function to be executed in the foreachBatch spark write method.\n    \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\nlogger = LoggingHandler(__name__).get_logger()\nlogger.warning(\"Skipping transform micro batch... nothing to do.\")\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writer_factory.html","title":"Writer factory","text":"<p>Module for writer factory.</p>"},{"location":"reference/packages/io/writer_factory.html#packages.io.writer_factory.WriterFactory","title":"<code>WriterFactory</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Class for writer factory.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer_factory.py</code> <pre><code>class WriterFactory(ABC):  # noqa: B024\n\"\"\"Class for writer factory.\"\"\"\nAVAILABLE_WRITERS = {\nOutputFormat.TABLE.value: TableWriter,\nOutputFormat.DELTAFILES.value: DeltaMergeWriter,\nOutputFormat.JDBC.value: JDBCWriter,\nOutputFormat.FILE.value: FileWriter,\nOutputFormat.KAFKA.value: KafkaWriter,\nOutputFormat.CONSOLE.value: ConsoleWriter,\nOutputFormat.DATAFRAME.value: DataFrameWriter,\nOutputFormat.REST_API.value: RestApiWriter,\nOutputFormat.SHAREPOINT.value: SharepointWriter,\n}\n@classmethod\ndef _get_writer_name(cls, spec: OutputSpec) -&gt; str:\n\"\"\"Get the writer name according to the output specification.\n        Args:\n            OutputSpec spec: output specification to write data.\n        Returns:\n            Writer: writer name that will be created to write the data.\n        \"\"\"\nif spec.db_table and spec.write_type != WriteType.MERGE.value:\nwriter_name = OutputFormat.TABLE.value\nelif (\nspec.data_format == OutputFormat.DELTAFILES.value or spec.db_table\n) and spec.write_type == WriteType.MERGE.value:\nwriter_name = OutputFormat.DELTAFILES.value\nelif spec.data_format in FILE_OUTPUT_FORMATS:\nwriter_name = OutputFormat.FILE.value\nelse:\nwriter_name = spec.data_format\nreturn writer_name\n@classmethod\ndef get_writer(cls, spec: OutputSpec, df: DataFrame, data: OrderedDict) -&gt; Writer:\n\"\"\"Get a writer according to the output specification using a factory pattern.\n        Args:\n            spec: output specification to write data.\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            Writer: writer that will write the data.\n        \"\"\"\nwriter_name = cls._get_writer_name(spec)\nwriter = cls.AVAILABLE_WRITERS.get(writer_name)\nif writer:\nreturn writer(output_spec=spec, df=df, data=data)  # type: ignore\nelse:\nraise NotImplementedError(\nf\"The requested output spec format {spec.data_format} is not supported.\"\n)\n</code></pre>"},{"location":"reference/packages/io/writer_factory.html#packages.io.writer_factory.WriterFactory.get_writer","title":"<code>get_writer(spec, df, data)</code>  <code>classmethod</code>","text":"<p>Get a writer according to the output specification using a factory pattern.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>OutputSpec</code> <p>output specification to write data.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <code>Writer</code> <p>writer that will write the data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writer_factory.py</code> <pre><code>@classmethod\ndef get_writer(cls, spec: OutputSpec, df: DataFrame, data: OrderedDict) -&gt; Writer:\n\"\"\"Get a writer according to the output specification using a factory pattern.\n    Args:\n        spec: output specification to write data.\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    Returns:\n        Writer: writer that will write the data.\n    \"\"\"\nwriter_name = cls._get_writer_name(spec)\nwriter = cls.AVAILABLE_WRITERS.get(writer_name)\nif writer:\nreturn writer(output_spec=spec, df=df, data=data)  # type: ignore\nelse:\nraise NotImplementedError(\nf\"The requested output spec format {spec.data_format} is not supported.\"\n)\n</code></pre>"},{"location":"reference/packages/io/readers/index.html","title":"Readers","text":"<p>Readers package to define reading behaviour.</p>"},{"location":"reference/packages/io/readers/dataframe_reader.html","title":"Dataframe reader","text":"<p>Module to define behaviour to read from dataframes.</p>"},{"location":"reference/packages/io/readers/dataframe_reader.html#packages.io.readers.dataframe_reader.DataFrameReader","title":"<code>DataFrameReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read data from a dataframe.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/dataframe_reader.py</code> <pre><code>class DataFrameReader(Reader):\n\"\"\"Class to read data from a dataframe.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct DataFrameReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from a dataframe.\n        Returns:\n            A dataframe containing the data from a dataframe previously\n            computed.\n        \"\"\"\nreturn self._input_spec.df_name\n</code></pre>"},{"location":"reference/packages/io/readers/dataframe_reader.html#packages.io.readers.dataframe_reader.DataFrameReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct DataFrameReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/dataframe_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct DataFrameReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/dataframe_reader.html#packages.io.readers.dataframe_reader.DataFrameReader.read","title":"<code>read()</code>","text":"<p>Read data from a dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from a dataframe previously</p> <code>DataFrame</code> <p>computed.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/dataframe_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from a dataframe.\n    Returns:\n        A dataframe containing the data from a dataframe previously\n        computed.\n    \"\"\"\nreturn self._input_spec.df_name\n</code></pre>"},{"location":"reference/packages/io/readers/file_reader.html","title":"File reader","text":"<p>Module to define behaviour to read from files.</p>"},{"location":"reference/packages/io/readers/file_reader.html#packages.io.readers.file_reader.FileReader","title":"<code>FileReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read from files.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/file_reader.py</code> <pre><code>class FileReader(Reader):\n\"\"\"Class to read from files.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct FileReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read file data.\n        Returns:\n            A dataframe containing the data from the files.\n        \"\"\"\nif (\nself._input_spec.read_type == ReadType.BATCH.value\nand self._input_spec.data_format in FILE_INPUT_FORMATS\n):\ndf = ExecEnv.SESSION.read.load(\npath=self._input_spec.location,\nformat=self._input_spec.data_format,\nschema=SchemaUtils.from_input_spec(self._input_spec),\n**self._input_spec.options if self._input_spec.options else {},\n)\nif self._input_spec.with_filepath:\n# _metadata contains hidden columns\ndf = df.selectExpr(\n\"*\", \"_metadata.file_path as lhe_extraction_filepath\"\n)\nreturn df\nelif (\nself._input_spec.read_type == ReadType.STREAMING.value\nand self._input_spec.data_format in FILE_INPUT_FORMATS\n):\ndf = ExecEnv.SESSION.readStream.load(\npath=self._input_spec.location,\nformat=self._input_spec.data_format,\nschema=SchemaUtils.from_input_spec(self._input_spec),\n**self._input_spec.options if self._input_spec.options else {},\n)\nif self._input_spec.with_filepath:\n# _metadata contains hidden columns\ndf = df.selectExpr(\n\"*\", \"_metadata.file_path as lhe_extraction_filepath\"\n)\nreturn df\nelse:\nraise NotImplementedError(\n\"The requested read type and format combination is not supported.\"\n)\n</code></pre>"},{"location":"reference/packages/io/readers/file_reader.html#packages.io.readers.file_reader.FileReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct FileReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/file_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct FileReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/file_reader.html#packages.io.readers.file_reader.FileReader.read","title":"<code>read()</code>","text":"<p>Read file data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from the files.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/file_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read file data.\n    Returns:\n        A dataframe containing the data from the files.\n    \"\"\"\nif (\nself._input_spec.read_type == ReadType.BATCH.value\nand self._input_spec.data_format in FILE_INPUT_FORMATS\n):\ndf = ExecEnv.SESSION.read.load(\npath=self._input_spec.location,\nformat=self._input_spec.data_format,\nschema=SchemaUtils.from_input_spec(self._input_spec),\n**self._input_spec.options if self._input_spec.options else {},\n)\nif self._input_spec.with_filepath:\n# _metadata contains hidden columns\ndf = df.selectExpr(\n\"*\", \"_metadata.file_path as lhe_extraction_filepath\"\n)\nreturn df\nelif (\nself._input_spec.read_type == ReadType.STREAMING.value\nand self._input_spec.data_format in FILE_INPUT_FORMATS\n):\ndf = ExecEnv.SESSION.readStream.load(\npath=self._input_spec.location,\nformat=self._input_spec.data_format,\nschema=SchemaUtils.from_input_spec(self._input_spec),\n**self._input_spec.options if self._input_spec.options else {},\n)\nif self._input_spec.with_filepath:\n# _metadata contains hidden columns\ndf = df.selectExpr(\n\"*\", \"_metadata.file_path as lhe_extraction_filepath\"\n)\nreturn df\nelse:\nraise NotImplementedError(\n\"The requested read type and format combination is not supported.\"\n)\n</code></pre>"},{"location":"reference/packages/io/readers/jdbc_reader.html","title":"Jdbc reader","text":"<p>Module to define behaviour to read from JDBC sources.</p>"},{"location":"reference/packages/io/readers/jdbc_reader.html#packages.io.readers.jdbc_reader.JDBCReader","title":"<code>JDBCReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read from JDBC source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/jdbc_reader.py</code> <pre><code>class JDBCReader(Reader):\n\"\"\"Class to read from JDBC source.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct JDBCReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from JDBC source.\n        Returns:\n            A dataframe containing the data from the JDBC source.\n        \"\"\"\nif (\nself._input_spec.options is not None\nand self._input_spec.options.get(\"predicates\", None) is not None\n):\nraise WrongArgumentsException(\"Predicates can only be used with jdbc_args.\")\noptions = self._input_spec.options if self._input_spec.options else {}\nif self._input_spec.calculate_upper_bound:\njdbc_util = JDBCExtractionUtils(\nJDBCExtraction(\nuser=options[\"user\"],\npassword=options[\"password\"],\nurl=options[\"url\"],\ndbtable=options[\"dbtable\"],\nextraction_type=options.get(\n\"extraction_type\", JDBCExtraction.extraction_type\n),\npartition_column=options[\"partitionColumn\"],\ncalc_upper_bound_schema=self._input_spec.calc_upper_bound_schema,\ndefault_upper_bound=options.get(\n\"default_upper_bound\", JDBCExtraction.default_upper_bound\n),\n)\n)  # type: ignore\noptions[\"upperBound\"] = jdbc_util.get_spark_jdbc_optimal_upper_bound()\nif self._input_spec.jdbc_args:\nreturn ExecEnv.SESSION.read.options(**options).jdbc(\n**self._input_spec.jdbc_args\n)\nelse:\nreturn (\nExecEnv.SESSION.read.format(InputFormat.JDBC.value)\n.options(**options)\n.load()\n)\n</code></pre>"},{"location":"reference/packages/io/readers/jdbc_reader.html#packages.io.readers.jdbc_reader.JDBCReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct JDBCReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/jdbc_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct JDBCReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/jdbc_reader.html#packages.io.readers.jdbc_reader.JDBCReader.read","title":"<code>read()</code>","text":"<p>Read data from JDBC source.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from the JDBC source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/jdbc_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from JDBC source.\n    Returns:\n        A dataframe containing the data from the JDBC source.\n    \"\"\"\nif (\nself._input_spec.options is not None\nand self._input_spec.options.get(\"predicates\", None) is not None\n):\nraise WrongArgumentsException(\"Predicates can only be used with jdbc_args.\")\noptions = self._input_spec.options if self._input_spec.options else {}\nif self._input_spec.calculate_upper_bound:\njdbc_util = JDBCExtractionUtils(\nJDBCExtraction(\nuser=options[\"user\"],\npassword=options[\"password\"],\nurl=options[\"url\"],\ndbtable=options[\"dbtable\"],\nextraction_type=options.get(\n\"extraction_type\", JDBCExtraction.extraction_type\n),\npartition_column=options[\"partitionColumn\"],\ncalc_upper_bound_schema=self._input_spec.calc_upper_bound_schema,\ndefault_upper_bound=options.get(\n\"default_upper_bound\", JDBCExtraction.default_upper_bound\n),\n)\n)  # type: ignore\noptions[\"upperBound\"] = jdbc_util.get_spark_jdbc_optimal_upper_bound()\nif self._input_spec.jdbc_args:\nreturn ExecEnv.SESSION.read.options(**options).jdbc(\n**self._input_spec.jdbc_args\n)\nelse:\nreturn (\nExecEnv.SESSION.read.format(InputFormat.JDBC.value)\n.options(**options)\n.load()\n)\n</code></pre>"},{"location":"reference/packages/io/readers/kafka_reader.html","title":"Kafka reader","text":"<p>Module to define behaviour to read from Kafka.</p>"},{"location":"reference/packages/io/readers/kafka_reader.html#packages.io.readers.kafka_reader.KafkaReader","title":"<code>KafkaReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read from Kafka.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/kafka_reader.py</code> <pre><code>class KafkaReader(Reader):\n\"\"\"Class to read from Kafka.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct KafkaReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read Kafka data.\n        Returns:\n            A dataframe containing the data from Kafka.\n        \"\"\"\ndf = ExecEnv.SESSION.readStream.load(\nformat=InputFormat.KAFKA.value,\n**self._input_spec.options if self._input_spec.options else {},\n)\nreturn df\n</code></pre>"},{"location":"reference/packages/io/readers/kafka_reader.html#packages.io.readers.kafka_reader.KafkaReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct KafkaReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/kafka_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct KafkaReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/kafka_reader.html#packages.io.readers.kafka_reader.KafkaReader.read","title":"<code>read()</code>","text":"<p>Read Kafka data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from Kafka.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/kafka_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read Kafka data.\n    Returns:\n        A dataframe containing the data from Kafka.\n    \"\"\"\ndf = ExecEnv.SESSION.readStream.load(\nformat=InputFormat.KAFKA.value,\n**self._input_spec.options if self._input_spec.options else {},\n)\nreturn df\n</code></pre>"},{"location":"reference/packages/io/readers/query_reader.html","title":"Query reader","text":"<p>Module to define behaviour to read from a query.</p>"},{"location":"reference/packages/io/readers/query_reader.html#packages.io.readers.query_reader.QueryReader","title":"<code>QueryReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read data from a query.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/query_reader.py</code> <pre><code>class QueryReader(Reader):\n\"\"\"Class to read data from a query.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct QueryReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from a query.\n        Returns:\n            A dataframe containing the data from the query.\n        \"\"\"\nreturn ExecEnv.SESSION.sql(self._input_spec.query)\n</code></pre>"},{"location":"reference/packages/io/readers/query_reader.html#packages.io.readers.query_reader.QueryReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct QueryReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/query_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct QueryReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/query_reader.html#packages.io.readers.query_reader.QueryReader.read","title":"<code>read()</code>","text":"<p>Read data from a query.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from the query.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/query_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from a query.\n    Returns:\n        A dataframe containing the data from the query.\n    \"\"\"\nreturn ExecEnv.SESSION.sql(self._input_spec.query)\n</code></pre>"},{"location":"reference/packages/io/readers/sap_b4_reader.html","title":"Sap b4 reader","text":"<p>Module to define behaviour to read from SAP B4 sources.</p>"},{"location":"reference/packages/io/readers/sap_b4_reader.html#packages.io.readers.sap_b4_reader.SAPB4Reader","title":"<code>SAPB4Reader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read from SAP B4 source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sap_b4_reader.py</code> <pre><code>class SAPB4Reader(Reader):\n\"\"\"Class to read from SAP B4 source.\"\"\"\n_LOGGER: Logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct SAPB4Reader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\nself.jdbc_utils = self._get_jdbc_utils()\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from SAP B4 source.\n        Returns:\n            A dataframe containing the data from the SAP B4 source.\n        \"\"\"\noptions_args, jdbc_args = self._get_options()\nreturn ExecEnv.SESSION.read.options(**options_args).jdbc(**jdbc_args)\ndef _get_jdbc_utils(self) -&gt; SAPB4ExtractionUtils:\njdbc_extraction = SAPB4Extraction(\nuser=self._input_spec.options[\"user\"],\npassword=self._input_spec.options[\"password\"],\nurl=self._input_spec.options[\"url\"],\ndbtable=self._input_spec.options[\"dbtable\"],\nadso_type=self._input_spec.options[\"adso_type\"],\nrequest_status_tbl=self._input_spec.options.get(\n\"request_status_tbl\", SAPB4Extraction.request_status_tbl\n),\nchangelog_table=self._input_spec.options.get(\n\"changelog_table\",\n(\nself._input_spec.options[\"dbtable\"]\nif self._input_spec.options[\"adso_type\"] == ADSOTypes.AQ.value\nelse self._input_spec.options[\"changelog_table\"]\n),\n),\ndata_target=SAPB4ExtractionUtils.get_data_target(self._input_spec.options),\nact_req_join_condition=self._input_spec.options.get(\n\"act_req_join_condition\", SAPB4Extraction.act_req_join_condition\n),\nlatest_timestamp_data_location=self._input_spec.options.get(\n\"latest_timestamp_data_location\",\nSAPB4Extraction.latest_timestamp_data_location,\n),\nlatest_timestamp_input_col=self._input_spec.options.get(\n\"latest_timestamp_input_col\",\nSAPB4Extraction.latest_timestamp_input_col,\n),\nlatest_timestamp_data_format=self._input_spec.options.get(\n\"latest_timestamp_data_format\",\nSAPB4Extraction.latest_timestamp_data_format,\n),\nextraction_type=self._input_spec.options.get(\n\"extraction_type\", SAPB4Extraction.extraction_type\n),\ndriver=self._input_spec.options.get(\"driver\", SAPB4Extraction.driver),\nnum_partitions=self._input_spec.options.get(\n\"numPartitions\", SAPB4Extraction.num_partitions\n),\npartition_column=self._input_spec.options.get(\n\"partitionColumn\", SAPB4Extraction.partition_column\n),\nlower_bound=self._input_spec.options.get(\n\"lowerBound\", SAPB4Extraction.lower_bound\n),\nupper_bound=self._input_spec.options.get(\n\"upperBound\", SAPB4Extraction.upper_bound\n),\ndefault_upper_bound=self._input_spec.options.get(\n\"default_upper_bound\", SAPB4Extraction.default_upper_bound\n),\nfetch_size=self._input_spec.options.get(\n\"fetchSize\", SAPB4Extraction.fetch_size\n),\ncompress=self._input_spec.options.get(\"compress\", SAPB4Extraction.compress),\ncustom_schema=self._input_spec.options.get(\n\"customSchema\", SAPB4Extraction.custom_schema\n),\nextraction_timestamp=self._input_spec.options.get(\n\"extraction_timestamp\",\nSAPB4Extraction.extraction_timestamp,\n),\nmin_timestamp=self._input_spec.options.get(\n\"min_timestamp\", SAPB4Extraction.min_timestamp\n),\nmax_timestamp=self._input_spec.options.get(\n\"max_timestamp\", SAPB4Extraction.max_timestamp\n),\ndefault_max_timestamp=self._input_spec.options.get(\n\"default_max_timestamp\", SAPB4Extraction.default_max_timestamp\n),\nmax_timestamp_custom_schema=self._input_spec.options.get(\n\"max_timestamp_custom_schema\",\nSAPB4Extraction.max_timestamp_custom_schema,\n),\ngenerate_predicates=self._input_spec.generate_predicates,\npredicates=self._input_spec.options.get(\n\"predicates\", SAPB4Extraction.predicates\n),\npredicates_add_null=self._input_spec.predicates_add_null,\nextra_cols_req_status_tbl=self._input_spec.options.get(\n\"extra_cols_req_status_tbl\", SAPB4Extraction.extra_cols_req_status_tbl\n),\ncalc_upper_bound_schema=self._input_spec.calc_upper_bound_schema,\ninclude_changelog_tech_cols=self._input_spec.options.get(\n\"include_changelog_tech_cols\",\n(\nFalse\nif self._input_spec.options[\"adso_type\"] == ADSOTypes.AQ.value\nelse True\n),\n),\n)\nreturn SAPB4ExtractionUtils(jdbc_extraction)\ndef _get_options(self) -&gt; Tuple[dict, dict]:\n\"\"\"Get Spark Options using JDBC utilities.\n        Returns:\n            A tuple dict containing the options args and\n            jdbc args to be passed to Spark.\n        \"\"\"\nself._LOGGER.info(\nf\"Initial options passed to the SAP B4 Reader: {self._input_spec.options}\"\n)\noptions_args, jdbc_args = self.jdbc_utils.get_spark_jdbc_options()\nif self._input_spec.generate_predicates or self._input_spec.options.get(\n\"predicates\", None\n):\noptions_args.update(\nself.jdbc_utils.get_additional_spark_options(\nself._input_spec,\noptions_args,\n[\"partitionColumn\", \"numPartitions\", \"lowerBound\", \"upperBound\"],\n)\n)\nelse:\nif self._input_spec.calculate_upper_bound:\noptions_args[\"upperBound\"] = (\nself.jdbc_utils.get_spark_jdbc_optimal_upper_bound()\n)\noptions_args.update(\nself.jdbc_utils.get_additional_spark_options(\nself._input_spec, options_args\n)\n)\nself._LOGGER.info(\nf\"Final options to fill SAP B4 Reader Options: {options_args}\"\n)\nself._LOGGER.info(f\"Final jdbc args to fill SAP B4 Reader JDBC: {jdbc_args}\")\nreturn options_args, jdbc_args\n</code></pre>"},{"location":"reference/packages/io/readers/sap_b4_reader.html#packages.io.readers.sap_b4_reader.SAPB4Reader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct SAPB4Reader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sap_b4_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct SAPB4Reader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\nself.jdbc_utils = self._get_jdbc_utils()\n</code></pre>"},{"location":"reference/packages/io/readers/sap_b4_reader.html#packages.io.readers.sap_b4_reader.SAPB4Reader.read","title":"<code>read()</code>","text":"<p>Read data from SAP B4 source.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from the SAP B4 source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sap_b4_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from SAP B4 source.\n    Returns:\n        A dataframe containing the data from the SAP B4 source.\n    \"\"\"\noptions_args, jdbc_args = self._get_options()\nreturn ExecEnv.SESSION.read.options(**options_args).jdbc(**jdbc_args)\n</code></pre>"},{"location":"reference/packages/io/readers/sap_bw_reader.html","title":"Sap bw reader","text":"<p>Module to define behaviour to read from SAP BW sources.</p>"},{"location":"reference/packages/io/readers/sap_bw_reader.html#packages.io.readers.sap_bw_reader.SAPBWReader","title":"<code>SAPBWReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read from SAP BW source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sap_bw_reader.py</code> <pre><code>class SAPBWReader(Reader):\n\"\"\"Class to read from SAP BW source.\"\"\"\n_LOGGER: Logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct SAPBWReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\nself.jdbc_utils = self._get_jdbc_utils()\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from SAP BW source.\n        Returns:\n            A dataframe containing the data from the SAP BW source.\n        \"\"\"\noptions_args, jdbc_args = self._get_options()\nreturn ExecEnv.SESSION.read.options(**options_args).jdbc(**jdbc_args)\ndef _get_jdbc_utils(self) -&gt; SAPBWExtractionUtils:\njdbc_extraction = SAPBWExtraction(\nuser=self._input_spec.options[\"user\"],\npassword=self._input_spec.options[\"password\"],\nurl=self._input_spec.options[\"url\"],\ndbtable=self._input_spec.options[\"dbtable\"],\nlatest_timestamp_data_location=self._input_spec.options.get(\n\"latest_timestamp_data_location\",\nSAPBWExtraction.latest_timestamp_data_location,\n),\nlatest_timestamp_input_col=self._input_spec.options.get(\n\"latest_timestamp_input_col\", SAPBWExtraction.latest_timestamp_input_col\n),\nlatest_timestamp_data_format=self._input_spec.options.get(\n\"latest_timestamp_data_format\",\nSAPBWExtraction.latest_timestamp_data_format,\n),\nextraction_type=self._input_spec.options.get(\n\"extraction_type\", SAPBWExtraction.extraction_type\n),\nact_request_table=self._input_spec.options.get(\n\"act_request_table\", SAPBWExtraction.act_request_table\n),\nrequest_col_name=self._input_spec.options.get(\n\"request_col_name\", SAPBWExtraction.request_col_name\n),\nact_req_join_condition=self._input_spec.options.get(\n\"act_req_join_condition\", SAPBWExtraction.act_req_join_condition\n),\ndriver=self._input_spec.options.get(\"driver\", SAPBWExtraction.driver),\nchangelog_table=self._input_spec.options.get(\n\"changelog_table\", SAPBWExtraction.changelog_table\n),\nnum_partitions=self._input_spec.options.get(\n\"numPartitions\", SAPBWExtraction.num_partitions\n),\npartition_column=self._input_spec.options.get(\n\"partitionColumn\", SAPBWExtraction.partition_column\n),\nlower_bound=self._input_spec.options.get(\n\"lowerBound\", SAPBWExtraction.lower_bound\n),\nupper_bound=self._input_spec.options.get(\n\"upperBound\", SAPBWExtraction.upper_bound\n),\ndefault_upper_bound=self._input_spec.options.get(\n\"default_upper_bound\", SAPBWExtraction.default_upper_bound\n),\nfetch_size=self._input_spec.options.get(\n\"fetchSize\", SAPBWExtraction.fetch_size\n),\ncompress=self._input_spec.options.get(\"compress\", SAPBWExtraction.compress),\ncustom_schema=self._input_spec.options.get(\n\"customSchema\", SAPBWExtraction.custom_schema\n),\nextraction_timestamp=self._input_spec.options.get(\n\"extraction_timestamp\",\nSAPBWExtraction.extraction_timestamp,\n),\nodsobject=self._input_spec.options.get(\n\"odsobject\",\nSAPBWExtractionUtils.get_odsobject(self._input_spec.options),\n),\nmin_timestamp=self._input_spec.options.get(\n\"min_timestamp\", SAPBWExtraction.min_timestamp\n),\nmax_timestamp=self._input_spec.options.get(\n\"max_timestamp\", SAPBWExtraction.max_timestamp\n),\ndefault_max_timestamp=self._input_spec.options.get(\n\"default_max_timestamp\", SAPBWExtraction.default_max_timestamp\n),\nmax_timestamp_custom_schema=self._input_spec.options.get(\n\"max_timestamp_custom_schema\",\nSAPBWExtraction.max_timestamp_custom_schema,\n),\ninclude_changelog_tech_cols=self._input_spec.options.get(\n\"include_changelog_tech_cols\",\nSAPBWExtraction.include_changelog_tech_cols,\n),\ngenerate_predicates=self._input_spec.generate_predicates,\npredicates=self._input_spec.options.get(\n\"predicates\", SAPBWExtraction.predicates\n),\npredicates_add_null=self._input_spec.predicates_add_null,\nextra_cols_act_request=self._input_spec.options.get(\n\"extra_cols_act_request\", SAPBWExtraction.extra_cols_act_request\n),\nget_timestamp_from_act_request=self._input_spec.options.get(\n\"get_timestamp_from_act_request\",\nSAPBWExtraction.get_timestamp_from_act_request,\n),\ncalc_upper_bound_schema=self._input_spec.calc_upper_bound_schema,\n)\nreturn SAPBWExtractionUtils(jdbc_extraction)\ndef _get_options(self) -&gt; Tuple[dict, dict]:\n\"\"\"Get Spark Options using JDBC utilities.\n        Returns:\n            A tuple dict containing the options args and\n            jdbc args to be passed to Spark.\n        \"\"\"\nself._LOGGER.info(\nf\"Initial options passed to the SAP BW Reader: {self._input_spec.options}\"\n)\noptions_args, jdbc_args = self.jdbc_utils.get_spark_jdbc_options()\nif self._input_spec.generate_predicates or self._input_spec.options.get(\n\"predicates\", None\n):\noptions_args.update(\nself.jdbc_utils.get_additional_spark_options(\nself._input_spec,\noptions_args,\n[\"partitionColumn\", \"numPartitions\", \"lowerBound\", \"upperBound\"],\n)\n)\nelse:\nif self._input_spec.calculate_upper_bound:\noptions_args[\"upperBound\"] = (\nself.jdbc_utils.get_spark_jdbc_optimal_upper_bound()\n)\noptions_args.update(\nself.jdbc_utils.get_additional_spark_options(\nself._input_spec, options_args\n)\n)\nself._LOGGER.info(\nf\"Final options to fill SAP BW Reader Options: {options_args}\"\n)\nself._LOGGER.info(f\"Final jdbc args to fill SAP BW Reader JDBC: {jdbc_args}\")\nreturn options_args, jdbc_args\n</code></pre>"},{"location":"reference/packages/io/readers/sap_bw_reader.html#packages.io.readers.sap_bw_reader.SAPBWReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct SAPBWReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sap_bw_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct SAPBWReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\nself.jdbc_utils = self._get_jdbc_utils()\n</code></pre>"},{"location":"reference/packages/io/readers/sap_bw_reader.html#packages.io.readers.sap_bw_reader.SAPBWReader.read","title":"<code>read()</code>","text":"<p>Read data from SAP BW source.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from the SAP BW source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sap_bw_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from SAP BW source.\n    Returns:\n        A dataframe containing the data from the SAP BW source.\n    \"\"\"\noptions_args, jdbc_args = self._get_options()\nreturn ExecEnv.SESSION.read.options(**options_args).jdbc(**jdbc_args)\n</code></pre>"},{"location":"reference/packages/io/readers/sftp_reader.html","title":"Sftp reader","text":"<p>Module to define behaviour to read from SFTP.</p>"},{"location":"reference/packages/io/readers/sftp_reader.html#packages.io.readers.sftp_reader.SFTPReader","title":"<code>SFTPReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read from SFTP.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sftp_reader.py</code> <pre><code>class SFTPReader(Reader):\n\"\"\"Class to read from SFTP.\"\"\"\n_logger: Logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct SFTPReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read SFTP data.\n        Returns:\n            A dataframe containing the data from SFTP.\n        \"\"\"\nif self._input_spec.read_type == ReadType.BATCH.value:\noptions_args = self._input_spec.options if self._input_spec.options else {}\nsftp_files_format = SFTPExtractionUtils.validate_format(\nself._input_spec.sftp_files_format.lower()\n)\nlocation = SFTPExtractionUtils.validate_location(self._input_spec.location)\nsftp, transport = SFTPExtractionUtils.get_sftp_client(options_args)\nfiles_list = SFTPExtractionUtils.get_files_list(\nsftp, location, options_args\n)\ndfs: List[PandasDataFrame] = []\ntry:\nfor filename in files_list:\nwith sftp.open(filename, \"r\") as sftp_file:\ntry:\npdf = self._read_files(\nfilename,\nsftp_file,\noptions_args.get(\"args\", {}),\nsftp_files_format,\n)\nif options_args.get(\"file_metadata\", None):\npdf[\"filename\"] = filename\npdf[\"modification_time\"] = datetime.fromtimestamp(\nsftp.stat(filename).st_mtime\n)\nself._append_files(pdf, dfs)\nexcept EmptyDataError:\nself._logger.info(f\"{filename} - Empty or malformed file.\")\nif dfs:\ndf = ExecEnv.SESSION.createDataFrame(pd.concat(dfs))\nelse:\nraise ValueError(\n\"No files were found with the specified parameters.\"\n)\nfinally:\nsftp.close()\ntransport.close()\nelse:\nraise NotImplementedError(\n\"The requested read type supports only BATCH mode.\"\n)\nreturn df\n@classmethod\ndef _append_files(cls, pdf: PandasDataFrame, dfs: List) -&gt; List:\n\"\"\"Append to the list dataframes with data.\n        Args:\n            pdf: a Pandas dataframe containing data from files.\n            dfs: a list of Pandas dataframes.\n        Returns:\n            A list of not empty Pandas dataframes.\n        \"\"\"\nif not pdf.empty:\ndfs.append(pdf)\nreturn dfs\n@classmethod\ndef _read_files(\ncls, filename: str, sftp_file: SFTPFile, option_args: dict, files_format: str\n) -&gt; PandasDataFrame:\n\"\"\"Open and decompress files to be extracted from SFTP.\n        For zip files, to avoid data type inferred issues\n        during the iteration, all data will be read as string.\n        Also, empty dataframes will NOT be considered to be processed.\n        For the not considered ones, the file names will be logged.\n        Args:\n            filename: the filename to be read.\n            sftp_file: SFTPFile object representing the open file.\n            option_args: options from the acon.\n            files_format: a string containing the file extension.\n        Returns:\n            A pandas dataframe with data from the file.\n        \"\"\"\nreader = getattr(pd, f\"read_{files_format}\")\nif filename.endswith(\".gz\"):\nwith gzip.GzipFile(fileobj=sftp_file, mode=\"rb\") as gz_file:\npdf = reader(\nTextIOWrapper(gz_file),  # type: ignore\n**option_args,\n)\nelif filename.endswith(\".zip\"):\nwith ZipFile(sftp_file, \"r\") as zf:  # type: ignore\ndfs = [\nreader(TextIOWrapper(zf.open(f)), **option_args).fillna(\"\")\nfor f in zf.namelist()\n]\nif not pd.concat(dfs, ignore_index=True).empty:\npdf = pd.concat(dfs, ignore_index=True).astype(str)\nelse:\npdf = pd.DataFrame()\ncls._logger.info(f\"{filename} - Empty or malformed file.\")\nelse:\npdf = reader(\nsftp_file,\n**option_args,\n)\nreturn pdf\n</code></pre>"},{"location":"reference/packages/io/readers/sftp_reader.html#packages.io.readers.sftp_reader.SFTPReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct SFTPReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sftp_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct SFTPReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/sftp_reader.html#packages.io.readers.sftp_reader.SFTPReader.read","title":"<code>read()</code>","text":"<p>Read SFTP data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from SFTP.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/sftp_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read SFTP data.\n    Returns:\n        A dataframe containing the data from SFTP.\n    \"\"\"\nif self._input_spec.read_type == ReadType.BATCH.value:\noptions_args = self._input_spec.options if self._input_spec.options else {}\nsftp_files_format = SFTPExtractionUtils.validate_format(\nself._input_spec.sftp_files_format.lower()\n)\nlocation = SFTPExtractionUtils.validate_location(self._input_spec.location)\nsftp, transport = SFTPExtractionUtils.get_sftp_client(options_args)\nfiles_list = SFTPExtractionUtils.get_files_list(\nsftp, location, options_args\n)\ndfs: List[PandasDataFrame] = []\ntry:\nfor filename in files_list:\nwith sftp.open(filename, \"r\") as sftp_file:\ntry:\npdf = self._read_files(\nfilename,\nsftp_file,\noptions_args.get(\"args\", {}),\nsftp_files_format,\n)\nif options_args.get(\"file_metadata\", None):\npdf[\"filename\"] = filename\npdf[\"modification_time\"] = datetime.fromtimestamp(\nsftp.stat(filename).st_mtime\n)\nself._append_files(pdf, dfs)\nexcept EmptyDataError:\nself._logger.info(f\"{filename} - Empty or malformed file.\")\nif dfs:\ndf = ExecEnv.SESSION.createDataFrame(pd.concat(dfs))\nelse:\nraise ValueError(\n\"No files were found with the specified parameters.\"\n)\nfinally:\nsftp.close()\ntransport.close()\nelse:\nraise NotImplementedError(\n\"The requested read type supports only BATCH mode.\"\n)\nreturn df\n</code></pre>"},{"location":"reference/packages/io/readers/table_reader.html","title":"Table reader","text":"<p>Module to define behaviour to read from tables.</p>"},{"location":"reference/packages/io/readers/table_reader.html#packages.io.readers.table_reader.TableReader","title":"<code>TableReader</code>","text":"<p>         Bases: <code>Reader</code></p> <p>Class to read data from a table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/table_reader.py</code> <pre><code>class TableReader(Reader):\n\"\"\"Class to read data from a table.\"\"\"\ndef __init__(self, input_spec: InputSpec):\n\"\"\"Construct TableReader instances.\n        Args:\n            input_spec: input specification.\n        \"\"\"\nsuper().__init__(input_spec)\ndef read(self) -&gt; DataFrame:\n\"\"\"Read data from a table.\n        Returns:\n            A dataframe containing the data from the table.\n        \"\"\"\nif self._input_spec.read_type == ReadType.BATCH.value:\nreturn ExecEnv.SESSION.read.options(\n**self._input_spec.options if self._input_spec.options else {}\n).table(self._input_spec.db_table)\nelif self._input_spec.read_type == ReadType.STREAMING.value:\nreturn ExecEnv.SESSION.readStream.options(\n**self._input_spec.options if self._input_spec.options else {}\n).table(self._input_spec.db_table)\nelse:\nself._logger.error(\"The requested read type is not supported.\")\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/io/readers/table_reader.html#packages.io.readers.table_reader.TableReader.__init__","title":"<code>__init__(input_spec)</code>","text":"<p>Construct TableReader instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/table_reader.py</code> <pre><code>def __init__(self, input_spec: InputSpec):\n\"\"\"Construct TableReader instances.\n    Args:\n        input_spec: input specification.\n    \"\"\"\nsuper().__init__(input_spec)\n</code></pre>"},{"location":"reference/packages/io/readers/table_reader.html#packages.io.readers.table_reader.TableReader.read","title":"<code>read()</code>","text":"<p>Read data from a table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the data from the table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/readers/table_reader.py</code> <pre><code>def read(self) -&gt; DataFrame:\n\"\"\"Read data from a table.\n    Returns:\n        A dataframe containing the data from the table.\n    \"\"\"\nif self._input_spec.read_type == ReadType.BATCH.value:\nreturn ExecEnv.SESSION.read.options(\n**self._input_spec.options if self._input_spec.options else {}\n).table(self._input_spec.db_table)\nelif self._input_spec.read_type == ReadType.STREAMING.value:\nreturn ExecEnv.SESSION.readStream.options(\n**self._input_spec.options if self._input_spec.options else {}\n).table(self._input_spec.db_table)\nelse:\nself._logger.error(\"The requested read type is not supported.\")\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/io/writers/index.html","title":"Writers","text":"<p>Package containing the writers responsible for writing data.</p>"},{"location":"reference/packages/io/writers/console_writer.html","title":"Console writer","text":"<p>Module to define behaviour to write to console.</p>"},{"location":"reference/packages/io/writers/console_writer.html#packages.io.writers.console_writer.ConsoleWriter","title":"<code>ConsoleWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write data to console.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/console_writer.py</code> <pre><code>class ConsoleWriter(Writer):\n\"\"\"Class to write data to console.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct ConsoleWriter instances.\n        Args:\n            output_spec: output specification\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Write data to console.\"\"\"\nself._output_spec.options = (\nself._output_spec.options if self._output_spec.options else {}\n)\nif not self._df.isStreaming:\nself._logger.info(\"Dataframe preview:\")\nself._show_df(self._df, self._output_spec)\nelse:\nself._logger.info(\"Stream Dataframe preview:\")\nself._write_to_console_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n@staticmethod\ndef _show_df(df: DataFrame, output_spec: OutputSpec) -&gt; None:\n\"\"\"Given a dataframe it applies Spark's show function to show it.\n        Args:\n            df: dataframe to be shown.\n            output_spec: output specification.\n        \"\"\"\ndf.show(\nn=output_spec.options.get(\"limit\", 20),\ntruncate=output_spec.options.get(\"truncate\", True),\nvertical=output_spec.options.get(\"vertical\", False),\n)\n@staticmethod\ndef _show_streaming_df(output_spec: OutputSpec) -&gt; Callable:\n\"\"\"Define how to show a streaming df.\n        Args:\n            output_spec: output specification.\n        Returns:\n            A function to show df in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\nConsoleWriter._logger.info(f\"Showing DF for batch {batch_id}\")\nConsoleWriter._show_df(batch_df, output_spec)\nreturn inner\n@staticmethod\ndef _write_to_console_in_streaming_mode(\ndf: DataFrame, output_spec: OutputSpec, data: OrderedDict\n) -&gt; None:\n\"\"\"Write to console in streaming mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\ndf_writer = df.writeStream.trigger(**Writer.get_streaming_trigger(output_spec))\nif (\noutput_spec.streaming_micro_batch_transformers\nor output_spec.streaming_micro_batch_dq_processors\n):\nstream_df = df_writer.foreachBatch(\nConsoleWriter._write_transformed_micro_batch(output_spec, data)\n).start()\nelse:\nstream_df = df_writer.foreachBatch(\nConsoleWriter._show_streaming_df(output_spec)\n).start()\nif output_spec.streaming_await_termination:\nstream_df.awaitTermination(output_spec.streaming_await_termination_timeout)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nConsoleWriter._show_df(transformed_df, output_spec)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/console_writer.html#packages.io.writers.console_writer.ConsoleWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct ConsoleWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/console_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct ConsoleWriter instances.\n    Args:\n        output_spec: output specification\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/console_writer.html#packages.io.writers.console_writer.ConsoleWriter.write","title":"<code>write()</code>","text":"<p>Write data to console.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/console_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Write data to console.\"\"\"\nself._output_spec.options = (\nself._output_spec.options if self._output_spec.options else {}\n)\nif not self._df.isStreaming:\nself._logger.info(\"Dataframe preview:\")\nself._show_df(self._df, self._output_spec)\nelse:\nself._logger.info(\"Stream Dataframe preview:\")\nself._write_to_console_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n</code></pre>"},{"location":"reference/packages/io/writers/dataframe_writer.html","title":"Dataframe writer","text":"<p>Module to define behaviour to write to dataframe.</p>"},{"location":"reference/packages/io/writers/dataframe_writer.html#packages.io.writers.dataframe_writer.DataFrameWriter","title":"<code>DataFrameWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write data to dataframe.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/dataframe_writer.py</code> <pre><code>class DataFrameWriter(Writer):\n\"\"\"Class to write data to dataframe.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct DataFrameWriter instances.\n        Args:\n            output_spec: output specification.\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; Optional[OrderedDict]:\n\"\"\"Write data to dataframe.\"\"\"\nself._output_spec.options = (\nself._output_spec.options if self._output_spec.options else {}\n)\nwritten_dfs: OrderedDict = OrderedDict({})\nif (\nself._output_spec.streaming_processing_time\nor self._output_spec.streaming_continuous\n):\nraise NotSupportedException(\nf\"DataFrame writer doesn't support \"\nf\"processing time or continuous streaming \"\nf\"for step ${self._output_spec.spec_id}.\"\n)\nif self._df.isStreaming:\noutput_df = self._write_to_dataframe_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\nelse:\noutput_df = self._df\nwritten_dfs[self._output_spec.spec_id] = output_df\nreturn written_dfs\n@staticmethod\ndef _create_global_view(df: DataFrame, stream_df_view_name: str) -&gt; None:\n\"\"\"Given a dataframe create a global temp view to be available for consumption.\n        Args:\n            df: dataframe to be shown.\n            stream_df_view_name: stream df view name.\n        \"\"\"\nif DataFrameWriter._table_exists(stream_df_view_name):\nDataFrameWriter._logger.info(\"Global temp view exists\")\nexisting_data = ExecEnv.SESSION.table(f\"global_temp.{stream_df_view_name}\")\ndf = existing_data.union(df)\ndf.createOrReplaceGlobalTempView(f\"{stream_df_view_name}\")\n@staticmethod\ndef _write_streaming_df(stream_df_view_name: str) -&gt; Callable:\n\"\"\"Define how to create a df from streaming df.\n        Args:\n            stream_df_view_name: stream df view name.\n        Returns:\n            A function to show df in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\nDataFrameWriter._create_global_view(batch_df, stream_df_view_name)\nreturn inner\n@staticmethod\ndef _write_to_dataframe_in_streaming_mode(\ndf: DataFrame, output_spec: OutputSpec, data: OrderedDict\n) -&gt; DataFrame:\n\"\"\"Write to DataFrame in streaming mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\napp_id = ExecEnv.SESSION.getActiveSession().conf.get(\"spark.app.id\")\nstream_df_view_name = f\"`{app_id}_{output_spec.spec_id}`\"\nDataFrameWriter._logger.info(\"Drop temp view if exists\")\nif DataFrameWriter._table_exists(stream_df_view_name):\n# Cleaning global temp view to not maintain state and impact other acon runs\nview_name = stream_df_view_name.strip(\"`\")\nExecEnv.SESSION.sql(f\"DROP VIEW global_temp.`{view_name}`\")\ndf_writer = df.writeStream.trigger(**Writer.get_streaming_trigger(output_spec))\nif (\noutput_spec.streaming_micro_batch_transformers\nor output_spec.streaming_micro_batch_dq_processors\n):\nstream_df = (\ndf_writer.options(**output_spec.options if output_spec.options else {})\n.format(OutputFormat.NOOP.value)\n.foreachBatch(\nDataFrameWriter._write_transformed_micro_batch(\noutput_spec, data, stream_df_view_name\n)\n)\n.start()\n)\nelse:\nstream_df = (\ndf_writer.options(**output_spec.options if output_spec.options else {})\n.format(OutputFormat.NOOP.value)\n.foreachBatch(DataFrameWriter._write_streaming_df(stream_df_view_name))\n.start()\n)\nif output_spec.streaming_await_termination:\nstream_df.awaitTermination(output_spec.streaming_await_termination_timeout)\nDataFrameWriter._logger.info(\"Reading stream data as df if exists\")\nif DataFrameWriter._table_exists(stream_df_view_name):\nstream_data_as_df = ExecEnv.SESSION.table(\nf\"global_temp.{stream_df_view_name}\"\n)\nelse:\nDataFrameWriter._logger.info(\nf\"DataFrame writer couldn't find any data to return \"\nf\"for streaming, check if you are using checkpoint \"\nf\"for step {output_spec.spec_id}.\"\n)\nstream_data_as_df = ExecEnv.SESSION.createDataFrame(\ndata=[], schema=StructType([])\n)\nreturn stream_data_as_df\n@staticmethod\ndef _table_exists(  # type: ignore\ntable_name: str, db_name: str = \"global_temp\"\n) -&gt; bool:\n\"\"\"Check if the table exists in the session catalog.\n        Args:\n            table_name: table/view name to check if exists in the session.\n            db_name: database name that you want to check if the table/view exists,\n                default value is the global_temp.\n        Returns:\n            A bool representing if the table/view exists.\n        \"\"\"\nExecEnv.get_or_create()\ntable_name = table_name.strip(\"`\")\nreturn (\nlen(\nExecEnv.SESSION.sql(f\"SHOW  TABLES  IN `{db_name}`\")\n.filter(f\"tableName = '{table_name}'\")\n.collect()\n)\n&gt; 0\n)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict, stream_as_df_view\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n            stream_as_df_view: stream df view name.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nDataFrameWriter._create_global_view(transformed_df, stream_as_df_view)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/dataframe_writer.html#packages.io.writers.dataframe_writer.DataFrameWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct DataFrameWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/dataframe_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct DataFrameWriter instances.\n    Args:\n        output_spec: output specification.\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/dataframe_writer.html#packages.io.writers.dataframe_writer.DataFrameWriter.write","title":"<code>write()</code>","text":"<p>Write data to dataframe.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/dataframe_writer.py</code> <pre><code>def write(self) -&gt; Optional[OrderedDict]:\n\"\"\"Write data to dataframe.\"\"\"\nself._output_spec.options = (\nself._output_spec.options if self._output_spec.options else {}\n)\nwritten_dfs: OrderedDict = OrderedDict({})\nif (\nself._output_spec.streaming_processing_time\nor self._output_spec.streaming_continuous\n):\nraise NotSupportedException(\nf\"DataFrame writer doesn't support \"\nf\"processing time or continuous streaming \"\nf\"for step ${self._output_spec.spec_id}.\"\n)\nif self._df.isStreaming:\noutput_df = self._write_to_dataframe_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\nelse:\noutput_df = self._df\nwritten_dfs[self._output_spec.spec_id] = output_df\nreturn written_dfs\n</code></pre>"},{"location":"reference/packages/io/writers/delta_merge_writer.html","title":"Delta merge writer","text":"<p>Module to define the behaviour of delta merges.</p>"},{"location":"reference/packages/io/writers/delta_merge_writer.html#packages.io.writers.delta_merge_writer.DeltaMergeWriter","title":"<code>DeltaMergeWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to merge data using delta lake.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/delta_merge_writer.py</code> <pre><code>class DeltaMergeWriter(Writer):\n\"\"\"Class to merge data using delta lake.\"\"\"\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct DeltaMergeWriter instances.\n        Args:\n            output_spec: output specification containing merge options and\n                relevant information.\n            df: the dataframe containing the new data to be merged.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Merge new data with current data.\"\"\"\ndelta_table = self._get_delta_table(self._output_spec)\nif self._df.isStreaming:\nstream_df = (\nself._df.writeStream.options(\n**self._output_spec.options if self._output_spec.options else {}\n)\n.foreachBatch(\nself._write_transformed_micro_batch(\nself._output_spec, self._data, delta_table\n)\n)\n.trigger(**Writer.get_streaming_trigger(self._output_spec))\n.start()\n)\nif self._output_spec.streaming_await_termination:\nstream_df.awaitTermination(\nself._output_spec.streaming_await_termination_timeout\n)\nelse:\nDeltaMergeWriter._merge(delta_table, self._output_spec, self._df)\n@staticmethod\ndef _get_delta_table(output_spec: OutputSpec) -&gt; DeltaTable:\n\"\"\"Get the delta table given an output specification w/ table name or location.\n        Args:\n            output_spec: output specification.\n        Returns:\n            DeltaTable: the delta table instance.\n        \"\"\"\nif output_spec.db_table:\ndelta_table = DeltaTable.forName(ExecEnv.SESSION, output_spec.db_table)\nelif output_spec.data_format == OutputFormat.DELTAFILES.value:\ndelta_table = DeltaTable.forPath(ExecEnv.SESSION, output_spec.location)\nelse:\nraise WrongIOFormatException(\nf\"{output_spec.data_format} is not compatible with Delta Merge \"\nf\"Writer.\"\n)\nreturn delta_table\n@staticmethod\ndef _insert(\ndelta_merge: DeltaMergeBuilder,\ninsert_predicate: Optional[str],\ninsert_column_set: Optional[dict],\n) -&gt; DeltaMergeBuilder:\n\"\"\"Get the builder of merge data with insert predicate and column set.\n        Args:\n            delta_merge: builder of the merge data.\n            insert_predicate: condition of the insert.\n            insert_column_set: rules for setting the values of\n                columns that need to be inserted.\n        Returns:\n            DeltaMergeBuilder: builder of the merge data with insert.\n        \"\"\"\nif insert_predicate:\nif insert_column_set:\ndelta_merge = delta_merge.whenNotMatchedInsert(\ncondition=insert_predicate,\nvalues=insert_column_set,\n)\nelse:\ndelta_merge = delta_merge.whenNotMatchedInsertAll(\ncondition=insert_predicate\n)\nelse:\nif insert_column_set:\ndelta_merge = delta_merge.whenNotMatchedInsert(values=insert_column_set)\nelse:\ndelta_merge = delta_merge.whenNotMatchedInsertAll()\nreturn delta_merge\n@staticmethod\ndef _merge(delta_table: DeltaTable, output_spec: OutputSpec, df: DataFrame) -&gt; None:\n\"\"\"Perform a delta lake merge according to several merge options.\n        Args:\n            delta_table: delta table to which to merge data.\n            output_spec: output specification containing the merge options.\n            df: dataframe with the new data to be merged into the delta table.\n        \"\"\"\ndelta_merge = delta_table.alias(\"current\").merge(\ndf.alias(\"new\"), output_spec.merge_opts.merge_predicate\n)\nif not output_spec.merge_opts.insert_only:\nif output_spec.merge_opts.delete_predicate:\ndelta_merge = delta_merge.whenMatchedDelete(\noutput_spec.merge_opts.delete_predicate\n)\ndelta_merge = DeltaMergeWriter._update(\ndelta_merge,\noutput_spec.merge_opts.update_predicate,\noutput_spec.merge_opts.update_column_set,\n)\ndelta_merge = DeltaMergeWriter._insert(\ndelta_merge,\noutput_spec.merge_opts.insert_predicate,\noutput_spec.merge_opts.insert_column_set,\n)\ndelta_merge.execute()\n@staticmethod\ndef _update(\ndelta_merge: DeltaMergeBuilder,\nupdate_predicate: Optional[str],\nupdate_column_set: Optional[dict],\n) -&gt; DeltaMergeBuilder:\n\"\"\"Get the builder of merge data with update predicate and column set.\n        Args:\n            delta_merge: builder of the merge data.\n            update_predicate: condition of the update.\n            update_column_set: rules for setting the values of\n                columns that need to be updated.\n        Returns:\n            DeltaMergeBuilder: builder of the merge data with update.\n        \"\"\"\nif update_predicate:\nif update_column_set:\ndelta_merge = delta_merge.whenMatchedUpdate(\ncondition=update_predicate,\nset=update_column_set,\n)\nelse:\ndelta_merge = delta_merge.whenMatchedUpdateAll(\ncondition=update_predicate\n)\nelse:\nif update_column_set:\ndelta_merge = delta_merge.whenMatchedUpdate(set=update_column_set)\nelse:\ndelta_merge = delta_merge.whenMatchedUpdateAll()\nreturn delta_merge\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec,\ndata: OrderedDict,\ndelta_table: Optional[DeltaTable] = None,\n) -&gt; Callable:\n\"\"\"Perform the merge in streaming mode by specifying a transform function.\n        This function returns a function that will be invoked in the foreachBatch in\n        streaming mode, performing a delta lake merge while streaming the micro batches.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n            delta_table: delta table for which to merge the streaming data\n                with.\n        Returns:\n            Function to call in .foreachBatch streaming function.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nDeltaMergeWriter._merge(delta_table, output_spec, transformed_df)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/delta_merge_writer.html#packages.io.writers.delta_merge_writer.DeltaMergeWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct DeltaMergeWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification containing merge options and relevant information.</p> required <code>df</code> <code>DataFrame</code> <p>the dataframe containing the new data to be merged.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/delta_merge_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct DeltaMergeWriter instances.\n    Args:\n        output_spec: output specification containing merge options and\n            relevant information.\n        df: the dataframe containing the new data to be merged.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/delta_merge_writer.html#packages.io.writers.delta_merge_writer.DeltaMergeWriter.write","title":"<code>write()</code>","text":"<p>Merge new data with current data.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/delta_merge_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Merge new data with current data.\"\"\"\ndelta_table = self._get_delta_table(self._output_spec)\nif self._df.isStreaming:\nstream_df = (\nself._df.writeStream.options(\n**self._output_spec.options if self._output_spec.options else {}\n)\n.foreachBatch(\nself._write_transformed_micro_batch(\nself._output_spec, self._data, delta_table\n)\n)\n.trigger(**Writer.get_streaming_trigger(self._output_spec))\n.start()\n)\nif self._output_spec.streaming_await_termination:\nstream_df.awaitTermination(\nself._output_spec.streaming_await_termination_timeout\n)\nelse:\nDeltaMergeWriter._merge(delta_table, self._output_spec, self._df)\n</code></pre>"},{"location":"reference/packages/io/writers/file_writer.html","title":"File writer","text":"<p>Module to define behaviour to write to files.</p>"},{"location":"reference/packages/io/writers/file_writer.html#packages.io.writers.file_writer.FileWriter","title":"<code>FileWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write data to files.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/file_writer.py</code> <pre><code>class FileWriter(Writer):\n\"\"\"Class to write data to files.\"\"\"\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct FileWriter instances.\n        Args:\n            output_spec: output specification\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Write data to files.\"\"\"\nif not self._df.isStreaming:\nself._write_to_files_in_batch_mode(self._df, self._output_spec)\nelse:\nself._write_to_files_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n@staticmethod\ndef _write_to_files_in_batch_mode(df: DataFrame, output_spec: OutputSpec) -&gt; None:\n\"\"\"Write to files in batch mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n        \"\"\"\ndf.write.format(output_spec.data_format).partitionBy(\noutput_spec.partitions\n).options(**output_spec.options if output_spec.options else {}).mode(\noutput_spec.write_type\n).save(\noutput_spec.location\n)\n@staticmethod\ndef _write_to_files_in_streaming_mode(\ndf: DataFrame, output_spec: OutputSpec, data: OrderedDict\n) -&gt; None:\n\"\"\"Write to files in streaming mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\ndf_writer = df.writeStream.trigger(**Writer.get_streaming_trigger(output_spec))\nif (\noutput_spec.streaming_micro_batch_transformers\nor output_spec.streaming_micro_batch_dq_processors\n):\nstream_df = (\ndf_writer.options(**output_spec.options if output_spec.options else {})\n.foreachBatch(\nFileWriter._write_transformed_micro_batch(output_spec, data)\n)\n.start()\n)\nelse:\nstream_df = (\ndf_writer.format(output_spec.data_format)\n.partitionBy(output_spec.partitions)\n.options(**output_spec.options if output_spec.options else {})\n.outputMode(output_spec.write_type)\n.start(output_spec.location)\n)\nif output_spec.streaming_await_termination:\nstream_df.awaitTermination(output_spec.streaming_await_termination_timeout)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nFileWriter._write_to_files_in_batch_mode(transformed_df, output_spec)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/file_writer.html#packages.io.writers.file_writer.FileWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct FileWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/file_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct FileWriter instances.\n    Args:\n        output_spec: output specification\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/file_writer.html#packages.io.writers.file_writer.FileWriter.write","title":"<code>write()</code>","text":"<p>Write data to files.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/file_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Write data to files.\"\"\"\nif not self._df.isStreaming:\nself._write_to_files_in_batch_mode(self._df, self._output_spec)\nelse:\nself._write_to_files_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n</code></pre>"},{"location":"reference/packages/io/writers/jdbc_writer.html","title":"Jdbc writer","text":"<p>Module that defines the behaviour to write to JDBC targets.</p>"},{"location":"reference/packages/io/writers/jdbc_writer.html#packages.io.writers.jdbc_writer.JDBCWriter","title":"<code>JDBCWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write to JDBC targets.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/jdbc_writer.py</code> <pre><code>class JDBCWriter(Writer):\n\"\"\"Class to write to JDBC targets.\"\"\"\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct JDBCWriter instances.\n        Args:\n            output_spec: output specification.\n            df: dataframe to be writen.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Write data into JDBC target.\"\"\"\nif not self._df.isStreaming:\nself._write_to_jdbc_in_batch_mode(self._df, self._output_spec)\nelse:\nstream_df = (\nself._df.writeStream.trigger(\n**Writer.get_streaming_trigger(self._output_spec)\n)\n.options(\n**self._output_spec.options if self._output_spec.options else {}\n)\n.foreachBatch(\nself._write_transformed_micro_batch(self._output_spec, self._data)\n)\n.start()\n)\nif self._output_spec.streaming_await_termination:\nstream_df.awaitTermination(\nself._output_spec.streaming_await_termination_timeout\n)\n@staticmethod\ndef _write_to_jdbc_in_batch_mode(df: DataFrame, output_spec: OutputSpec) -&gt; None:\n\"\"\"Write to jdbc in batch mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n        \"\"\"\ndf.write.format(output_spec.data_format).partitionBy(\noutput_spec.partitions\n).options(**output_spec.options if output_spec.options else {}).mode(\noutput_spec.write_type\n).save(\noutput_spec.location\n)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nJDBCWriter._write_to_jdbc_in_batch_mode(transformed_df, output_spec)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/jdbc_writer.html#packages.io.writers.jdbc_writer.JDBCWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct JDBCWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be writen.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/jdbc_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct JDBCWriter instances.\n    Args:\n        output_spec: output specification.\n        df: dataframe to be writen.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/jdbc_writer.html#packages.io.writers.jdbc_writer.JDBCWriter.write","title":"<code>write()</code>","text":"<p>Write data into JDBC target.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/jdbc_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Write data into JDBC target.\"\"\"\nif not self._df.isStreaming:\nself._write_to_jdbc_in_batch_mode(self._df, self._output_spec)\nelse:\nstream_df = (\nself._df.writeStream.trigger(\n**Writer.get_streaming_trigger(self._output_spec)\n)\n.options(\n**self._output_spec.options if self._output_spec.options else {}\n)\n.foreachBatch(\nself._write_transformed_micro_batch(self._output_spec, self._data)\n)\n.start()\n)\nif self._output_spec.streaming_await_termination:\nstream_df.awaitTermination(\nself._output_spec.streaming_await_termination_timeout\n)\n</code></pre>"},{"location":"reference/packages/io/writers/kafka_writer.html","title":"Kafka writer","text":"<p>Module that defines the behaviour to write to Kafka.</p>"},{"location":"reference/packages/io/writers/kafka_writer.html#packages.io.writers.kafka_writer.KafkaWriter","title":"<code>KafkaWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write to a Kafka target.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/kafka_writer.py</code> <pre><code>class KafkaWriter(Writer):\n\"\"\"Class to write to a Kafka target.\"\"\"\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct KafkaWriter instances.\n        Args:\n            output_spec: output specification.\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Write data to Kafka.\"\"\"\nif not self._df.isStreaming:\nself._write_to_kafka_in_batch_mode(self._df, self._output_spec)\nelse:\nself._write_to_kafka_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n@staticmethod\ndef _write_to_kafka_in_batch_mode(df: DataFrame, output_spec: OutputSpec) -&gt; None:\n\"\"\"Write to Kafka in batch mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n        \"\"\"\ndf.write.format(output_spec.data_format).options(\n**output_spec.options if output_spec.options else {}\n).mode(output_spec.write_type).save()\n@staticmethod\ndef _write_to_kafka_in_streaming_mode(\ndf: DataFrame, output_spec: OutputSpec, data: OrderedDict\n) -&gt; None:\n\"\"\"Write to kafka in streaming mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\ndf_writer = df.writeStream.trigger(**Writer.get_streaming_trigger(output_spec))\nif (\noutput_spec.streaming_micro_batch_transformers\nor output_spec.streaming_micro_batch_dq_processors\n):\nstream_df = (\ndf_writer.options(**output_spec.options if output_spec.options else {})\n.foreachBatch(\nKafkaWriter._write_transformed_micro_batch(output_spec, data)\n)\n.start()\n)\nelse:\nstream_df = (\ndf_writer.format(output_spec.data_format)\n.options(**output_spec.options if output_spec.options else {})\n.start()\n)\nif output_spec.streaming_await_termination:\nstream_df.awaitTermination(output_spec.streaming_await_termination_timeout)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nKafkaWriter._write_to_kafka_in_batch_mode(transformed_df, output_spec)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/kafka_writer.html#packages.io.writers.kafka_writer.KafkaWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct KafkaWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/kafka_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct KafkaWriter instances.\n    Args:\n        output_spec: output specification.\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/kafka_writer.html#packages.io.writers.kafka_writer.KafkaWriter.write","title":"<code>write()</code>","text":"<p>Write data to Kafka.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/kafka_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Write data to Kafka.\"\"\"\nif not self._df.isStreaming:\nself._write_to_kafka_in_batch_mode(self._df, self._output_spec)\nelse:\nself._write_to_kafka_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n</code></pre>"},{"location":"reference/packages/io/writers/rest_api_writer.html","title":"Rest api writer","text":"<p>Module to define behaviour to write to REST APIs.</p>"},{"location":"reference/packages/io/writers/rest_api_writer.html#packages.io.writers.rest_api_writer.RestApiWriter","title":"<code>RestApiWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write data to a REST API.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/rest_api_writer.py</code> <pre><code>class RestApiWriter(Writer):\n\"\"\"Class to write data to a REST API.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct RestApiWriter instances.\n        Args:\n            output_spec: output specification.\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Write data to REST API.\"\"\"\nif not self._df.isStreaming:\nself._write_to_rest_api_in_batch_mode(self._df, self._output_spec)\nelse:\nself._write_to_rest_api_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n@staticmethod\ndef _get_func_to_send_payload_to_rest_api(output_spec: OutputSpec) -&gt; Callable:\n\"\"\"Define and return a function to send the payload to the REST api.\n        Args:\n            output_spec: Output Specification containing configurations to\n                communicate with the REST api. Within the output_spec, the user\n                can specify several options:\n                    - rest_api_header: http headers.\n                    - rest_api_basic_auth: basic http authentication details\n                        (e.g., {\"username\": \"x\", \"password\": \"y\"}).\n                    - rest_api_url: url of the api.\n                    - rest_api_method: REST method (e.g., POST or PUT).\n                    - rest_api_sleep_seconds: sleep seconds to avoid throttling.\n                    - rest_api_is_file_payload: if the payload to be sent to the\n                        api is in the format of a file using multipart encoding\n                        upload. if this is true, then the payload will always be\n                        sent using the \"files\" parameter in Python's requests\n                        library.\n                    - rest_api_file_payload_name: when rest_api_is_file_payload\n                        is true, this option can be used to define the file\n                        identifier in Python's requests library.\n                    - extra_json_payload: when rest_api_file_payload_name is False,\n                        can be used to provide additional JSON variables to add to\n                        the original payload. This is useful to complement\n                        the original payload with some extra input to better\n                        configure the final payload to send to the REST api. An\n                        example can be to add a constant configuration value to\n                        add to the payload data.\n        Returns:\n            Function to be called inside Spark dataframe.foreach.\n        \"\"\"\nheaders = output_spec.options.get(\"rest_api_header\", None)\nbasic_auth_dict = output_spec.options.get(\"rest_api_basic_auth\", None)\nurl = output_spec.options[\"rest_api_url\"]\nmethod = output_spec.options.get(\"rest_api_method\", RestMethods.POST.value)\nsleep_seconds = output_spec.options.get(\"rest_api_sleep_seconds\", 0)\nis_file_payload = output_spec.options.get(\"rest_api_is_file_payload\", False)\nfile_payload_name = output_spec.options.get(\n\"rest_api_file_payload_name\", \"file\"\n)\nextra_json_payload = output_spec.options.get(\n\"rest_api_extra_json_payload\", None\n)\nsuccess_status_codes = output_spec.options.get(\n\"rest_api_success_status_codes\", RestStatusCodes.OK_STATUS_CODES.value\n)\ndef send_payload_to_rest_api(row: Row) -&gt; Any:\n\"\"\"Send payload to the REST API.\n            The payload needs to be prepared as a JSON string column in a dataframe.\n            E.g., {\"a\": \"a value\", \"b\": \"b value\"}.\n            Args:\n                row: a row in a dataframe.\n            \"\"\"\nif \"payload\" not in row:\nraise ValueError(\"Input DataFrame must contain 'payload' column.\")\nstr_payload = row.payload\npayload = None\nif not is_file_payload:\npayload = json.loads(str_payload)\nelse:\npayload = {file_payload_name: str_payload}\nif extra_json_payload:\npayload.update(extra_json_payload)\nRestApiWriter._logger.debug(f\"Original payload: {str_payload}\")\nRestApiWriter._logger.debug(f\"Final payload: {payload}\")\nresponse = execute_api_request(\nmethod=method,\nurl=url,\nheaders=headers,\nbasic_auth_dict=basic_auth_dict,\njson=payload if not is_file_payload else None,\nfiles=payload if is_file_payload else None,\nsleep_seconds=sleep_seconds,\n)\nRestApiWriter._logger.debug(\nf\"Response: {response.status_code} - {response.text}\"\n)\nif response.status_code not in success_status_codes:\nraise RESTApiException(\nf\"API response status code {response.status_code} is not in\"\nf\" {success_status_codes}. Got {response.text}\"\n)\nreturn send_payload_to_rest_api\n@staticmethod\ndef _write_to_rest_api_in_batch_mode(\ndf: DataFrame, output_spec: OutputSpec\n) -&gt; None:\n\"\"\"Write to REST API in Spark batch mode.\n        This function uses the dataframe.foreach function to generate a payload\n        for each row of the dataframe and send it to the REST API endpoint.\n        Warning! Make sure your execution environment supports RDD api operations,\n        as there are environments where RDD operation may not be supported. As,\n        df.foreach() is a shorthand for df.rdd.foreach(), this can bring issues\n        in such environments.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n        \"\"\"\ndf.foreach(RestApiWriter._get_func_to_send_payload_to_rest_api(output_spec))\n@staticmethod\ndef _write_to_rest_api_in_streaming_mode(\ndf: DataFrame, output_spec: OutputSpec, data: OrderedDict\n) -&gt; None:\n\"\"\"Write to REST API in streaming mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\ndf_writer = df.writeStream.trigger(**Writer.get_streaming_trigger(output_spec))\nstream_df = (\ndf_writer.options(**output_spec.options if output_spec.options else {})\n.foreachBatch(\nRestApiWriter._write_transformed_micro_batch(output_spec, data)\n)\n.start()\n)\nif output_spec.streaming_await_termination:\nstream_df.awaitTermination(output_spec.streaming_await_termination_timeout)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nRestApiWriter._write_to_rest_api_in_batch_mode(transformed_df, output_spec)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/rest_api_writer.html#packages.io.writers.rest_api_writer.RestApiWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct RestApiWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/rest_api_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct RestApiWriter instances.\n    Args:\n        output_spec: output specification.\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/rest_api_writer.html#packages.io.writers.rest_api_writer.RestApiWriter.write","title":"<code>write()</code>","text":"<p>Write data to REST API.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/rest_api_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Write data to REST API.\"\"\"\nif not self._df.isStreaming:\nself._write_to_rest_api_in_batch_mode(self._df, self._output_spec)\nelse:\nself._write_to_rest_api_in_streaming_mode(\nself._df, self._output_spec, self._data\n)\n</code></pre>"},{"location":"reference/packages/io/writers/sharepoint_writer.html","title":"Sharepoint writer","text":"<p>Module to define the behaviour to write to Sharepoint.</p>"},{"location":"reference/packages/io/writers/sharepoint_writer.html#packages.io.writers.sharepoint_writer.SharepointWriter","title":"<code>SharepointWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write data to SharePoint.</p> <p>This writer is designed specifically for uploading a single file to SharePoint. It first writes the data locally before uploading it to the specified SharePoint location. Since it handles only a single file at a time, any logic for writing multiple files must be implemented on the notebook-side.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/sharepoint_writer.py</code> <pre><code>class SharepointWriter(Writer):\n\"\"\"Class to write data to SharePoint.\n    This writer is designed specifically for uploading a single file\n    to SharePoint. It first writes the data locally before uploading\n    it to the specified SharePoint location. Since it handles only\n    a single file at a time, any logic for writing multiple files\n    must be implemented on the notebook-side.\n    \"\"\"\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct FileWriter instances.\n        Args:\n            output_spec: output specification\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\nself.sharepoint_utils = self._get_sharepoint_utils()\nself._logger = LoggingHandler(__name__).get_logger()\ndef write(self) -&gt; None:\n\"\"\"Upload data to sharepoint.\"\"\"\nif self._df.isStreaming:\nraise NotSupportedException(\"Sharepoint writer doesn't support streaming!\")\nelif any(\nnot getattr(self._output_spec.sharepoint_opts, attr)\nfor attr in [\"site_name\", \"drive_name\", \"local_path\"]\n):\nraise InputNotFoundException(\nf\"Please provide all mandatory Sharepoint options. \\n\"\nf\"Expected: site_name, drive_name and local_path. \"\nf\"Value should not be None.\\n\"\nf\"Provided: site_name={self._output_spec.sharepoint_opts.site_name}, \\n\"\nf\"drive_name={self._output_spec.sharepoint_opts.drive_name}, \\n\"\nf\"local_path={self._output_spec.sharepoint_opts.local_path}\"\n)\nelif not self.sharepoint_utils.check_if_endpoint_exists(\nsite_name=self._output_spec.sharepoint_opts.site_name,\ndrive_name=self._output_spec.sharepoint_opts.drive_name,\nfolder_root_path=self._output_spec.sharepoint_opts.folder_relative_path,\n):\nraise EndpointNotFoundException(\"The provided endpoint does not exist!\")\nelse:\nself._write_to_sharepoint_in_batch_mode(self._df)\ndef _get_sharepoint_utils(self) -&gt; SharepointUtils:\nsharepoint_utils = SharepointUtils(\nclient_id=self._output_spec.sharepoint_opts.client_id,\ntenant_id=self._output_spec.sharepoint_opts.tenant_id,\nlocal_path=self._output_spec.sharepoint_opts.local_path,\napi_version=self._output_spec.sharepoint_opts.api_version,\nsite_name=self._output_spec.sharepoint_opts.site_name,\ndrive_name=self._output_spec.sharepoint_opts.drive_name,\nfile_name=self._output_spec.sharepoint_opts.file_name,\nfolder_relative_path=self._output_spec.sharepoint_opts.folder_relative_path,\nchunk_size=self._output_spec.sharepoint_opts.chunk_size,\nlocal_options=self._output_spec.sharepoint_opts.local_options,\nsecret=self._output_spec.sharepoint_opts.secret,\nconflict_behaviour=self._output_spec.sharepoint_opts.conflict_behaviour,\n)\nreturn sharepoint_utils\ndef _write_to_sharepoint_in_batch_mode(self, df: DataFrame) -&gt; None:\n\"\"\"Write to Sharepoint in batch mode.\n        This method first writes the provided DataFrame to a local file using the\n        SharePointUtils `write_to_local_path` method. If the local file is successfully\n        written, it then uploads the file to SharePoint using the `write_to_sharepoint`\n        method, logging the process and outcome.\n        Args:\n            df: The DataFrame to write to a local file and subsequently\n                upload to SharePoint.\n        \"\"\"\nlocal_path = self._output_spec.sharepoint_opts.local_path\nfile_name = self._output_spec.sharepoint_opts.file_name\nself._logger.info(f\"Starting to write the data to the local path: {local_path}\")\ntry:\nself.sharepoint_utils.write_to_local_path(df)\nexcept IOError as err:\nself.sharepoint_utils.delete_local_path()\nself._logger.info(f\"Deleted the local folder: {local_path}\")\nraise WriteToLocalException(\nf\"The data was not written on the local path: {local_path}\"\n) from err\nself._logger.info(f\"The data was written to the local path: {local_path}\")\nfile_size = os.path.getsize(local_path)\nself._logger.info(\nf\"Uploading the {file_name} ({file_size} bytes) to SharePoint.\"\n)\nself.sharepoint_utils.write_to_sharepoint()\nself._logger.info(f\"The {file_name} was uploaded to SharePoint with success!\")\nself.sharepoint_utils.delete_local_path()\nself._logger.info(f\"Deleted the local folder: {local_path}\")\n</code></pre>"},{"location":"reference/packages/io/writers/sharepoint_writer.html#packages.io.writers.sharepoint_writer.SharepointWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct FileWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/sharepoint_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct FileWriter instances.\n    Args:\n        output_spec: output specification\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\nself.sharepoint_utils = self._get_sharepoint_utils()\nself._logger = LoggingHandler(__name__).get_logger()\n</code></pre>"},{"location":"reference/packages/io/writers/sharepoint_writer.html#packages.io.writers.sharepoint_writer.SharepointWriter.write","title":"<code>write()</code>","text":"<p>Upload data to sharepoint.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/sharepoint_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Upload data to sharepoint.\"\"\"\nif self._df.isStreaming:\nraise NotSupportedException(\"Sharepoint writer doesn't support streaming!\")\nelif any(\nnot getattr(self._output_spec.sharepoint_opts, attr)\nfor attr in [\"site_name\", \"drive_name\", \"local_path\"]\n):\nraise InputNotFoundException(\nf\"Please provide all mandatory Sharepoint options. \\n\"\nf\"Expected: site_name, drive_name and local_path. \"\nf\"Value should not be None.\\n\"\nf\"Provided: site_name={self._output_spec.sharepoint_opts.site_name}, \\n\"\nf\"drive_name={self._output_spec.sharepoint_opts.drive_name}, \\n\"\nf\"local_path={self._output_spec.sharepoint_opts.local_path}\"\n)\nelif not self.sharepoint_utils.check_if_endpoint_exists(\nsite_name=self._output_spec.sharepoint_opts.site_name,\ndrive_name=self._output_spec.sharepoint_opts.drive_name,\nfolder_root_path=self._output_spec.sharepoint_opts.folder_relative_path,\n):\nraise EndpointNotFoundException(\"The provided endpoint does not exist!\")\nelse:\nself._write_to_sharepoint_in_batch_mode(self._df)\n</code></pre>"},{"location":"reference/packages/io/writers/table_writer.html","title":"Table writer","text":"<p>Module that defines the behaviour to write to tables.</p>"},{"location":"reference/packages/io/writers/table_writer.html#packages.io.writers.table_writer.TableWriter","title":"<code>TableWriter</code>","text":"<p>         Bases: <code>Writer</code></p> <p>Class to write to a table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/table_writer.py</code> <pre><code>class TableWriter(Writer):\n\"\"\"Class to write to a table.\"\"\"\ndef __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct TableWriter instances.\n        Args:\n            output_spec: output specification.\n            df: dataframe to be written.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nsuper().__init__(output_spec, df, data)\ndef write(self) -&gt; None:\n\"\"\"Write data to a table.\n        After the write operation we repair the table (e.g., update partitions).\n        However, there's a caveat to this, which is the fact that this repair\n        operation is not reachable if we are running long-running streaming mode.\n        Therefore, we recommend not using the TableWriter with formats other than\n        delta lake for those scenarios (as delta lake does not need msck repair).\n        So, you can: 1) use delta lake format for the table; 2) use the FileWriter\n        and run the repair with a certain frequency in a separate task of your\n        pipeline.\n        \"\"\"\nif not self._df.isStreaming:\nself._write_to_table_in_batch_mode(self._df, self._output_spec)\nelse:\ndf_writer = self._df.writeStream.trigger(\n**Writer.get_streaming_trigger(self._output_spec)\n)\nif (\nself._output_spec.streaming_micro_batch_transformers\nor self._output_spec.streaming_micro_batch_dq_processors\n):\nstream_df = (\ndf_writer.options(\n**self._output_spec.options if self._output_spec.options else {}\n)\n.foreachBatch(\nself._write_transformed_micro_batch(\nself._output_spec, self._data\n)\n)\n.start()\n)\nif self._output_spec.streaming_await_termination:\nstream_df.awaitTermination(\nself._output_spec.streaming_await_termination_timeout\n)\nelse:\nself._write_to_table_in_streaming_mode(df_writer, self._output_spec)\nif (\nself._output_spec.data_format != OutputFormat.DELTAFILES.value\nand self._output_spec.partitions\n):\nExecEnv.SESSION.sql(f\"MSCK REPAIR TABLE {self._output_spec.db_table}\")\n@staticmethod\ndef _write_to_table_in_batch_mode(df: DataFrame, output_spec: OutputSpec) -&gt; None:\n\"\"\"Write to a metastore table in batch mode.\n        Args:\n            df: dataframe to write.\n            output_spec: output specification.\n        \"\"\"\ndf_writer = df.write.format(output_spec.data_format)\nif output_spec.partitions:\ndf_writer = df_writer.partitionBy(output_spec.partitions)\nif output_spec.location:\ndf_writer = df_writer.options(\npath=output_spec.location,\n**output_spec.options if output_spec.options else {},\n)\nelse:\ndf_writer = df_writer.options(\n**output_spec.options if output_spec.options else {}\n)\ndf_writer.mode(output_spec.write_type).saveAsTable(output_spec.db_table)\n@staticmethod\ndef _write_to_table_in_streaming_mode(\ndf_writer: Any, output_spec: OutputSpec\n) -&gt; None:\n\"\"\"Write to a metastore table in streaming mode.\n        Args:\n            df_writer: dataframe writer.\n            output_spec: output specification.\n        \"\"\"\ndf_writer = df_writer.outputMode(output_spec.write_type).format(\noutput_spec.data_format\n)\nif output_spec.partitions:\ndf_writer = df_writer.partitionBy(output_spec.partitions)\nif output_spec.location:\ndf_writer = df_writer.options(\npath=output_spec.location,\n**output_spec.options if output_spec.options else {},\n)\nelse:\ndf_writer = df_writer.options(\n**output_spec.options if output_spec.options else {}\n)\nif output_spec.streaming_await_termination:\ndf_writer.toTable(output_spec.db_table).awaitTermination(\noutput_spec.streaming_await_termination_timeout\n)\nelse:\ndf_writer.toTable(output_spec.db_table)\n@staticmethod\ndef _write_transformed_micro_batch(  # type: ignore\noutput_spec: OutputSpec, data: OrderedDict\n) -&gt; Callable:\n\"\"\"Define how to write a streaming micro batch after transforming it.\n        Args:\n            output_spec: output specification.\n            data: list of all dfs generated on previous steps before writer.\n        Returns:\n            A function to be executed in the foreachBatch spark write method.\n        \"\"\"\ndef inner(batch_df: DataFrame, batch_id: int) -&gt; None:\ntransformed_df = Writer.get_transformed_micro_batch(\noutput_spec, batch_df, batch_id, data\n)\nif output_spec.streaming_micro_batch_dq_processors:\ntransformed_df = Writer.run_micro_batch_dq_process(\ntransformed_df, output_spec.streaming_micro_batch_dq_processors\n)\nTableWriter._write_to_table_in_batch_mode(transformed_df, output_spec)\nreturn inner\n</code></pre>"},{"location":"reference/packages/io/writers/table_writer.html#packages.io.writers.table_writer.TableWriter.__init__","title":"<code>__init__(output_spec, df, data)</code>","text":"<p>Construct TableWriter instances.</p> <p>Parameters:</p> Name Type Description Default <code>output_spec</code> <code>OutputSpec</code> <p>output specification.</p> required <code>df</code> <code>DataFrame</code> <p>dataframe to be written.</p> required <code>data</code> <code>OrderedDict</code> <p>list of all dfs generated on previous steps before writer.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/table_writer.py</code> <pre><code>def __init__(self, output_spec: OutputSpec, df: DataFrame, data: OrderedDict):\n\"\"\"Construct TableWriter instances.\n    Args:\n        output_spec: output specification.\n        df: dataframe to be written.\n        data: list of all dfs generated on previous steps before writer.\n    \"\"\"\nsuper().__init__(output_spec, df, data)\n</code></pre>"},{"location":"reference/packages/io/writers/table_writer.html#packages.io.writers.table_writer.TableWriter.write","title":"<code>write()</code>","text":"<p>Write data to a table.</p> <p>After the write operation we repair the table (e.g., update partitions). However, there's a caveat to this, which is the fact that this repair operation is not reachable if we are running long-running streaming mode. Therefore, we recommend not using the TableWriter with formats other than delta lake for those scenarios (as delta lake does not need msck repair). So, you can: 1) use delta lake format for the table; 2) use the FileWriter and run the repair with a certain frequency in a separate task of your pipeline.</p> Source code in <code>mkdocs/lakehouse_engine/packages/io/writers/table_writer.py</code> <pre><code>def write(self) -&gt; None:\n\"\"\"Write data to a table.\n    After the write operation we repair the table (e.g., update partitions).\n    However, there's a caveat to this, which is the fact that this repair\n    operation is not reachable if we are running long-running streaming mode.\n    Therefore, we recommend not using the TableWriter with formats other than\n    delta lake for those scenarios (as delta lake does not need msck repair).\n    So, you can: 1) use delta lake format for the table; 2) use the FileWriter\n    and run the repair with a certain frequency in a separate task of your\n    pipeline.\n    \"\"\"\nif not self._df.isStreaming:\nself._write_to_table_in_batch_mode(self._df, self._output_spec)\nelse:\ndf_writer = self._df.writeStream.trigger(\n**Writer.get_streaming_trigger(self._output_spec)\n)\nif (\nself._output_spec.streaming_micro_batch_transformers\nor self._output_spec.streaming_micro_batch_dq_processors\n):\nstream_df = (\ndf_writer.options(\n**self._output_spec.options if self._output_spec.options else {}\n)\n.foreachBatch(\nself._write_transformed_micro_batch(\nself._output_spec, self._data\n)\n)\n.start()\n)\nif self._output_spec.streaming_await_termination:\nstream_df.awaitTermination(\nself._output_spec.streaming_await_termination_timeout\n)\nelse:\nself._write_to_table_in_streaming_mode(df_writer, self._output_spec)\nif (\nself._output_spec.data_format != OutputFormat.DELTAFILES.value\nand self._output_spec.partitions\n):\nExecEnv.SESSION.sql(f\"MSCK REPAIR TABLE {self._output_spec.db_table}\")\n</code></pre>"},{"location":"reference/packages/terminators/index.html","title":"Terminators","text":"<p>Package to define algorithm terminators (e.g., vacuum, optimize, compute stats).</p>"},{"location":"reference/packages/terminators/cdf_processor.html","title":"Cdf processor","text":"<p>Defines change data feed processor behaviour.</p>"},{"location":"reference/packages/terminators/cdf_processor.html#packages.terminators.cdf_processor.CDFProcessor","title":"<code>CDFProcessor</code>","text":"<p>         Bases: <code>object</code></p> <p>Change data feed processor class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/cdf_processor.py</code> <pre><code>class CDFProcessor(object):\n\"\"\"Change data feed processor class.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef expose_cdf(cls, spec: TerminatorSpec) -&gt; None:\n\"\"\"Expose CDF to external location.\n        Args:\n            spec: terminator specification.\n        \"\"\"\ncls._logger.info(\"Reading CDF from input table...\")\ndf_cdf = ReaderFactory.get_data(cls._get_table_cdf_input_specs(spec))\nnew_df_cdf = df_cdf.withColumn(\n\"_commit_timestamp\",\ndate_format(col(\"_commit_timestamp\"), \"yyyyMMddHHmmss\"),\n)\ncls._logger.info(\"Writing CDF to external table...\")\ncls._write_cdf_to_external(\nspec,\nnew_df_cdf.repartition(\nspec.args.get(\n\"materialized_cdf_num_partitions\", col(\"_commit_timestamp\")\n)\n),\n)\n# used to delete old data on CDF table (don't remove parquet).\nif spec.args.get(\"clean_cdf\", True):\ncls._logger.info(\"Cleaning CDF table...\")\ncls.delete_old_data(spec)\n# used to delete old parquet files.\nif spec.args.get(\"vacuum_cdf\", False):\ncls._logger.info(\"Vacuuming CDF table...\")\ncls.vacuum_cdf_data(spec)\n@staticmethod\ndef _write_cdf_to_external(\nspec: TerminatorSpec, df: DataFrame, data: OrderedDict = None\n) -&gt; None:\n\"\"\"Write cdf results dataframe.\n        Args:\n            spec: terminator specification.\n            df: dataframe with cdf results to write.\n            data: list of all dfs generated on previous steps before writer.\n        \"\"\"\nWriterFactory.get_writer(\nspec=OutputSpec(\nspec_id=\"materialized_cdf\",\ninput_id=\"input_table\",\nlocation=spec.args[\"materialized_cdf_location\"],\nwrite_type=WriteType.APPEND.value,\ndata_format=spec.args.get(\"data_format\", OutputFormat.DELTAFILES.value),\noptions=spec.args[\"materialized_cdf_options\"],\npartitions=[\"_commit_timestamp\"],\n),\ndf=df,\ndata=data,\n).write()\n@staticmethod\ndef _get_table_cdf_input_specs(spec: TerminatorSpec) -&gt; InputSpec:\n\"\"\"Get the input specifications from a terminator spec.\n        Args:\n            spec: terminator specifications.\n        Returns:\n            List of input specifications.\n        \"\"\"\noptions = {\n\"readChangeFeed\": \"true\",\n**spec.args.get(\"db_table_options\", {}),\n}\ninput_specs = InputSpec(\nspec_id=\"input_table\",\ndb_table=spec.args[\"db_table\"],\nread_type=ReadType.STREAMING.value,\ndata_format=OutputFormat.DELTAFILES.value,\noptions=options,\n)\nreturn input_specs\n@classmethod\ndef delete_old_data(cls, spec: TerminatorSpec) -&gt; None:\n\"\"\"Delete old data from cdf delta table.\n        Args:\n            spec: terminator specifications.\n        \"\"\"\ntoday_datetime = datetime.today()\nlimit_date = today_datetime + timedelta(\ndays=spec.args.get(\"days_to_keep\", 30) * -1\n)\nlimit_timestamp = limit_date.strftime(\"%Y%m%d%H%M%S\")\ncdf_delta_table = DeltaTable.forPath(\nExecEnv.SESSION, spec.args[\"materialized_cdf_location\"]\n)\ncdf_delta_table.delete(col(\"_commit_timestamp\") &lt; limit_timestamp)\n@classmethod\ndef vacuum_cdf_data(cls, spec: TerminatorSpec) -&gt; None:\n\"\"\"Vacuum old data from cdf delta table.\n        Args:\n            spec: terminator specifications.\n        \"\"\"\ncdf_delta_table = DeltaTable.forPath(\nExecEnv.SESSION, spec.args[\"materialized_cdf_location\"]\n)\ncdf_delta_table.vacuum(spec.args.get(\"vacuum_hours\", 168))\n</code></pre>"},{"location":"reference/packages/terminators/cdf_processor.html#packages.terminators.cdf_processor.CDFProcessor.delete_old_data","title":"<code>delete_old_data(spec)</code>  <code>classmethod</code>","text":"<p>Delete old data from cdf delta table.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TerminatorSpec</code> <p>terminator specifications.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/terminators/cdf_processor.py</code> <pre><code>@classmethod\ndef delete_old_data(cls, spec: TerminatorSpec) -&gt; None:\n\"\"\"Delete old data from cdf delta table.\n    Args:\n        spec: terminator specifications.\n    \"\"\"\ntoday_datetime = datetime.today()\nlimit_date = today_datetime + timedelta(\ndays=spec.args.get(\"days_to_keep\", 30) * -1\n)\nlimit_timestamp = limit_date.strftime(\"%Y%m%d%H%M%S\")\ncdf_delta_table = DeltaTable.forPath(\nExecEnv.SESSION, spec.args[\"materialized_cdf_location\"]\n)\ncdf_delta_table.delete(col(\"_commit_timestamp\") &lt; limit_timestamp)\n</code></pre>"},{"location":"reference/packages/terminators/cdf_processor.html#packages.terminators.cdf_processor.CDFProcessor.expose_cdf","title":"<code>expose_cdf(spec)</code>  <code>classmethod</code>","text":"<p>Expose CDF to external location.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TerminatorSpec</code> <p>terminator specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/terminators/cdf_processor.py</code> <pre><code>@classmethod\ndef expose_cdf(cls, spec: TerminatorSpec) -&gt; None:\n\"\"\"Expose CDF to external location.\n    Args:\n        spec: terminator specification.\n    \"\"\"\ncls._logger.info(\"Reading CDF from input table...\")\ndf_cdf = ReaderFactory.get_data(cls._get_table_cdf_input_specs(spec))\nnew_df_cdf = df_cdf.withColumn(\n\"_commit_timestamp\",\ndate_format(col(\"_commit_timestamp\"), \"yyyyMMddHHmmss\"),\n)\ncls._logger.info(\"Writing CDF to external table...\")\ncls._write_cdf_to_external(\nspec,\nnew_df_cdf.repartition(\nspec.args.get(\n\"materialized_cdf_num_partitions\", col(\"_commit_timestamp\")\n)\n),\n)\n# used to delete old data on CDF table (don't remove parquet).\nif spec.args.get(\"clean_cdf\", True):\ncls._logger.info(\"Cleaning CDF table...\")\ncls.delete_old_data(spec)\n# used to delete old parquet files.\nif spec.args.get(\"vacuum_cdf\", False):\ncls._logger.info(\"Vacuuming CDF table...\")\ncls.vacuum_cdf_data(spec)\n</code></pre>"},{"location":"reference/packages/terminators/cdf_processor.html#packages.terminators.cdf_processor.CDFProcessor.vacuum_cdf_data","title":"<code>vacuum_cdf_data(spec)</code>  <code>classmethod</code>","text":"<p>Vacuum old data from cdf delta table.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TerminatorSpec</code> <p>terminator specifications.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/terminators/cdf_processor.py</code> <pre><code>@classmethod\ndef vacuum_cdf_data(cls, spec: TerminatorSpec) -&gt; None:\n\"\"\"Vacuum old data from cdf delta table.\n    Args:\n        spec: terminator specifications.\n    \"\"\"\ncdf_delta_table = DeltaTable.forPath(\nExecEnv.SESSION, spec.args[\"materialized_cdf_location\"]\n)\ncdf_delta_table.vacuum(spec.args.get(\"vacuum_hours\", 168))\n</code></pre>"},{"location":"reference/packages/terminators/dataset_optimizer.html","title":"Dataset optimizer","text":"<p>Module with dataset optimizer terminator.</p>"},{"location":"reference/packages/terminators/dataset_optimizer.html#packages.terminators.dataset_optimizer.DatasetOptimizer","title":"<code>DatasetOptimizer</code>","text":"<p>         Bases: <code>object</code></p> <p>Class with dataset optimizer terminator.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/dataset_optimizer.py</code> <pre><code>class DatasetOptimizer(object):\n\"\"\"Class with dataset optimizer terminator.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef optimize_dataset(\ncls,\ndb_table: Optional[str] = None,\nlocation: Optional[str] = None,\ncompute_table_stats: bool = True,\nvacuum: bool = True,\nvacuum_hours: int = 720,\noptimize: bool = True,\noptimize_where: Optional[str] = None,\noptimize_zorder_col_list: Optional[List[str]] = None,\ndebug: bool = False,\n) -&gt; None:\n\"\"\"Optimize a dataset based on a set of pre-conceived optimizations.\n        Most of the time the dataset is a table, but it can be a file-based one only.\n        Args:\n            db_table: `database_name.table_name`.\n            location: dataset/table filesystem location.\n            compute_table_stats: to compute table statistics or not.\n            vacuum: (delta lake tables only) whether to vacuum the delta lake\n                table or not.\n            vacuum_hours: (delta lake tables only) number of hours to consider\n                in vacuum operation.\n            optimize: (delta lake tables only) whether to optimize the table or\n                not. Custom optimize parameters can be supplied through ExecEnv (Spark)\n                configs\n            optimize_where: expression to use in the optimize function.\n            optimize_zorder_col_list: (delta lake tables only) list of\n                columns to consider in the zorder optimization process. Custom optimize\n                parameters can be supplied through ExecEnv (Spark) configs.\n            debug: flag indicating if we are just debugging this for local\n                tests and therefore pass through all the exceptions to perform some\n                assertions in local tests.\n        \"\"\"\nif optimize:\nif debug:\ntry:\ncls._optimize(\ndb_table, location, optimize_where, optimize_zorder_col_list\n)\nexcept ParseException:\npass\nelse:\ncls._optimize(\ndb_table, location, optimize_where, optimize_zorder_col_list\n)\nif vacuum:\ncls._vacuum(db_table, location, vacuum_hours)\nif compute_table_stats:\nif debug:\ntry:\ncls._compute_table_stats(db_table)\nexcept AnalysisException:\npass\nelse:\ncls._compute_table_stats(db_table)\n@classmethod\ndef _compute_table_stats(cls, db_table: str) -&gt; None:\n\"\"\"Compute table statistics.\n        Args:\n            db_table: `&lt;db&gt;.&lt;table&gt;` string.\n        \"\"\"\nif not db_table:\nraise WrongArgumentsException(\"A table needs to be provided.\")\nconfig = {\"function\": \"compute_table_statistics\", \"table_or_view\": db_table}\ncls._logger.info(f\"Computing table statistics for {db_table}...\")\nTableManager(config).compute_table_statistics()\n@classmethod\ndef _vacuum(cls, db_table: str, location: str, hours: int) -&gt; None:\n\"\"\"Vacuum a delta table.\n        Args:\n            db_table: `&lt;db&gt;.&lt;table&gt;` string. Takes precedence over location.\n            location: location of the delta table.\n            hours: number of hours to consider in vacuum operation.\n        \"\"\"\nif not db_table and not location:\nraise WrongArgumentsException(\"A table or location need to be provided.\")\ntable_or_location = db_table if db_table else f\"delta.`{location}`\"\nconfig = {\n\"function\": \"compute_table_statistics\",\n\"table_or_view\": table_or_location,\n\"vacuum_hours\": hours,\n}\ncls._logger.info(f\"Vacuuming table {table_or_location}...\")\nTableManager(config).vacuum()\n@classmethod\ndef _optimize(\ncls, db_table: str, location: str, where: str, zorder_cols: List[str]\n) -&gt; None:\n\"\"\"Optimize a delta table.\n        Args:\n            db_table: `&lt;db&gt;.&lt;table&gt;` string. Takes precedence over location.\n            location: location of the delta table.\n            where: expression to use in the optimize function.\n            zorder_cols: list of columns to consider in the zorder optimization process.\n        \"\"\"\nif not db_table and not location:\nraise WrongArgumentsException(\"A table or location needs to be provided.\")\ntable_or_location = db_table if db_table else f\"delta.`{location}`\"\nconfig = {\n\"function\": \"compute_table_statistics\",\n\"table_or_view\": table_or_location,\n\"optimize_where\": where,\n\"optimize_zorder_col_list\": \",\".join(zorder_cols if zorder_cols else []),\n}\ncls._logger.info(f\"Optimizing table {table_or_location}...\")\nTableManager(config).optimize()\n</code></pre>"},{"location":"reference/packages/terminators/dataset_optimizer.html#packages.terminators.dataset_optimizer.DatasetOptimizer.optimize_dataset","title":"<code>optimize_dataset(db_table=None, location=None, compute_table_stats=True, vacuum=True, vacuum_hours=720, optimize=True, optimize_where=None, optimize_zorder_col_list=None, debug=False)</code>  <code>classmethod</code>","text":"<p>Optimize a dataset based on a set of pre-conceived optimizations.</p> <p>Most of the time the dataset is a table, but it can be a file-based one only.</p> <p>Parameters:</p> Name Type Description Default <code>db_table</code> <code>Optional[str]</code> <p><code>database_name.table_name</code>.</p> <code>None</code> <code>location</code> <code>Optional[str]</code> <p>dataset/table filesystem location.</p> <code>None</code> <code>compute_table_stats</code> <code>bool</code> <p>to compute table statistics or not.</p> <code>True</code> <code>vacuum</code> <code>bool</code> <p>(delta lake tables only) whether to vacuum the delta lake table or not.</p> <code>True</code> <code>vacuum_hours</code> <code>int</code> <p>(delta lake tables only) number of hours to consider in vacuum operation.</p> <code>720</code> <code>optimize</code> <code>bool</code> <p>(delta lake tables only) whether to optimize the table or not. Custom optimize parameters can be supplied through ExecEnv (Spark) configs</p> <code>True</code> <code>optimize_where</code> <code>Optional[str]</code> <p>expression to use in the optimize function.</p> <code>None</code> <code>optimize_zorder_col_list</code> <code>Optional[List[str]]</code> <p>(delta lake tables only) list of columns to consider in the zorder optimization process. Custom optimize parameters can be supplied through ExecEnv (Spark) configs.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>flag indicating if we are just debugging this for local tests and therefore pass through all the exceptions to perform some assertions in local tests.</p> <code>False</code> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/dataset_optimizer.py</code> <pre><code>@classmethod\ndef optimize_dataset(\ncls,\ndb_table: Optional[str] = None,\nlocation: Optional[str] = None,\ncompute_table_stats: bool = True,\nvacuum: bool = True,\nvacuum_hours: int = 720,\noptimize: bool = True,\noptimize_where: Optional[str] = None,\noptimize_zorder_col_list: Optional[List[str]] = None,\ndebug: bool = False,\n) -&gt; None:\n\"\"\"Optimize a dataset based on a set of pre-conceived optimizations.\n    Most of the time the dataset is a table, but it can be a file-based one only.\n    Args:\n        db_table: `database_name.table_name`.\n        location: dataset/table filesystem location.\n        compute_table_stats: to compute table statistics or not.\n        vacuum: (delta lake tables only) whether to vacuum the delta lake\n            table or not.\n        vacuum_hours: (delta lake tables only) number of hours to consider\n            in vacuum operation.\n        optimize: (delta lake tables only) whether to optimize the table or\n            not. Custom optimize parameters can be supplied through ExecEnv (Spark)\n            configs\n        optimize_where: expression to use in the optimize function.\n        optimize_zorder_col_list: (delta lake tables only) list of\n            columns to consider in the zorder optimization process. Custom optimize\n            parameters can be supplied through ExecEnv (Spark) configs.\n        debug: flag indicating if we are just debugging this for local\n            tests and therefore pass through all the exceptions to perform some\n            assertions in local tests.\n    \"\"\"\nif optimize:\nif debug:\ntry:\ncls._optimize(\ndb_table, location, optimize_where, optimize_zorder_col_list\n)\nexcept ParseException:\npass\nelse:\ncls._optimize(\ndb_table, location, optimize_where, optimize_zorder_col_list\n)\nif vacuum:\ncls._vacuum(db_table, location, vacuum_hours)\nif compute_table_stats:\nif debug:\ntry:\ncls._compute_table_stats(db_table)\nexcept AnalysisException:\npass\nelse:\ncls._compute_table_stats(db_table)\n</code></pre>"},{"location":"reference/packages/terminators/notifier.html","title":"Notifier","text":"<p>Module with notification terminator.</p>"},{"location":"reference/packages/terminators/notifier.html#packages.terminators.notifier.Notifier","title":"<code>Notifier</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract Notification class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier.py</code> <pre><code>class Notifier(ABC):\n\"\"\"Abstract Notification class.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, notification_spec: TerminatorSpec):\n\"\"\"Construct Notification instances.\n        Args:\n            notification_spec: notification specification.\n        \"\"\"\nself.type = notification_spec.args.get(\"type\")\nself.notification = notification_spec.args\n@abstractmethod\ndef create_notification(self) -&gt; None:\n\"\"\"Abstract create notification method.\"\"\"\nraise NotImplementedError\n@abstractmethod\ndef send_notification(self) -&gt; None:\n\"\"\"Abstract send notification method.\"\"\"\nraise NotImplementedError\n@staticmethod\ndef _check_args_are_correct(given_args: dict, template_args: list) -&gt; None:\n\"\"\"Checking if all arguments in template were set.\n        Args:\n            given_args: args set in the terminator spec.\n            template_args: args needed to be set in the template.\n        \"\"\"\nextra_args = []\nfor key in given_args.keys():\nif key in template_args:\ntemplate_args.remove(key)\nelse:\nextra_args.append(key)\nif set(template_args) - set(NOTIFICATION_RUNTIME_PARAMETERS):\nraise ValueError(\n\"The following template args have not been set: \"\n+ \", \".join(template_args)\n)\nif extra_args:\nNotifier._logger.info(\n\"Extra parameters sent to template: \" + \", \".join(extra_args)\n)\n@staticmethod\ndef _render_notification_field(template_field: str, args: dict) -&gt; str:\n\"\"\"Render the notification given args.\n        Args:\n            template_field: Message with templates to be replaced.\n            args: key/value pairs to be replaced in the message.\n        Returns:\n            Rendered field\n        \"\"\"\nfield_template = Template(template_field)\nif (\nNotificationRuntimeParameters.DATABRICKS_JOB_NAME.value in template_field\nor NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID.value\nin template_field\n):\nworkspace_id, job_name = DatabricksUtils.get_databricks_job_information(\nExecEnv.SESSION\n)\nargs[\"databricks_job_name\"] = job_name\nargs[\"databricks_workspace_id\"] = workspace_id\nreturn field_template.render(args)\n@staticmethod\ndef check_if_notification_is_failure_notification(\nspec: TerminatorSpec,\n) -&gt; bool:\n\"\"\"Check if given notification is a failure notification.\n        Args:\n            spec: spec to validate if it is a failure notification.\n        Returns:\n            A boolean telling if the notification is a failure notification\n        \"\"\"\nnotification = spec.args\nis_notification_failure_notification: bool = False\nif \"template\" in notification.keys():\ntemplate: dict = NotificationsTemplates.EMAIL_NOTIFICATIONS_TEMPLATES.get(\nnotification[\"template\"], {}\n)\nif template:\nis_notification_failure_notification = notification.get(\n\"on_failure\", True\n)\nelse:\nraise ValueError(f\"\"\"Template {notification[\"template\"]} not found.\"\"\")\nelse:\nis_notification_failure_notification = notification.get(\"on_failure\", True)\nreturn is_notification_failure_notification\n</code></pre>"},{"location":"reference/packages/terminators/notifier.html#packages.terminators.notifier.Notifier.__init__","title":"<code>__init__(notification_spec)</code>","text":"<p>Construct Notification instances.</p> <p>Parameters:</p> Name Type Description Default <code>notification_spec</code> <code>TerminatorSpec</code> <p>notification specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier.py</code> <pre><code>def __init__(self, notification_spec: TerminatorSpec):\n\"\"\"Construct Notification instances.\n    Args:\n        notification_spec: notification specification.\n    \"\"\"\nself.type = notification_spec.args.get(\"type\")\nself.notification = notification_spec.args\n</code></pre>"},{"location":"reference/packages/terminators/notifier.html#packages.terminators.notifier.Notifier.check_if_notification_is_failure_notification","title":"<code>check_if_notification_is_failure_notification(spec)</code>  <code>staticmethod</code>","text":"<p>Check if given notification is a failure notification.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TerminatorSpec</code> <p>spec to validate if it is a failure notification.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean telling if the notification is a failure notification</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier.py</code> <pre><code>@staticmethod\ndef check_if_notification_is_failure_notification(\nspec: TerminatorSpec,\n) -&gt; bool:\n\"\"\"Check if given notification is a failure notification.\n    Args:\n        spec: spec to validate if it is a failure notification.\n    Returns:\n        A boolean telling if the notification is a failure notification\n    \"\"\"\nnotification = spec.args\nis_notification_failure_notification: bool = False\nif \"template\" in notification.keys():\ntemplate: dict = NotificationsTemplates.EMAIL_NOTIFICATIONS_TEMPLATES.get(\nnotification[\"template\"], {}\n)\nif template:\nis_notification_failure_notification = notification.get(\n\"on_failure\", True\n)\nelse:\nraise ValueError(f\"\"\"Template {notification[\"template\"]} not found.\"\"\")\nelse:\nis_notification_failure_notification = notification.get(\"on_failure\", True)\nreturn is_notification_failure_notification\n</code></pre>"},{"location":"reference/packages/terminators/notifier.html#packages.terminators.notifier.Notifier.create_notification","title":"<code>create_notification()</code>  <code>abstractmethod</code>","text":"<p>Abstract create notification method.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier.py</code> <pre><code>@abstractmethod\ndef create_notification(self) -&gt; None:\n\"\"\"Abstract create notification method.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/terminators/notifier.html#packages.terminators.notifier.Notifier.send_notification","title":"<code>send_notification()</code>  <code>abstractmethod</code>","text":"<p>Abstract send notification method.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier.py</code> <pre><code>@abstractmethod\ndef send_notification(self) -&gt; None:\n\"\"\"Abstract send notification method.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/packages/terminators/notifier_factory.html","title":"Notifier factory","text":"<p>Module for notifier factory.</p>"},{"location":"reference/packages/terminators/notifier_factory.html#packages.terminators.notifier_factory.NotifierFactory","title":"<code>NotifierFactory</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Class for notification factory.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier_factory.py</code> <pre><code>class NotifierFactory(ABC):\n\"\"\"Class for notification factory.\"\"\"\nNOTIFIER_TYPES = {NotifierType.EMAIL.value: EmailNotifier}\n@classmethod\ndef get_notifier(cls, spec: TerminatorSpec) -&gt; Notifier:\n\"\"\"Get a notifier according to the terminator specs using a factory.\n        Args:\n            spec: terminator specification.\n        Returns:\n            Notifier: notifier that will handle notifications.\n        \"\"\"\nnotifier_name = spec.args.get(\"type\")\nnotifier = cls.NOTIFIER_TYPES.get(notifier_name)\nif notifier:\nreturn notifier(notification_spec=spec)\nelse:\nraise NotImplementedError(\nf\"The requested notification format {notifier_name} is not supported.\"\n)\n@staticmethod\ndef generate_failure_notification(spec: list, exception: Exception) -&gt; None:\n\"\"\"Check if it is necessary to send a failure notification and generate it.\n        Args:\n            spec: List of termination specs\n            exception: Exception that caused the failure.\n        \"\"\"\nnotification_specs = []\nfor terminator in spec:\nif terminator.function == \"notify\":\nnotification_specs.append(terminator)\nfor notification in notification_specs:\nnotification_args = notification.args\ngenerate_failure_notification = notification_args.get(\n\"generate_failure_notification\", False\n)\nif generate_failure_notification or (\nNotifier.check_if_notification_is_failure_notification(notification)\n):\nfailure_notification_spec = notification_args\nfailure_notification_spec_args = notification_args.get(\"args\", {})\nfailure_notification_spec_args[\"exception\"] = str(exception)\nfailure_notification_spec[\"args\"] = failure_notification_spec_args\nif generate_failure_notification:\nfailure_notification_spec[\"template\"] = (\nf\"\"\"failure_notification_{notification_args[\"type\"]}\"\"\"\n)\nelif \"template\" in notification_args.keys():\nfailure_notification_spec[\"template\"] = notification_args[\n\"template\"\n]\nfailure_spec = TerminatorSpec(\nfunction=\"notification\", args=failure_notification_spec\n)\nnotifier = NotifierFactory.get_notifier(failure_spec)\nnotifier.create_notification()\nnotifier.send_notification()\n</code></pre>"},{"location":"reference/packages/terminators/notifier_factory.html#packages.terminators.notifier_factory.NotifierFactory.generate_failure_notification","title":"<code>generate_failure_notification(spec, exception)</code>  <code>staticmethod</code>","text":"<p>Check if it is necessary to send a failure notification and generate it.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>list</code> <p>List of termination specs</p> required <code>exception</code> <code>Exception</code> <p>Exception that caused the failure.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier_factory.py</code> <pre><code>@staticmethod\ndef generate_failure_notification(spec: list, exception: Exception) -&gt; None:\n\"\"\"Check if it is necessary to send a failure notification and generate it.\n    Args:\n        spec: List of termination specs\n        exception: Exception that caused the failure.\n    \"\"\"\nnotification_specs = []\nfor terminator in spec:\nif terminator.function == \"notify\":\nnotification_specs.append(terminator)\nfor notification in notification_specs:\nnotification_args = notification.args\ngenerate_failure_notification = notification_args.get(\n\"generate_failure_notification\", False\n)\nif generate_failure_notification or (\nNotifier.check_if_notification_is_failure_notification(notification)\n):\nfailure_notification_spec = notification_args\nfailure_notification_spec_args = notification_args.get(\"args\", {})\nfailure_notification_spec_args[\"exception\"] = str(exception)\nfailure_notification_spec[\"args\"] = failure_notification_spec_args\nif generate_failure_notification:\nfailure_notification_spec[\"template\"] = (\nf\"\"\"failure_notification_{notification_args[\"type\"]}\"\"\"\n)\nelif \"template\" in notification_args.keys():\nfailure_notification_spec[\"template\"] = notification_args[\n\"template\"\n]\nfailure_spec = TerminatorSpec(\nfunction=\"notification\", args=failure_notification_spec\n)\nnotifier = NotifierFactory.get_notifier(failure_spec)\nnotifier.create_notification()\nnotifier.send_notification()\n</code></pre>"},{"location":"reference/packages/terminators/notifier_factory.html#packages.terminators.notifier_factory.NotifierFactory.get_notifier","title":"<code>get_notifier(spec)</code>  <code>classmethod</code>","text":"<p>Get a notifier according to the terminator specs using a factory.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TerminatorSpec</code> <p>terminator specification.</p> required <p>Returns:</p> Name Type Description <code>Notifier</code> <code>Notifier</code> <p>notifier that will handle notifications.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifier_factory.py</code> <pre><code>@classmethod\ndef get_notifier(cls, spec: TerminatorSpec) -&gt; Notifier:\n\"\"\"Get a notifier according to the terminator specs using a factory.\n    Args:\n        spec: terminator specification.\n    Returns:\n        Notifier: notifier that will handle notifications.\n    \"\"\"\nnotifier_name = spec.args.get(\"type\")\nnotifier = cls.NOTIFIER_TYPES.get(notifier_name)\nif notifier:\nreturn notifier(notification_spec=spec)\nelse:\nraise NotImplementedError(\nf\"The requested notification format {notifier_name} is not supported.\"\n)\n</code></pre>"},{"location":"reference/packages/terminators/sensor_terminator.html","title":"Sensor terminator","text":"<p>Module with sensor terminator.</p>"},{"location":"reference/packages/terminators/sensor_terminator.html#packages.terminators.sensor_terminator.SensorTerminator","title":"<code>SensorTerminator</code>","text":"<p>         Bases: <code>object</code></p> <p>Sensor Terminator class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/sensor_terminator.py</code> <pre><code>class SensorTerminator(object):\n\"\"\"Sensor Terminator class.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef update_sensor_status(\ncls,\nsensor_id: str,\ncontrol_db_table_name: str,\nstatus: str = SensorStatus.PROCESSED_NEW_DATA.value,\nassets: List[str] = None,\n) -&gt; None:\n\"\"\"Update internal sensor status.\n        Update the sensor status in the control table, it should be used to tell the\n        system that the sensor has processed all new data that was previously\n        identified, hence updating the shifted sensor status.\n        Usually used to move from `SensorStatus.ACQUIRED_NEW_DATA` to\n        `SensorStatus.PROCESSED_NEW_DATA`, but there might be scenarios - still\n        to identify - where we can update the sensor status from/to different statuses.\n        Args:\n            sensor_id: sensor id.\n            control_db_table_name: `db.table` to store sensor checkpoints.\n            status: status of the sensor.\n            assets: a list of assets that are considered as available to\n                consume downstream after this sensor has status\n                PROCESSED_NEW_DATA.\n        \"\"\"\nif status not in [s.value for s in SensorStatus]:\nraise NotImplementedError(f\"Status {status} not accepted in sensor.\")\nExecEnv.get_or_create(app_name=\"update_sensor_status\")\nSensorControlTableManager.update_sensor_status(\nsensor_spec=SensorSpec(\nsensor_id=sensor_id,\ncontrol_db_table_name=control_db_table_name,\nassets=assets,\ninput_spec=None,\npreprocess_query=None,\ncheckpoint_location=None,\n),\nstatus=status,\n)\n</code></pre>"},{"location":"reference/packages/terminators/sensor_terminator.html#packages.terminators.sensor_terminator.SensorTerminator.update_sensor_status","title":"<code>update_sensor_status(sensor_id, control_db_table_name, status=SensorStatus.PROCESSED_NEW_DATA.value, assets=None)</code>  <code>classmethod</code>","text":"<p>Update internal sensor status.</p> <p>Update the sensor status in the control table, it should be used to tell the system that the sensor has processed all new data that was previously identified, hence updating the shifted sensor status. Usually used to move from <code>SensorStatus.ACQUIRED_NEW_DATA</code> to <code>SensorStatus.PROCESSED_NEW_DATA</code>, but there might be scenarios - still to identify - where we can update the sensor status from/to different statuses.</p> <p>Parameters:</p> Name Type Description Default <code>sensor_id</code> <code>str</code> <p>sensor id.</p> required <code>control_db_table_name</code> <code>str</code> <p><code>db.table</code> to store sensor checkpoints.</p> required <code>status</code> <code>str</code> <p>status of the sensor.</p> <code>SensorStatus.PROCESSED_NEW_DATA.value</code> <code>assets</code> <code>List[str]</code> <p>a list of assets that are considered as available to consume downstream after this sensor has status PROCESSED_NEW_DATA.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/sensor_terminator.py</code> <pre><code>@classmethod\ndef update_sensor_status(\ncls,\nsensor_id: str,\ncontrol_db_table_name: str,\nstatus: str = SensorStatus.PROCESSED_NEW_DATA.value,\nassets: List[str] = None,\n) -&gt; None:\n\"\"\"Update internal sensor status.\n    Update the sensor status in the control table, it should be used to tell the\n    system that the sensor has processed all new data that was previously\n    identified, hence updating the shifted sensor status.\n    Usually used to move from `SensorStatus.ACQUIRED_NEW_DATA` to\n    `SensorStatus.PROCESSED_NEW_DATA`, but there might be scenarios - still\n    to identify - where we can update the sensor status from/to different statuses.\n    Args:\n        sensor_id: sensor id.\n        control_db_table_name: `db.table` to store sensor checkpoints.\n        status: status of the sensor.\n        assets: a list of assets that are considered as available to\n            consume downstream after this sensor has status\n            PROCESSED_NEW_DATA.\n    \"\"\"\nif status not in [s.value for s in SensorStatus]:\nraise NotImplementedError(f\"Status {status} not accepted in sensor.\")\nExecEnv.get_or_create(app_name=\"update_sensor_status\")\nSensorControlTableManager.update_sensor_status(\nsensor_spec=SensorSpec(\nsensor_id=sensor_id,\ncontrol_db_table_name=control_db_table_name,\nassets=assets,\ninput_spec=None,\npreprocess_query=None,\ncheckpoint_location=None,\n),\nstatus=status,\n)\n</code></pre>"},{"location":"reference/packages/terminators/spark_terminator.html","title":"Spark terminator","text":"<p>Module with spark terminator.</p>"},{"location":"reference/packages/terminators/spark_terminator.html#packages.terminators.spark_terminator.SparkTerminator","title":"<code>SparkTerminator</code>","text":"<p>         Bases: <code>object</code></p> <p>Spark Terminator class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/spark_terminator.py</code> <pre><code>class SparkTerminator(object):\n\"\"\"Spark Terminator class.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef terminate_spark(cls) -&gt; None:\n\"\"\"Terminate spark session.\"\"\"\ncls._logger.info(\"Terminating spark session...\")\nExecEnv.SESSION.stop()\n</code></pre>"},{"location":"reference/packages/terminators/spark_terminator.html#packages.terminators.spark_terminator.SparkTerminator.terminate_spark","title":"<code>terminate_spark()</code>  <code>classmethod</code>","text":"<p>Terminate spark session.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/spark_terminator.py</code> <pre><code>@classmethod\ndef terminate_spark(cls) -&gt; None:\n\"\"\"Terminate spark session.\"\"\"\ncls._logger.info(\"Terminating spark session...\")\nExecEnv.SESSION.stop()\n</code></pre>"},{"location":"reference/packages/terminators/terminator_factory.html","title":"Terminator factory","text":"<p>Module with the factory pattern to return terminators.</p>"},{"location":"reference/packages/terminators/terminator_factory.html#packages.terminators.terminator_factory.TerminatorFactory","title":"<code>TerminatorFactory</code>","text":"<p>         Bases: <code>object</code></p> <p>TerminatorFactory class following the factory pattern.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/terminator_factory.py</code> <pre><code>class TerminatorFactory(object):\n\"\"\"TerminatorFactory class following the factory pattern.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef execute_terminator(\nspec: TerminatorSpec, df: Optional[DataFrame] = None\n) -&gt; None:\n\"\"\"Execute a terminator following the factory pattern.\n        Args:\n            spec: terminator specification.\n            df: dataframe to be used in the terminator. Needed when a\n                terminator requires one dataframe as input.\n        Returns:\n            Transformer function to be executed in .transform() spark function.\n        \"\"\"\nif spec.function == \"optimize_dataset\":\nfrom lakehouse_engine.terminators.dataset_optimizer import DatasetOptimizer\nDatasetOptimizer.optimize_dataset(**spec.args)\nelif spec.function == \"terminate_spark\":\nfrom lakehouse_engine.terminators.spark_terminator import SparkTerminator\nSparkTerminator.terminate_spark()\nelif spec.function == \"expose_cdf\":\nfrom lakehouse_engine.terminators.cdf_processor import CDFProcessor\nCDFProcessor.expose_cdf(spec)\nelif spec.function == \"notify\":\nif not Notifier.check_if_notification_is_failure_notification(spec):\nnotifier = NotifierFactory.get_notifier(spec)\nnotifier.create_notification()\nnotifier.send_notification()\nelse:\nraise NotImplementedError(\nf\"The requested terminator {spec.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/terminators/terminator_factory.html#packages.terminators.terminator_factory.TerminatorFactory.execute_terminator","title":"<code>execute_terminator(spec, df=None)</code>  <code>staticmethod</code>","text":"<p>Execute a terminator following the factory pattern.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TerminatorSpec</code> <p>terminator specification.</p> required <code>df</code> <code>Optional[DataFrame]</code> <p>dataframe to be used in the terminator. Needed when a terminator requires one dataframe as input.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Transformer function to be executed in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/terminator_factory.py</code> <pre><code>@staticmethod\ndef execute_terminator(\nspec: TerminatorSpec, df: Optional[DataFrame] = None\n) -&gt; None:\n\"\"\"Execute a terminator following the factory pattern.\n    Args:\n        spec: terminator specification.\n        df: dataframe to be used in the terminator. Needed when a\n            terminator requires one dataframe as input.\n    Returns:\n        Transformer function to be executed in .transform() spark function.\n    \"\"\"\nif spec.function == \"optimize_dataset\":\nfrom lakehouse_engine.terminators.dataset_optimizer import DatasetOptimizer\nDatasetOptimizer.optimize_dataset(**spec.args)\nelif spec.function == \"terminate_spark\":\nfrom lakehouse_engine.terminators.spark_terminator import SparkTerminator\nSparkTerminator.terminate_spark()\nelif spec.function == \"expose_cdf\":\nfrom lakehouse_engine.terminators.cdf_processor import CDFProcessor\nCDFProcessor.expose_cdf(spec)\nelif spec.function == \"notify\":\nif not Notifier.check_if_notification_is_failure_notification(spec):\nnotifier = NotifierFactory.get_notifier(spec)\nnotifier.create_notification()\nnotifier.send_notification()\nelse:\nraise NotImplementedError(\nf\"The requested terminator {spec.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/terminators/notifiers/index.html","title":"Notifiers","text":"<p>Notifications module.</p>"},{"location":"reference/packages/terminators/notifiers/email_notifier.html","title":"Email notifier","text":"<p>Module with email notifier.</p>"},{"location":"reference/packages/terminators/notifiers/email_notifier.html#packages.terminators.notifiers.email_notifier.EmailNotifier","title":"<code>EmailNotifier</code>","text":"<p>         Bases: <code>Notifier</code></p> <p>Base Notification class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifiers/email_notifier.py</code> <pre><code>class EmailNotifier(Notifier):\n\"\"\"Base Notification class.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\ndef __init__(self, notification_spec: TerminatorSpec):\n\"\"\"Construct Email Notification instance.\n        Args:\n            notification_spec: notification specification.\n        \"\"\"\nsuper().__init__(notification_spec)\ndef create_notification(self) -&gt; None:\n\"\"\"Creates the notification to be sent.\"\"\"\ngiven_args = self.notification.get(\"args\", {})\nif \"template\" in self.notification.keys():\ntemplate: dict = NotificationsTemplates.EMAIL_NOTIFICATIONS_TEMPLATES.get(\nself.notification[\"template\"], {}\n)\nif template:\ntemplate_args = template[\"args\"]\nNotifier._check_args_are_correct(given_args, copy(template_args))\nself.notification[\"message\"] = self._render_notification_field(\ntemplate[\"message\"], given_args\n)\nself.notification[\"subject\"] = self._render_notification_field(\ntemplate[\"subject\"], given_args\n)\nself.notification[\"mimetype\"] = template[\"mimetype\"]\nelse:\nraise ValueError(\nf\"\"\"Template {self.notification[\"template\"]} does not exist\"\"\"\n)\nelif \"message\" in self.notification.keys():\nif given_args:\nself.notification[\"message\"] = self._render_notification_field(\nself.notification[\"message\"], given_args\n)\nself.notification[\"subject\"] = self._render_notification_field(\nself.notification[\"subject\"], given_args\n)\nelse:\nraise ValueError(\"Malformed Notification Definition\")\ndef send_notification(self) -&gt; None:\n\"\"\"Sends the notification by using a series of methods.\"\"\"\nserver = self.notification[\"server\"]\nnotification_office_email_servers = [\"smtp.office365.com\"]\nif (\nExecEnv.ENGINE_CONFIG.notif_disallowed_email_servers is not None\nand server in ExecEnv.ENGINE_CONFIG.notif_disallowed_email_servers\n):\nraise ValueError(\nf\"Trying to use disallowed smtp server: '{server}'.\\n\"\nf\"Disallowed smtp servers: \"\nf\"{str(ExecEnv.ENGINE_CONFIG.notif_disallowed_email_servers)}\"\n)\nelif server in notification_office_email_servers:\nself._authenticate_and_send_office365()\nelse:\nself._authenticate_and_send_simple_smtp()\ndef _authenticate_and_send_office365(self) -&gt; None:\n\"\"\"Authenticates and sends an email notification using Graph API.\"\"\"\nfrom azure.identity.aio import ClientSecretCredential\nfrom msgraph import GraphServiceClient\nfrom msgraph.generated.models.email_address import EmailAddress\nfrom msgraph.generated.models.item_body import ItemBody\nfrom msgraph.generated.models.message import Message\nfrom msgraph.generated.models.recipient import Recipient\nfrom msgraph.generated.users.item.send_mail.send_mail_post_request_body import (\nSendMailPostRequestBody,\n)\nself._logger.info(\"Attempting authentication using Graph API.\")\ncredential = ClientSecretCredential(\ntenant_id=self.notification[\"tenant_id\"],\nclient_id=self.notification[\"user\"],\nclient_secret=self.notification[\"password\"],\n)\nclient = GraphServiceClient(credentials=credential)\nrequest_body = SendMailPostRequestBody()\nmessage = Message()\nmessage.subject = self.notification[\"subject\"]\nmessage_body = ItemBody()\nmessage_body.content = self.notification[\"message\"]\nmessage.body = message_body\nrecipients = []\nfor email in self.notification[\"to\"]:\nrecipient = Recipient()\nrecipient_address = EmailAddress()\nrecipient_address.address = email\nrecipient.email_address = recipient_address\nrecipients.append(recipient)\nmessage.to_recipients = recipients\nrequest_body.message = message\nrequest_body.save_to_sent_items = False\nself._logger.info(f\"Sending notification email with body: {request_body}\")\nimport nest_asyncio\nnest_asyncio.apply()\nasyncio.get_event_loop().run_until_complete(\nclient.users.by_user_id(self.notification[\"from\"]).send_mail.post(\nbody=request_body\n)\n)\nself._logger.info(\"Notification email sent successfully.\")\ndef _authenticate_and_send_simple_smtp(self) -&gt; None:\n\"\"\"Authenticates and sends an email notification using simple authentication.\"\"\"\nwith smtplib.SMTP(\nself.notification[\"server\"], self.notification[\"port\"]\n) as smtp:\ntry:\nsmtp.starttls()\nsmtp.login(\nself.notification.get(\"user\", \"\"),\nself.notification.get(\"password\", \"\"),\n)\nexcept smtplib.SMTPException as e:\nself._logger.exception(\nf\"Exception while authenticating to smtp: {str(e)}\"\n)\nself._logger.exception(\n\"Attempting to send the notification without authentication\"\n)\nmesg = MIMEMultipart()\nmesg[\"From\"] = self.notification[\"from\"]\nmesg[\"To\"] = \", \".join(self.notification[\"to\"])\nmesg[\"Subject\"] = self.notification[\"subject\"]\nbody = MIMEText(self.notification[\"message\"], self.notification[\"mimetype\"])\nmesg.attach(body)\ntry:\nsmtp.sendmail(\nself.notification[\"from\"], self.notification[\"to\"], mesg.as_string()\n)\nself._logger.info(\"Email sent successfully.\")\nexcept smtplib.SMTPException as e:\nself._logger.exception(f\"Exception while sending email: {str(e)}\")\n</code></pre>"},{"location":"reference/packages/terminators/notifiers/email_notifier.html#packages.terminators.notifiers.email_notifier.EmailNotifier.__init__","title":"<code>__init__(notification_spec)</code>","text":"<p>Construct Email Notification instance.</p> <p>Parameters:</p> Name Type Description Default <code>notification_spec</code> <code>TerminatorSpec</code> <p>notification specification.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifiers/email_notifier.py</code> <pre><code>def __init__(self, notification_spec: TerminatorSpec):\n\"\"\"Construct Email Notification instance.\n    Args:\n        notification_spec: notification specification.\n    \"\"\"\nsuper().__init__(notification_spec)\n</code></pre>"},{"location":"reference/packages/terminators/notifiers/email_notifier.html#packages.terminators.notifiers.email_notifier.EmailNotifier.create_notification","title":"<code>create_notification()</code>","text":"<p>Creates the notification to be sent.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifiers/email_notifier.py</code> <pre><code>def create_notification(self) -&gt; None:\n\"\"\"Creates the notification to be sent.\"\"\"\ngiven_args = self.notification.get(\"args\", {})\nif \"template\" in self.notification.keys():\ntemplate: dict = NotificationsTemplates.EMAIL_NOTIFICATIONS_TEMPLATES.get(\nself.notification[\"template\"], {}\n)\nif template:\ntemplate_args = template[\"args\"]\nNotifier._check_args_are_correct(given_args, copy(template_args))\nself.notification[\"message\"] = self._render_notification_field(\ntemplate[\"message\"], given_args\n)\nself.notification[\"subject\"] = self._render_notification_field(\ntemplate[\"subject\"], given_args\n)\nself.notification[\"mimetype\"] = template[\"mimetype\"]\nelse:\nraise ValueError(\nf\"\"\"Template {self.notification[\"template\"]} does not exist\"\"\"\n)\nelif \"message\" in self.notification.keys():\nif given_args:\nself.notification[\"message\"] = self._render_notification_field(\nself.notification[\"message\"], given_args\n)\nself.notification[\"subject\"] = self._render_notification_field(\nself.notification[\"subject\"], given_args\n)\nelse:\nraise ValueError(\"Malformed Notification Definition\")\n</code></pre>"},{"location":"reference/packages/terminators/notifiers/email_notifier.html#packages.terminators.notifiers.email_notifier.EmailNotifier.send_notification","title":"<code>send_notification()</code>","text":"<p>Sends the notification by using a series of methods.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifiers/email_notifier.py</code> <pre><code>def send_notification(self) -&gt; None:\n\"\"\"Sends the notification by using a series of methods.\"\"\"\nserver = self.notification[\"server\"]\nnotification_office_email_servers = [\"smtp.office365.com\"]\nif (\nExecEnv.ENGINE_CONFIG.notif_disallowed_email_servers is not None\nand server in ExecEnv.ENGINE_CONFIG.notif_disallowed_email_servers\n):\nraise ValueError(\nf\"Trying to use disallowed smtp server: '{server}'.\\n\"\nf\"Disallowed smtp servers: \"\nf\"{str(ExecEnv.ENGINE_CONFIG.notif_disallowed_email_servers)}\"\n)\nelif server in notification_office_email_servers:\nself._authenticate_and_send_office365()\nelse:\nself._authenticate_and_send_simple_smtp()\n</code></pre>"},{"location":"reference/packages/terminators/notifiers/notification_templates.html","title":"Notification templates","text":"<p>Email notification templates.</p>"},{"location":"reference/packages/terminators/notifiers/notification_templates.html#packages.terminators.notifiers.notification_templates.NotificationsTemplates","title":"<code>NotificationsTemplates</code>","text":"<p>         Bases: <code>object</code></p> <p>Templates for notifications.</p> Source code in <code>mkdocs/lakehouse_engine/packages/terminators/notifiers/notification_templates.py</code> <pre><code>class NotificationsTemplates(object):\n\"\"\"Templates for notifications.\"\"\"\nEMAIL_NOTIFICATIONS_TEMPLATES = {\n\"failure_notification_email\": {\n\"subject\": \"Service Failure\",\n\"mimetype\": \"plain\",\n\"message\": \"\"\"\n            Job {{ databricks_job_name }} in workspace {{ databricks_workspace_id }} has\n            failed with the exception: {{ exception }}\"\"\",\n\"args\": [\n\"exception\",\n],\n\"on_failure\": True,\n},\n\"opsgenie_notification\": {\n\"subject\": \"{{ data_product }} - Failure Notification\",\n\"mimetype\": \"plain\",\n\"message\": \"\"\"\n                Opsgenie notification:\n                    - Data Product: {{ data_product }}\n                    - Job name: {{ databricks_job_name }}\n                    - Alias: {{ alias }}\n                    - Priority: {{ priority }}\n                    - Entity: {{ entity }}\n                    - Tags: {{ tags }}\n                    - Action: {{ action }}\n                    - Description: {{ description }}\n                    - Exception: {{ exception }}\"\"\",\n\"args\": [\n\"data_product\",\n\"alias\",\n\"priority\",\n\"entity\",\n\"tags\",\n\"action\",\n\"description\",\n\"exception\",\n],\n\"on_failure\": True,\n},\n}\n</code></pre>"},{"location":"reference/packages/transformers/index.html","title":"Transformers","text":"<p>Package to define transformers available in the lakehouse engine.</p>"},{"location":"reference/packages/transformers/aggregators.html","title":"Aggregators","text":"<p>Aggregators module.</p>"},{"location":"reference/packages/transformers/aggregators.html#packages.transformers.aggregators.Aggregators","title":"<code>Aggregators</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing all aggregation functions.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/aggregators.py</code> <pre><code>class Aggregators(object):\n\"\"\"Class containing all aggregation functions.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef get_max_value(input_col: str, output_col: str = \"latest\") -&gt; Callable:\n\"\"\"Get the maximum value of a given column of a dataframe.\n        Args:\n            input_col: name of the input column.\n            output_col: name of the output column (defaults to \"latest\").\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='get_max_value')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.select(col(input_col)).agg(max(input_col).alias(output_col))\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/aggregators.html#packages.transformers.aggregators.Aggregators.get_max_value","title":"<code>get_max_value(input_col, output_col='latest')</code>  <code>staticmethod</code>","text":"<p>Get the maximum value of a given column of a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input_col</code> <code>str</code> <p>name of the input column.</p> required <code>output_col</code> <code>str</code> <p>name of the output column (defaults to \"latest\").</p> <code>'latest'</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of get_max_value (See full example here)<pre><code><pre>28{\n29    \"function\": \"get_max_value\",\n30    \"args\": {\n31        \"input_col\": \"extraction_date\"\n32    }\n33}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/aggregators.py</code> <pre><code>@staticmethod\ndef get_max_value(input_col: str, output_col: str = \"latest\") -&gt; Callable:\n\"\"\"Get the maximum value of a given column of a dataframe.\n    Args:\n        input_col: name of the input column.\n        output_col: name of the output column (defaults to \"latest\").\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='get_max_value')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.select(col(input_col)).agg(max(input_col).alias(output_col))\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_creators.html","title":"Column creators","text":"<p>Column creators transformers module.</p>"},{"location":"reference/packages/transformers/column_creators.html#packages.transformers.column_creators.ColumnCreators","title":"<code>ColumnCreators</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing all functions that can create columns to add value.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_creators.py</code> <pre><code>class ColumnCreators(object):\n\"\"\"Class containing all functions that can create columns to add value.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef with_row_id(\ncls,\noutput_col: str = \"lhe_row_id\",\n) -&gt; Callable:\n\"\"\"Create a sequential but not consecutive id.\n        Args:\n            output_col: optional name of the output column.\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='with_row_id')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\nreturn df.withColumn(output_col, monotonically_increasing_id())\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer with_row_id is not supported in streaming mode.\"\n)\nreturn inner\n@classmethod\ndef with_auto_increment_id(\ncls, output_col: str = \"lhe_row_id\", rdd: bool = True\n) -&gt; Callable:\n\"\"\"Create a sequential and consecutive id.\n        Args:\n            output_col: optional name of the output column.\n            rdd: optional parameter to use spark rdd.\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='with_auto_increment_id')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\nif len(df.take(1)) == 0:\n# if df is empty we have to prevent the algorithm from failing\nreturn df.withColumn(output_col, lit(None).cast(IntegerType()))\nelif rdd:\nreturn (\ndf.rdd.zipWithIndex()\n.toDF()\n.select(col(\"_1.*\"), col(\"_2\").alias(output_col))\n)\nelse:\nw = Window.orderBy(monotonically_increasing_id())\nreturn df.withColumn(output_col, (row_number().over(w)) - 1)\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer with_auto_increment_id is not supported in \"\n\"streaming mode.\"\n)\nreturn inner\n@classmethod\ndef with_literals(\ncls,\nliterals: Dict[str, Any],\n) -&gt; Callable:\n\"\"\"Create columns given a map of column names and literal values (constants).\n        Args:\n            Dict[str, Any] literals: map of column names and literal values (constants).\n        Returns:\n            Callable: A function to be executed in the .transform() spark function.\n        {{get_example(method_name='with_literals')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ndf_with_literals = df\nfor name, value in literals.items():\ndf_with_literals = df_with_literals.withColumn(name, lit(value))\nreturn df_with_literals\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_creators.html#packages.transformers.column_creators.ColumnCreators.with_auto_increment_id","title":"<code>with_auto_increment_id(output_col='lhe_row_id', rdd=True)</code>  <code>classmethod</code>","text":"<p>Create a sequential and consecutive id.</p> <p>Parameters:</p> Name Type Description Default <code>output_col</code> <code>str</code> <p>optional name of the output column.</p> <code>'lhe_row_id'</code> <code>rdd</code> <code>bool</code> <p>optional parameter to use spark rdd.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of with_auto_increment_id (See full example here)<pre><code><pre>56{\n57    \"function\": \"with_auto_increment_id\"\n58}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_creators.py</code> <pre><code>@classmethod\ndef with_auto_increment_id(\ncls, output_col: str = \"lhe_row_id\", rdd: bool = True\n) -&gt; Callable:\n\"\"\"Create a sequential and consecutive id.\n    Args:\n        output_col: optional name of the output column.\n        rdd: optional parameter to use spark rdd.\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='with_auto_increment_id')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\nif len(df.take(1)) == 0:\n# if df is empty we have to prevent the algorithm from failing\nreturn df.withColumn(output_col, lit(None).cast(IntegerType()))\nelif rdd:\nreturn (\ndf.rdd.zipWithIndex()\n.toDF()\n.select(col(\"_1.*\"), col(\"_2\").alias(output_col))\n)\nelse:\nw = Window.orderBy(monotonically_increasing_id())\nreturn df.withColumn(output_col, (row_number().over(w)) - 1)\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer with_auto_increment_id is not supported in \"\n\"streaming mode.\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_creators.html#packages.transformers.column_creators.ColumnCreators.with_literals","title":"<code>with_literals(literals)</code>  <code>classmethod</code>","text":"<p>Create columns given a map of column names and literal values (constants).</p> <p>Parameters:</p> Name Type Description Default <code>Dict[str,</code> <code>Any] literals</code> <p>map of column names and literal values (constants).</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of with_literals (See full example here)<pre><code><pre>21{\n22    \"function\": \"with_literals\",\n23    \"args\": {\n24        \"literals\": {\n25            \"dummy_string\": \"this is a string\",\n26            \"dummy_int\": 100,\n27            \"dummy_double\": 10.2,\n28            \"dummy_boolean\": true\n29        }\n30    }\n31}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_creators.py</code> <pre><code>@classmethod\ndef with_literals(\ncls,\nliterals: Dict[str, Any],\n) -&gt; Callable:\n\"\"\"Create columns given a map of column names and literal values (constants).\n    Args:\n        Dict[str, Any] literals: map of column names and literal values (constants).\n    Returns:\n        Callable: A function to be executed in the .transform() spark function.\n    {{get_example(method_name='with_literals')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ndf_with_literals = df\nfor name, value in literals.items():\ndf_with_literals = df_with_literals.withColumn(name, lit(value))\nreturn df_with_literals\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_creators.html#packages.transformers.column_creators.ColumnCreators.with_row_id","title":"<code>with_row_id(output_col='lhe_row_id')</code>  <code>classmethod</code>","text":"<p>Create a sequential but not consecutive id.</p> <p>Parameters:</p> Name Type Description Default <code>output_col</code> <code>str</code> <p>optional name of the output column.</p> <code>'lhe_row_id'</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of with_row_id (See full example here)<pre><code><pre>92{\n93    \"function\": \"with_row_id\"\n94}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_creators.py</code> <pre><code>@classmethod\ndef with_row_id(\ncls,\noutput_col: str = \"lhe_row_id\",\n) -&gt; Callable:\n\"\"\"Create a sequential but not consecutive id.\n    Args:\n        output_col: optional name of the output column.\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='with_row_id')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\nreturn df.withColumn(output_col, monotonically_increasing_id())\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer with_row_id is not supported in streaming mode.\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html","title":"Column reshapers","text":"<p>Module with column reshaping transformers.</p>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers","title":"<code>ColumnReshapers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing column reshaping transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>class ColumnReshapers(object):\n\"\"\"Class containing column reshaping transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef cast(cls, cols: Dict[str, str]) -&gt; Callable:\n\"\"\"Cast specific columns into the designated type.\n        Args:\n            cols: dict with columns and respective target types.\n                Target types need to have the exact name of spark types:\n                https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='cast')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ncast_df = df\nfor c, t in cols.items():\ncast_df = cast_df.withColumn(c, col(c).cast(getattr(spark_types, t)()))\nreturn cast_df\nreturn inner\n@classmethod\ndef column_selector(cls, cols: OrderedDict) -&gt; Callable:\n\"\"\"Select specific columns with specific output aliases.\n        Args:\n            cols: dict with columns to select and respective aliases.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='column_selector')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.select(*[col(c).alias(a) for c, a in cols.items()])\nreturn inner\n@classmethod\ndef flatten_schema(\ncls,\nmax_level: int = None,\nshorten_names: bool = False,\nalias: bool = True,\nnum_chars: int = 7,\nignore_cols: List = None,\n) -&gt; Callable:\n\"\"\"Flatten the schema of the dataframe.\n        Args:\n            max_level: level until which you want to flatten the schema.\n                Default: None.\n            shorten_names: whether to shorten the names of the prefixes\n                of the fields being flattened or not. Default: False.\n            alias: whether to define alias for the columns being flattened\n                or not. Default: True.\n            num_chars: number of characters to consider when shortening\n                the names of the fields. Default: 7.\n            ignore_cols: columns which you don't want to flatten.\n                Default: None.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='flatten_schema')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.select(\nSchemaUtils.schema_flattener(\nschema=df.schema,\nmax_level=max_level,\nshorten_names=shorten_names,\nalias=alias,\nnum_chars=num_chars,\nignore_cols=ignore_cols,\n)\n)\nreturn inner\n@classmethod\ndef explode_columns(\ncls,\nexplode_arrays: bool = False,\narray_cols_to_explode: List[str] = None,\nexplode_maps: bool = False,\nmap_cols_to_explode: List[str] = None,\n) -&gt; Callable:\n\"\"\"Explode columns with types like ArrayType and MapType.\n        After it can be applied the flatten_schema transformation,\n        if we desired for example to explode the map (as we explode a StructType)\n        or to explode a StructType inside the array.\n        We recommend you to specify always the columns desired to explode\n        and not explode all columns.\n        Args:\n            explode_arrays: whether you want to explode array columns (True)\n                or not (False). Default: False.\n            array_cols_to_explode: array columns which you want to explode.\n                If you don't specify it will get all array columns and explode them.\n                Default: None.\n            explode_maps: whether you want to explode map columns (True)\n                or not (False). Default: False.\n            map_cols_to_explode: map columns which you want to explode.\n                If you don't specify it will get all map columns and explode them.\n                Default: None.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='explode_columns')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif explode_arrays or (array_cols_to_explode is not None):\ndf = cls._explode_arrays(df, array_cols_to_explode)\nif explode_maps or (map_cols_to_explode is not None):\ndf = cls._explode_maps(df, map_cols_to_explode)\nreturn df\nreturn inner\n@classmethod\ndef _get_columns(\ncls,\ndf: DataFrame,\ndata_type: Any,\n) -&gt; List:\n\"\"\"Get a list of columns from the dataframe of the data types specified.\n        Args:\n            df: input dataframe.\n            data_type: data type specified.\n        Returns:\n            List of columns with the datatype specified.\n        \"\"\"\ncols = []\nfor field in df.schema.fields:\nif isinstance(field.dataType, data_type):\ncols.append(field.name)\nreturn cols\n@classmethod\ndef with_expressions(cls, cols_and_exprs: Dict[str, str]) -&gt; Callable:\n\"\"\"Execute Spark SQL expressions to create the specified columns.\n        This function uses the Spark expr function. [Check here](\n        https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html).\n        Args:\n            cols_and_exprs: dict with columns and respective expressions to compute\n                (Spark SQL expressions).\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='with_expressions')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nenriched_df = df\nfor c, e in cols_and_exprs.items():\nenriched_df = enriched_df.withColumn(c, expr(e))\nreturn enriched_df\nreturn inner\n@classmethod\ndef rename(cls, cols: Dict[str, str], escape_col_names: bool = True) -&gt; Callable:\n\"\"\"Rename specific columns into the designated name.\n        Args:\n            cols: dict with columns and respective target names.\n            escape_col_names: whether to escape column names (e.g. `/BIC/COL1`) or not.\n                If True it creates a column with the new name and drop the old one.\n                If False, uses the native withColumnRenamed Spark function.\n                Default: True.\n        Returns:\n            Function to be called in .transform() spark function.\n        {{get_example(method_name='rename')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nrenamed_df = df\nif escape_col_names:\nfor old_name, new_name in cols.items():\nrenamed_df = renamed_df.withColumn(new_name, col(old_name))\nrenamed_df = renamed_df.drop(old_name)\nelse:\nfor old_name, new_name in cols.items():\nrenamed_df = df.withColumnRenamed(old_name, new_name)\nreturn renamed_df\nreturn inner\n@classmethod\ndef from_avro(\ncls,\nschema: str = None,\nkey_col: str = \"key\",\nvalue_col: str = \"value\",\noptions: dict = None,\nexpand_key: bool = False,\nexpand_value: bool = True,\n) -&gt; Callable:\n\"\"\"Select all attributes from avro.\n        Args:\n            schema: the schema string.\n            key_col: the name of the key column.\n            value_col: the name of the value column.\n            options: extra options (e.g., mode: \"PERMISSIVE\").\n            expand_key: whether you want to expand the content inside the key\n                column or not. Default: false.\n            expand_value: whether you want to expand the content inside the value\n                column or not. Default: true.\n        Returns:\n            Function to be called in .transform() spark function.\n        {{get_example(method_name='from_avro')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ncols_to_select = [\ncolumn for column in df.columns if column not in [key_col, value_col]\n]\nreturn df.select(\n*cols_to_select,\nkey_col,\nfrom_avro(col(value_col), schema, options if options else {}).alias(\nvalue_col\n),\n).select(\n*cols_to_select,\nf\"{key_col}.*\" if expand_key else key_col,\nf\"{value_col}.*\" if expand_value else value_col,\n)\nreturn inner\n@classmethod\ndef from_avro_with_registry(\ncls,\nschema_registry: str,\nvalue_schema: str,\nvalue_col: str = \"value\",\nkey_schema: str = None,\nkey_col: str = \"key\",\nexpand_key: bool = False,\nexpand_value: bool = True,\n) -&gt; Callable:\n\"\"\"Select all attributes from avro using a schema registry.\n        Args:\n            schema_registry: the url to the schema registry.\n            value_schema: the name of the value schema entry in the schema registry.\n            value_col: the name of the value column.\n            key_schema: the name of the key schema entry in the schema\n                registry. Default: None.\n            key_col: the name of the key column.\n            expand_key: whether you want to expand the content inside the key\n                column or not. Default: false.\n            expand_value: whether you want to expand the content inside the value\n                column or not. Default: true.\n        Returns:\n            Function to be called in .transform() spark function.\n        {{get_example(method_name='from_avro_with_registry')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ncols_to_select = [\ncolumn for column in df.columns if column not in [key_col, value_col]\n]\nreturn df.select(  # type: ignore\n*cols_to_select,\n(\nfrom_avro(\ndata=col(key_col),\nsubject=key_schema,\nschemaRegistryAddress=schema_registry,  # type: ignore\n).alias(key_col)\nif key_schema\nelse key_col\n),\nfrom_avro(\ndata=col(value_col),\nsubject=value_schema,\nschemaRegistryAddress=schema_registry,  # type: ignore\n).alias(value_col),\n).select(\n*cols_to_select,\nf\"{key_col}.*\" if expand_key else key_col,\nf\"{value_col}.*\" if expand_value else value_col,\n)\nreturn inner\n@classmethod\ndef from_json(\ncls,\ninput_col: str,\nschema_path: Optional[str] = None,\nschema: Optional[dict] = None,\njson_options: Optional[dict] = None,\ndrop_all_cols: bool = False,\ndisable_dbfs_retry: bool = False,\n) -&gt; Callable:\n\"\"\"Convert a json string into a json column (struct).\n        The new json column can be added to the existing columns (default) or it can\n        replace all the others, being the only one to output. The new column gets the\n        same name as the original one suffixed with '_json'.\n        Args:\n            input_col: dict with columns and respective target names.\n            schema_path: path to the StructType schema (spark schema).\n            schema: dict with the StructType schema (spark schema).\n            json_options: options to parse the json value.\n            drop_all_cols: whether to drop all the input columns or not.\n                Defaults to False.\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='from_json')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif schema_path:\njson_schema = SchemaUtils.from_file(schema_path, disable_dbfs_retry)\nelif schema:\njson_schema = SchemaUtils.from_dict(schema)\nelse:\nraise WrongArgumentsException(\n\"A file or dict schema needs to be provided.\"\n)\nif drop_all_cols:\ndf_with_json = df.select(\nfrom_json(\ncol(input_col).cast(\"string\").alias(f\"{input_col}_json\"),\njson_schema,\njson_options if json_options else {},\n).alias(f\"{input_col}_json\")\n)\nelse:\ndf_with_json = df.select(\n\"*\",\nfrom_json(\ncol(input_col).cast(\"string\").alias(f\"{input_col}_json\"),\njson_schema,\njson_options if json_options else {},\n).alias(f\"{input_col}_json\"),\n)\nreturn df_with_json\nreturn inner\n@classmethod\ndef to_json(\ncls, in_cols: List[str], out_col: str, json_options: Optional[dict] = None\n) -&gt; Callable:\n\"\"\"Convert dataframe columns into a json value.\n        Args:\n            in_cols: name(s) of the input column(s).\n                Example values:\n                \"*\" - all\n                columns; \"my_col\" - one column named \"my_col\";\n                \"my_col1, my_col2\" - two columns.\n            out_col: name of the output column.\n            json_options: options to parse the json value.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='to_json')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.withColumn(\nout_col, to_json(struct(*in_cols), json_options if json_options else {})\n)\nreturn inner\n@classmethod\ndef _explode_arrays(cls, df: DataFrame, cols_to_explode: List[str]) -&gt; DataFrame:\n\"\"\"Explode array columns from dataframe.\n        Args:\n            df: the dataframe to apply the explode operation.\n            cols_to_explode: list of array columns to perform explode.\n        Returns:\n            A dataframe with array columns exploded.\n        \"\"\"\nif cols_to_explode is None:\ncols_to_explode = cls._get_columns(df, spark_types.ArrayType)\nfor column in cols_to_explode:\ndf = df.withColumn(column, explode_outer(column))\nreturn df\n@classmethod\ndef _explode_maps(cls, df: DataFrame, cols_to_explode: List[str]) -&gt; DataFrame:\n\"\"\"Explode map columns from dataframe.\n        Args:\n            df: the dataframe to apply the explode operation.\n            cols_to_explode: list of map columns to perform explode.\n        Returns:\n            A dataframe with map columns exploded.\n        \"\"\"\nif cols_to_explode is None:\ncols_to_explode = cls._get_columns(df, spark_types.MapType)\nfor column in cols_to_explode:\ndf = df.withColumn(column, explode_outer(map_entries(col(column))))\nreturn df\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.cast","title":"<code>cast(cols)</code>  <code>classmethod</code>","text":"<p>Cast specific columns into the designated type.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>Dict[str, str]</code> <p>dict with columns and respective target types. Target types need to have the exact name of spark types: https://spark.apache.org/docs/latest/sql-ref-datatypes.html</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of cast (See full example here)<pre><code><pre>38{\n39    \"function\": \"cast\",\n40    \"args\": {\n41        \"cols\": {\n42            \"code\": \"StringType\"\n43        }\n44    }\n45}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef cast(cls, cols: Dict[str, str]) -&gt; Callable:\n\"\"\"Cast specific columns into the designated type.\n    Args:\n        cols: dict with columns and respective target types.\n            Target types need to have the exact name of spark types:\n            https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='cast')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ncast_df = df\nfor c, t in cols.items():\ncast_df = cast_df.withColumn(c, col(c).cast(getattr(spark_types, t)()))\nreturn cast_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.column_selector","title":"<code>column_selector(cols)</code>  <code>classmethod</code>","text":"<p>Select specific columns with specific output aliases.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>OrderedDict</code> <p>dict with columns to select and respective aliases.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef column_selector(cls, cols: OrderedDict) -&gt; Callable:\n\"\"\"Select specific columns with specific output aliases.\n    Args:\n        cols: dict with columns to select and respective aliases.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='column_selector')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.select(*[col(c).alias(a) for c, a in cols.items()])\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.explode_columns","title":"<code>explode_columns(explode_arrays=False, array_cols_to_explode=None, explode_maps=False, map_cols_to_explode=None)</code>  <code>classmethod</code>","text":"<p>Explode columns with types like ArrayType and MapType.</p> <p>After it can be applied the flatten_schema transformation, if we desired for example to explode the map (as we explode a StructType) or to explode a StructType inside the array. We recommend you to specify always the columns desired to explode and not explode all columns.</p> <p>Parameters:</p> Name Type Description Default <code>explode_arrays</code> <code>bool</code> <p>whether you want to explode array columns (True) or not (False). Default: False.</p> <code>False</code> <code>array_cols_to_explode</code> <code>List[str]</code> <p>array columns which you want to explode. If you don't specify it will get all array columns and explode them. Default: None.</p> <code>None</code> <code>explode_maps</code> <code>bool</code> <p>whether you want to explode map columns (True) or not (False). Default: False.</p> <code>False</code> <code>map_cols_to_explode</code> <code>List[str]</code> <p>map columns which you want to explode. If you don't specify it will get all map columns and explode them. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of explode_columns (See full example here)<pre><code><pre>44{\n45    \"function\": \"explode_columns\",\n46    \"args\": {\n47        \"explode_arrays\": true\n48    }\n49}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef explode_columns(\ncls,\nexplode_arrays: bool = False,\narray_cols_to_explode: List[str] = None,\nexplode_maps: bool = False,\nmap_cols_to_explode: List[str] = None,\n) -&gt; Callable:\n\"\"\"Explode columns with types like ArrayType and MapType.\n    After it can be applied the flatten_schema transformation,\n    if we desired for example to explode the map (as we explode a StructType)\n    or to explode a StructType inside the array.\n    We recommend you to specify always the columns desired to explode\n    and not explode all columns.\n    Args:\n        explode_arrays: whether you want to explode array columns (True)\n            or not (False). Default: False.\n        array_cols_to_explode: array columns which you want to explode.\n            If you don't specify it will get all array columns and explode them.\n            Default: None.\n        explode_maps: whether you want to explode map columns (True)\n            or not (False). Default: False.\n        map_cols_to_explode: map columns which you want to explode.\n            If you don't specify it will get all map columns and explode them.\n            Default: None.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='explode_columns')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif explode_arrays or (array_cols_to_explode is not None):\ndf = cls._explode_arrays(df, array_cols_to_explode)\nif explode_maps or (map_cols_to_explode is not None):\ndf = cls._explode_maps(df, map_cols_to_explode)\nreturn df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.flatten_schema","title":"<code>flatten_schema(max_level=None, shorten_names=False, alias=True, num_chars=7, ignore_cols=None)</code>  <code>classmethod</code>","text":"<p>Flatten the schema of the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>max_level</code> <code>int</code> <p>level until which you want to flatten the schema. Default: None.</p> <code>None</code> <code>shorten_names</code> <code>bool</code> <p>whether to shorten the names of the prefixes of the fields being flattened or not. Default: False.</p> <code>False</code> <code>alias</code> <code>bool</code> <p>whether to define alias for the columns being flattened or not. Default: True.</p> <code>True</code> <code>num_chars</code> <code>int</code> <p>number of characters to consider when shortening the names of the fields. Default: 7.</p> <code>7</code> <code>ignore_cols</code> <code>List</code> <p>columns which you don't want to flatten. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of flatten_schema (See full example here)<pre><code><pre> 95{\n 96    \"function\": \"flatten_schema\",\n 97    \"args\": {\n 98        \"max_level\": 2\n 99    }\n100}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef flatten_schema(\ncls,\nmax_level: int = None,\nshorten_names: bool = False,\nalias: bool = True,\nnum_chars: int = 7,\nignore_cols: List = None,\n) -&gt; Callable:\n\"\"\"Flatten the schema of the dataframe.\n    Args:\n        max_level: level until which you want to flatten the schema.\n            Default: None.\n        shorten_names: whether to shorten the names of the prefixes\n            of the fields being flattened or not. Default: False.\n        alias: whether to define alias for the columns being flattened\n            or not. Default: True.\n        num_chars: number of characters to consider when shortening\n            the names of the fields. Default: 7.\n        ignore_cols: columns which you don't want to flatten.\n            Default: None.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='flatten_schema')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.select(\nSchemaUtils.schema_flattener(\nschema=df.schema,\nmax_level=max_level,\nshorten_names=shorten_names,\nalias=alias,\nnum_chars=num_chars,\nignore_cols=ignore_cols,\n)\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.from_avro","title":"<code>from_avro(schema=None, key_col='key', value_col='value', options=None, expand_key=False, expand_value=True)</code>  <code>classmethod</code>","text":"<p>Select all attributes from avro.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>the schema string.</p> <code>None</code> <code>key_col</code> <code>str</code> <p>the name of the key column.</p> <code>'key'</code> <code>value_col</code> <code>str</code> <p>the name of the value column.</p> <code>'value'</code> <code>options</code> <code>dict</code> <p>extra options (e.g., mode: \"PERMISSIVE\").</p> <code>None</code> <code>expand_key</code> <code>bool</code> <p>whether you want to expand the content inside the key column or not. Default: false.</p> <code>False</code> <code>expand_value</code> <code>bool</code> <p>whether you want to expand the content inside the value column or not. Default: true.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef from_avro(\ncls,\nschema: str = None,\nkey_col: str = \"key\",\nvalue_col: str = \"value\",\noptions: dict = None,\nexpand_key: bool = False,\nexpand_value: bool = True,\n) -&gt; Callable:\n\"\"\"Select all attributes from avro.\n    Args:\n        schema: the schema string.\n        key_col: the name of the key column.\n        value_col: the name of the value column.\n        options: extra options (e.g., mode: \"PERMISSIVE\").\n        expand_key: whether you want to expand the content inside the key\n            column or not. Default: false.\n        expand_value: whether you want to expand the content inside the value\n            column or not. Default: true.\n    Returns:\n        Function to be called in .transform() spark function.\n    {{get_example(method_name='from_avro')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ncols_to_select = [\ncolumn for column in df.columns if column not in [key_col, value_col]\n]\nreturn df.select(\n*cols_to_select,\nkey_col,\nfrom_avro(col(value_col), schema, options if options else {}).alias(\nvalue_col\n),\n).select(\n*cols_to_select,\nf\"{key_col}.*\" if expand_key else key_col,\nf\"{value_col}.*\" if expand_value else value_col,\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.from_avro_with_registry","title":"<code>from_avro_with_registry(schema_registry, value_schema, value_col='value', key_schema=None, key_col='key', expand_key=False, expand_value=True)</code>  <code>classmethod</code>","text":"<p>Select all attributes from avro using a schema registry.</p> <p>Parameters:</p> Name Type Description Default <code>schema_registry</code> <code>str</code> <p>the url to the schema registry.</p> required <code>value_schema</code> <code>str</code> <p>the name of the value schema entry in the schema registry.</p> required <code>value_col</code> <code>str</code> <p>the name of the value column.</p> <code>'value'</code> <code>key_schema</code> <code>str</code> <p>the name of the key schema entry in the schema registry. Default: None.</p> <code>None</code> <code>key_col</code> <code>str</code> <p>the name of the key column.</p> <code>'key'</code> <code>expand_key</code> <code>bool</code> <p>whether you want to expand the content inside the key column or not. Default: false.</p> <code>False</code> <code>expand_value</code> <code>bool</code> <p>whether you want to expand the content inside the value column or not. Default: true.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef from_avro_with_registry(\ncls,\nschema_registry: str,\nvalue_schema: str,\nvalue_col: str = \"value\",\nkey_schema: str = None,\nkey_col: str = \"key\",\nexpand_key: bool = False,\nexpand_value: bool = True,\n) -&gt; Callable:\n\"\"\"Select all attributes from avro using a schema registry.\n    Args:\n        schema_registry: the url to the schema registry.\n        value_schema: the name of the value schema entry in the schema registry.\n        value_col: the name of the value column.\n        key_schema: the name of the key schema entry in the schema\n            registry. Default: None.\n        key_col: the name of the key column.\n        expand_key: whether you want to expand the content inside the key\n            column or not. Default: false.\n        expand_value: whether you want to expand the content inside the value\n            column or not. Default: true.\n    Returns:\n        Function to be called in .transform() spark function.\n    {{get_example(method_name='from_avro_with_registry')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ncols_to_select = [\ncolumn for column in df.columns if column not in [key_col, value_col]\n]\nreturn df.select(  # type: ignore\n*cols_to_select,\n(\nfrom_avro(\ndata=col(key_col),\nsubject=key_schema,\nschemaRegistryAddress=schema_registry,  # type: ignore\n).alias(key_col)\nif key_schema\nelse key_col\n),\nfrom_avro(\ndata=col(value_col),\nsubject=value_schema,\nschemaRegistryAddress=schema_registry,  # type: ignore\n).alias(value_col),\n).select(\n*cols_to_select,\nf\"{key_col}.*\" if expand_key else key_col,\nf\"{value_col}.*\" if expand_value else value_col,\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.from_json","title":"<code>from_json(input_col, schema_path=None, schema=None, json_options=None, drop_all_cols=False, disable_dbfs_retry=False)</code>  <code>classmethod</code>","text":"<p>Convert a json string into a json column (struct).</p> <p>The new json column can be added to the existing columns (default) or it can replace all the others, being the only one to output. The new column gets the same name as the original one suffixed with '_json'.</p> <p>Parameters:</p> Name Type Description Default <code>input_col</code> <code>str</code> <p>dict with columns and respective target names.</p> required <code>schema_path</code> <code>Optional[str]</code> <p>path to the StructType schema (spark schema).</p> <code>None</code> <code>schema</code> <code>Optional[dict]</code> <p>dict with the StructType schema (spark schema).</p> <code>None</code> <code>json_options</code> <code>Optional[dict]</code> <p>options to parse the json value.</p> <code>None</code> <code>drop_all_cols</code> <code>bool</code> <p>whether to drop all the input columns or not. Defaults to False.</p> <code>False</code> <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of from_json (See full example here)<pre><code><pre>34{\n35    \"function\": \"from_json\",\n36    \"args\": {\n37        \"input_col\": \"sample\",\n38        \"schema\": {\n39            \"type\": \"struct\",\n40            \"fields\": [\n41                {\n42                    \"name\": \"field1\",\n43                    \"type\": \"string\",\n44                    \"nullable\": true,\n45                    \"metadata\": {}\n46                },\n47                {\n48                    \"name\": \"field2\",\n49                    \"type\": \"string\",\n50                    \"nullable\": true,\n51                    \"metadata\": {}\n52                },\n53                {\n54                    \"name\": \"field3\",\n55                    \"type\": \"double\",\n56                    \"nullable\": true,\n57                    \"metadata\": {}\n58                },\n59                {\n60                    \"name\": \"field4\",\n61                    \"type\": {\n62                        \"type\": \"struct\",\n63                        \"fields\": [\n64                            {\n65                                \"name\": \"field1\",\n66                                \"type\": \"string\",\n67                                \"nullable\": true,\n68                                \"metadata\": {}\n69                            },\n70                            {\n71                                \"name\": \"field2\",\n72                                \"type\": \"string\",\n73                                \"nullable\": true,\n74                                \"metadata\": {}\n75                            }\n76                        ]\n77                    },\n78                    \"nullable\": true,\n79                    \"metadata\": {}\n80                }\n81            ]\n82        }\n83    }\n84}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef from_json(\ncls,\ninput_col: str,\nschema_path: Optional[str] = None,\nschema: Optional[dict] = None,\njson_options: Optional[dict] = None,\ndrop_all_cols: bool = False,\ndisable_dbfs_retry: bool = False,\n) -&gt; Callable:\n\"\"\"Convert a json string into a json column (struct).\n    The new json column can be added to the existing columns (default) or it can\n    replace all the others, being the only one to output. The new column gets the\n    same name as the original one suffixed with '_json'.\n    Args:\n        input_col: dict with columns and respective target names.\n        schema_path: path to the StructType schema (spark schema).\n        schema: dict with the StructType schema (spark schema).\n        json_options: options to parse the json value.\n        drop_all_cols: whether to drop all the input columns or not.\n            Defaults to False.\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='from_json')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif schema_path:\njson_schema = SchemaUtils.from_file(schema_path, disable_dbfs_retry)\nelif schema:\njson_schema = SchemaUtils.from_dict(schema)\nelse:\nraise WrongArgumentsException(\n\"A file or dict schema needs to be provided.\"\n)\nif drop_all_cols:\ndf_with_json = df.select(\nfrom_json(\ncol(input_col).cast(\"string\").alias(f\"{input_col}_json\"),\njson_schema,\njson_options if json_options else {},\n).alias(f\"{input_col}_json\")\n)\nelse:\ndf_with_json = df.select(\n\"*\",\nfrom_json(\ncol(input_col).cast(\"string\").alias(f\"{input_col}_json\"),\njson_schema,\njson_options if json_options else {},\n).alias(f\"{input_col}_json\"),\n)\nreturn df_with_json\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.rename","title":"<code>rename(cols, escape_col_names=True)</code>  <code>classmethod</code>","text":"<p>Rename specific columns into the designated name.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>Dict[str, str]</code> <p>dict with columns and respective target names.</p> required <code>escape_col_names</code> <code>bool</code> <p>whether to escape column names (e.g. <code>/BIC/COL1</code>) or not. If True it creates a column with the new name and drop the old one. If False, uses the native withColumnRenamed Spark function. Default: True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Function to be called in .transform() spark function.</p> View Example of rename (See full example here)<pre><code><pre>37{\n38    \"function\": \"rename\",\n39    \"args\": {\n40        \"cols\": {\n41            \"ARTICLE\": \"article\"\n42        }\n43    }\n44}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef rename(cls, cols: Dict[str, str], escape_col_names: bool = True) -&gt; Callable:\n\"\"\"Rename specific columns into the designated name.\n    Args:\n        cols: dict with columns and respective target names.\n        escape_col_names: whether to escape column names (e.g. `/BIC/COL1`) or not.\n            If True it creates a column with the new name and drop the old one.\n            If False, uses the native withColumnRenamed Spark function.\n            Default: True.\n    Returns:\n        Function to be called in .transform() spark function.\n    {{get_example(method_name='rename')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nrenamed_df = df\nif escape_col_names:\nfor old_name, new_name in cols.items():\nrenamed_df = renamed_df.withColumn(new_name, col(old_name))\nrenamed_df = renamed_df.drop(old_name)\nelse:\nfor old_name, new_name in cols.items():\nrenamed_df = df.withColumnRenamed(old_name, new_name)\nreturn renamed_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.to_json","title":"<code>to_json(in_cols, out_col, json_options=None)</code>  <code>classmethod</code>","text":"<p>Convert dataframe columns into a json value.</p> <p>Parameters:</p> Name Type Description Default <code>in_cols</code> <code>List[str]</code> <p>name(s) of the input column(s). Example values: \"*\" - all columns; \"my_col\" - one column named \"my_col\"; \"my_col1, my_col2\" - two columns.</p> required <code>out_col</code> <code>str</code> <p>name of the output column.</p> required <code>json_options</code> <code>Optional[dict]</code> <p>options to parse the json value.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of to_json (See full example here)<pre><code><pre>85{\n86    \"function\": \"to_json\",\n87    \"args\": {\n88        \"in_cols\": [\n89            \"item\",\n90            \"amount\"\n91        ],\n92        \"out_col\": \"item_amount_json\"\n93    }\n94}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef to_json(\ncls, in_cols: List[str], out_col: str, json_options: Optional[dict] = None\n) -&gt; Callable:\n\"\"\"Convert dataframe columns into a json value.\n    Args:\n        in_cols: name(s) of the input column(s).\n            Example values:\n            \"*\" - all\n            columns; \"my_col\" - one column named \"my_col\";\n            \"my_col1, my_col2\" - two columns.\n        out_col: name of the output column.\n        json_options: options to parse the json value.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='to_json')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.withColumn(\nout_col, to_json(struct(*in_cols), json_options if json_options else {})\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/column_reshapers.html#packages.transformers.column_reshapers.ColumnReshapers.with_expressions","title":"<code>with_expressions(cols_and_exprs)</code>  <code>classmethod</code>","text":"<p>Execute Spark SQL expressions to create the specified columns.</p> <p>This function uses the Spark expr function. Check here.</p> <p>Parameters:</p> Name Type Description Default <code>cols_and_exprs</code> <code>Dict[str, str]</code> <p>dict with columns and respective expressions to compute (Spark SQL expressions).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of with_expressions (See full example here)<pre><code><pre>25{\n26    \"function\": \"with_expressions\",\n27    \"args\": {\n28        \"cols_and_exprs\": {\n29            \"constant\": \"'just a constant'\",\n30            \"length_customer2\": \"length(customer2)\"\n31        }\n32    }\n33}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/column_reshapers.py</code> <pre><code>@classmethod\ndef with_expressions(cls, cols_and_exprs: Dict[str, str]) -&gt; Callable:\n\"\"\"Execute Spark SQL expressions to create the specified columns.\n    This function uses the Spark expr function. [Check here](\n    https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html).\n    Args:\n        cols_and_exprs: dict with columns and respective expressions to compute\n            (Spark SQL expressions).\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='with_expressions')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nenriched_df = df\nfor c, e in cols_and_exprs.items():\nenriched_df = enriched_df.withColumn(c, expr(e))\nreturn enriched_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/condensers.html","title":"Condensers","text":"<p>Condensers module.</p>"},{"location":"reference/packages/transformers/condensers.html#packages.transformers.condensers.Condensers","title":"<code>Condensers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing all the functions to condensate data for later merges.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/condensers.py</code> <pre><code>class Condensers(object):\n\"\"\"Class containing all the functions to condensate data for later merges.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef condense_record_mode_cdc(\ncls,\nbusiness_key: List[str],\nrecord_mode_col: str,\nvalid_record_modes: List[str],\nranking_key_desc: Optional[List[str]] = None,\nranking_key_asc: Optional[List[str]] = None,\n) -&gt; Callable:\n\"\"\"Condense Change Data Capture (CDC) based on record_mode strategy.\n        This CDC data is particularly seen in some CDC enabled systems. Other systems\n        may have different CDC strategies.\n        Args:\n            business_key: The business key (logical primary key) of the data.\n            ranking_key_desc: In this type of CDC condensation the data needs to be\n                in descending order in a certain way, using columns specified in this\n                parameter.\n            ranking_key_asc: In this type of CDC condensation the data needs to be\n                in ascending order in a certain way, using columns specified in\n                this parameter.\n            record_mode_col: Name of the record mode input_col.\n            valid_record_modes: Depending on the context, not all record modes may be\n                considered for condensation. Use this parameter to skip those.\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='condense_record_mode_cdc')}}\n        \"\"\"\nif not ranking_key_desc and not ranking_key_asc:\nraise WrongArgumentsException(\n\"The condense_record_mode_cdc transformer requires data to be either\"\n\"in descending or ascending order, but no arguments for ordering\"\n\"were provided.\"\n)\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\npartition_window = Window.partitionBy(\n[col(c) for c in business_key]\n).orderBy(\n[\ncol(c).desc()\nfor c in (ranking_key_desc if ranking_key_desc else [])\n]  # type: ignore\n+ [\ncol(c).asc()\nfor c in (ranking_key_asc if ranking_key_asc else [])\n]  # type: ignore\n)\nreturn (\ndf.withColumn(\"ranking\", row_number().over(partition_window))\n.filter(\ncol(record_mode_col).isNull()\n| col(record_mode_col).isin(valid_record_modes)\n)\n.filter(col(\"ranking\") == 1)\n.drop(\"ranking\")\n)\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer condense_record_mode_cdc is not supported in \"\n\"streaming mode.\"\n)\nreturn inner\n@classmethod\ndef group_and_rank(\ncls, group_key: List[str], ranking_key: List[str], descending: bool = True\n) -&gt; Callable:\n\"\"\"Condense data based on a simple group by + take latest mechanism.\n        Args:\n            group_key: list of column names to use in the group by.\n            ranking_key: the data needs to be in descending order using columns\n                specified in this parameter.\n            descending: if the ranking considers descending order or not. Defaults to\n                True.\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='group_and_rank')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\npartition_window = Window.partitionBy(\n[col(c) for c in group_key]\n).orderBy(\n[\ncol(c).desc() if descending else col(c).asc()\nfor c in (ranking_key if ranking_key else [])\n]  # type: ignore\n)\nreturn (\ndf.withColumn(\"ranking\", row_number().over(partition_window))\n.filter(col(\"ranking\") == 1)\n.drop(\"ranking\")\n)\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer group_and_rank is not supported in streaming mode.\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/condensers.html#packages.transformers.condensers.Condensers.condense_record_mode_cdc","title":"<code>condense_record_mode_cdc(business_key, record_mode_col, valid_record_modes, ranking_key_desc=None, ranking_key_asc=None)</code>  <code>classmethod</code>","text":"<p>Condense Change Data Capture (CDC) based on record_mode strategy.</p> <p>This CDC data is particularly seen in some CDC enabled systems. Other systems may have different CDC strategies.</p> <p>Parameters:</p> Name Type Description Default <code>business_key</code> <code>List[str]</code> <p>The business key (logical primary key) of the data.</p> required <code>ranking_key_desc</code> <code>Optional[List[str]]</code> <p>In this type of CDC condensation the data needs to be in descending order in a certain way, using columns specified in this parameter.</p> <code>None</code> <code>ranking_key_asc</code> <code>Optional[List[str]]</code> <p>In this type of CDC condensation the data needs to be in ascending order in a certain way, using columns specified in this parameter.</p> <code>None</code> <code>record_mode_col</code> <code>str</code> <p>Name of the record mode input_col.</p> required <code>valid_record_modes</code> <code>List[str]</code> <p>Depending on the context, not all record modes may be considered for condensation. Use this parameter to skip those.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of condense_record_mode_cdc (See full example here)<pre><code><pre>20{\n21    \"function\": \"condense_record_mode_cdc\",\n22    \"args\": {\n23        \"business_key\": [\n24            \"salesorder\",\n25            \"item\"\n26        ],\n27        \"ranking_key_desc\": [\n28            \"extraction_timestamp\",\n29            \"actrequest_timestamp\",\n30            \"datapakid\",\n31            \"partno\",\n32            \"record\"\n33        ],\n34        \"record_mode_col\": \"recordmode\",\n35        \"valid_record_modes\": [\n36            \"\",\n37            \"N\",\n38            \"R\",\n39            \"D\",\n40            \"X\"\n41        ]\n42    }\n43}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/condensers.py</code> <pre><code>@classmethod\ndef condense_record_mode_cdc(\ncls,\nbusiness_key: List[str],\nrecord_mode_col: str,\nvalid_record_modes: List[str],\nranking_key_desc: Optional[List[str]] = None,\nranking_key_asc: Optional[List[str]] = None,\n) -&gt; Callable:\n\"\"\"Condense Change Data Capture (CDC) based on record_mode strategy.\n    This CDC data is particularly seen in some CDC enabled systems. Other systems\n    may have different CDC strategies.\n    Args:\n        business_key: The business key (logical primary key) of the data.\n        ranking_key_desc: In this type of CDC condensation the data needs to be\n            in descending order in a certain way, using columns specified in this\n            parameter.\n        ranking_key_asc: In this type of CDC condensation the data needs to be\n            in ascending order in a certain way, using columns specified in\n            this parameter.\n        record_mode_col: Name of the record mode input_col.\n        valid_record_modes: Depending on the context, not all record modes may be\n            considered for condensation. Use this parameter to skip those.\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='condense_record_mode_cdc')}}\n    \"\"\"\nif not ranking_key_desc and not ranking_key_asc:\nraise WrongArgumentsException(\n\"The condense_record_mode_cdc transformer requires data to be either\"\n\"in descending or ascending order, but no arguments for ordering\"\n\"were provided.\"\n)\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\npartition_window = Window.partitionBy(\n[col(c) for c in business_key]\n).orderBy(\n[\ncol(c).desc()\nfor c in (ranking_key_desc if ranking_key_desc else [])\n]  # type: ignore\n+ [\ncol(c).asc()\nfor c in (ranking_key_asc if ranking_key_asc else [])\n]  # type: ignore\n)\nreturn (\ndf.withColumn(\"ranking\", row_number().over(partition_window))\n.filter(\ncol(record_mode_col).isNull()\n| col(record_mode_col).isin(valid_record_modes)\n)\n.filter(col(\"ranking\") == 1)\n.drop(\"ranking\")\n)\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer condense_record_mode_cdc is not supported in \"\n\"streaming mode.\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/condensers.html#packages.transformers.condensers.Condensers.group_and_rank","title":"<code>group_and_rank(group_key, ranking_key, descending=True)</code>  <code>classmethod</code>","text":"<p>Condense data based on a simple group by + take latest mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>group_key</code> <code>List[str]</code> <p>list of column names to use in the group by.</p> required <code>ranking_key</code> <code>List[str]</code> <p>the data needs to be in descending order using columns specified in this parameter.</p> required <code>descending</code> <code>bool</code> <p>if the ranking considers descending order or not. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of group_and_rank (See full example here)<pre><code><pre>59{\n60    \"function\": \"group_and_rank\",\n61    \"args\": {\n62        \"group_key\": [\n63            \"salesorder\",\n64            \"item\"\n65        ],\n66        \"ranking_key\": [\n67            \"extraction_date\",\n68            \"changed_on\",\n69            \"lhe_row_id\"\n70        ]\n71    }\n72}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/condensers.py</code> <pre><code>@classmethod\ndef group_and_rank(\ncls, group_key: List[str], ranking_key: List[str], descending: bool = True\n) -&gt; Callable:\n\"\"\"Condense data based on a simple group by + take latest mechanism.\n    Args:\n        group_key: list of column names to use in the group by.\n        ranking_key: the data needs to be in descending order using columns\n            specified in this parameter.\n        descending: if the ranking considers descending order or not. Defaults to\n            True.\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='group_and_rank')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif not df.isStreaming:\npartition_window = Window.partitionBy(\n[col(c) for c in group_key]\n).orderBy(\n[\ncol(c).desc() if descending else col(c).asc()\nfor c in (ranking_key if ranking_key else [])\n]  # type: ignore\n)\nreturn (\ndf.withColumn(\"ranking\", row_number().over(partition_window))\n.filter(col(\"ranking\") == 1)\n.drop(\"ranking\")\n)\nelse:\nraise UnsupportedStreamingTransformerException(\n\"Transformer group_and_rank is not supported in streaming mode.\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/custom_transformers.html","title":"Custom transformers","text":"<p>Custom transformers module.</p>"},{"location":"reference/packages/transformers/custom_transformers.html#packages.transformers.custom_transformers.CustomTransformers","title":"<code>CustomTransformers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class representing a CustomTransformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/custom_transformers.py</code> <pre><code>class CustomTransformers(object):\n\"\"\"Class representing a CustomTransformers.\"\"\"\n@staticmethod\ndef custom_transformation(custom_transformer: Callable) -&gt; Callable:\n\"\"\"Execute a custom transformation provided by the user.\n        This transformer can be very useful whenever the user cannot use our provided\n        transformers, or they want to write complex logic in the transform step of the\n        algorithm.\n        .. warning:: Attention!\n            Please bear in mind that the custom_transformer function provided\n            as argument needs to receive a DataFrame and return a DataFrame,\n            because it is how Spark's .transform method is able to chain the\n            transformations.\n        Example:\n        ```python\n        def my_custom_logic(df: DataFrame) -&gt; DataFrame:\n        ```\n        Args:\n            custom_transformer: custom transformer function. A python function with all\n                required pyspark logic provided by the user.\n        Returns:\n            Callable: the same function provided as parameter, in order to e called\n                later in the TransformerFactory.\n        {{get_example(method_name='custom_transformation')}}\n        \"\"\"\nreturn custom_transformer\n@staticmethod\ndef sql_transformation(sql: str) -&gt; Callable:\n\"\"\"Execute a SQL transformation provided by the user.\n        This transformer can be very useful whenever the user wants to perform\n        SQL-based transformations that are not natively supported by the\n        lakehouse engine transformers.\n        Args:\n            sql: the SQL query to be executed. This can read from any table or\n                view from the catalog, or any dataframe registered as a temp\n                view.\n        Returns:\n            Callable: A function to be called in .transform() spark function.\n        {{get_example(method_name='sql_transformation')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.sparkSession.sql(sql)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/custom_transformers.html#packages.transformers.custom_transformers.CustomTransformers.custom_transformation","title":"<code>custom_transformation(custom_transformer)</code>  <code>staticmethod</code>","text":"<p>Execute a custom transformation provided by the user.</p> <p>This transformer can be very useful whenever the user cannot use our provided transformers, or they want to write complex logic in the transform step of the algorithm.</p> <p>.. warning:: Attention!     Please bear in mind that the custom_transformer function provided     as argument needs to receive a DataFrame and return a DataFrame,     because it is how Spark's .transform method is able to chain the     transformations.</p> <p>Example: <pre><code>def my_custom_logic(df: DataFrame) -&gt; DataFrame:\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>custom_transformer</code> <code>Callable</code> <p>custom transformer function. A python function with all required pyspark logic provided by the user.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>the same function provided as parameter, in order to e called later in the TransformerFactory.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/custom_transformers.py</code> <pre><code>@staticmethod\ndef custom_transformation(custom_transformer: Callable) -&gt; Callable:\n\"\"\"Execute a custom transformation provided by the user.\n    This transformer can be very useful whenever the user cannot use our provided\n    transformers, or they want to write complex logic in the transform step of the\n    algorithm.\n    .. warning:: Attention!\n        Please bear in mind that the custom_transformer function provided\n        as argument needs to receive a DataFrame and return a DataFrame,\n        because it is how Spark's .transform method is able to chain the\n        transformations.\n    Example:\n    ```python\n    def my_custom_logic(df: DataFrame) -&gt; DataFrame:\n    ```\n    Args:\n        custom_transformer: custom transformer function. A python function with all\n            required pyspark logic provided by the user.\n    Returns:\n        Callable: the same function provided as parameter, in order to e called\n            later in the TransformerFactory.\n    {{get_example(method_name='custom_transformation')}}\n    \"\"\"\nreturn custom_transformer\n</code></pre>"},{"location":"reference/packages/transformers/custom_transformers.html#packages.transformers.custom_transformers.CustomTransformers.sql_transformation","title":"<code>sql_transformation(sql)</code>  <code>staticmethod</code>","text":"<p>Execute a SQL transformation provided by the user.</p> <p>This transformer can be very useful whenever the user wants to perform SQL-based transformations that are not natively supported by the lakehouse engine transformers.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>the SQL query to be executed. This can read from any table or view from the catalog, or any dataframe registered as a temp view.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/custom_transformers.py</code> <pre><code>@staticmethod\ndef sql_transformation(sql: str) -&gt; Callable:\n\"\"\"Execute a SQL transformation provided by the user.\n    This transformer can be very useful whenever the user wants to perform\n    SQL-based transformations that are not natively supported by the\n    lakehouse engine transformers.\n    Args:\n        sql: the SQL query to be executed. This can read from any table or\n            view from the catalog, or any dataframe registered as a temp\n            view.\n    Returns:\n        Callable: A function to be called in .transform() spark function.\n    {{get_example(method_name='sql_transformation')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.sparkSession.sql(sql)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/data_maskers.html","title":"Data maskers","text":"<p>Module with data masking transformers.</p>"},{"location":"reference/packages/transformers/data_maskers.html#packages.transformers.data_maskers.DataMaskers","title":"<code>DataMaskers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing data masking transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/data_maskers.py</code> <pre><code>class DataMaskers(object):\n\"\"\"Class containing data masking transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef hash_masker(\ncls,\ncols: List[str],\napproach: str = \"SHA\",\nnum_bits: int = 256,\nsuffix: str = \"_hash\",\n) -&gt; Callable:\n\"\"\"Mask specific columns using an hashing approach.\n        Args:\n            cols: list of column names to mask.\n            approach: hashing approach. Defaults to 'SHA'. There's \"MURMUR3\" as well.\n            num_bits: number of bits of the SHA approach. Only applies to SHA approach.\n            suffix: suffix to apply to new column name. Defaults to \"_hash\".\n                Note: you can pass an empty suffix to have the original column replaced.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='hash_masker')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nmasked_df = df\nfor col in cols:\nif approach == \"MURMUR3\":\nmasked_df = masked_df.withColumn(col + suffix, hash(col))\nelif approach == \"SHA\":\nmasked_df = masked_df.withColumn(col + suffix, sha2(col, num_bits))\nelse:\nraise WrongArgumentsException(\"Hashing approach is not supported.\")\nreturn masked_df\nreturn inner\n@classmethod\ndef column_dropper(cls, cols: List[str]) -&gt; Callable:\n\"\"\"Drop specific columns.\n        Args:\n            cols: list of column names to drop.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='column_dropper')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ndrop_df = df\nfor col in cols:\ndrop_df = drop_df.drop(col)\nreturn drop_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/data_maskers.html#packages.transformers.data_maskers.DataMaskers.column_dropper","title":"<code>column_dropper(cols)</code>  <code>classmethod</code>","text":"<p>Drop specific columns.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>list of column names to drop.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of column_dropper (See full example here)<pre><code><pre>21{\n22    \"function\": \"column_dropper\",\n23    \"args\": {\n24        \"cols\": [\n25            \"customer\",\n26            \"article\"\n27        ]\n28    }\n29}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/data_maskers.py</code> <pre><code>@classmethod\ndef column_dropper(cls, cols: List[str]) -&gt; Callable:\n\"\"\"Drop specific columns.\n    Args:\n        cols: list of column names to drop.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='column_dropper')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ndrop_df = df\nfor col in cols:\ndrop_df = drop_df.drop(col)\nreturn drop_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/data_maskers.html#packages.transformers.data_maskers.DataMaskers.hash_masker","title":"<code>hash_masker(cols, approach='SHA', num_bits=256, suffix='_hash')</code>  <code>classmethod</code>","text":"<p>Mask specific columns using an hashing approach.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>list of column names to mask.</p> required <code>approach</code> <code>str</code> <p>hashing approach. Defaults to 'SHA'. There's \"MURMUR3\" as well.</p> <code>'SHA'</code> <code>num_bits</code> <code>int</code> <p>number of bits of the SHA approach. Only applies to SHA approach.</p> <code>256</code> <code>suffix</code> <code>str</code> <p>suffix to apply to new column name. Defaults to \"_hash\". Note: you can pass an empty suffix to have the original column replaced.</p> <code>'_hash'</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of hash_masker (See full example here)<pre><code><pre>21{\n22    \"function\": \"hash_masker\",\n23    \"args\": {\n24        \"cols\": [\n25            \"customer\",\n26            \"article\"\n27        ]\n28    }\n29}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/data_maskers.py</code> <pre><code>@classmethod\ndef hash_masker(\ncls,\ncols: List[str],\napproach: str = \"SHA\",\nnum_bits: int = 256,\nsuffix: str = \"_hash\",\n) -&gt; Callable:\n\"\"\"Mask specific columns using an hashing approach.\n    Args:\n        cols: list of column names to mask.\n        approach: hashing approach. Defaults to 'SHA'. There's \"MURMUR3\" as well.\n        num_bits: number of bits of the SHA approach. Only applies to SHA approach.\n        suffix: suffix to apply to new column name. Defaults to \"_hash\".\n            Note: you can pass an empty suffix to have the original column replaced.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='hash_masker')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nmasked_df = df\nfor col in cols:\nif approach == \"MURMUR3\":\nmasked_df = masked_df.withColumn(col + suffix, hash(col))\nelif approach == \"SHA\":\nmasked_df = masked_df.withColumn(col + suffix, sha2(col, num_bits))\nelse:\nraise WrongArgumentsException(\"Hashing approach is not supported.\")\nreturn masked_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/date_transformers.html","title":"Date transformers","text":"<p>Module containing date transformers.</p>"},{"location":"reference/packages/transformers/date_transformers.html#packages.transformers.date_transformers.DateTransformers","title":"<code>DateTransformers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class with set of transformers to transform dates in several forms.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/date_transformers.py</code> <pre><code>class DateTransformers(object):\n\"\"\"Class with set of transformers to transform dates in several forms.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef add_current_date(output_col: str) -&gt; Callable:\n\"\"\"Add column with current date.\n        The current date comes from the driver as a constant, not from every executor.\n        Args:\n            output_col: name of the output column.\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='add_current_date')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.withColumn(output_col, lit(datetime.now()))\nreturn inner\n@staticmethod\ndef convert_to_date(\ncols: List[str], source_format: Optional[str] = None\n) -&gt; Callable:\n\"\"\"Convert multiple string columns with a source format into dates.\n        Args:\n            cols: list of names of the string columns to convert.\n            source_format: dates source format (e.g., YYYY-MM-dd). [Check here](\n                https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='convert_to_date')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nconverted_df = df\nfor c in cols:\nconverted_df = converted_df.withColumn(\nc, to_date(col(c), source_format)\n)\nreturn converted_df\nreturn inner\n@staticmethod\ndef convert_to_timestamp(\ncols: List[str], source_format: Optional[str] = None\n) -&gt; Callable:\n\"\"\"Convert multiple string columns with a source format into timestamps.\n        Args:\n            cols: list of names of the string columns to convert.\n            source_format: dates source format (e.g., MM-dd-yyyy HH:mm:ss.SSS).\n                [Check here](\n                https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='convert_to_timestamp')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nconverted_df = df\nfor c in cols:\nconverted_df = converted_df.withColumn(\nc, to_timestamp(col(c), source_format)\n)\nreturn converted_df\nreturn inner\n@staticmethod\ndef format_date(cols: List[str], target_format: Optional[str] = None) -&gt; Callable:\n\"\"\"Convert multiple date/timestamp columns into strings with the target format.\n        Args:\n            cols: list of names of the string columns to convert.\n            target_format: strings target format (e.g., YYYY-MM-dd). [Check here](\n                https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='format_date')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nconverted_df = df\nfor c in cols:\nconverted_df = converted_df.withColumn(\nc, date_format(col(c), target_format)\n)\nreturn converted_df\nreturn inner\n@staticmethod\ndef get_date_hierarchy(cols: List[str], formats: Optional[dict] = None) -&gt; Callable:\n\"\"\"Create day/month/week/quarter/year hierarchy for the provided date columns.\n        Uses Spark's extract function.\n        Args:\n            cols: list of names of the date columns to create the hierarchy.\n            formats: dict with the correspondence between the hierarchy and the format\n                to apply. [Check here](\n                https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n                Example: {\n                    \"year\": \"year\",\n                    \"month\": \"month\",\n                    \"day\": \"day\",\n                    \"week\": \"week\",\n                    \"quarter\": \"quarter\"\n                }\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='get_date_hierarchy')}}\n        \"\"\"\nif not formats:\nformats = {\n\"year\": \"year\",\n\"month\": \"month\",\n\"day\": \"day\",\n\"week\": \"week\",\n\"quarter\": \"quarter\",\n}\ndef inner(df: DataFrame) -&gt; DataFrame:\ntransformer_df = df\nfor c in cols:\ntransformer_df = transformer_df.selectExpr(\n\"*\",\nf\"extract({formats['day']} from {c}) as {c}_day\",\nf\"extract({formats['month']} from {c}) as {c}_month\",\nf\"extract({formats['week']} from {c}) as {c}_week\",\nf\"extract({formats['quarter']} from {c}) as {c}_quarter\",\nf\"extract({formats['year']} from {c}) as {c}_year\",\n)\nreturn transformer_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/date_transformers.html#packages.transformers.date_transformers.DateTransformers.add_current_date","title":"<code>add_current_date(output_col)</code>  <code>staticmethod</code>","text":"<p>Add column with current date.</p> <p>The current date comes from the driver as a constant, not from every executor.</p> <p>Parameters:</p> Name Type Description Default <code>output_col</code> <code>str</code> <p>name of the output column.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of add_current_date (See full example here)<pre><code><pre>21{\n22    \"function\": \"add_current_date\",\n23    \"args\": {\n24        \"output_col\": \"curr_date\"\n25    }\n26}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/date_transformers.py</code> <pre><code>@staticmethod\ndef add_current_date(output_col: str) -&gt; Callable:\n\"\"\"Add column with current date.\n    The current date comes from the driver as a constant, not from every executor.\n    Args:\n        output_col: name of the output column.\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='add_current_date')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.withColumn(output_col, lit(datetime.now()))\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/date_transformers.html#packages.transformers.date_transformers.DateTransformers.convert_to_date","title":"<code>convert_to_date(cols, source_format=None)</code>  <code>staticmethod</code>","text":"<p>Convert multiple string columns with a source format into dates.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>list of names of the string columns to convert.</p> required <code>source_format</code> <code>Optional[str]</code> <p>dates source format (e.g., YYYY-MM-dd). Check here.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of convert_to_date (See full example here)<pre><code><pre>27{\n28    \"function\": \"convert_to_date\",\n29    \"args\": {\n30        \"cols\": [\n31            \"order_date2\"\n32        ],\n33        \"source_format\": \"dd-MM-yyyy\"\n34    }\n35}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/date_transformers.py</code> <pre><code>@staticmethod\ndef convert_to_date(\ncols: List[str], source_format: Optional[str] = None\n) -&gt; Callable:\n\"\"\"Convert multiple string columns with a source format into dates.\n    Args:\n        cols: list of names of the string columns to convert.\n        source_format: dates source format (e.g., YYYY-MM-dd). [Check here](\n            https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='convert_to_date')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nconverted_df = df\nfor c in cols:\nconverted_df = converted_df.withColumn(\nc, to_date(col(c), source_format)\n)\nreturn converted_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/date_transformers.html#packages.transformers.date_transformers.DateTransformers.convert_to_timestamp","title":"<code>convert_to_timestamp(cols, source_format=None)</code>  <code>staticmethod</code>","text":"<p>Convert multiple string columns with a source format into timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>list of names of the string columns to convert.</p> required <code>source_format</code> <code>Optional[str]</code> <p>dates source format (e.g., MM-dd-yyyy HHss.SSS). Check here.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of convert_to_timestamp (See full example here)<pre><code><pre>41{\n42    \"function\": \"convert_to_timestamp\",\n43    \"args\": {\n44        \"cols\": [\n45            \"ship_date\"\n46        ],\n47        \"source_format\": \"yyyy-dd-MM HH:mm:ss\"\n48    }\n49}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/date_transformers.py</code> <pre><code>@staticmethod\ndef convert_to_timestamp(\ncols: List[str], source_format: Optional[str] = None\n) -&gt; Callable:\n\"\"\"Convert multiple string columns with a source format into timestamps.\n    Args:\n        cols: list of names of the string columns to convert.\n        source_format: dates source format (e.g., MM-dd-yyyy HH:mm:ss.SSS).\n            [Check here](\n            https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='convert_to_timestamp')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nconverted_df = df\nfor c in cols:\nconverted_df = converted_df.withColumn(\nc, to_timestamp(col(c), source_format)\n)\nreturn converted_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/date_transformers.html#packages.transformers.date_transformers.DateTransformers.format_date","title":"<code>format_date(cols, target_format=None)</code>  <code>staticmethod</code>","text":"<p>Convert multiple date/timestamp columns into strings with the target format.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>list of names of the string columns to convert.</p> required <code>target_format</code> <code>Optional[str]</code> <p>strings target format (e.g., YYYY-MM-dd). Check here.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of format_date (See full example here)<pre><code><pre>48{\n49    \"function\": \"format_date\",\n50    \"args\": {\n51        \"cols\": [\n52            \"order_date3\",\n53            \"ship_date\"\n54        ],\n55        \"target_format\": \"yy-d-M\"\n56    }\n57}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/date_transformers.py</code> <pre><code>@staticmethod\ndef format_date(cols: List[str], target_format: Optional[str] = None) -&gt; Callable:\n\"\"\"Convert multiple date/timestamp columns into strings with the target format.\n    Args:\n        cols: list of names of the string columns to convert.\n        target_format: strings target format (e.g., YYYY-MM-dd). [Check here](\n            https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='format_date')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nconverted_df = df\nfor c in cols:\nconverted_df = converted_df.withColumn(\nc, date_format(col(c), target_format)\n)\nreturn converted_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/date_transformers.html#packages.transformers.date_transformers.DateTransformers.get_date_hierarchy","title":"<code>get_date_hierarchy(cols, formats=None)</code>  <code>staticmethod</code>","text":"<p>Create day/month/week/quarter/year hierarchy for the provided date columns.</p> <p>Uses Spark's extract function.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>list of names of the date columns to create the hierarchy.</p> required <code>formats</code> <code>Optional[dict]</code> <p>dict with the correspondence between the hierarchy and the format to apply. Check here. Example: {     \"year\": \"year\",     \"month\": \"month\",     \"day\": \"day\",     \"week\": \"week\",     \"quarter\": \"quarter\" }</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of get_date_hierarchy (See full example here)<pre><code><pre>55{\n56    \"function\": \"get_date_hierarchy\",\n57    \"args\": {\n58        \"cols\": [\n59            \"order_date2\",\n60            \"ship_date2\"\n61        ]\n62    }\n63}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/date_transformers.py</code> <pre><code>@staticmethod\ndef get_date_hierarchy(cols: List[str], formats: Optional[dict] = None) -&gt; Callable:\n\"\"\"Create day/month/week/quarter/year hierarchy for the provided date columns.\n    Uses Spark's extract function.\n    Args:\n        cols: list of names of the date columns to create the hierarchy.\n        formats: dict with the correspondence between the hierarchy and the format\n            to apply. [Check here](\n            https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html).\n            Example: {\n                \"year\": \"year\",\n                \"month\": \"month\",\n                \"day\": \"day\",\n                \"week\": \"week\",\n                \"quarter\": \"quarter\"\n            }\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='get_date_hierarchy')}}\n    \"\"\"\nif not formats:\nformats = {\n\"year\": \"year\",\n\"month\": \"month\",\n\"day\": \"day\",\n\"week\": \"week\",\n\"quarter\": \"quarter\",\n}\ndef inner(df: DataFrame) -&gt; DataFrame:\ntransformer_df = df\nfor c in cols:\ntransformer_df = transformer_df.selectExpr(\n\"*\",\nf\"extract({formats['day']} from {c}) as {c}_day\",\nf\"extract({formats['month']} from {c}) as {c}_month\",\nf\"extract({formats['week']} from {c}) as {c}_week\",\nf\"extract({formats['quarter']} from {c}) as {c}_quarter\",\nf\"extract({formats['year']} from {c}) as {c}_year\",\n)\nreturn transformer_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/exceptions.html","title":"Exceptions","text":"<p>Module for all the transformers exceptions.</p>"},{"location":"reference/packages/transformers/exceptions.html#packages.transformers.exceptions.UnsupportedStreamingTransformerException","title":"<code>UnsupportedStreamingTransformerException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a user requests a transformer not supported in streaming.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/exceptions.py</code> <pre><code>class UnsupportedStreamingTransformerException(Exception):\n\"\"\"Exception for when a user requests a transformer not supported in streaming.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/transformers/exceptions.html#packages.transformers.exceptions.WrongArgumentsException","title":"<code>WrongArgumentsException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Exception for when a user provides wrong arguments to a transformer.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/exceptions.py</code> <pre><code>class WrongArgumentsException(Exception):\n\"\"\"Exception for when a user provides wrong arguments to a transformer.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/transformers/filters.html","title":"Filters","text":"<p>Module containing the filters transformers.</p>"},{"location":"reference/packages/transformers/filters.html#packages.transformers.filters.Filters","title":"<code>Filters</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing the filters transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/filters.py</code> <pre><code>class Filters(object):\n\"\"\"Class containing the filters transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef incremental_filter(\ncls,\ninput_col: str,\nincrement_value: Optional[Any] = None,\nincrement_df: Optional[DataFrame] = None,\nincrement_col: str = \"latest\",\ngreater_or_equal: bool = False,\n) -&gt; Callable:\n\"\"\"Incrementally Filter a certain dataframe given an increment logic.\n        This logic can either be an increment value or an increment dataframe from\n        which the get the latest value from. By default, the operator for the\n        filtering process is greater or equal to cover cases where we receive late\n        arriving data not cover in a previous load. You can change greater_or_equal\n        to false to use greater, when you trust the source will never output more data\n        with the increment after you have load the data (e.g., you will never load\n        data until the source is still dumping data, which may cause you to get an\n        incomplete picture of the last arrived data).\n        Args:\n            input_col: input column name\n            increment_value: value to which to filter the data, considering the\n                provided input_Col.\n            increment_df: a dataframe to get the increment value from.\n                you either specify this or the increment_value (this takes precedence).\n                This is a good approach to get the latest value from a given dataframe\n                that was read and apply that value as filter here. In this way you can\n                perform incremental loads based on the last value of a given dataframe\n                (e.g., table or file based). Can be used together with the\n                get_max_value transformer to accomplish these incremental based loads.\n                See our append load feature tests  to see how to provide an acon for\n                incremental loads, taking advantage of the scenario explained here.\n            increment_col: name of the column from which to get the increment\n                value from (when using increment_df approach). This assumes there's\n                only one row in the increment_df, reason why is a good idea to use\n                together with the get_max_value transformer. Defaults to \"latest\"\n                because that's the default output column name provided by the\n                get_max_value transformer.\n            greater_or_equal: if filtering should be done by also including the\n                increment value or not (useful for scenarios where you are performing\n                increment loads but still want to include data considering the increment\n                value, and not only values greater than that increment... examples may\n                include scenarios where you already loaded data including those values,\n                but the source produced more data containing those values).\n                Defaults to false.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='incremental_filter')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif increment_df:\nif greater_or_equal:\nreturn df.filter(  # type: ignore\ncol(input_col) &gt;= increment_df.collect()[0][increment_col]\n)\nelse:\nreturn df.filter(  # type: ignore\ncol(input_col) &gt; increment_df.collect()[0][increment_col]\n)\nelse:\nif greater_or_equal:\nreturn df.filter(col(input_col) &gt;= increment_value)  # type: ignore\nelse:\nreturn df.filter(col(input_col) &gt; increment_value)  # type: ignore\nreturn inner\n@staticmethod\ndef expression_filter(exp: str) -&gt; Callable:\n\"\"\"Filter a dataframe based on an expression.\n        Args:\n            exp: filter expression.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='expression_filter')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.filter(exp)  # type: ignore\nreturn inner\n@staticmethod\ndef column_filter_exp(exp: List[str]) -&gt; Callable:\n\"\"\"Filter a dataframe's columns based on a list of SQL expressions.\n        Args:\n            exp: column filter expressions.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='column_filter_exp')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.selectExpr(*exp)  # type: ignore\nreturn inner\n@staticmethod\ndef drop_duplicate_rows(\ncols: List[str] = None, watermarker: dict = None\n) -&gt; Callable:\n\"\"\"Drop duplicate rows using spark function dropDuplicates().\n        This transformer can be used with or without arguments.\n        The provided argument needs to be a list of columns.\n        For example: [\u201cName\u201d,\u201dVAT\u201d] will drop duplicate records within\n        \"Name\" and \"VAT\" columns.\n        If the transformer is used without providing any columns list or providing\n        an empty list, such as [] the result will be the same as using\n        the distinct() pyspark function. If the watermark dict is present it will\n        ensure that the drop operation will apply to rows within the watermark timeline\n        window.\n        Args:\n            cols: column names.\n            watermarker: properties to apply watermarker to the transformer.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='drop_duplicate_rows')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif watermarker:\ndf = Watermarker.with_watermark(\nwatermarker[\"col\"], watermarker[\"watermarking_time\"]\n)(df)\nif not cols:\nreturn df.dropDuplicates()\nelse:\nreturn df.dropDuplicates(cols)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/filters.html#packages.transformers.filters.Filters.column_filter_exp","title":"<code>column_filter_exp(exp)</code>  <code>staticmethod</code>","text":"<p>Filter a dataframe's columns based on a list of SQL expressions.</p> <p>Parameters:</p> Name Type Description Default <code>exp</code> <code>List[str]</code> <p>column filter expressions.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of column_filter_exp (See full example here)<pre><code><pre>21{\n22    \"function\": \"column_filter_exp\",\n23    \"args\": {\n24        \"exp\": [\n25            \"date\",\n26            \"country\",\n27            \"customer_number\"\n28        ]\n29    }\n30}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/filters.py</code> <pre><code>@staticmethod\ndef column_filter_exp(exp: List[str]) -&gt; Callable:\n\"\"\"Filter a dataframe's columns based on a list of SQL expressions.\n    Args:\n        exp: column filter expressions.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='column_filter_exp')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.selectExpr(*exp)  # type: ignore\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/filters.html#packages.transformers.filters.Filters.drop_duplicate_rows","title":"<code>drop_duplicate_rows(cols=None, watermarker=None)</code>  <code>staticmethod</code>","text":"<p>Drop duplicate rows using spark function dropDuplicates().</p> <p>This transformer can be used with or without arguments. The provided argument needs to be a list of columns. For example: [\u201cName\u201d,\u201dVAT\u201d] will drop duplicate records within \"Name\" and \"VAT\" columns. If the transformer is used without providing any columns list or providing an empty list, such as [] the result will be the same as using the distinct() pyspark function. If the watermark dict is present it will ensure that the drop operation will apply to rows within the watermark timeline window.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>List[str]</code> <p>column names.</p> <code>None</code> <code>watermarker</code> <code>dict</code> <p>properties to apply watermarker to the transformer.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/filters.py</code> <pre><code>@staticmethod\ndef drop_duplicate_rows(\ncols: List[str] = None, watermarker: dict = None\n) -&gt; Callable:\n\"\"\"Drop duplicate rows using spark function dropDuplicates().\n    This transformer can be used with or without arguments.\n    The provided argument needs to be a list of columns.\n    For example: [\u201cName\u201d,\u201dVAT\u201d] will drop duplicate records within\n    \"Name\" and \"VAT\" columns.\n    If the transformer is used without providing any columns list or providing\n    an empty list, such as [] the result will be the same as using\n    the distinct() pyspark function. If the watermark dict is present it will\n    ensure that the drop operation will apply to rows within the watermark timeline\n    window.\n    Args:\n        cols: column names.\n        watermarker: properties to apply watermarker to the transformer.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='drop_duplicate_rows')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif watermarker:\ndf = Watermarker.with_watermark(\nwatermarker[\"col\"], watermarker[\"watermarking_time\"]\n)(df)\nif not cols:\nreturn df.dropDuplicates()\nelse:\nreturn df.dropDuplicates(cols)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/filters.html#packages.transformers.filters.Filters.expression_filter","title":"<code>expression_filter(exp)</code>  <code>staticmethod</code>","text":"<p>Filter a dataframe based on an expression.</p> <p>Parameters:</p> Name Type Description Default <code>exp</code> <code>str</code> <p>filter expression.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of expression_filter (See full example here)<pre><code><pre>20{\n21    \"function\": \"expression_filter\",\n22    \"args\": {\n23        \"exp\": \"date like '2016%'\"\n24    }\n25}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/filters.py</code> <pre><code>@staticmethod\ndef expression_filter(exp: str) -&gt; Callable:\n\"\"\"Filter a dataframe based on an expression.\n    Args:\n        exp: filter expression.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='expression_filter')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.filter(exp)  # type: ignore\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/filters.html#packages.transformers.filters.Filters.incremental_filter","title":"<code>incremental_filter(input_col, increment_value=None, increment_df=None, increment_col='latest', greater_or_equal=False)</code>  <code>classmethod</code>","text":"<p>Incrementally Filter a certain dataframe given an increment logic.</p> <p>This logic can either be an increment value or an increment dataframe from which the get the latest value from. By default, the operator for the filtering process is greater or equal to cover cases where we receive late arriving data not cover in a previous load. You can change greater_or_equal to false to use greater, when you trust the source will never output more data with the increment after you have load the data (e.g., you will never load data until the source is still dumping data, which may cause you to get an incomplete picture of the last arrived data).</p> <p>Parameters:</p> Name Type Description Default <code>input_col</code> <code>str</code> <p>input column name</p> required <code>increment_value</code> <code>Optional[Any]</code> <p>value to which to filter the data, considering the provided input_Col.</p> <code>None</code> <code>increment_df</code> <code>Optional[DataFrame]</code> <p>a dataframe to get the increment value from. you either specify this or the increment_value (this takes precedence). This is a good approach to get the latest value from a given dataframe that was read and apply that value as filter here. In this way you can perform incremental loads based on the last value of a given dataframe (e.g., table or file based). Can be used together with the get_max_value transformer to accomplish these incremental based loads. See our append load feature tests  to see how to provide an acon for incremental loads, taking advantage of the scenario explained here.</p> <code>None</code> <code>increment_col</code> <code>str</code> <p>name of the column from which to get the increment value from (when using increment_df approach). This assumes there's only one row in the increment_df, reason why is a good idea to use together with the get_max_value transformer. Defaults to \"latest\" because that's the default output column name provided by the get_max_value transformer.</p> <code>'latest'</code> <code>greater_or_equal</code> <code>bool</code> <p>if filtering should be done by also including the increment value or not (useful for scenarios where you are performing increment loads but still want to include data considering the increment value, and not only values greater than that increment... examples may include scenarios where you already loaded data including those values, but the source produced more data containing those values). Defaults to false.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of incremental_filter (See full example here)<pre><code><pre>38{\n39    \"function\": \"incremental_filter\",\n40    \"args\": {\n41        \"input_col\": \"actrequest_timestamp\",\n42        \"increment_df\": \"max_sales_bronze_timestamp\"\n43    }\n44}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/filters.py</code> <pre><code>@classmethod\ndef incremental_filter(\ncls,\ninput_col: str,\nincrement_value: Optional[Any] = None,\nincrement_df: Optional[DataFrame] = None,\nincrement_col: str = \"latest\",\ngreater_or_equal: bool = False,\n) -&gt; Callable:\n\"\"\"Incrementally Filter a certain dataframe given an increment logic.\n    This logic can either be an increment value or an increment dataframe from\n    which the get the latest value from. By default, the operator for the\n    filtering process is greater or equal to cover cases where we receive late\n    arriving data not cover in a previous load. You can change greater_or_equal\n    to false to use greater, when you trust the source will never output more data\n    with the increment after you have load the data (e.g., you will never load\n    data until the source is still dumping data, which may cause you to get an\n    incomplete picture of the last arrived data).\n    Args:\n        input_col: input column name\n        increment_value: value to which to filter the data, considering the\n            provided input_Col.\n        increment_df: a dataframe to get the increment value from.\n            you either specify this or the increment_value (this takes precedence).\n            This is a good approach to get the latest value from a given dataframe\n            that was read and apply that value as filter here. In this way you can\n            perform incremental loads based on the last value of a given dataframe\n            (e.g., table or file based). Can be used together with the\n            get_max_value transformer to accomplish these incremental based loads.\n            See our append load feature tests  to see how to provide an acon for\n            incremental loads, taking advantage of the scenario explained here.\n        increment_col: name of the column from which to get the increment\n            value from (when using increment_df approach). This assumes there's\n            only one row in the increment_df, reason why is a good idea to use\n            together with the get_max_value transformer. Defaults to \"latest\"\n            because that's the default output column name provided by the\n            get_max_value transformer.\n        greater_or_equal: if filtering should be done by also including the\n            increment value or not (useful for scenarios where you are performing\n            increment loads but still want to include data considering the increment\n            value, and not only values greater than that increment... examples may\n            include scenarios where you already loaded data including those values,\n            but the source produced more data containing those values).\n            Defaults to false.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='incremental_filter')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif increment_df:\nif greater_or_equal:\nreturn df.filter(  # type: ignore\ncol(input_col) &gt;= increment_df.collect()[0][increment_col]\n)\nelse:\nreturn df.filter(  # type: ignore\ncol(input_col) &gt; increment_df.collect()[0][increment_col]\n)\nelse:\nif greater_or_equal:\nreturn df.filter(col(input_col) &gt;= increment_value)  # type: ignore\nelse:\nreturn df.filter(col(input_col) &gt; increment_value)  # type: ignore\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/joiners.html","title":"Joiners","text":"<p>Module with join transformers.</p>"},{"location":"reference/packages/transformers/joiners.html#packages.transformers.joiners.Joiners","title":"<code>Joiners</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing join transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/joiners.py</code> <pre><code>class Joiners(object):\n\"\"\"Class containing join transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef join(\ncls,\njoin_with: DataFrame,\njoin_condition: str,\nleft_df_alias: str = \"a\",\nright_df_alias: str = \"b\",\njoin_type: str = \"inner\",\nbroadcast_join: bool = True,\nselect_cols: Optional[List[str]] = None,\nwatermarker: Optional[dict] = None,\n) -&gt; Callable:\n\"\"\"Join two dataframes based on specified type and columns.\n        Some stream to stream joins are only possible if you apply Watermark, so this\n        method also provides a parameter to enable watermarking specification.\n        Args:\n            left_df_alias: alias of the first dataframe.\n            join_with: right dataframe.\n            right_df_alias: alias of the second dataframe.\n            join_condition: condition to join dataframes.\n            join_type: type of join. Defaults to inner.\n                Available values: inner, cross, outer, full, full outer,\n                left, left outer, right, right outer, semi,\n                left semi, anti, and left anti.\n            broadcast_join: whether to perform a broadcast join or not.\n            select_cols: list of columns to select at the end.\n            watermarker: properties to apply watermarking.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='join')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\n# To enable join on foreachBatch processing we had\n# to change to global temp view. The goal here is to\n# avoid problems on simultaneously running process,\n# so we added application id on table name.\napp_id = ExecEnv.SESSION.getActiveSession().conf.get(\"spark.app.id\")\nleft = f\"`{app_id}_{left_df_alias}`\"\nright = f\"`{app_id}_{right_df_alias}`\"\ndf_join_with = join_with\nif watermarker:\nleft_df_watermarking = watermarker.get(left_df_alias, None)\nright_df_watermarking = watermarker.get(right_df_alias, None)\nif left_df_watermarking:\ndf = Watermarker.with_watermark(\nleft_df_watermarking[\"col\"],\nleft_df_watermarking[\"watermarking_time\"],\n)(df)\nif right_df_watermarking:\ndf_join_with = Watermarker.with_watermark(\nright_df_watermarking[\"col\"],\nright_df_watermarking[\"watermarking_time\"],\n)(df_join_with)\ndf.createOrReplaceGlobalTempView(left)  # type: ignore\ndf_join_with.createOrReplaceGlobalTempView(right)  # type: ignore\nquery = f\"\"\"\n                SELECT {f\"/*+ BROADCAST({right_df_alias}) */\" if broadcast_join else \"\"}\n{\", \".join(select_cols)}\n                FROM global_temp.{left} AS {left_df_alias}\n{join_type.upper()}\n                JOIN global_temp.{right} AS {right_df_alias}\n                ON {join_condition}\n            \"\"\"  # nosec: B608\ncls._logger.info(f\"Execution query: {query}\")\nreturn ExecEnv.SESSION.sql(query)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/joiners.html#packages.transformers.joiners.Joiners.join","title":"<code>join(join_with, join_condition, left_df_alias='a', right_df_alias='b', join_type='inner', broadcast_join=True, select_cols=None, watermarker=None)</code>  <code>classmethod</code>","text":"<p>Join two dataframes based on specified type and columns.</p> <p>Some stream to stream joins are only possible if you apply Watermark, so this method also provides a parameter to enable watermarking specification.</p> <p>Parameters:</p> Name Type Description Default <code>left_df_alias</code> <code>str</code> <p>alias of the first dataframe.</p> <code>'a'</code> <code>join_with</code> <code>DataFrame</code> <p>right dataframe.</p> required <code>right_df_alias</code> <code>str</code> <p>alias of the second dataframe.</p> <code>'b'</code> <code>join_condition</code> <code>str</code> <p>condition to join dataframes.</p> required <code>join_type</code> <code>str</code> <p>type of join. Defaults to inner. Available values: inner, cross, outer, full, full outer, left, left outer, right, right outer, semi, left semi, anti, and left anti.</p> <code>'inner'</code> <code>broadcast_join</code> <code>bool</code> <p>whether to perform a broadcast join or not.</p> <code>True</code> <code>select_cols</code> <code>Optional[List[str]]</code> <p>list of columns to select at the end.</p> <code>None</code> <code>watermarker</code> <code>Optional[dict]</code> <p>properties to apply watermarking.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of join (See full example here)<pre><code><pre>33{\n34    \"function\": \"join\",\n35    \"args\": {\n36        \"join_with\": \"customers\",\n37        \"join_type\": \"left outer\",\n38        \"join_condition\": \"a.customer = b.customer\",\n39        \"select_cols\": [\n40            \"a.*\",\n41            \"b.name as customer_name\"\n42        ]\n43    }\n44}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/joiners.py</code> <pre><code>@classmethod\ndef join(\ncls,\njoin_with: DataFrame,\njoin_condition: str,\nleft_df_alias: str = \"a\",\nright_df_alias: str = \"b\",\njoin_type: str = \"inner\",\nbroadcast_join: bool = True,\nselect_cols: Optional[List[str]] = None,\nwatermarker: Optional[dict] = None,\n) -&gt; Callable:\n\"\"\"Join two dataframes based on specified type and columns.\n    Some stream to stream joins are only possible if you apply Watermark, so this\n    method also provides a parameter to enable watermarking specification.\n    Args:\n        left_df_alias: alias of the first dataframe.\n        join_with: right dataframe.\n        right_df_alias: alias of the second dataframe.\n        join_condition: condition to join dataframes.\n        join_type: type of join. Defaults to inner.\n            Available values: inner, cross, outer, full, full outer,\n            left, left outer, right, right outer, semi,\n            left semi, anti, and left anti.\n        broadcast_join: whether to perform a broadcast join or not.\n        select_cols: list of columns to select at the end.\n        watermarker: properties to apply watermarking.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='join')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\n# To enable join on foreachBatch processing we had\n# to change to global temp view. The goal here is to\n# avoid problems on simultaneously running process,\n# so we added application id on table name.\napp_id = ExecEnv.SESSION.getActiveSession().conf.get(\"spark.app.id\")\nleft = f\"`{app_id}_{left_df_alias}`\"\nright = f\"`{app_id}_{right_df_alias}`\"\ndf_join_with = join_with\nif watermarker:\nleft_df_watermarking = watermarker.get(left_df_alias, None)\nright_df_watermarking = watermarker.get(right_df_alias, None)\nif left_df_watermarking:\ndf = Watermarker.with_watermark(\nleft_df_watermarking[\"col\"],\nleft_df_watermarking[\"watermarking_time\"],\n)(df)\nif right_df_watermarking:\ndf_join_with = Watermarker.with_watermark(\nright_df_watermarking[\"col\"],\nright_df_watermarking[\"watermarking_time\"],\n)(df_join_with)\ndf.createOrReplaceGlobalTempView(left)  # type: ignore\ndf_join_with.createOrReplaceGlobalTempView(right)  # type: ignore\nquery = f\"\"\"\n            SELECT {f\"/*+ BROADCAST({right_df_alias}) */\" if broadcast_join else \"\"}\n{\", \".join(select_cols)}\n            FROM global_temp.{left} AS {left_df_alias}\n{join_type.upper()}\n            JOIN global_temp.{right} AS {right_df_alias}\n            ON {join_condition}\n        \"\"\"  # nosec: B608\ncls._logger.info(f\"Execution query: {query}\")\nreturn ExecEnv.SESSION.sql(query)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/null_handlers.html","title":"Null handlers","text":"<p>Module with null handlers transformers.</p>"},{"location":"reference/packages/transformers/null_handlers.html#packages.transformers.null_handlers.NullHandlers","title":"<code>NullHandlers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing null handler transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/null_handlers.py</code> <pre><code>class NullHandlers(object):\n\"\"\"Class containing null handler transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef replace_nulls(\ncls,\nreplace_on_nums: bool = True,\ndefault_num_value: int = -999,\nreplace_on_strings: bool = True,\ndefault_string_value: str = \"UNKNOWN\",\nsubset_cols: List[str] = None,\n) -&gt; Callable:\n\"\"\"Replace nulls in a dataframe.\n        Args:\n            replace_on_nums: if it is to replace nulls on numeric columns.\n                Applies to ints, longs and floats.\n            default_num_value: default integer value to use as replacement.\n            replace_on_strings: if it is to replace nulls on string columns.\n            default_string_value: default string value to use as replacement.\n            subset_cols: list of columns in which to replace nulls. If not\n                provided, all nulls in all columns will be replaced as specified.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='replace_nulls')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif replace_on_nums:\ndf = df.na.fill(default_num_value, subset_cols)\nif replace_on_strings:\ndf = df.na.fill(default_string_value, subset_cols)\nreturn df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/null_handlers.html#packages.transformers.null_handlers.NullHandlers.replace_nulls","title":"<code>replace_nulls(replace_on_nums=True, default_num_value=-999, replace_on_strings=True, default_string_value='UNKNOWN', subset_cols=None)</code>  <code>classmethod</code>","text":"<p>Replace nulls in a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>replace_on_nums</code> <code>bool</code> <p>if it is to replace nulls on numeric columns. Applies to ints, longs and floats.</p> <code>True</code> <code>default_num_value</code> <code>int</code> <p>default integer value to use as replacement.</p> <code>-999</code> <code>replace_on_strings</code> <code>bool</code> <p>if it is to replace nulls on string columns.</p> <code>True</code> <code>default_string_value</code> <code>str</code> <p>default string value to use as replacement.</p> <code>'UNKNOWN'</code> <code>subset_cols</code> <code>List[str]</code> <p>list of columns in which to replace nulls. If not provided, all nulls in all columns will be replaced as specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of replace_nulls (See full example here)<pre><code><pre>21{\n22    \"function\": \"replace_nulls\",\n23    \"args\": {\n24        \"subset_cols\": [\n25            \"amount\"\n26        ]\n27    }\n28}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/null_handlers.py</code> <pre><code>@classmethod\ndef replace_nulls(\ncls,\nreplace_on_nums: bool = True,\ndefault_num_value: int = -999,\nreplace_on_strings: bool = True,\ndefault_string_value: str = \"UNKNOWN\",\nsubset_cols: List[str] = None,\n) -&gt; Callable:\n\"\"\"Replace nulls in a dataframe.\n    Args:\n        replace_on_nums: if it is to replace nulls on numeric columns.\n            Applies to ints, longs and floats.\n        default_num_value: default integer value to use as replacement.\n        replace_on_strings: if it is to replace nulls on string columns.\n        default_string_value: default string value to use as replacement.\n        subset_cols: list of columns in which to replace nulls. If not\n            provided, all nulls in all columns will be replaced as specified.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='replace_nulls')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif replace_on_nums:\ndf = df.na.fill(default_num_value, subset_cols)\nif replace_on_strings:\ndf = df.na.fill(default_string_value, subset_cols)\nreturn df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/optimizers.html","title":"Optimizers","text":"<p>Optimizers module.</p>"},{"location":"reference/packages/transformers/optimizers.html#packages.transformers.optimizers.Optimizers","title":"<code>Optimizers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing all the functions that can provide optimizations.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/optimizers.py</code> <pre><code>class Optimizers(object):\n\"\"\"Class containing all the functions that can provide optimizations.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef cache(cls) -&gt; Callable:\n\"\"\"Caches the current dataframe.\n        The default storage level used is MEMORY_AND_DISK.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='cache')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.cache()\nreturn inner\n@classmethod\ndef persist(cls, storage_level: str = None) -&gt; Callable:\n\"\"\"Caches the current dataframe with a specific StorageLevel.\n        Args:\n            storage_level: the type of StorageLevel, as default MEMORY_AND_DISK_DESER.\n                [More options here](\n                https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html).\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='persist')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nlevel = getattr(\nStorageLevel, storage_level, StorageLevel.MEMORY_AND_DISK_DESER\n)\nreturn df.persist(level)\nreturn inner\n@classmethod\ndef unpersist(cls, blocking: bool = False) -&gt; Callable:\n\"\"\"Removes the dataframe from the disk and memory.\n        Args:\n            blocking: whether to block until all the data blocks are\n                removed from disk/memory or run asynchronously.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='unpersist')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.unpersist(blocking)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/optimizers.html#packages.transformers.optimizers.Optimizers.cache","title":"<code>cache()</code>  <code>classmethod</code>","text":"<p>Caches the current dataframe.</p> <p>The default storage level used is MEMORY_AND_DISK.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/optimizers.py</code> <pre><code>@classmethod\ndef cache(cls) -&gt; Callable:\n\"\"\"Caches the current dataframe.\n    The default storage level used is MEMORY_AND_DISK.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='cache')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.cache()\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/optimizers.html#packages.transformers.optimizers.Optimizers.persist","title":"<code>persist(storage_level=None)</code>  <code>classmethod</code>","text":"<p>Caches the current dataframe with a specific StorageLevel.</p> <p>Parameters:</p> Name Type Description Default <code>storage_level</code> <code>str</code> <p>the type of StorageLevel, as default MEMORY_AND_DISK_DESER. More options here.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/optimizers.py</code> <pre><code>@classmethod\ndef persist(cls, storage_level: str = None) -&gt; Callable:\n\"\"\"Caches the current dataframe with a specific StorageLevel.\n    Args:\n        storage_level: the type of StorageLevel, as default MEMORY_AND_DISK_DESER.\n            [More options here](\n            https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html).\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='persist')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nlevel = getattr(\nStorageLevel, storage_level, StorageLevel.MEMORY_AND_DISK_DESER\n)\nreturn df.persist(level)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/optimizers.html#packages.transformers.optimizers.Optimizers.unpersist","title":"<code>unpersist(blocking=False)</code>  <code>classmethod</code>","text":"<p>Removes the dataframe from the disk and memory.</p> <p>Parameters:</p> Name Type Description Default <code>blocking</code> <code>bool</code> <p>whether to block until all the data blocks are removed from disk/memory or run asynchronously.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/optimizers.py</code> <pre><code>@classmethod\ndef unpersist(cls, blocking: bool = False) -&gt; Callable:\n\"\"\"Removes the dataframe from the disk and memory.\n    Args:\n        blocking: whether to block until all the data blocks are\n            removed from disk/memory or run asynchronously.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='unpersist')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.unpersist(blocking)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/regex_transformers.html","title":"Regex transformers","text":"<p>Regex transformers module.</p>"},{"location":"reference/packages/transformers/regex_transformers.html#packages.transformers.regex_transformers.RegexTransformers","title":"<code>RegexTransformers</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing all regex functions.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/regex_transformers.py</code> <pre><code>class RegexTransformers(object):\n\"\"\"Class containing all regex functions.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef with_regex_value(\ninput_col: str,\noutput_col: str,\nregex: str,\ndrop_input_col: bool = False,\nidx: int = 1,\n) -&gt; Callable:\n\"\"\"Get the result of applying a regex to an input column (via regexp_extract).\n        Args:\n            input_col: name of the input column.\n            output_col: name of the output column.\n            regex: regular expression.\n            drop_input_col: whether to drop input_col or not.\n            idx: index to return.\n        Returns:\n            A function to be executed in the .transform() spark function.\n        {{get_example(method_name='with_regex_value')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ndf = df.withColumn(output_col, regexp_extract(col(input_col), regex, idx))\nif drop_input_col:\ndf = df.drop(input_col)\nreturn df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/regex_transformers.html#packages.transformers.regex_transformers.RegexTransformers.with_regex_value","title":"<code>with_regex_value(input_col, output_col, regex, drop_input_col=False, idx=1)</code>  <code>staticmethod</code>","text":"<p>Get the result of applying a regex to an input column (via regexp_extract).</p> <p>Parameters:</p> Name Type Description Default <code>input_col</code> <code>str</code> <p>name of the input column.</p> required <code>output_col</code> <code>str</code> <p>name of the output column.</p> required <code>regex</code> <code>str</code> <p>regular expression.</p> required <code>drop_input_col</code> <code>bool</code> <p>whether to drop input_col or not.</p> <code>False</code> <code>idx</code> <code>int</code> <p>index to return.</p> <code>1</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed in the .transform() spark function.</p> View Example of with_regex_value (See full example here)<pre><code><pre>40{\n41    \"function\": \"with_regex_value\",\n42    \"args\": {\n43        \"input_col\": \"lhe_extraction_filepath\",\n44        \"output_col\": \"extraction_date\",\n45        \"drop_input_col\": true,\n46        \"regex\": \".*WE_SO_SCL_(\\\\d+).csv\"\n47    }\n48}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/regex_transformers.py</code> <pre><code>@staticmethod\ndef with_regex_value(\ninput_col: str,\noutput_col: str,\nregex: str,\ndrop_input_col: bool = False,\nidx: int = 1,\n) -&gt; Callable:\n\"\"\"Get the result of applying a regex to an input column (via regexp_extract).\n    Args:\n        input_col: name of the input column.\n        output_col: name of the output column.\n        regex: regular expression.\n        drop_input_col: whether to drop input_col or not.\n        idx: index to return.\n    Returns:\n        A function to be executed in the .transform() spark function.\n    {{get_example(method_name='with_regex_value')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\ndf = df.withColumn(output_col, regexp_extract(col(input_col), regex, idx))\nif drop_input_col:\ndf = df.drop(input_col)\nreturn df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/repartitioners.html","title":"Repartitioners","text":"<p>Module with repartitioners transformers.</p>"},{"location":"reference/packages/transformers/repartitioners.html#packages.transformers.repartitioners.Repartitioners","title":"<code>Repartitioners</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing repartitioners transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/repartitioners.py</code> <pre><code>class Repartitioners(object):\n\"\"\"Class containing repartitioners transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef coalesce(cls, num_partitions: int) -&gt; Callable:\n\"\"\"Coalesce a dataframe into n partitions.\n        Args:\n            num_partitions: num of partitions to coalesce.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='coalesce')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.coalesce(num_partitions)\nreturn inner\n@classmethod\ndef repartition(\ncls, num_partitions: Optional[int] = None, cols: Optional[List[str]] = None\n) -&gt; Callable:\n\"\"\"Repartition a dataframe into n partitions.\n        If num_partitions is provided repartitioning happens based on the provided\n        number, otherwise it happens based on the values of the provided cols (columns).\n        Args:\n            num_partitions: num of partitions to repartition.\n            cols: list of columns to use for repartitioning.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='repartition')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif cols:\nreturn df.repartition(num_partitions, *cols)\nelif num_partitions:\nreturn df.repartition(num_partitions)\nelse:\nraise WrongArgumentsException(\n\"num_partitions or cols should be specified\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/repartitioners.html#packages.transformers.repartitioners.Repartitioners.coalesce","title":"<code>coalesce(num_partitions)</code>  <code>classmethod</code>","text":"<p>Coalesce a dataframe into n partitions.</p> <p>Parameters:</p> Name Type Description Default <code>num_partitions</code> <code>int</code> <p>num of partitions to coalesce.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of coalesce (See full example here)<pre><code><pre>39{\n40    \"function\": \"coalesce\",\n41    \"args\": {\n42        \"num_partitions\": 1\n43    }\n44}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/repartitioners.py</code> <pre><code>@classmethod\ndef coalesce(cls, num_partitions: int) -&gt; Callable:\n\"\"\"Coalesce a dataframe into n partitions.\n    Args:\n        num_partitions: num of partitions to coalesce.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='coalesce')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.coalesce(num_partitions)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/repartitioners.html#packages.transformers.repartitioners.Repartitioners.repartition","title":"<code>repartition(num_partitions=None, cols=None)</code>  <code>classmethod</code>","text":"<p>Repartition a dataframe into n partitions.</p> <p>If num_partitions is provided repartitioning happens based on the provided number, otherwise it happens based on the values of the provided cols (columns).</p> <p>Parameters:</p> Name Type Description Default <code>num_partitions</code> <code>Optional[int]</code> <p>num of partitions to repartition.</p> <code>None</code> <code>cols</code> <code>Optional[List[str]]</code> <p>list of columns to use for repartitioning.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> View Example of repartition (See full example here)<pre><code><pre>48{\n49    \"function\": \"repartition\",\n50    \"args\": {\n51        \"num_partitions\": 1\n52    }\n53}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/repartitioners.py</code> <pre><code>@classmethod\ndef repartition(\ncls, num_partitions: Optional[int] = None, cols: Optional[List[str]] = None\n) -&gt; Callable:\n\"\"\"Repartition a dataframe into n partitions.\n    If num_partitions is provided repartitioning happens based on the provided\n    number, otherwise it happens based on the values of the provided cols (columns).\n    Args:\n        num_partitions: num of partitions to repartition.\n        cols: list of columns to use for repartitioning.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='repartition')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nif cols:\nreturn df.repartition(num_partitions, *cols)\nelif num_partitions:\nreturn df.repartition(num_partitions)\nelse:\nraise WrongArgumentsException(\n\"num_partitions or cols should be specified\"\n)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/transformer_factory.html","title":"Transformer factory","text":"<p>Module with the factory pattern to return transformers.</p>"},{"location":"reference/packages/transformers/transformer_factory.html#packages.transformers.transformer_factory.TransformerFactory","title":"<code>TransformerFactory</code>","text":"<p>         Bases: <code>object</code></p> <p>TransformerFactory class following the factory pattern.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/transformer_factory.py</code> <pre><code>class TransformerFactory(object):\n\"\"\"TransformerFactory class following the factory pattern.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\nUNSUPPORTED_STREAMING_TRANSFORMERS = [\n\"condense_record_mode_cdc\",\n\"group_and_rank\",\n\"with_auto_increment_id\",\n\"with_row_id\",\n]\nAVAILABLE_TRANSFORMERS = {\n\"add_current_date\": DateTransformers.add_current_date,\n\"cache\": Optimizers.cache,\n\"cast\": ColumnReshapers.cast,\n\"coalesce\": Repartitioners.coalesce,\n\"column_dropper\": DataMaskers.column_dropper,\n\"column_filter_exp\": Filters.column_filter_exp,\n\"column_selector\": ColumnReshapers.column_selector,\n\"condense_record_mode_cdc\": Condensers.condense_record_mode_cdc,\n\"convert_to_date\": DateTransformers.convert_to_date,\n\"convert_to_timestamp\": DateTransformers.convert_to_timestamp,\n\"custom_transformation\": CustomTransformers.custom_transformation,\n\"drop_duplicate_rows\": Filters.drop_duplicate_rows,\n\"expression_filter\": Filters.expression_filter,\n\"format_date\": DateTransformers.format_date,\n\"flatten_schema\": ColumnReshapers.flatten_schema,\n\"explode_columns\": ColumnReshapers.explode_columns,\n\"from_avro\": ColumnReshapers.from_avro,\n\"from_avro_with_registry\": ColumnReshapers.from_avro_with_registry,\n\"from_json\": ColumnReshapers.from_json,\n\"get_date_hierarchy\": DateTransformers.get_date_hierarchy,\n\"get_max_value\": Aggregators.get_max_value,\n\"group_and_rank\": Condensers.group_and_rank,\n\"hash_masker\": DataMaskers.hash_masker,\n\"incremental_filter\": Filters.incremental_filter,\n\"join\": Joiners.join,\n\"persist\": Optimizers.persist,\n\"rename\": ColumnReshapers.rename,\n\"repartition\": Repartitioners.repartition,\n\"replace_nulls\": NullHandlers.replace_nulls,\n\"sql_transformation\": CustomTransformers.sql_transformation,\n\"to_json\": ColumnReshapers.to_json,\n\"union\": Unions.union,\n\"union_by_name\": Unions.union_by_name,\n\"with_watermark\": Watermarker.with_watermark,\n\"unpersist\": Optimizers.unpersist,\n\"with_auto_increment_id\": ColumnCreators.with_auto_increment_id,\n\"with_expressions\": ColumnReshapers.with_expressions,\n\"with_literals\": ColumnCreators.with_literals,\n\"with_regex_value\": RegexTransformers.with_regex_value,\n\"with_row_id\": ColumnCreators.with_row_id,\n}\n@staticmethod\ndef get_transformer(spec: TransformerSpec, data: OrderedDict = None) -&gt; Callable:\n\"\"\"Get a transformer following the factory pattern.\n        Args:\n            spec: transformer specification (individual transformation... not to be\n                confused with list of all transformations).\n            data: ordered dict of dataframes to be transformed. Needed when a\n                transformer requires more than one dataframe as input.\n        Returns:\n            Transformer function to be executed in .transform() spark function.\n        {{get_example(method_name='get_transformer')}}\n        \"\"\"\nif spec.function == \"incremental_filter\":\n# incremental_filter optionally expects a DataFrame as input, so find it.\nif \"increment_df\" in spec.args:\nspec.args[\"increment_df\"] = data[spec.args[\"increment_df\"]]\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelif spec.function == \"join\":\n# get the dataframe given the input_id in the input specs of the acon.\nspec.args[\"join_with\"] = data[spec.args[\"join_with\"]]\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelif spec.function == \"union\" or spec.function == \"union_by_name\":\n# get the list of dataframes given the input_id in the input specs\n# of the acon.\ndf_to_transform = spec.args[\"union_with\"]\nspec.args[\"union_with\"] = []\nfor item in df_to_transform:\nspec.args[\"union_with\"].append(data[item])\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelif spec.function in TransformerFactory.AVAILABLE_TRANSFORMERS:\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelse:\nraise NotImplementedError(\nf\"The requested transformer {spec.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/transformers/transformer_factory.html#packages.transformers.transformer_factory.TransformerFactory.get_transformer","title":"<code>get_transformer(spec, data=None)</code>  <code>staticmethod</code>","text":"<p>Get a transformer following the factory pattern.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TransformerSpec</code> <p>transformer specification (individual transformation... not to be confused with list of all transformations).</p> required <code>data</code> <code>OrderedDict</code> <p>ordered dict of dataframes to be transformed. Needed when a transformer requires more than one dataframe as input.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Transformer function to be executed in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/transformer_factory.py</code> <pre><code>@staticmethod\ndef get_transformer(spec: TransformerSpec, data: OrderedDict = None) -&gt; Callable:\n\"\"\"Get a transformer following the factory pattern.\n    Args:\n        spec: transformer specification (individual transformation... not to be\n            confused with list of all transformations).\n        data: ordered dict of dataframes to be transformed. Needed when a\n            transformer requires more than one dataframe as input.\n    Returns:\n        Transformer function to be executed in .transform() spark function.\n    {{get_example(method_name='get_transformer')}}\n    \"\"\"\nif spec.function == \"incremental_filter\":\n# incremental_filter optionally expects a DataFrame as input, so find it.\nif \"increment_df\" in spec.args:\nspec.args[\"increment_df\"] = data[spec.args[\"increment_df\"]]\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelif spec.function == \"join\":\n# get the dataframe given the input_id in the input specs of the acon.\nspec.args[\"join_with\"] = data[spec.args[\"join_with\"]]\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelif spec.function == \"union\" or spec.function == \"union_by_name\":\n# get the list of dataframes given the input_id in the input specs\n# of the acon.\ndf_to_transform = spec.args[\"union_with\"]\nspec.args[\"union_with\"] = []\nfor item in df_to_transform:\nspec.args[\"union_with\"].append(data[item])\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelif spec.function in TransformerFactory.AVAILABLE_TRANSFORMERS:\nreturn TransformerFactory.AVAILABLE_TRANSFORMERS[  # type: ignore\nspec.function\n](**spec.args)\nelse:\nraise NotImplementedError(\nf\"The requested transformer {spec.function} is not implemented.\"\n)\n</code></pre>"},{"location":"reference/packages/transformers/unions.html","title":"Unions","text":"<p>Module with union transformers.</p>"},{"location":"reference/packages/transformers/unions.html#packages.transformers.unions.Unions","title":"<code>Unions</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing union transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/unions.py</code> <pre><code>class Unions(object):\n\"\"\"Class containing union transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef union(\ncls,\nunion_with: List[DataFrame],\ndeduplication: bool = True,\n) -&gt; Callable:\n\"\"\"Union dataframes, resolving columns by position (not by name).\n        Args:\n            union_with: list of dataframes to union.\n            deduplication: whether to perform deduplication of elements or not.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='union')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nunion_df = reduce(lambda x, y: x.union(y), [df] + union_with)\nreturn union_df.distinct() if deduplication else union_df\nreturn inner\n@classmethod\ndef union_by_name(\ncls,\nunion_with: List[DataFrame],\ndeduplication: bool = True,\nallow_missing_columns: bool = True,\n) -&gt; Callable:\n\"\"\"Union dataframes, resolving columns by name (not by position).\n        Args:\n            union_with: list of dataframes to union.\n            deduplication: whether to perform deduplication of elements or not.\n            allow_missing_columns: allow the union of DataFrames with different\n                schemas.\n        Returns:\n            A function to be called in .transform() spark function.\n        {{get_example(method_name='union_by_name')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nunion_df = reduce(\nlambda x, y: x.unionByName(\ny, allowMissingColumns=allow_missing_columns\n),\n[df] + union_with,\n)\nreturn union_df.distinct() if deduplication else union_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/unions.html#packages.transformers.unions.Unions.union","title":"<code>union(union_with, deduplication=True)</code>  <code>classmethod</code>","text":"<p>Union dataframes, resolving columns by position (not by name).</p> <p>Parameters:</p> Name Type Description Default <code>union_with</code> <code>List[DataFrame]</code> <p>list of dataframes to union.</p> required <code>deduplication</code> <code>bool</code> <p>whether to perform deduplication of elements or not.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/unions.py</code> <pre><code>@classmethod\ndef union(\ncls,\nunion_with: List[DataFrame],\ndeduplication: bool = True,\n) -&gt; Callable:\n\"\"\"Union dataframes, resolving columns by position (not by name).\n    Args:\n        union_with: list of dataframes to union.\n        deduplication: whether to perform deduplication of elements or not.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='union')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nunion_df = reduce(lambda x, y: x.union(y), [df] + union_with)\nreturn union_df.distinct() if deduplication else union_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/unions.html#packages.transformers.unions.Unions.union_by_name","title":"<code>union_by_name(union_with, deduplication=True, allow_missing_columns=True)</code>  <code>classmethod</code>","text":"<p>Union dataframes, resolving columns by name (not by position).</p> <p>Parameters:</p> Name Type Description Default <code>union_with</code> <code>List[DataFrame]</code> <p>list of dataframes to union.</p> required <code>deduplication</code> <code>bool</code> <p>whether to perform deduplication of elements or not.</p> <code>True</code> <code>allow_missing_columns</code> <code>bool</code> <p>allow the union of DataFrames with different schemas.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/unions.py</code> <pre><code>@classmethod\ndef union_by_name(\ncls,\nunion_with: List[DataFrame],\ndeduplication: bool = True,\nallow_missing_columns: bool = True,\n) -&gt; Callable:\n\"\"\"Union dataframes, resolving columns by name (not by position).\n    Args:\n        union_with: list of dataframes to union.\n        deduplication: whether to perform deduplication of elements or not.\n        allow_missing_columns: allow the union of DataFrames with different\n            schemas.\n    Returns:\n        A function to be called in .transform() spark function.\n    {{get_example(method_name='union_by_name')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nunion_df = reduce(\nlambda x, y: x.unionByName(\ny, allowMissingColumns=allow_missing_columns\n),\n[df] + union_with,\n)\nreturn union_df.distinct() if deduplication else union_df\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/watermarker.html","title":"Watermarker","text":"<p>Watermarker module.</p>"},{"location":"reference/packages/transformers/watermarker.html#packages.transformers.watermarker.Watermarker","title":"<code>Watermarker</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing all watermarker transformers.</p> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/watermarker.py</code> <pre><code>class Watermarker(object):\n\"\"\"Class containing all watermarker transformers.\"\"\"\n_logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef with_watermark(watermarker_column: str, watermarker_time: str) -&gt; Callable:\n\"\"\"Get the dataframe with watermarker defined.\n        Args:\n            watermarker_column: name of the input column to be considered for\n                the watermarking. Note: it must be a timestamp.\n            watermarker_time: time window to define the watermark value.\n        Returns:\n            A function to be executed on other transformers.\n        {{get_example(method_name='with_watermark')}}\n        \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.withWatermark(watermarker_column, watermarker_time)\nreturn inner\n</code></pre>"},{"location":"reference/packages/transformers/watermarker.html#packages.transformers.watermarker.Watermarker.with_watermark","title":"<code>with_watermark(watermarker_column, watermarker_time)</code>  <code>staticmethod</code>","text":"<p>Get the dataframe with watermarker defined.</p> <p>Parameters:</p> Name Type Description Default <code>watermarker_column</code> <code>str</code> <p>name of the input column to be considered for the watermarking. Note: it must be a timestamp.</p> required <code>watermarker_time</code> <code>str</code> <p>time window to define the watermark value.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function to be executed on other transformers.</p> View Example of with_watermark (See full example here)<pre><code><pre>21{\n22    \"function\": \"with_watermark\",\n23    \"args\": {\n24        \"watermarker_column\": \"date\",\n25        \"watermarker_time\": \"2 days\"\n26    }\n27}\n</pre>\n</code></pre> Source code in <code>mkdocs/lakehouse_engine/packages/transformers/watermarker.py</code> <pre><code>@staticmethod\ndef with_watermark(watermarker_column: str, watermarker_time: str) -&gt; Callable:\n\"\"\"Get the dataframe with watermarker defined.\n    Args:\n        watermarker_column: name of the input column to be considered for\n            the watermarking. Note: it must be a timestamp.\n        watermarker_time: time window to define the watermark value.\n    Returns:\n        A function to be executed on other transformers.\n    {{get_example(method_name='with_watermark')}}\n    \"\"\"\ndef inner(df: DataFrame) -&gt; DataFrame:\nreturn df.withWatermark(watermarker_column, watermarker_time)\nreturn inner\n</code></pre>"},{"location":"reference/packages/utils/index.html","title":"Utils","text":"<p>Utilities package.</p>"},{"location":"reference/packages/utils/acon_utils.html","title":"Acon utils","text":"<p>Module to perform validations and resolve the acon.</p>"},{"location":"reference/packages/utils/acon_utils.html#packages.utils.acon_utils.resolve_dq_functions","title":"<code>resolve_dq_functions(acon, execution_point)</code>","text":"<p>Function to resolve the dq functions in the acon.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>Acon to resolve the dq functions.</p> required <code>execution_point</code> <code>str</code> <p>Execution point of the dq_functions.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Acon after resolving the dq functions.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/acon_utils.py</code> <pre><code>def resolve_dq_functions(acon: dict, execution_point: str) -&gt; dict:\n\"\"\"Function to resolve the dq functions in the acon.\n    Args:\n        acon: Acon to resolve the dq functions.\n        execution_point: Execution point of the dq_functions.\n    Returns:\n        Acon after resolving the dq functions.\n    \"\"\"\nif acon.get(\"dq_spec\"):\nif acon.get(\"dq_spec\").get(\"dq_type\") == DQType.PRISMA.value:\nacon[\"dq_spec\"] = PrismaUtils.build_prisma_dq_spec(\nspec=acon.get(\"dq_spec\"), execution_point=execution_point\n)\nelif acon.get(\"dq_specs\"):\nresolved_dq_specs = []\nfor spec in acon.get(\"dq_specs\", []):\nif spec.get(\"dq_type\") == DQType.PRISMA.value:\nresolved_dq_specs.append(\nPrismaUtils.build_prisma_dq_spec(\nspec=spec, execution_point=execution_point\n)\n)\nelse:\nresolved_dq_specs.append(spec)\nacon[\"dq_specs\"] = resolved_dq_specs\nreturn acon\n</code></pre>"},{"location":"reference/packages/utils/acon_utils.html#packages.utils.acon_utils.validate_and_resolve_acon","title":"<code>validate_and_resolve_acon(acon, execution_point='')</code>","text":"<p>Function to validate and resolve the acon.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>Acon to be validated and resolved.</p> required <code>execution_point</code> <code>str</code> <p>Execution point to resolve the dq functions.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>Acon after validation and resolution.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/acon_utils.py</code> <pre><code>def validate_and_resolve_acon(acon: dict, execution_point: str = \"\") -&gt; dict:\n\"\"\"Function to validate and resolve the acon.\n    Args:\n        acon: Acon to be validated and resolved.\n        execution_point: Execution point to resolve the dq functions.\n    Returns:\n        Acon after validation and resolution.\n    \"\"\"\n# Performing validations\nvalidate_readers(acon)\nvalidate_writers(acon)\n# Resolving the acon\nif execution_point:\nacon = resolve_dq_functions(acon, execution_point)\n_LOGGER.info(f\"Read Algorithm Configuration: {str(acon)}\")\nreturn acon\n</code></pre>"},{"location":"reference/packages/utils/acon_utils.html#packages.utils.acon_utils.validate_readers","title":"<code>validate_readers(acon)</code>","text":"<p>Function to validate the readers in the acon.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>Acon to be validated.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the input format is not supported.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/acon_utils.py</code> <pre><code>def validate_readers(acon: dict) -&gt; None:\n\"\"\"Function to validate the readers in the acon.\n    Args:\n        acon: Acon to be validated.\n    Raises:\n        RuntimeError: If the input format is not supported.\n    \"\"\"\nif \"input_specs\" in acon.keys() or \"input_spec\" in acon.keys():\nfor spec in acon.get(\"input_specs\", []) or [acon.get(\"input_spec\", {})]:\nif (\nnot InputFormat.exists(spec.get(\"data_format\"))\nand \"db_table\" not in spec.keys()\n):\nraise WrongIOFormatException(\nf\"Input format not supported: {spec.get('data_format')}\"\n)\n</code></pre>"},{"location":"reference/packages/utils/acon_utils.html#packages.utils.acon_utils.validate_writers","title":"<code>validate_writers(acon)</code>","text":"<p>Function to validate the writers in the acon.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>Acon to be validated.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the output format is not supported.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/acon_utils.py</code> <pre><code>def validate_writers(acon: dict) -&gt; None:\n\"\"\"Function to validate the writers in the acon.\n    Args:\n        acon: Acon to be validated.\n    Raises:\n        RuntimeError: If the output format is not supported.\n    \"\"\"\nif \"output_specs\" in acon.keys() or \"output_spec\" in acon.keys():\nfor spec in acon.get(\"output_specs\", []) or [acon.get(\"output_spec\", {})]:\nif not OutputFormat.exists(spec.get(\"data_format\")):\nraise WrongIOFormatException(\nf\"Output format not supported: {spec.get('data_format')}\"\n)\n</code></pre>"},{"location":"reference/packages/utils/databricks_utils.html","title":"Databricks utils","text":"<p>Utilities for databricks operations.</p>"},{"location":"reference/packages/utils/databricks_utils.html#packages.utils.databricks_utils.DatabricksUtils","title":"<code>DatabricksUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Databricks utilities class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/databricks_utils.py</code> <pre><code>class DatabricksUtils(object):\n\"\"\"Databricks utilities class.\"\"\"\n@staticmethod\ndef get_db_utils(spark: SparkSession) -&gt; Any:\n\"\"\"Get db utils on databricks.\n        Args:\n            spark: spark session.\n        Returns:\n            Dbutils from databricks.\n        \"\"\"\ntry:\nfrom pyspark.dbutils import DBUtils\nif \"dbutils\" not in locals():\ndbutils = DBUtils(spark)\nelse:\ndbutils = locals().get(\"dbutils\")\nexcept ImportError:\nimport IPython\ndbutils = IPython.get_ipython().user_ns[\"dbutils\"]\nreturn dbutils\n@staticmethod\ndef get_databricks_job_information(spark: SparkSession) -&gt; Tuple[str, str]:\n\"\"\"Get notebook context from running acon.\n        Args:\n            spark: spark session.\n        Returns:\n            Dict containing databricks notebook context.\n        \"\"\"\nif \"local\" in spark.getActiveSession().conf.get(\"spark.app.id\"):\nreturn \"local\", \"local\"\nelse:\ndbutils = DatabricksUtils.get_db_utils(spark)\nnotebook_context = json.loads(\n(\ndbutils.notebook.entry_point.getDbutils()\n.notebook()\n.getContext()\n.safeToJson()\n)\n)\nreturn notebook_context[\"attributes\"].get(\"orgId\"), notebook_context[\n\"attributes\"\n].get(\"jobName\")\n</code></pre>"},{"location":"reference/packages/utils/databricks_utils.html#packages.utils.databricks_utils.DatabricksUtils.get_databricks_job_information","title":"<code>get_databricks_job_information(spark)</code>  <code>staticmethod</code>","text":"<p>Get notebook context from running acon.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>spark session.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Dict containing databricks notebook context.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/databricks_utils.py</code> <pre><code>@staticmethod\ndef get_databricks_job_information(spark: SparkSession) -&gt; Tuple[str, str]:\n\"\"\"Get notebook context from running acon.\n    Args:\n        spark: spark session.\n    Returns:\n        Dict containing databricks notebook context.\n    \"\"\"\nif \"local\" in spark.getActiveSession().conf.get(\"spark.app.id\"):\nreturn \"local\", \"local\"\nelse:\ndbutils = DatabricksUtils.get_db_utils(spark)\nnotebook_context = json.loads(\n(\ndbutils.notebook.entry_point.getDbutils()\n.notebook()\n.getContext()\n.safeToJson()\n)\n)\nreturn notebook_context[\"attributes\"].get(\"orgId\"), notebook_context[\n\"attributes\"\n].get(\"jobName\")\n</code></pre>"},{"location":"reference/packages/utils/databricks_utils.html#packages.utils.databricks_utils.DatabricksUtils.get_db_utils","title":"<code>get_db_utils(spark)</code>  <code>staticmethod</code>","text":"<p>Get db utils on databricks.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>spark session.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dbutils from databricks.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/databricks_utils.py</code> <pre><code>@staticmethod\ndef get_db_utils(spark: SparkSession) -&gt; Any:\n\"\"\"Get db utils on databricks.\n    Args:\n        spark: spark session.\n    Returns:\n        Dbutils from databricks.\n    \"\"\"\ntry:\nfrom pyspark.dbutils import DBUtils\nif \"dbutils\" not in locals():\ndbutils = DBUtils(spark)\nelse:\ndbutils = locals().get(\"dbutils\")\nexcept ImportError:\nimport IPython\ndbutils = IPython.get_ipython().user_ns[\"dbutils\"]\nreturn dbutils\n</code></pre>"},{"location":"reference/packages/utils/dq_utils.html","title":"Dq utils","text":"<p>Module containing utils for DQ processing.</p>"},{"location":"reference/packages/utils/dq_utils.html#packages.utils.dq_utils.DQUtils","title":"<code>DQUtils</code>","text":"<p>Utils related to the data quality process.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/dq_utils.py</code> <pre><code>class DQUtils:\n\"\"\"Utils related to the data quality process.\"\"\"\n@staticmethod\ndef import_dq_rules_from_table(\nspec: dict,\nexecution_point: str,\nbase_expectation_arguments: list,\nextra_meta_arguments: list,\n) -&gt; dict:\n\"\"\"Import dq rules from a table.\n        Args:\n            spec: data quality specification.\n            execution_point: if the execution is in_motion or at_rest.\n            base_expectation_arguments: base arguments for dq functions.\n            extra_meta_arguments: extra meta arguments for dq functions.\n        Returns:\n            The dictionary containing the dq spec with dq functions defined.\n        \"\"\"\ndq_db_table = spec[\"dq_db_table\"]\ndq_functions = []\nif spec.get(\"dq_table_table_filter\"):\ndq_table_table_filter = spec[\"dq_table_table_filter\"]\nelse:\nraise DQSpecMalformedException(\n\"When importing rules from a table \"\n\"dq_table_table_filter must be defined.\"\n)\nextra_filters_query = (\nf\"\"\" and {spec[\"dq_table_extra_filters\"]}\"\"\"\nif spec.get(\"dq_table_extra_filters\")\nelse \"\"\n)\nfields = base_expectation_arguments + extra_meta_arguments\ndq_functions_query = f\"\"\"\n            SELECT {\", \".join(fields)}\n            FROM {dq_db_table}\n            WHERE\n            execution_point='{execution_point}' and table = '{dq_table_table_filter}'\n{extra_filters_query}\"\"\"  # nosec: B608\nraw_dq_functions = ExecEnv.SESSION.sql(dq_functions_query)\narguments = raw_dq_functions.select(\"arguments\").collect()\nparsed_arguments = [loads(argument.arguments) for argument in arguments]\ncombined_dict: dict = {}\nfor argument in parsed_arguments:\ncombined_dict = {**combined_dict, **argument}\ndq_function_arguments_schema = schema_of_json(str(combined_dict))\nprocessed_dq_functions = (\nraw_dq_functions.withColumn(\n\"json_data\", from_json(col(\"arguments\"), dq_function_arguments_schema)\n)\n.withColumn(\n\"parsed_arguments\",\nstruct(\ncol(\"json_data.*\"),\nstruct(extra_meta_arguments).alias(\"meta\"),\n),\n)\n.drop(col(\"json_data\"))\n)\nunique_dq_functions = processed_dq_functions.drop_duplicates(\n[\"dq_tech_function\", \"arguments\"]\n)\nduplicated_rows = processed_dq_functions.subtract(unique_dq_functions)\nif duplicated_rows.count() &gt; 0:\n_LOGGER.warning(\"Found Duplicates Rows:\")\nduplicated_rows.show(truncate=False)\nprocessed_dq_functions_list = unique_dq_functions.collect()\nfor processed_dq_function in processed_dq_functions_list:\ndq_functions.append(\n{\n\"function\": f\"{processed_dq_function.dq_tech_function}\",\n\"args\": {\nk: v\nfor k, v in processed_dq_function.parsed_arguments.asDict(\nrecursive=True\n).items()\nif v is not None\n},\n}\n)\nspec[\"dq_functions\"] = dq_functions\nreturn spec\n@staticmethod\ndef validate_dq_functions(\nspec: dict, execution_point: str = \"\", extra_meta_arguments: list = None\n) -&gt; None:\n\"\"\"Function to validate the dq functions defined in the dq_spec.\n        This function validates that the defined dq_functions contain all\n        the fields defined in the extra_meta_arguments parameter.\n        Args:\n            spec: data quality specification.\n            execution_point: if the execution is in_motion or at_rest.\n            extra_meta_arguments: extra meta arguments for dq functions.\n        Raises:\n            DQSpecMalformedException: If the dq spec is malformed.\n        \"\"\"\ndq_functions = spec[\"dq_functions\"]\nif not extra_meta_arguments:\n_LOGGER.info(\n\"No extra meta parameters defined. \"\n\"Skipping validation of imported dq rule.\"\n)\nreturn\nfor dq_function in dq_functions:\nif not dq_function.get(\"args\").get(\"meta\", None):\nraise DQSpecMalformedException(\n\"The dq function must have a meta field containing all \"\nf\"the fields defined: {extra_meta_arguments}.\"\n)\nelse:\nmeta = dq_function[\"args\"][\"meta\"]\ngiven_keys = meta.keys()\nmissing_keys = sorted(set(extra_meta_arguments) - set(given_keys))\nif missing_keys:\nraise DQSpecMalformedException(\n\"The dq function meta field must contain all the \"\nf\"fields defined: {extra_meta_arguments}.\\n\"\nf\"Found fields: {list(given_keys)}.\\n\"\nf\"Diff: {list(missing_keys)}\"\n)\nif execution_point and meta[\"execution_point\"] != execution_point:\nraise DQSpecMalformedException(\n\"The dq function execution point must be the same as \"\n\"the execution point of the dq spec.\"\n)\n</code></pre>"},{"location":"reference/packages/utils/dq_utils.html#packages.utils.dq_utils.DQUtils.import_dq_rules_from_table","title":"<code>import_dq_rules_from_table(spec, execution_point, base_expectation_arguments, extra_meta_arguments)</code>  <code>staticmethod</code>","text":"<p>Import dq rules from a table.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>dict</code> <p>data quality specification.</p> required <code>execution_point</code> <code>str</code> <p>if the execution is in_motion or at_rest.</p> required <code>base_expectation_arguments</code> <code>list</code> <p>base arguments for dq functions.</p> required <code>extra_meta_arguments</code> <code>list</code> <p>extra meta arguments for dq functions.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary containing the dq spec with dq functions defined.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/dq_utils.py</code> <pre><code>@staticmethod\ndef import_dq_rules_from_table(\nspec: dict,\nexecution_point: str,\nbase_expectation_arguments: list,\nextra_meta_arguments: list,\n) -&gt; dict:\n\"\"\"Import dq rules from a table.\n    Args:\n        spec: data quality specification.\n        execution_point: if the execution is in_motion or at_rest.\n        base_expectation_arguments: base arguments for dq functions.\n        extra_meta_arguments: extra meta arguments for dq functions.\n    Returns:\n        The dictionary containing the dq spec with dq functions defined.\n    \"\"\"\ndq_db_table = spec[\"dq_db_table\"]\ndq_functions = []\nif spec.get(\"dq_table_table_filter\"):\ndq_table_table_filter = spec[\"dq_table_table_filter\"]\nelse:\nraise DQSpecMalformedException(\n\"When importing rules from a table \"\n\"dq_table_table_filter must be defined.\"\n)\nextra_filters_query = (\nf\"\"\" and {spec[\"dq_table_extra_filters\"]}\"\"\"\nif spec.get(\"dq_table_extra_filters\")\nelse \"\"\n)\nfields = base_expectation_arguments + extra_meta_arguments\ndq_functions_query = f\"\"\"\n        SELECT {\", \".join(fields)}\n        FROM {dq_db_table}\n        WHERE\n        execution_point='{execution_point}' and table = '{dq_table_table_filter}'\n{extra_filters_query}\"\"\"  # nosec: B608\nraw_dq_functions = ExecEnv.SESSION.sql(dq_functions_query)\narguments = raw_dq_functions.select(\"arguments\").collect()\nparsed_arguments = [loads(argument.arguments) for argument in arguments]\ncombined_dict: dict = {}\nfor argument in parsed_arguments:\ncombined_dict = {**combined_dict, **argument}\ndq_function_arguments_schema = schema_of_json(str(combined_dict))\nprocessed_dq_functions = (\nraw_dq_functions.withColumn(\n\"json_data\", from_json(col(\"arguments\"), dq_function_arguments_schema)\n)\n.withColumn(\n\"parsed_arguments\",\nstruct(\ncol(\"json_data.*\"),\nstruct(extra_meta_arguments).alias(\"meta\"),\n),\n)\n.drop(col(\"json_data\"))\n)\nunique_dq_functions = processed_dq_functions.drop_duplicates(\n[\"dq_tech_function\", \"arguments\"]\n)\nduplicated_rows = processed_dq_functions.subtract(unique_dq_functions)\nif duplicated_rows.count() &gt; 0:\n_LOGGER.warning(\"Found Duplicates Rows:\")\nduplicated_rows.show(truncate=False)\nprocessed_dq_functions_list = unique_dq_functions.collect()\nfor processed_dq_function in processed_dq_functions_list:\ndq_functions.append(\n{\n\"function\": f\"{processed_dq_function.dq_tech_function}\",\n\"args\": {\nk: v\nfor k, v in processed_dq_function.parsed_arguments.asDict(\nrecursive=True\n).items()\nif v is not None\n},\n}\n)\nspec[\"dq_functions\"] = dq_functions\nreturn spec\n</code></pre>"},{"location":"reference/packages/utils/dq_utils.html#packages.utils.dq_utils.DQUtils.validate_dq_functions","title":"<code>validate_dq_functions(spec, execution_point='', extra_meta_arguments=None)</code>  <code>staticmethod</code>","text":"<p>Function to validate the dq functions defined in the dq_spec.</p> <p>This function validates that the defined dq_functions contain all the fields defined in the extra_meta_arguments parameter.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>dict</code> <p>data quality specification.</p> required <code>execution_point</code> <code>str</code> <p>if the execution is in_motion or at_rest.</p> <code>''</code> <code>extra_meta_arguments</code> <code>list</code> <p>extra meta arguments for dq functions.</p> <code>None</code> <p>Raises:</p> Type Description <code>DQSpecMalformedException</code> <p>If the dq spec is malformed.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/dq_utils.py</code> <pre><code>@staticmethod\ndef validate_dq_functions(\nspec: dict, execution_point: str = \"\", extra_meta_arguments: list = None\n) -&gt; None:\n\"\"\"Function to validate the dq functions defined in the dq_spec.\n    This function validates that the defined dq_functions contain all\n    the fields defined in the extra_meta_arguments parameter.\n    Args:\n        spec: data quality specification.\n        execution_point: if the execution is in_motion or at_rest.\n        extra_meta_arguments: extra meta arguments for dq functions.\n    Raises:\n        DQSpecMalformedException: If the dq spec is malformed.\n    \"\"\"\ndq_functions = spec[\"dq_functions\"]\nif not extra_meta_arguments:\n_LOGGER.info(\n\"No extra meta parameters defined. \"\n\"Skipping validation of imported dq rule.\"\n)\nreturn\nfor dq_function in dq_functions:\nif not dq_function.get(\"args\").get(\"meta\", None):\nraise DQSpecMalformedException(\n\"The dq function must have a meta field containing all \"\nf\"the fields defined: {extra_meta_arguments}.\"\n)\nelse:\nmeta = dq_function[\"args\"][\"meta\"]\ngiven_keys = meta.keys()\nmissing_keys = sorted(set(extra_meta_arguments) - set(given_keys))\nif missing_keys:\nraise DQSpecMalformedException(\n\"The dq function meta field must contain all the \"\nf\"fields defined: {extra_meta_arguments}.\\n\"\nf\"Found fields: {list(given_keys)}.\\n\"\nf\"Diff: {list(missing_keys)}\"\n)\nif execution_point and meta[\"execution_point\"] != execution_point:\nraise DQSpecMalformedException(\n\"The dq function execution point must be the same as \"\n\"the execution point of the dq spec.\"\n)\n</code></pre>"},{"location":"reference/packages/utils/dq_utils.html#packages.utils.dq_utils.PrismaUtils","title":"<code>PrismaUtils</code>","text":"<p>Prisma related utils.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/dq_utils.py</code> <pre><code>class PrismaUtils:\n\"\"\"Prisma related utils.\"\"\"\n@staticmethod\ndef build_prisma_dq_spec(spec: dict, execution_point: str) -&gt; dict:\n\"\"\"Fetch dq functions from given table.\n        Args:\n            spec: data quality specification.\n            execution_point: if the execution is in_motion or at_rest.\n        Returns:\n            The dictionary containing the dq spec with dq functions defined.\n        \"\"\"\nif spec.get(\"dq_db_table\"):\nspec = DQUtils.import_dq_rules_from_table(\nspec,\nexecution_point,\nDQTableBaseParameters.PRISMA_BASE_PARAMETERS.value,\nExecEnv.ENGINE_CONFIG.dq_functions_column_list,\n)\nelif spec.get(\"dq_functions\"):\nDQUtils.validate_dq_functions(\nspec,\nexecution_point,\nExecEnv.ENGINE_CONFIG.dq_functions_column_list,\n)\nelse:\nraise DQSpecMalformedException(\n\"When using PRISMA either dq_db_table or \"\n\"dq_functions needs to be defined.\"\n)\ndq_bucket = (\nExecEnv.ENGINE_CONFIG.dq_bucket\nif ExecEnv.get_environment() == \"prod\"\nelse ExecEnv.ENGINE_CONFIG.dq_dev_bucket\n)\nspec[\"critical_functions\"] = []\nspec[\"execution_point\"] = execution_point\nspec[\"result_sink_db_table\"] = None\nspec[\"result_sink_explode\"] = True\nspec[\"fail_on_error\"] = spec.get(\"fail_on_error\", False)\nspec[\"max_percentage_failure\"] = spec.get(\"max_percentage_failure\", 1)\nif not spec.get(\"result_sink_extra_columns\", None):\nspec[\"result_sink_extra_columns\"] = [\n\"validation_results.expectation_config.meta\",\n]\nelse:\nspec[\"result_sink_extra_columns\"] = [\n\"validation_results.expectation_config.meta\",\n] + spec[\"result_sink_extra_columns\"]\nif not spec.get(\"data_product_name\", None):\nraise DQSpecMalformedException(\n\"When using PRISMA DQ data_product_name must be defined.\"\n)\nspec[\"result_sink_location\"] = (\nf\"{dq_bucket}/{spec['data_product_name']}/result_sink/\"\n)\nif not spec.get(\"tbl_to_derive_pk\", None) and not spec.get(\n\"unexpected_rows_pk\", None\n):\nraise DQSpecMalformedException(\n\"When using PRISMA DQ either \"\n\"tbl_to_derive_pk or unexpected_rows_pk need to be defined.\"\n)\nreturn spec\n@staticmethod\ndef validate_rule_id_duplication(\nspecs: list[DQSpec],\n) -&gt; dict[str, str]:\n\"\"\"Verify uniqueness of the dq_rule_id.\n        Args:\n            specs: a list of DQSpec to be validated\n        Returns:\n             A dictionary with the spec_id as key and\n             rule_id as value for any duplicates.\n        \"\"\"\nerror_dict = {}\nfor spec in specs:\ndq_db_table = spec.dq_db_table\ndq_functions = spec.dq_functions\nspec_id = spec.spec_id\nif spec.dq_type == DQType.PRISMA.value and dq_db_table:\ndq_rule_id_query = f\"\"\"\n                    SELECT dq_rule_id, COUNT(*) AS count\n                    FROM {dq_db_table}\n                    GROUP BY dq_rule_id\n                    HAVING COUNT(*) &gt; 1;\n                    \"\"\"  # nosec: B608\nduplicate_rule_id_table = ExecEnv.SESSION.sql(dq_rule_id_query)\nif not duplicate_rule_id_table.isEmpty():\nrows = duplicate_rule_id_table.collect()\ndf_str = \"; \".join([str(row) for row in rows])\nerror_dict[f\"dq_spec_id: {spec_id}\"] = df_str\nelif spec.dq_type == DQType.PRISMA.value and dq_functions:\ndq_rules_id_list = []\nfor dq_function in dq_functions:\ndq_rules_id_list.append(dq_function.args[\"meta\"][\"dq_rule_id\"])\nif len(dq_rules_id_list) != len(set(dq_rules_id_list)):\nerror_dict[f\"dq_spec_id: {spec_id}\"] = \"; \".join(\n[str(dq_rule_id) for dq_rule_id in dq_rules_id_list]\n)\nreturn error_dict\n</code></pre>"},{"location":"reference/packages/utils/dq_utils.html#packages.utils.dq_utils.PrismaUtils.build_prisma_dq_spec","title":"<code>build_prisma_dq_spec(spec, execution_point)</code>  <code>staticmethod</code>","text":"<p>Fetch dq functions from given table.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>dict</code> <p>data quality specification.</p> required <code>execution_point</code> <code>str</code> <p>if the execution is in_motion or at_rest.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary containing the dq spec with dq functions defined.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/dq_utils.py</code> <pre><code>@staticmethod\ndef build_prisma_dq_spec(spec: dict, execution_point: str) -&gt; dict:\n\"\"\"Fetch dq functions from given table.\n    Args:\n        spec: data quality specification.\n        execution_point: if the execution is in_motion or at_rest.\n    Returns:\n        The dictionary containing the dq spec with dq functions defined.\n    \"\"\"\nif spec.get(\"dq_db_table\"):\nspec = DQUtils.import_dq_rules_from_table(\nspec,\nexecution_point,\nDQTableBaseParameters.PRISMA_BASE_PARAMETERS.value,\nExecEnv.ENGINE_CONFIG.dq_functions_column_list,\n)\nelif spec.get(\"dq_functions\"):\nDQUtils.validate_dq_functions(\nspec,\nexecution_point,\nExecEnv.ENGINE_CONFIG.dq_functions_column_list,\n)\nelse:\nraise DQSpecMalformedException(\n\"When using PRISMA either dq_db_table or \"\n\"dq_functions needs to be defined.\"\n)\ndq_bucket = (\nExecEnv.ENGINE_CONFIG.dq_bucket\nif ExecEnv.get_environment() == \"prod\"\nelse ExecEnv.ENGINE_CONFIG.dq_dev_bucket\n)\nspec[\"critical_functions\"] = []\nspec[\"execution_point\"] = execution_point\nspec[\"result_sink_db_table\"] = None\nspec[\"result_sink_explode\"] = True\nspec[\"fail_on_error\"] = spec.get(\"fail_on_error\", False)\nspec[\"max_percentage_failure\"] = spec.get(\"max_percentage_failure\", 1)\nif not spec.get(\"result_sink_extra_columns\", None):\nspec[\"result_sink_extra_columns\"] = [\n\"validation_results.expectation_config.meta\",\n]\nelse:\nspec[\"result_sink_extra_columns\"] = [\n\"validation_results.expectation_config.meta\",\n] + spec[\"result_sink_extra_columns\"]\nif not spec.get(\"data_product_name\", None):\nraise DQSpecMalformedException(\n\"When using PRISMA DQ data_product_name must be defined.\"\n)\nspec[\"result_sink_location\"] = (\nf\"{dq_bucket}/{spec['data_product_name']}/result_sink/\"\n)\nif not spec.get(\"tbl_to_derive_pk\", None) and not spec.get(\n\"unexpected_rows_pk\", None\n):\nraise DQSpecMalformedException(\n\"When using PRISMA DQ either \"\n\"tbl_to_derive_pk or unexpected_rows_pk need to be defined.\"\n)\nreturn spec\n</code></pre>"},{"location":"reference/packages/utils/dq_utils.html#packages.utils.dq_utils.PrismaUtils.validate_rule_id_duplication","title":"<code>validate_rule_id_duplication(specs)</code>  <code>staticmethod</code>","text":"<p>Verify uniqueness of the dq_rule_id.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>list[DQSpec]</code> <p>a list of DQSpec to be validated</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dictionary with the spec_id as key and</p> <code>dict[str, str]</code> <p>rule_id as value for any duplicates.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/dq_utils.py</code> <pre><code>@staticmethod\ndef validate_rule_id_duplication(\nspecs: list[DQSpec],\n) -&gt; dict[str, str]:\n\"\"\"Verify uniqueness of the dq_rule_id.\n    Args:\n        specs: a list of DQSpec to be validated\n    Returns:\n         A dictionary with the spec_id as key and\n         rule_id as value for any duplicates.\n    \"\"\"\nerror_dict = {}\nfor spec in specs:\ndq_db_table = spec.dq_db_table\ndq_functions = spec.dq_functions\nspec_id = spec.spec_id\nif spec.dq_type == DQType.PRISMA.value and dq_db_table:\ndq_rule_id_query = f\"\"\"\n                SELECT dq_rule_id, COUNT(*) AS count\n                FROM {dq_db_table}\n                GROUP BY dq_rule_id\n                HAVING COUNT(*) &gt; 1;\n                \"\"\"  # nosec: B608\nduplicate_rule_id_table = ExecEnv.SESSION.sql(dq_rule_id_query)\nif not duplicate_rule_id_table.isEmpty():\nrows = duplicate_rule_id_table.collect()\ndf_str = \"; \".join([str(row) for row in rows])\nerror_dict[f\"dq_spec_id: {spec_id}\"] = df_str\nelif spec.dq_type == DQType.PRISMA.value and dq_functions:\ndq_rules_id_list = []\nfor dq_function in dq_functions:\ndq_rules_id_list.append(dq_function.args[\"meta\"][\"dq_rule_id\"])\nif len(dq_rules_id_list) != len(set(dq_rules_id_list)):\nerror_dict[f\"dq_spec_id: {spec_id}\"] = \"; \".join(\n[str(dq_rule_id) for dq_rule_id in dq_rules_id_list]\n)\nreturn error_dict\n</code></pre>"},{"location":"reference/packages/utils/engine_usage_stats.html","title":"Engine usage stats","text":"<p>Utilities for recording the engine activity.</p>"},{"location":"reference/packages/utils/engine_usage_stats.html#packages.utils.engine_usage_stats.EngineUsageStats","title":"<code>EngineUsageStats</code>","text":"<p>         Bases: <code>object</code></p> <p>Engine Usage utilities class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/engine_usage_stats.py</code> <pre><code>class EngineUsageStats(object):\n\"\"\"Engine Usage utilities class.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef store_engine_usage(\ncls,\nacon: dict,\nfunc_name: str,\ncollect_engine_usage: str = None,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Collects and store Lakehouse Engine usage statistics.\n        These statistics include the acon and other relevant information, such as\n        the lakehouse engine version and the functions/algorithms being used.\n        Args:\n            acon: acon dictionary file.\n            func_name: function name that called this log acon.\n            collect_engine_usage: Lakehouse usage statistics collection strategy.\n            spark_confs: optional dictionary with the spark confs to be used when\n                collecting the engine usage.\n        \"\"\"\ntry:\nif (\ncollect_engine_usage\nin [\nCollectEngineUsage.ENABLED.value,\nCollectEngineUsage.PROD_ONLY.value,\n]\nor ExecEnv.ENGINE_CONFIG.collect_engine_usage\nin CollectEngineUsage.ENABLED.value\n):\nstart_timestamp = datetime.now()\ntimestamp_str = start_timestamp.strftime(\"%Y%m%d%H%M%S\")\nusage_stats: Dict = {\"acon\": ConfigUtils.remove_sensitive_info(acon)}\nEngineUsageStats.get_spark_conf_values(usage_stats, spark_confs)\nengine_usage_path = None\nif usage_stats[\"environment\"] == \"prod\":\nengine_usage_path = ExecEnv.ENGINE_CONFIG.engine_usage_path\nelif collect_engine_usage != CollectEngineUsage.PROD_ONLY.value:\nengine_usage_path = ExecEnv.ENGINE_CONFIG.engine_dev_usage_path\nif engine_usage_path is not None:\nusage_stats[\"function\"] = func_name\nusage_stats[\"engine_version\"] = ConfigUtils.get_engine_version()\nusage_stats[\"start_timestamp\"] = start_timestamp\nusage_stats[\"year\"] = start_timestamp.year\nusage_stats[\"month\"] = start_timestamp.month\nrun_id_extracted = re.search(\n\"run-([1-9]\\\\w+)\", usage_stats[\"run_id\"]\n)\nusage_stats[\"run_id\"] = (\nrun_id_extracted.group(1) if run_id_extracted else \"\"\n)\nlog_file_name = f\"eng_usage_{func_name}_{timestamp_str}.json\"\nusage_stats_str = json.dumps(usage_stats, default=str)\nurl = urlparse(\nf\"{engine_usage_path}/{usage_stats['dp_name']}/\"\nf\"{start_timestamp.year}/{start_timestamp.month}/\"\nf\"{log_file_name}\",\nallow_fragments=False,\n)\ntry:\nFileStorageFunctions.write_payload(\nengine_usage_path, url, usage_stats_str\n)\ncls._LOGGER.info(\"Storing Lakehouse Engine usage statistics\")\nexcept FileNotFoundError as e:\ncls._LOGGER.error(\nf\"Could not write engine stats into file: {e}.\"\n)\nexcept Exception as e:\ncls._LOGGER.error(\n\"Failed while collecting the lakehouse engine stats: \"\nf\"Unexpected {e=}, {type(e)=}.\"\n)\n@classmethod\ndef get_spark_conf_values(cls, usage_stats: dict, spark_confs: dict) -&gt; None:\n\"\"\"Get information from spark session configurations.\n        Args:\n            usage_stats: usage_stats dictionary file.\n            spark_confs: optional dictionary with the spark tags to be used when\n                collecting the engine usage.\n        \"\"\"\nspark_confs = (\nEngineStats.DEF_SPARK_CONFS.value\nif spark_confs is None\nelse EngineStats.DEF_SPARK_CONFS.value | spark_confs\n)\nfor spark_conf_key, spark_conf_value in spark_confs.items():\n# whenever the spark_conf_value has #, it means it is an array, so we need\n# to split it and adequately process it\nif \"#\" in spark_conf_value:\narray_key = spark_conf_value.split(\"#\")\narray_values = ast.literal_eval(\nExecEnv.SESSION.conf.get(array_key[0], \"[]\")\n)\nfinal_value = [\nkey_val[\"value\"]\nfor key_val in array_values\nif key_val[\"key\"] == array_key[1]\n]\nusage_stats[spark_conf_key] = (\nfinal_value[0] if len(final_value) &gt; 0 else \"\"\n)\nelse:\nusage_stats[spark_conf_key] = ExecEnv.SESSION.conf.get(\nspark_conf_value, \"\"\n)\n</code></pre>"},{"location":"reference/packages/utils/engine_usage_stats.html#packages.utils.engine_usage_stats.EngineUsageStats.get_spark_conf_values","title":"<code>get_spark_conf_values(usage_stats, spark_confs)</code>  <code>classmethod</code>","text":"<p>Get information from spark session configurations.</p> <p>Parameters:</p> Name Type Description Default <code>usage_stats</code> <code>dict</code> <p>usage_stats dictionary file.</p> required <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark tags to be used when collecting the engine usage.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/engine_usage_stats.py</code> <pre><code>@classmethod\ndef get_spark_conf_values(cls, usage_stats: dict, spark_confs: dict) -&gt; None:\n\"\"\"Get information from spark session configurations.\n    Args:\n        usage_stats: usage_stats dictionary file.\n        spark_confs: optional dictionary with the spark tags to be used when\n            collecting the engine usage.\n    \"\"\"\nspark_confs = (\nEngineStats.DEF_SPARK_CONFS.value\nif spark_confs is None\nelse EngineStats.DEF_SPARK_CONFS.value | spark_confs\n)\nfor spark_conf_key, spark_conf_value in spark_confs.items():\n# whenever the spark_conf_value has #, it means it is an array, so we need\n# to split it and adequately process it\nif \"#\" in spark_conf_value:\narray_key = spark_conf_value.split(\"#\")\narray_values = ast.literal_eval(\nExecEnv.SESSION.conf.get(array_key[0], \"[]\")\n)\nfinal_value = [\nkey_val[\"value\"]\nfor key_val in array_values\nif key_val[\"key\"] == array_key[1]\n]\nusage_stats[spark_conf_key] = (\nfinal_value[0] if len(final_value) &gt; 0 else \"\"\n)\nelse:\nusage_stats[spark_conf_key] = ExecEnv.SESSION.conf.get(\nspark_conf_value, \"\"\n)\n</code></pre>"},{"location":"reference/packages/utils/engine_usage_stats.html#packages.utils.engine_usage_stats.EngineUsageStats.store_engine_usage","title":"<code>store_engine_usage(acon, func_name, collect_engine_usage=None, spark_confs=None)</code>  <code>classmethod</code>","text":"<p>Collects and store Lakehouse Engine usage statistics.</p> <p>These statistics include the acon and other relevant information, such as the lakehouse engine version and the functions/algorithms being used.</p> <p>Parameters:</p> Name Type Description Default <code>acon</code> <code>dict</code> <p>acon dictionary file.</p> required <code>func_name</code> <code>str</code> <p>function name that called this log acon.</p> required <code>collect_engine_usage</code> <code>str</code> <p>Lakehouse usage statistics collection strategy.</p> <code>None</code> <code>spark_confs</code> <code>dict</code> <p>optional dictionary with the spark confs to be used when collecting the engine usage.</p> <code>None</code> Source code in <code>mkdocs/lakehouse_engine/packages/utils/engine_usage_stats.py</code> <pre><code>@classmethod\ndef store_engine_usage(\ncls,\nacon: dict,\nfunc_name: str,\ncollect_engine_usage: str = None,\nspark_confs: dict = None,\n) -&gt; None:\n\"\"\"Collects and store Lakehouse Engine usage statistics.\n    These statistics include the acon and other relevant information, such as\n    the lakehouse engine version and the functions/algorithms being used.\n    Args:\n        acon: acon dictionary file.\n        func_name: function name that called this log acon.\n        collect_engine_usage: Lakehouse usage statistics collection strategy.\n        spark_confs: optional dictionary with the spark confs to be used when\n            collecting the engine usage.\n    \"\"\"\ntry:\nif (\ncollect_engine_usage\nin [\nCollectEngineUsage.ENABLED.value,\nCollectEngineUsage.PROD_ONLY.value,\n]\nor ExecEnv.ENGINE_CONFIG.collect_engine_usage\nin CollectEngineUsage.ENABLED.value\n):\nstart_timestamp = datetime.now()\ntimestamp_str = start_timestamp.strftime(\"%Y%m%d%H%M%S\")\nusage_stats: Dict = {\"acon\": ConfigUtils.remove_sensitive_info(acon)}\nEngineUsageStats.get_spark_conf_values(usage_stats, spark_confs)\nengine_usage_path = None\nif usage_stats[\"environment\"] == \"prod\":\nengine_usage_path = ExecEnv.ENGINE_CONFIG.engine_usage_path\nelif collect_engine_usage != CollectEngineUsage.PROD_ONLY.value:\nengine_usage_path = ExecEnv.ENGINE_CONFIG.engine_dev_usage_path\nif engine_usage_path is not None:\nusage_stats[\"function\"] = func_name\nusage_stats[\"engine_version\"] = ConfigUtils.get_engine_version()\nusage_stats[\"start_timestamp\"] = start_timestamp\nusage_stats[\"year\"] = start_timestamp.year\nusage_stats[\"month\"] = start_timestamp.month\nrun_id_extracted = re.search(\n\"run-([1-9]\\\\w+)\", usage_stats[\"run_id\"]\n)\nusage_stats[\"run_id\"] = (\nrun_id_extracted.group(1) if run_id_extracted else \"\"\n)\nlog_file_name = f\"eng_usage_{func_name}_{timestamp_str}.json\"\nusage_stats_str = json.dumps(usage_stats, default=str)\nurl = urlparse(\nf\"{engine_usage_path}/{usage_stats['dp_name']}/\"\nf\"{start_timestamp.year}/{start_timestamp.month}/\"\nf\"{log_file_name}\",\nallow_fragments=False,\n)\ntry:\nFileStorageFunctions.write_payload(\nengine_usage_path, url, usage_stats_str\n)\ncls._LOGGER.info(\"Storing Lakehouse Engine usage statistics\")\nexcept FileNotFoundError as e:\ncls._LOGGER.error(\nf\"Could not write engine stats into file: {e}.\"\n)\nexcept Exception as e:\ncls._LOGGER.error(\n\"Failed while collecting the lakehouse engine stats: \"\nf\"Unexpected {e=}, {type(e)=}.\"\n)\n</code></pre>"},{"location":"reference/packages/utils/expectations_utils.html","title":"Expectations utils","text":"<p>Utilities to be used by custom expectations.</p>"},{"location":"reference/packages/utils/expectations_utils.html#packages.utils.expectations_utils.validate_result","title":"<code>validate_result(expectation, configuration, metrics, runtime_configuration, execution_engine, base_expectation)</code>","text":"<p>Validates that the unexpected_index_list in the tests is corretly defined.</p> <p>Additionally, it validates the expectation using the GE _validate method.</p> <p>Parameters:</p> Name Type Description Default <code>expectation</code> <code>Expectation</code> <p>Expectation to validate.</p> required <code>configuration</code> <code>ExpectationConfiguration</code> <p>Configuration used in the test.</p> required <code>metrics</code> <code>dict</code> <p>Test result metrics.</p> required <code>runtime_configuration</code> <code>Optional[dict]</code> <p>Configuration used when running the expectation.</p> required <code>execution_engine</code> <code>Optional[ExecutionEngine]</code> <p>Execution engine used in the expectation.</p> required <code>base_expectation</code> <code>Expectation</code> <p>Base expectation to validate.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/expectations_utils.py</code> <pre><code>def validate_result(\nexpectation: Expectation,\nconfiguration: ExpectationConfiguration,\nmetrics: dict,\nruntime_configuration: Optional[dict],\nexecution_engine: Optional[ExecutionEngine],\nbase_expectation: Expectation,\n) -&gt; Any:\n\"\"\"Validates that the unexpected_index_list in the tests is corretly defined.\n    Additionally, it validates the expectation using the GE _validate method.\n    Args:\n        expectation: Expectation to validate.\n        configuration: Configuration used in the test.\n        metrics: Test result metrics.\n        runtime_configuration: Configuration used when running the expectation.\n        execution_engine: Execution engine used in the expectation.\n        base_expectation: Base expectation to validate.\n    \"\"\"\nexample_unexpected_index_list = _get_example_unexpected_index_list(\nexpectation, configuration\n)\ntest_unexpected_index_list = _get_test_unexpected_index_list(\nexpectation.map_metric, metrics\n)\nif example_unexpected_index_list:\nif example_unexpected_index_list != test_unexpected_index_list:\nraise AssertionError(\nf\"Example unexpected_index_list: {example_unexpected_index_list}\\n\"\nf\"Test unexpected_index_list: {test_unexpected_index_list}\"\n)\nreturn base_expectation._validate(\nexpectation, configuration, metrics, runtime_configuration, execution_engine\n)\n</code></pre>"},{"location":"reference/packages/utils/file_utils.html","title":"File utils","text":"<p>Utilities for file name based operations.</p>"},{"location":"reference/packages/utils/file_utils.html#packages.utils.file_utils.get_directory_path","title":"<code>get_directory_path(path)</code>","text":"<p>Add '/' to the end of the path of a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>directory to be processed</p> required <p>Returns:</p> Type Description <code>str</code> <p>Directory path stripped and with '/' at the end.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/file_utils.py</code> <pre><code>def get_directory_path(path: str) -&gt; str:\n\"\"\"Add '/' to the end of the path of a directory.\n    Args:\n        path: directory to be processed\n    Returns:\n        Directory path stripped and with '/' at the end.\n    \"\"\"\npath = path.strip()\nreturn path if path[-1] == \"/\" else path + \"/\"\n</code></pre>"},{"location":"reference/packages/utils/file_utils.html#packages.utils.file_utils.get_file_names_without_file_type","title":"<code>get_file_names_without_file_type(path, file_type, exclude_regex)</code>","text":"<p>Function to retrieve list of file names in a folder.</p> <p>This function filters by file type and removes the extension of the file name it returns.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the folder to list files</p> required <code>file_type</code> <code>str</code> <p>type of the file to include in list</p> required <code>exclude_regex</code> <code>str</code> <p>regex of file names to exclude</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of file names without file type.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/file_utils.py</code> <pre><code>def get_file_names_without_file_type(\npath: str, file_type: str, exclude_regex: str\n) -&gt; list:\n\"\"\"Function to retrieve list of file names in a folder.\n    This function filters by file type and removes the extension of the file name\n    it returns.\n    Args:\n        path: path to the folder to list files\n        file_type: type of the file to include in list\n        exclude_regex: regex of file names to exclude\n    Returns:\n        A list of file names without file type.\n    \"\"\"\nfile_list: List[str] = []\nfor file in listdir(path):\nif not re.search(exclude_regex, file) and file.endswith(file_type):\nfile_list.append(file.split(\".\")[0])\nreturn file_list\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html","title":"Gab utils","text":"<p>Module to define GAB Utility classes.</p>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABPartitionUtils","title":"<code>GABPartitionUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Class to extract a partition based in a date period.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>class GABPartitionUtils(object):\n\"\"\"Class to extract a partition based in a date period.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef get_years(cls, start_date: str, end_date: str) -&gt; list[str]:\n\"\"\"Return a list of distinct years from the input parameters.\n        Args:\n            start_date: start of the period.\n            end_date: end of the period.\n        \"\"\"\nyear = []\nif start_date &gt; end_date:\nraise ValueError(\n\"Input Error: Invalid start_date and end_date. \"\n\"Start_date is greater than end_date\"\n)\nfor i in range(int(start_date[0:4]), int(end_date[0:4]) + 1):\nyear.append(str(i))\nreturn year\n@classmethod\ndef get_partition_condition(cls, start_date: str, end_date: str) -&gt; str:\n\"\"\"Return year,month and day partition statement from the input parameters.\n        Args:\n            start_date: start of the period.\n            end_date: end of the period.\n        \"\"\"\nyears = cls.get_years(start_date, end_date)\nif len(years) &gt; 1:\npartition_condition = cls._get_multiple_years_partition(\nstart_date, end_date, years\n)\nelse:\npartition_condition = cls._get_single_year_partition(start_date, end_date)\nreturn partition_condition\n@classmethod\ndef _get_multiple_years_partition(\ncls, start_date: str, end_date: str, years: list[str]\n) -&gt; str:\n\"\"\"Return partition when executing multiple years (&gt;1).\n        Args:\n            start_date: start of the period.\n            end_date: end of the period.\n            years: list of years.\n        \"\"\"\nstart_date_month = cls._extract_date_part_from_date(\"MONTH\", start_date)\nstart_date_day = cls._extract_date_part_from_date(\"DAY\", start_date)\nend_date_month = cls._extract_date_part_from_date(\"MONTH\", end_date)\nend_date_day = cls._extract_date_part_from_date(\"DAY\", end_date)\nyear_statement = \"(year = {0} and (\".format(years[0]) + \"{})\"\nif start_date_month != \"12\":\nstart_date_partition = year_statement.format(\n\"(month = {0} and day between {1} and 31)\".format(\nstart_date_month, start_date_day\n)\n+ \" or (month between {0} and 12)\".format(int(start_date_month) + 1)\n)\nelse:\nstart_date_partition = year_statement.format(\n\"month = {0} and day between {1} and 31\".format(\nstart_date_month, start_date_day\n)\n)\nperiod_years_partition = \"\"\nif len(years) == 3:\nperiod_years_partition = \") or (year = {0}\".format(years[1])\nelif len(years) &gt; 3:\nperiod_years_partition = \") or (year between {0} and {1})\".format(\nyears[1], years[-2]\n)\nif end_date_month != \"01\":\nend_date_partition = (\n\") or (year = {0} and ((month between 01 and {1})\".format(\nyears[-1], int(end_date_month) - 1\n)\n+ \" or (month = {0} and day between 1 and {1})))\".format(\nend_date_month, end_date_day\n)\n)\nelse:\nend_date_partition = (\n\") or (year = {0} and month = 1 and day between 01 and {1})\".format(\nyears[-1], end_date_day\n)\n)\npartition_condition = (\nstart_date_partition + period_years_partition + end_date_partition\n)\nreturn partition_condition\n@classmethod\ndef _get_single_year_partition(cls, start_date: str, end_date: str) -&gt; str:\n\"\"\"Return partition when executing a single year.\n        Args:\n            start_date: start of the period.\n            end_date: end of the period.\n        \"\"\"\nstart_date_year = cls._extract_date_part_from_date(\"YEAR\", start_date)\nstart_date_month = cls._extract_date_part_from_date(\"MONTH\", start_date)\nstart_date_day = cls._extract_date_part_from_date(\"DAY\", start_date)\nend_date_year = cls._extract_date_part_from_date(\"YEAR\", end_date)\nend_date_month = cls._extract_date_part_from_date(\"MONTH\", end_date)\nend_date_day = cls._extract_date_part_from_date(\"DAY\", end_date)\nif start_date_month != end_date_month:\nmonths = []\nfor i in range(int(start_date_month), int(end_date_month) + 1):\nmonths.append(i)\nstart_date_partition = (\n\"year = {0} and ((month={1} and day between {2} and 31)\".format(\nstart_date_year, months[0], start_date_day\n)\n)\nperiod_years_partition = \"\"\nif len(months) == 2:\nperiod_years_partition = start_date_partition\nelif len(months) == 3:\nperiod_years_partition = (\nstart_date_partition + \" or (month = {0})\".format(months[1])\n)\nelif len(months) &gt; 3:\nperiod_years_partition = (\nstart_date_partition\n+ \" or (month between {0} and {1})\".format(months[1], months[-2])\n)\npartition_condition = (\nperiod_years_partition\n+ \" or (month = {0} and day between 1 and {1}))\".format(\nend_date_month, end_date_day\n)\n)\nelse:\npartition_condition = (\n\"year = {0} and month = {1} and day between {2} and {3}\".format(\nend_date_year, end_date_month, start_date_day, end_date_day\n)\n)\nreturn partition_condition\n@classmethod\ndef _extract_date_part_from_date(cls, part: str, date: str) -&gt; str:\n\"\"\"Extract date part from string date.\n        Args:\n            part: date part (possible values: DAY, MONTH, YEAR)\n            date: string date.\n        \"\"\"\nif \"DAY\" == part.upper():\nreturn date[8:10]\nelif \"MONTH\" == part.upper():\nreturn date[5:7]\nelse:\nreturn date[0:4]\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABPartitionUtils.get_partition_condition","title":"<code>get_partition_condition(start_date, end_date)</code>  <code>classmethod</code>","text":"<p>Return year,month and day partition statement from the input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>start of the period.</p> required <code>end_date</code> <code>str</code> <p>end of the period.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>@classmethod\ndef get_partition_condition(cls, start_date: str, end_date: str) -&gt; str:\n\"\"\"Return year,month and day partition statement from the input parameters.\n    Args:\n        start_date: start of the period.\n        end_date: end of the period.\n    \"\"\"\nyears = cls.get_years(start_date, end_date)\nif len(years) &gt; 1:\npartition_condition = cls._get_multiple_years_partition(\nstart_date, end_date, years\n)\nelse:\npartition_condition = cls._get_single_year_partition(start_date, end_date)\nreturn partition_condition\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABPartitionUtils.get_years","title":"<code>get_years(start_date, end_date)</code>  <code>classmethod</code>","text":"<p>Return a list of distinct years from the input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>start of the period.</p> required <code>end_date</code> <code>str</code> <p>end of the period.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>@classmethod\ndef get_years(cls, start_date: str, end_date: str) -&gt; list[str]:\n\"\"\"Return a list of distinct years from the input parameters.\n    Args:\n        start_date: start of the period.\n        end_date: end of the period.\n    \"\"\"\nyear = []\nif start_date &gt; end_date:\nraise ValueError(\n\"Input Error: Invalid start_date and end_date. \"\n\"Start_date is greater than end_date\"\n)\nfor i in range(int(start_date[0:4]), int(end_date[0:4]) + 1):\nyear.append(str(i))\nreturn year\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils","title":"<code>GABUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Class containing utility functions for GAB.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>class GABUtils(object):\n\"\"\"Class containing utility functions for GAB.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\ndef logger(\nself,\nrun_start_time: datetime,\nrun_end_time: datetime,\nstart: str,\nend: str,\nquery_id: str,\nquery_label: str,\ncadence: str,\nstage_file_path: str,\nquery: str,\nstatus: str,\nerror_message: Union[Exception, str],\ntarget_database: str,\n) -&gt; None:\n\"\"\"Store the execution of each stage in the log events table.\n        Args:\n            run_start_time: execution start time.\n            run_end_time: execution end time.\n            start: use case start date.\n            end: use case end date.\n            query_id: gab configuration table use case identifier.\n            query_label: gab configuration table use case name.\n            cadence: cadence to process.\n            stage_file_path: stage file path.\n            query: query to execute.\n            status: status of the query execution.\n            error_message: error message if present.\n            target_database: target database to write.\n        \"\"\"\nins = \"\"\"\n        INSERT INTO {database}.gab_log_events\n        VALUES (\n            '{run_start_time}',\n            '{run_end_time}',\n            '{start}',\n            '{end}',\n{query_id},\n            '{query_label}',\n            '{cadence}',\n            '{stage_file_path}',\n            '{query}',\n            '{status}',\n            '{error_message}'\n        )\"\"\".format(  # nosec: B608\ndatabase=target_database,\nrun_start_time=run_start_time,\nrun_end_time=run_end_time,\nstart=start,\nend=end,\nquery_id=query_id,\nquery_label=query_label,\ncadence=cadence,\nstage_file_path=stage_file_path,\nquery=self._escape_quote(query),\nstatus=status,\nerror_message=(\nself._escape_quote(str(error_message))\nif status == \"Failed\"\nelse error_message\n),\n)\nExecEnv.SESSION.sql(ins)\n@classmethod\ndef _escape_quote(cls, to_escape: str) -&gt; str:\n\"\"\"Escape quote on string.\n        Args:\n            to_escape: string to escape.\n        \"\"\"\nreturn to_escape.replace(\"'\", r\"\\'\").replace('\"', r\"\\\"\")\n@classmethod\ndef get_json_column_as_dict(\ncls, lookup_query_builder: DataFrame, query_id: str, query_column: str\n) -&gt; dict:  # type: ignore\n\"\"\"Get JSON column as dictionary.\n        Args:\n            lookup_query_builder: gab configuration data.\n            query_id: gab configuration table use case identifier.\n            query_column: column to get as json.\n        \"\"\"\ncolumn_df = lookup_query_builder.filter(\ncol(\"query_id\") == lit(query_id)\n).select(col(query_column))\ncolumn_df_json = column_df.select(\nto_json(struct([column_df[x] for x in column_df.columns]))\n).collect()[0][0]\njson_column = json.loads(column_df_json)\nfor mapping in json_column.values():\ncolumn_as_json = ast.literal_eval(mapping)\nreturn column_as_json  # type: ignore\n@classmethod\ndef extract_columns_from_mapping(\ncls,\ncolumns: dict,\nis_dimension: bool,\nextract_column_without_alias: bool = False,\ntable_alias: Optional[str] = None,\nis_extracted_value_as_name: bool = True,\n) -&gt; Union[tuple[list[str], list[str]], list[str]]:\n\"\"\"Extract and transform columns to SQL select statement.\n        Args:\n            columns: data to extract the columns.\n            is_dimension: flag identifying if is a dimension or a metric.\n            extract_column_without_alias: flag to inform if it's to extract columns\n                without aliases.\n            table_alias: name or alias from the source table.\n            is_extracted_value_as_name: identify if the extracted value is the\n                column name.\n        \"\"\"\ncolumn_with_alias = (\n\"\".join([table_alias, \".\", \"{} as {}\"]) if table_alias else \"{} as {}\"\n)\ncolumn_without_alias = (\n\"\".join([table_alias, \".\", \"{}\"]) if table_alias else \"{}\"\n)\nextracted_columns_with_alias = []\nextracted_columns_without_alias = []\nfor column_name, column_value in columns.items():\nif extract_column_without_alias:\nextracted_column_without_alias = column_without_alias.format(\ncls._get_column_format_without_alias(\nis_dimension,\ncolumn_name,\ncolumn_value,\nis_extracted_value_as_name,\n)\n)\nextracted_columns_without_alias.append(extracted_column_without_alias)\nextracted_column_with_alias = column_with_alias.format(\n*cls._extract_column_with_alias(\nis_dimension,\ncolumn_name,\ncolumn_value,\nis_extracted_value_as_name,\n)\n)\nextracted_columns_with_alias.append(extracted_column_with_alias)\nreturn (\n(extracted_columns_with_alias, extracted_columns_without_alias)\nif extract_column_without_alias\nelse extracted_columns_with_alias\n)\n@classmethod\ndef _extract_column_with_alias(\ncls,\nis_dimension: bool,\ncolumn_name: str,\ncolumn_value: Union[str, dict],\nis_extracted_value_as_name: bool = True,\n) -&gt; tuple[str, str]:\n\"\"\"Extract column name with alias.\n        Args:\n            is_dimension: flag indicating if the column is a dimension.\n            column_name: name of the column.\n            column_value: value of the column.\n            is_extracted_value_as_name: flag indicating if the name of the column is the\n                extracted value.\n        \"\"\"\nextracted_value = (\ncolumn_value\nif is_dimension\nelse (column_value[\"metric_name\"])  # type: ignore\n)\nreturn (\n(extracted_value, column_name)  # type: ignore\nif is_extracted_value_as_name\nelse (column_name, extracted_value)\n)\n@classmethod\ndef _get_column_format_without_alias(\ncls,\nis_dimension: bool,\ncolumn_name: str,\ncolumn_value: Union[str, dict],\nis_extracted_value_as_name: bool = True,\n) -&gt; str:\n\"\"\"Extract column name without alias.\n        Args:\n            is_dimension: flag indicating if the column is a dimension.\n            column_name: name of the column.\n            column_value: value of the column.\n            is_extracted_value_as_name: flag indicating if the name of the column is the\n                extracted value.\n        \"\"\"\nextracted_value: str = (\ncolumn_value\nif is_dimension\nelse (column_value[\"metric_name\"])  # type: ignore\n)\nreturn extracted_value if is_extracted_value_as_name else column_name\n@classmethod\ndef get_cadence_configuration_at_end_date(cls, end_date: datetime) -&gt; dict:\n\"\"\"A dictionary that corresponds to the conclusion of a cadence.\n        Any end date inputted by the user we check this end date is actually end of\n            a cadence (YEAR, QUARTER, MONTH, WEEK).\n        If the user input is 2024-03-31 this is a month end and a quarter end that\n            means any use cases configured as month or quarter need to be calculated.\n        Args:\n            end_date: base end date.\n        \"\"\"\ninit_end_date_dict = {}\nexpected_end_cadence_date = pendulum.datetime(\nint(end_date.strftime(\"%Y\")),\nint(end_date.strftime(\"%m\")),\nint(end_date.strftime(\"%d\")),\n).replace(tzinfo=None)\n# Validating YEAR cadence\nif end_date == expected_end_cadence_date.last_of(\"year\"):\ninit_end_date_dict[\"YEAR\"] = \"N\"\n# Validating QUARTER cadence\nif end_date == expected_end_cadence_date.last_of(\"quarter\"):\ninit_end_date_dict[\"QUARTER\"] = \"N\"\n# Validating MONTH cadence\nif end_date == datetime(\nint(end_date.strftime(\"%Y\")),\nint(end_date.strftime(\"%m\")),\ncalendar.monthrange(\nint(end_date.strftime(\"%Y\")), int(end_date.strftime(\"%m\"))\n)[1],\n):\ninit_end_date_dict[\"MONTH\"] = \"N\"\n# Validating WEEK cadence\nif end_date == expected_end_cadence_date.end_of(\"week\").replace(\nhour=0, minute=0, second=0, microsecond=0\n):\ninit_end_date_dict[\"WEEK\"] = \"N\"\ninit_end_date_dict[\"DAY\"] = \"N\"\nreturn init_end_date_dict\ndef get_reconciliation_cadences(\nself,\ncadence: str,\nselected_reconciliation_window: dict,\ncadence_configuration_at_end_date: dict,\nrerun_flag: str,\n) -&gt; dict:\n\"\"\"Get reconciliation cadences based on the use case configuration.\n        Args:\n            cadence: cadence to process.\n            selected_reconciliation_window: configured use case reconciliation window.\n            cadence_configuration_at_end_date: cadences to execute at the end date.\n            rerun_flag: flag indicating if it's a rerun or a normal run.\n        \"\"\"\nconfigured_cadences = self._get_configured_cadences_by_snapshot(\ncadence, selected_reconciliation_window, cadence_configuration_at_end_date\n)\nreturn self._get_cadences_to_execute(\nconfigured_cadences, cadence, cadence_configuration_at_end_date, rerun_flag\n)\n@classmethod\ndef _get_cadences_to_execute(\ncls,\nconfigured_cadences: dict,\ncadence: str,\ncadence_configuration_at_end_date: dict,\nrerun_flag: str,\n) -&gt; dict:\n\"\"\"Get cadences to execute.\n        Args:\n            cadence: cadence to process.\n            configured_cadences: configured use case reconciliation window.\n            cadence_configuration_at_end_date: cadences to execute at the end date.\n            rerun_flag: flag indicating if it's a rerun or a normal run.\n        \"\"\"\ncadences_to_execute = {}\ncad_order = GABCadence.get_ordered_cadences()\nfor snapshot_cadence, snapshot_flag in configured_cadences.items():\nif (\n(cad_order[cadence] &gt; cad_order[snapshot_cadence])\nand (rerun_flag == \"Y\")\n) or snapshot_cadence in cadence_configuration_at_end_date:\ncadences_to_execute[snapshot_cadence] = snapshot_flag\nelif snapshot_cadence not in cadence_configuration_at_end_date:\ncontinue\nreturn cls._sort_cadences_to_execute(cadences_to_execute, cad_order)\n@classmethod\ndef _sort_cadences_to_execute(\ncls, cadences_to_execute: dict, cad_order: dict\n) -&gt; dict:\n\"\"\"Sort the cadences to execute.\n        Args:\n            cadences_to_execute: cadences to execute.\n            cad_order: all cadences with order.\n        \"\"\"\n# ordering it because when grouping cadences with snapshot and without snapshot\n# can impact the cadence ordering.\nsorted_cadences_to_execute: dict = dict(\nsorted(\ncadences_to_execute.items(),\nkey=lambda item: cad_order.get(item[0]),  # type: ignore\n)\n)\n# ordering cadences to execute it from bigger (YEAR) to smaller (DAY)\ncadences_to_execute_items = []\nfor cadence_name, cadence_value in sorted_cadences_to_execute.items():\ncadences_to_execute_items.append((cadence_name, cadence_value))\ncadences_sorted_by_bigger_cadence_to_execute: dict = dict(\nreversed(cadences_to_execute_items)\n)\nreturn cadences_sorted_by_bigger_cadence_to_execute\n@classmethod\ndef _get_configured_cadences_by_snapshot(\ncls,\ncadence: str,\nselected_reconciliation_window: dict,\ncadence_configuration_at_end_date: dict,\n) -&gt; dict:\n\"\"\"Get configured cadences to execute.\n        Args:\n            cadence: selected cadence.\n            selected_reconciliation_window: configured use case reconciliation window.\n            cadence_configuration_at_end_date: cadences to execute at the end date.\n        Returns:\n            Each cadence with the corresponding information if it's to execute with\n                snapshot or not.\n        \"\"\"\ncadences_by_snapshot = {}\n(\nno_snapshot_cadences,\nsnapshot_cadences,\n) = cls._generate_reconciliation_by_snapshot(\ncadence, selected_reconciliation_window\n)\nfor snapshot_cadence, snapshot_flag in no_snapshot_cadences.items():\nif snapshot_cadence in cadence_configuration_at_end_date:\ncadences_by_snapshot[snapshot_cadence] = snapshot_flag\ncls._LOGGER.info(f\"{snapshot_cadence} is present in {cadence} cadence\")\nbreak\ncadences_by_snapshot.update(snapshot_cadences)\nif (not cadences_by_snapshot) and (\ncadence in cadence_configuration_at_end_date\n):\ncadences_by_snapshot[cadence] = \"N\"\nreturn cadences_by_snapshot\n@classmethod\ndef _generate_reconciliation_by_snapshot(\ncls, cadence: str, selected_reconciliation_window: dict\n) -&gt; tuple[dict, dict]:\n\"\"\"Generate reconciliation by snapshot.\n        Args:\n            cadence: cadence to process.\n            selected_reconciliation_window: configured use case reconciliation window.\n        \"\"\"\ncadence_snapshot_configuration = {cadence: \"N\"}\nfor cadence in GABCadence.get_cadences():\ncls._add_cadence_snapshot_to_cadence_snapshot_config(\ncadence, selected_reconciliation_window, cadence_snapshot_configuration\n)\ncadence_snapshot_configuration = dict(\nsorted(\ncadence_snapshot_configuration.items(),\nkey=(\nlambda item: GABCadence.get_ordered_cadences().get(  # type: ignore\nitem[0]\n)\n),\n)\n)\ncadence_snapshot_configuration = dict(\nreversed(list(cadence_snapshot_configuration.items()))\n)\ncadences_without_snapshot = {\nkey: value\nfor key, value in cadence_snapshot_configuration.items()\nif value == \"N\"\n}\ncadences_with_snapshot = {\nkey: value\nfor key, value in cadence_snapshot_configuration.items()\nif value == \"Y\"\n}\nreturn cadences_with_snapshot, cadences_without_snapshot\n@classmethod\ndef _add_cadence_snapshot_to_cadence_snapshot_config(\ncls,\ncadence: str,\nselected_reconciliation_window: dict,\ncadence_snapshot_configuration: dict,\n) -&gt; None:\n\"\"\"Add the selected reconciliation to cadence snapshot configuration.\n        Args:\n            cadence: selected cadence.\n            selected_reconciliation_window:  configured use case reconciliation window.\n            cadence_snapshot_configuration: cadence snapshot configuration dictionary\n                who will be updated with the new value.\n        \"\"\"\nif cadence in selected_reconciliation_window:\ncadence_snapshot_configuration[cadence] = selected_reconciliation_window[\ncadence\n][\"snapshot\"]\n@classmethod\ndef format_datetime_to_default(cls, date_to_format: datetime) -&gt; str:\n\"\"\"Format datetime to GAB default format.\n        Args:\n            date_to_format: date to format.\n        \"\"\"\nreturn datetime.date(date_to_format).strftime(GABDefaults.DATE_FORMAT.value)\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils.extract_columns_from_mapping","title":"<code>extract_columns_from_mapping(columns, is_dimension, extract_column_without_alias=False, table_alias=None, is_extracted_value_as_name=True)</code>  <code>classmethod</code>","text":"<p>Extract and transform columns to SQL select statement.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>dict</code> <p>data to extract the columns.</p> required <code>is_dimension</code> <code>bool</code> <p>flag identifying if is a dimension or a metric.</p> required <code>extract_column_without_alias</code> <code>bool</code> <p>flag to inform if it's to extract columns without aliases.</p> <code>False</code> <code>table_alias</code> <code>Optional[str]</code> <p>name or alias from the source table.</p> <code>None</code> <code>is_extracted_value_as_name</code> <code>bool</code> <p>identify if the extracted value is the column name.</p> <code>True</code> Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>@classmethod\ndef extract_columns_from_mapping(\ncls,\ncolumns: dict,\nis_dimension: bool,\nextract_column_without_alias: bool = False,\ntable_alias: Optional[str] = None,\nis_extracted_value_as_name: bool = True,\n) -&gt; Union[tuple[list[str], list[str]], list[str]]:\n\"\"\"Extract and transform columns to SQL select statement.\n    Args:\n        columns: data to extract the columns.\n        is_dimension: flag identifying if is a dimension or a metric.\n        extract_column_without_alias: flag to inform if it's to extract columns\n            without aliases.\n        table_alias: name or alias from the source table.\n        is_extracted_value_as_name: identify if the extracted value is the\n            column name.\n    \"\"\"\ncolumn_with_alias = (\n\"\".join([table_alias, \".\", \"{} as {}\"]) if table_alias else \"{} as {}\"\n)\ncolumn_without_alias = (\n\"\".join([table_alias, \".\", \"{}\"]) if table_alias else \"{}\"\n)\nextracted_columns_with_alias = []\nextracted_columns_without_alias = []\nfor column_name, column_value in columns.items():\nif extract_column_without_alias:\nextracted_column_without_alias = column_without_alias.format(\ncls._get_column_format_without_alias(\nis_dimension,\ncolumn_name,\ncolumn_value,\nis_extracted_value_as_name,\n)\n)\nextracted_columns_without_alias.append(extracted_column_without_alias)\nextracted_column_with_alias = column_with_alias.format(\n*cls._extract_column_with_alias(\nis_dimension,\ncolumn_name,\ncolumn_value,\nis_extracted_value_as_name,\n)\n)\nextracted_columns_with_alias.append(extracted_column_with_alias)\nreturn (\n(extracted_columns_with_alias, extracted_columns_without_alias)\nif extract_column_without_alias\nelse extracted_columns_with_alias\n)\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils.format_datetime_to_default","title":"<code>format_datetime_to_default(date_to_format)</code>  <code>classmethod</code>","text":"<p>Format datetime to GAB default format.</p> <p>Parameters:</p> Name Type Description Default <code>date_to_format</code> <code>datetime</code> <p>date to format.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>@classmethod\ndef format_datetime_to_default(cls, date_to_format: datetime) -&gt; str:\n\"\"\"Format datetime to GAB default format.\n    Args:\n        date_to_format: date to format.\n    \"\"\"\nreturn datetime.date(date_to_format).strftime(GABDefaults.DATE_FORMAT.value)\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils.get_cadence_configuration_at_end_date","title":"<code>get_cadence_configuration_at_end_date(end_date)</code>  <code>classmethod</code>","text":"<p>A dictionary that corresponds to the conclusion of a cadence.</p> <p>Any end date inputted by the user we check this end date is actually end of     a cadence (YEAR, QUARTER, MONTH, WEEK). If the user input is 2024-03-31 this is a month end and a quarter end that     means any use cases configured as month or quarter need to be calculated.</p> <p>Parameters:</p> Name Type Description Default <code>end_date</code> <code>datetime</code> <p>base end date.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>@classmethod\ndef get_cadence_configuration_at_end_date(cls, end_date: datetime) -&gt; dict:\n\"\"\"A dictionary that corresponds to the conclusion of a cadence.\n    Any end date inputted by the user we check this end date is actually end of\n        a cadence (YEAR, QUARTER, MONTH, WEEK).\n    If the user input is 2024-03-31 this is a month end and a quarter end that\n        means any use cases configured as month or quarter need to be calculated.\n    Args:\n        end_date: base end date.\n    \"\"\"\ninit_end_date_dict = {}\nexpected_end_cadence_date = pendulum.datetime(\nint(end_date.strftime(\"%Y\")),\nint(end_date.strftime(\"%m\")),\nint(end_date.strftime(\"%d\")),\n).replace(tzinfo=None)\n# Validating YEAR cadence\nif end_date == expected_end_cadence_date.last_of(\"year\"):\ninit_end_date_dict[\"YEAR\"] = \"N\"\n# Validating QUARTER cadence\nif end_date == expected_end_cadence_date.last_of(\"quarter\"):\ninit_end_date_dict[\"QUARTER\"] = \"N\"\n# Validating MONTH cadence\nif end_date == datetime(\nint(end_date.strftime(\"%Y\")),\nint(end_date.strftime(\"%m\")),\ncalendar.monthrange(\nint(end_date.strftime(\"%Y\")), int(end_date.strftime(\"%m\"))\n)[1],\n):\ninit_end_date_dict[\"MONTH\"] = \"N\"\n# Validating WEEK cadence\nif end_date == expected_end_cadence_date.end_of(\"week\").replace(\nhour=0, minute=0, second=0, microsecond=0\n):\ninit_end_date_dict[\"WEEK\"] = \"N\"\ninit_end_date_dict[\"DAY\"] = \"N\"\nreturn init_end_date_dict\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils.get_json_column_as_dict","title":"<code>get_json_column_as_dict(lookup_query_builder, query_id, query_column)</code>  <code>classmethod</code>","text":"<p>Get JSON column as dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>lookup_query_builder</code> <code>DataFrame</code> <p>gab configuration data.</p> required <code>query_id</code> <code>str</code> <p>gab configuration table use case identifier.</p> required <code>query_column</code> <code>str</code> <p>column to get as json.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>@classmethod\ndef get_json_column_as_dict(\ncls, lookup_query_builder: DataFrame, query_id: str, query_column: str\n) -&gt; dict:  # type: ignore\n\"\"\"Get JSON column as dictionary.\n    Args:\n        lookup_query_builder: gab configuration data.\n        query_id: gab configuration table use case identifier.\n        query_column: column to get as json.\n    \"\"\"\ncolumn_df = lookup_query_builder.filter(\ncol(\"query_id\") == lit(query_id)\n).select(col(query_column))\ncolumn_df_json = column_df.select(\nto_json(struct([column_df[x] for x in column_df.columns]))\n).collect()[0][0]\njson_column = json.loads(column_df_json)\nfor mapping in json_column.values():\ncolumn_as_json = ast.literal_eval(mapping)\nreturn column_as_json  # type: ignore\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils.get_reconciliation_cadences","title":"<code>get_reconciliation_cadences(cadence, selected_reconciliation_window, cadence_configuration_at_end_date, rerun_flag)</code>","text":"<p>Get reconciliation cadences based on the use case configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cadence</code> <code>str</code> <p>cadence to process.</p> required <code>selected_reconciliation_window</code> <code>dict</code> <p>configured use case reconciliation window.</p> required <code>cadence_configuration_at_end_date</code> <code>dict</code> <p>cadences to execute at the end date.</p> required <code>rerun_flag</code> <code>str</code> <p>flag indicating if it's a rerun or a normal run.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>def get_reconciliation_cadences(\nself,\ncadence: str,\nselected_reconciliation_window: dict,\ncadence_configuration_at_end_date: dict,\nrerun_flag: str,\n) -&gt; dict:\n\"\"\"Get reconciliation cadences based on the use case configuration.\n    Args:\n        cadence: cadence to process.\n        selected_reconciliation_window: configured use case reconciliation window.\n        cadence_configuration_at_end_date: cadences to execute at the end date.\n        rerun_flag: flag indicating if it's a rerun or a normal run.\n    \"\"\"\nconfigured_cadences = self._get_configured_cadences_by_snapshot(\ncadence, selected_reconciliation_window, cadence_configuration_at_end_date\n)\nreturn self._get_cadences_to_execute(\nconfigured_cadences, cadence, cadence_configuration_at_end_date, rerun_flag\n)\n</code></pre>"},{"location":"reference/packages/utils/gab_utils.html#packages.utils.gab_utils.GABUtils.logger","title":"<code>logger(run_start_time, run_end_time, start, end, query_id, query_label, cadence, stage_file_path, query, status, error_message, target_database)</code>","text":"<p>Store the execution of each stage in the log events table.</p> <p>Parameters:</p> Name Type Description Default <code>run_start_time</code> <code>datetime</code> <p>execution start time.</p> required <code>run_end_time</code> <code>datetime</code> <p>execution end time.</p> required <code>start</code> <code>str</code> <p>use case start date.</p> required <code>end</code> <code>str</code> <p>use case end date.</p> required <code>query_id</code> <code>str</code> <p>gab configuration table use case identifier.</p> required <code>query_label</code> <code>str</code> <p>gab configuration table use case name.</p> required <code>cadence</code> <code>str</code> <p>cadence to process.</p> required <code>stage_file_path</code> <code>str</code> <p>stage file path.</p> required <code>query</code> <code>str</code> <p>query to execute.</p> required <code>status</code> <code>str</code> <p>status of the query execution.</p> required <code>error_message</code> <code>Union[Exception, str]</code> <p>error message if present.</p> required <code>target_database</code> <code>str</code> <p>target database to write.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/gab_utils.py</code> <pre><code>def logger(\nself,\nrun_start_time: datetime,\nrun_end_time: datetime,\nstart: str,\nend: str,\nquery_id: str,\nquery_label: str,\ncadence: str,\nstage_file_path: str,\nquery: str,\nstatus: str,\nerror_message: Union[Exception, str],\ntarget_database: str,\n) -&gt; None:\n\"\"\"Store the execution of each stage in the log events table.\n    Args:\n        run_start_time: execution start time.\n        run_end_time: execution end time.\n        start: use case start date.\n        end: use case end date.\n        query_id: gab configuration table use case identifier.\n        query_label: gab configuration table use case name.\n        cadence: cadence to process.\n        stage_file_path: stage file path.\n        query: query to execute.\n        status: status of the query execution.\n        error_message: error message if present.\n        target_database: target database to write.\n    \"\"\"\nins = \"\"\"\n    INSERT INTO {database}.gab_log_events\n    VALUES (\n        '{run_start_time}',\n        '{run_end_time}',\n        '{start}',\n        '{end}',\n{query_id},\n        '{query_label}',\n        '{cadence}',\n        '{stage_file_path}',\n        '{query}',\n        '{status}',\n        '{error_message}'\n    )\"\"\".format(  # nosec: B608\ndatabase=target_database,\nrun_start_time=run_start_time,\nrun_end_time=run_end_time,\nstart=start,\nend=end,\nquery_id=query_id,\nquery_label=query_label,\ncadence=cadence,\nstage_file_path=stage_file_path,\nquery=self._escape_quote(query),\nstatus=status,\nerror_message=(\nself._escape_quote(str(error_message))\nif status == \"Failed\"\nelse error_message\n),\n)\nExecEnv.SESSION.sql(ins)\n</code></pre>"},{"location":"reference/packages/utils/logging_handler.html","title":"Logging handler","text":"<p>Module to configure project logging.</p>"},{"location":"reference/packages/utils/logging_handler.html#packages.utils.logging_handler.FilterSensitiveData","title":"<code>FilterSensitiveData</code>","text":"<p>         Bases: <code>logging.Filter</code></p> <p>Logging filter to hide sensitive data from being shown in the logs.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/logging_handler.py</code> <pre><code>class FilterSensitiveData(logging.Filter):\n\"\"\"Logging filter to hide sensitive data from being shown in the logs.\"\"\"\ndef filter(self, record: logging.LogRecord) -&gt; bool:  # noqa: A003\n\"\"\"Hide sensitive information from being shown in the logs.\n        Based on the configured regex and replace strings, the content of the log\n        records is replaced and then all the records are allowed to be logged\n        (return True).\n        Args:\n            record: the LogRecord event being logged.\n        Returns:\n            The transformed record to be logged.\n        \"\"\"\nfor key_reg in SENSITIVE_KEYS_REG:\nrecord.msg = re.sub(key_reg[\"regex\"], key_reg[\"replace\"], str(record.msg))\nreturn True\n</code></pre>"},{"location":"reference/packages/utils/logging_handler.html#packages.utils.logging_handler.FilterSensitiveData.filter","title":"<code>filter(record)</code>","text":"<p>Hide sensitive information from being shown in the logs.</p> <p>Based on the configured regex and replace strings, the content of the log records is replaced and then all the records are allowed to be logged (return True).</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>logging.LogRecord</code> <p>the LogRecord event being logged.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>The transformed record to be logged.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/logging_handler.py</code> <pre><code>def filter(self, record: logging.LogRecord) -&gt; bool:  # noqa: A003\n\"\"\"Hide sensitive information from being shown in the logs.\n    Based on the configured regex and replace strings, the content of the log\n    records is replaced and then all the records are allowed to be logged\n    (return True).\n    Args:\n        record: the LogRecord event being logged.\n    Returns:\n        The transformed record to be logged.\n    \"\"\"\nfor key_reg in SENSITIVE_KEYS_REG:\nrecord.msg = re.sub(key_reg[\"regex\"], key_reg[\"replace\"], str(record.msg))\nreturn True\n</code></pre>"},{"location":"reference/packages/utils/logging_handler.html#packages.utils.logging_handler.LoggingHandler","title":"<code>LoggingHandler</code>","text":"<p>         Bases: <code>object</code></p> <p>Handle the logging of the lakehouse engine project.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/logging_handler.py</code> <pre><code>class LoggingHandler(object):\n\"\"\"Handle the logging of the lakehouse engine project.\"\"\"\ndef __init__(self, class_name: str):\n\"\"\"Construct a LoggingHandler instance.\n        Args:\n            class_name: name of the class to be indicated in the logs.\n        \"\"\"\nself._logger: logging.Logger = logging.getLogger(class_name)\nself._logger.setLevel(logging.DEBUG)\nself._logger.addFilter(FilterSensitiveData())\nlsh = logging.StreamHandler()\nlsh.setLevel(logging.DEBUG)\nlsh.setFormatter(FORMATTER)\nif not self._logger.hasHandlers():\n# avoid keep adding handlers and therefore duplicate messages\nself._logger.addHandler(lsh)\ndef get_logger(self) -&gt; logging.Logger:\n\"\"\"Get the _logger instance variable.\n        Returns:\n            logging.Logger: the logger object.\n        \"\"\"\nreturn self._logger\n</code></pre>"},{"location":"reference/packages/utils/logging_handler.html#packages.utils.logging_handler.LoggingHandler.__init__","title":"<code>__init__(class_name)</code>","text":"<p>Construct a LoggingHandler instance.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>name of the class to be indicated in the logs.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/logging_handler.py</code> <pre><code>def __init__(self, class_name: str):\n\"\"\"Construct a LoggingHandler instance.\n    Args:\n        class_name: name of the class to be indicated in the logs.\n    \"\"\"\nself._logger: logging.Logger = logging.getLogger(class_name)\nself._logger.setLevel(logging.DEBUG)\nself._logger.addFilter(FilterSensitiveData())\nlsh = logging.StreamHandler()\nlsh.setLevel(logging.DEBUG)\nlsh.setFormatter(FORMATTER)\nif not self._logger.hasHandlers():\n# avoid keep adding handlers and therefore duplicate messages\nself._logger.addHandler(lsh)\n</code></pre>"},{"location":"reference/packages/utils/logging_handler.html#packages.utils.logging_handler.LoggingHandler.get_logger","title":"<code>get_logger()</code>","text":"<p>Get the _logger instance variable.</p> <p>Returns:</p> Type Description <code>logging.Logger</code> <p>logging.Logger: the logger object.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/logging_handler.py</code> <pre><code>def get_logger(self) -&gt; logging.Logger:\n\"\"\"Get the _logger instance variable.\n    Returns:\n        logging.Logger: the logger object.\n    \"\"\"\nreturn self._logger\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html","title":"Rest api","text":"<p>Module to handle REST API operations.</p>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.RESTApiException","title":"<code>RESTApiException</code>","text":"<p>         Bases: <code>requests.RequestException</code></p> <p>Class representing any possible REST API Exception.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>class RESTApiException(requests.RequestException):\n\"\"\"Class representing any possible REST API Exception.\"\"\"\ndef __init__(self, message: str) -&gt; None:\n\"\"\"Construct RESTApiException instances.\n        Args:\n            message: message to display on exception event.\n        \"\"\"\nsuper().__init__(message)\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.RESTApiException.__init__","title":"<code>__init__(message)</code>","text":"<p>Construct RESTApiException instances.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>message to display on exception event.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n\"\"\"Construct RESTApiException instances.\n    Args:\n        message: message to display on exception event.\n    \"\"\"\nsuper().__init__(message)\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.RestMethods","title":"<code>RestMethods</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Methods for REST API calls.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>class RestMethods(Enum):\n\"\"\"Methods for REST API calls.\"\"\"\nPOST = \"POST\"\nPUT = \"PUT\"\nALLOWED_METHODS = [\"POST\", \"PUT\"]\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.RestStatusCodes","title":"<code>RestStatusCodes</code>","text":"<p>         Bases: <code>Enum</code></p> <p>REST Status Code.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>class RestStatusCodes(Enum):\n\"\"\"REST Status Code.\"\"\"\nRETRY_STATUS_CODES = [429, 500, 502, 503, 504]\nOK_STATUS_CODES = [200]\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.execute_api_request","title":"<code>execute_api_request(method, url, headers=None, basic_auth_dict=None, json=None, files=None, sleep_seconds=0.2)</code>","text":"<p>Execute a REST API request.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>REST method (e.g., POST or PUT).</p> required <code>url</code> <code>str</code> <p>url of the api.</p> required <code>headers</code> <code>dict</code> <p>request headers.</p> <code>None</code> <code>basic_auth_dict</code> <code>dict</code> <p>basic http authentication details (e.g., {\"username\": \"x\", \"password\": \"y\"}).</p> <code>None</code> <code>json</code> <code>dict</code> <p>json payload to send in the request.</p> <code>None</code> <code>files</code> <code>dict</code> <p>files payload to send in the request.</p> <code>None</code> <code>sleep_seconds</code> <code>float</code> <p>for how many seconds to sleep to avoid error 429.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>requests.Response</code> <p>response from the HTTP request.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>def execute_api_request(\nmethod: str,\nurl: str,\nheaders: dict = None,\nbasic_auth_dict: dict = None,\njson: dict = None,\nfiles: dict = None,\nsleep_seconds: float = 0.2,\n) -&gt; requests.Response:\n\"\"\"Execute a REST API request.\n    Args:\n        method: REST method (e.g., POST or PUT).\n        url: url of the api.\n        headers: request headers.\n        basic_auth_dict: basic http authentication details\n            (e.g., {\"username\": \"x\", \"password\": \"y\"}).\n        json: json payload to send in the request.\n        files: files payload to send in the request.\n        sleep_seconds: for how many seconds to sleep to avoid error 429.\n    Returns:\n        response from the HTTP request.\n    \"\"\"\nbasic_auth: requests.auth.HTTPBasicAuth = None\nif basic_auth_dict:\nbasic_auth = get_basic_auth(\nbasic_auth_dict[\"username\"], basic_auth_dict[\"password\"]\n)\nreturn get_configured_session(sleep_seconds=sleep_seconds).request(\nmethod=method,\nurl=url,\nheaders=headers,\nauth=basic_auth,\njson=json,\nfiles=files,\n)\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.get_basic_auth","title":"<code>get_basic_auth(username, password)</code>","text":"<p>Get the basic authentication object to authenticate REST requests.</p> <p>Parameters:</p> Name Type Description Default <code>username</code> <code>str</code> <p>username.</p> required <code>password</code> <code>str</code> <p>password.</p> required <p>Returns:</p> Type Description <code>requests.auth.HTTPBasicAuth</code> <p>requests.auth.HTTPBasicAuth: the HTTPBasicAuth object.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>def get_basic_auth(username: str, password: str) -&gt; requests.auth.HTTPBasicAuth:\n\"\"\"Get the basic authentication object to authenticate REST requests.\n    Args:\n        username: username.\n        password: password.\n    Returns:\n        requests.auth.HTTPBasicAuth: the HTTPBasicAuth object.\n    \"\"\"\nreturn requests.auth.HTTPBasicAuth(username, password)\n</code></pre>"},{"location":"reference/packages/utils/rest_api.html#packages.utils.rest_api.get_configured_session","title":"<code>get_configured_session(sleep_seconds=0.2, total_retries=5, backoff_factor=2, retry_status_codes=None, allowed_methods=None, protocol='https://')</code>","text":"<p>Get a configured requests Session with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>sleep_seconds</code> <code>float</code> <p>seconds to sleep before each request to avoid rate limits.</p> <code>0.2</code> <code>total_retries</code> <code>int</code> <p>number of times to retry.</p> <code>5</code> <code>backoff_factor</code> <code>int</code> <p>factor for the exponential backoff.</p> <code>2</code> <code>retry_status_codes</code> <code>list</code> <p>list of status code that triggers a retry.</p> <code>None</code> <code>allowed_methods</code> <code>list</code> <p>http methods that are allowed for retry.</p> <code>None</code> <code>protocol</code> <code>str</code> <p>http:// or https://.</p> <code>'https://'</code> <p>Returns     requests.Session: the configured session.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/rest_api.py</code> <pre><code>def get_configured_session(\nsleep_seconds: float = 0.2,\ntotal_retries: int = 5,\nbackoff_factor: int = 2,\nretry_status_codes: list = None,\nallowed_methods: list = None,\nprotocol: str = \"https://\",\n) -&gt; requests.Session:\n\"\"\"Get a configured requests Session with exponential backoff.\n    Args:\n        sleep_seconds: seconds to sleep before each request to avoid rate limits.\n        total_retries: number of times to retry.\n        backoff_factor: factor for the exponential backoff.\n        retry_status_codes: list of status code that triggers a retry.\n        allowed_methods: http methods that are allowed for retry.\n        protocol: http:// or https://.\n    Returns\n        requests.Session: the configured session.\n    \"\"\"\nretry_status_codes = (\nretry_status_codes\nif retry_status_codes\nelse RestStatusCodes.RETRY_STATUS_CODES.value\n)\nallowed_methods = (\nallowed_methods if allowed_methods else RestMethods.ALLOWED_METHODS.value\n)\ntime.sleep(sleep_seconds)\nsession = requests.Session()\nretries = Retry(\ntotal=total_retries,\nbackoff_factor=backoff_factor,\nstatus_forcelist=retry_status_codes,\nallowed_methods=allowed_methods,\n)\nsession.mount(protocol, HTTPAdapter(max_retries=retries))\nreturn session\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html","title":"Schema utils","text":"<p>Utilities to facilitate dataframe schema management.</p>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils","title":"<code>SchemaUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Schema utils that help retrieve and manage schemas of dataframes.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>class SchemaUtils(object):\n\"\"\"Schema utils that help retrieve and manage schemas of dataframes.\"\"\"\n_logger: Logger = LoggingHandler(__name__).get_logger()\n@staticmethod\ndef from_file(file_path: str, disable_dbfs_retry: bool = False) -&gt; StructType:\n\"\"\"Get a spark schema from a file (spark StructType json file) in a file system.\n        Args:\n            file_path: path of the file in a file system. [Check here](\n                https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html).\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            Spark schema struct type.\n        \"\"\"\nreturn StructType.fromJson(\nFileStorageFunctions.read_json(file_path, disable_dbfs_retry)\n)\n@staticmethod\ndef from_file_to_dict(file_path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Get a dict with the spark schema from a file in a file system.\n        Args:\n            file_path: path of the file in a file system. [Check here](\n                https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html).\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n             Spark schema in a dict.\n        \"\"\"\nreturn FileStorageFunctions.read_json(file_path, disable_dbfs_retry)\n@staticmethod\ndef from_dict(struct_type: dict) -&gt; StructType:\n\"\"\"Get a spark schema from a dict.\n        Args:\n            struct_type: dict containing a spark schema structure. [Check here](\n                https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html).\n        Returns:\n             Spark schema struct type.\n        \"\"\"\nreturn StructType.fromJson(struct_type)\n@staticmethod\ndef from_table_schema(table: str) -&gt; StructType:\n\"\"\"Get a spark schema from a table.\n        Args:\n            table: table name from which to inherit the schema.\n        Returns:\n            Spark schema struct type.\n        \"\"\"\nreturn ExecEnv.SESSION.read.table(table).schema\n@classmethod\ndef from_input_spec(cls, input_spec: InputSpec) -&gt; Optional[StructType]:\n\"\"\"Get a spark schema from an input specification.\n        This covers scenarios where the schema is provided as part of the input\n        specification of the algorithm. Schema can come from the table specified in the\n        input specification (enforce_schema_from_table) or by the dict with the spark\n        schema provided there also.\n        Args:\n            input_spec: input specification.\n        Returns:\n            spark schema struct type.\n        \"\"\"\nif input_spec.enforce_schema_from_table:\ncls._logger.info(\nf\"Reading schema from table: {input_spec.enforce_schema_from_table}\"\n)\nreturn SchemaUtils.from_table_schema(input_spec.enforce_schema_from_table)\nelif input_spec.schema_path:\ncls._logger.info(f\"Reading schema from file: {input_spec.schema_path}\")\nreturn SchemaUtils.from_file(\ninput_spec.schema_path, input_spec.disable_dbfs_retry\n)\nelif input_spec.schema:\ncls._logger.info(\nf\"Reading schema from configuration file: {input_spec.schema}\"\n)\nreturn SchemaUtils.from_dict(input_spec.schema)\nelse:\ncls._logger.info(\"No schema was provided... skipping enforce schema\")\nreturn None\n@staticmethod\ndef _get_prefix_alias(num_chars: int, prefix: str, shorten_names: bool) -&gt; str:\n\"\"\"Get prefix alias for a field.\"\"\"\nreturn (\nf\"\"\"{'_'.join(\n[item[:num_chars] for item in prefix.split('.')]\n)}_\"\"\"\nif shorten_names\nelse f\"{prefix}_\".replace(\".\", \"_\")\n)\n@staticmethod\ndef schema_flattener(\nschema: StructType,\nprefix: str = None,\nlevel: int = 1,\nmax_level: int = None,\nshorten_names: bool = False,\nalias: bool = True,\nnum_chars: int = 7,\nignore_cols: List = None,\n) -&gt; List:\n\"\"\"Recursive method to flatten the schema of the dataframe.\n        Args:\n            schema: schema to be flattened.\n            prefix: prefix of the struct to get the value for. Only relevant\n                for being used in the internal recursive logic.\n            level: level of the depth in the schema being flattened. Only relevant\n                for being used in the internal recursive logic.\n            max_level: level until which you want to flatten the schema. Default: None.\n            shorten_names: whether to shorten the names of the prefixes of the fields\n                being flattened or not. Default: False.\n            alias: whether to define alias for the columns being flattened or\n                not. Default: True.\n            num_chars: number of characters to consider when shortening the names of\n                the fields. Default: 7.\n            ignore_cols: columns which you don't want to flatten. Default: None.\n        Returns:\n            A function to be called in .transform() spark function.\n        \"\"\"\ncols = []\nignore_cols = ignore_cols if ignore_cols else []\nfor field in schema.fields:\nname = prefix + \".\" + field.name if prefix else field.name\nfield_type = field.dataType\nif (\nisinstance(field_type, StructType)\nand name not in ignore_cols\nand (max_level is None or level &lt;= max_level)\n):\ncols += SchemaUtils.schema_flattener(\nschema=field_type,\nprefix=name,\nlevel=level + 1,\nmax_level=max_level,\nshorten_names=shorten_names,\nalias=alias,\nnum_chars=num_chars,\nignore_cols=ignore_cols,\n)\nelse:\nif alias and prefix:\nprefix_alias = SchemaUtils._get_prefix_alias(\nnum_chars, prefix, shorten_names\n)\ncols.append(col(name).alias(f\"{prefix_alias}{field.name}\"))\nelse:\ncols.append(col(name))\nreturn cols\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils.from_dict","title":"<code>from_dict(struct_type)</code>  <code>staticmethod</code>","text":"<p>Get a spark schema from a dict.</p> <p>Parameters:</p> Name Type Description Default <code>struct_type</code> <code>dict</code> <p>dict containing a spark schema structure. Check here.</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>Spark schema struct type.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>@staticmethod\ndef from_dict(struct_type: dict) -&gt; StructType:\n\"\"\"Get a spark schema from a dict.\n    Args:\n        struct_type: dict containing a spark schema structure. [Check here](\n            https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html).\n    Returns:\n         Spark schema struct type.\n    \"\"\"\nreturn StructType.fromJson(struct_type)\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils.from_file","title":"<code>from_file(file_path, disable_dbfs_retry=False)</code>  <code>staticmethod</code>","text":"<p>Get a spark schema from a file (spark StructType json file) in a file system.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path of the file in a file system. Check here.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>StructType</code> <p>Spark schema struct type.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>@staticmethod\ndef from_file(file_path: str, disable_dbfs_retry: bool = False) -&gt; StructType:\n\"\"\"Get a spark schema from a file (spark StructType json file) in a file system.\n    Args:\n        file_path: path of the file in a file system. [Check here](\n            https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html).\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        Spark schema struct type.\n    \"\"\"\nreturn StructType.fromJson(\nFileStorageFunctions.read_json(file_path, disable_dbfs_retry)\n)\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils.from_file_to_dict","title":"<code>from_file_to_dict(file_path, disable_dbfs_retry=False)</code>  <code>staticmethod</code>","text":"<p>Get a dict with the spark schema from a file in a file system.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path of the file in a file system. Check here.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Spark schema in a dict.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>@staticmethod\ndef from_file_to_dict(file_path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Get a dict with the spark schema from a file in a file system.\n    Args:\n        file_path: path of the file in a file system. [Check here](\n            https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html).\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n         Spark schema in a dict.\n    \"\"\"\nreturn FileStorageFunctions.read_json(file_path, disable_dbfs_retry)\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils.from_input_spec","title":"<code>from_input_spec(input_spec)</code>  <code>classmethod</code>","text":"<p>Get a spark schema from an input specification.</p> <p>This covers scenarios where the schema is provided as part of the input specification of the algorithm. Schema can come from the table specified in the input specification (enforce_schema_from_table) or by the dict with the spark schema provided there also.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>input specification.</p> required <p>Returns:</p> Type Description <code>Optional[StructType]</code> <p>spark schema struct type.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>@classmethod\ndef from_input_spec(cls, input_spec: InputSpec) -&gt; Optional[StructType]:\n\"\"\"Get a spark schema from an input specification.\n    This covers scenarios where the schema is provided as part of the input\n    specification of the algorithm. Schema can come from the table specified in the\n    input specification (enforce_schema_from_table) or by the dict with the spark\n    schema provided there also.\n    Args:\n        input_spec: input specification.\n    Returns:\n        spark schema struct type.\n    \"\"\"\nif input_spec.enforce_schema_from_table:\ncls._logger.info(\nf\"Reading schema from table: {input_spec.enforce_schema_from_table}\"\n)\nreturn SchemaUtils.from_table_schema(input_spec.enforce_schema_from_table)\nelif input_spec.schema_path:\ncls._logger.info(f\"Reading schema from file: {input_spec.schema_path}\")\nreturn SchemaUtils.from_file(\ninput_spec.schema_path, input_spec.disable_dbfs_retry\n)\nelif input_spec.schema:\ncls._logger.info(\nf\"Reading schema from configuration file: {input_spec.schema}\"\n)\nreturn SchemaUtils.from_dict(input_spec.schema)\nelse:\ncls._logger.info(\"No schema was provided... skipping enforce schema\")\nreturn None\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils.from_table_schema","title":"<code>from_table_schema(table)</code>  <code>staticmethod</code>","text":"<p>Get a spark schema from a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>table name from which to inherit the schema.</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>Spark schema struct type.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>@staticmethod\ndef from_table_schema(table: str) -&gt; StructType:\n\"\"\"Get a spark schema from a table.\n    Args:\n        table: table name from which to inherit the schema.\n    Returns:\n        Spark schema struct type.\n    \"\"\"\nreturn ExecEnv.SESSION.read.table(table).schema\n</code></pre>"},{"location":"reference/packages/utils/schema_utils.html#packages.utils.schema_utils.SchemaUtils.schema_flattener","title":"<code>schema_flattener(schema, prefix=None, level=1, max_level=None, shorten_names=False, alias=True, num_chars=7, ignore_cols=None)</code>  <code>staticmethod</code>","text":"<p>Recursive method to flatten the schema of the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>StructType</code> <p>schema to be flattened.</p> required <code>prefix</code> <code>str</code> <p>prefix of the struct to get the value for. Only relevant for being used in the internal recursive logic.</p> <code>None</code> <code>level</code> <code>int</code> <p>level of the depth in the schema being flattened. Only relevant for being used in the internal recursive logic.</p> <code>1</code> <code>max_level</code> <code>int</code> <p>level until which you want to flatten the schema. Default: None.</p> <code>None</code> <code>shorten_names</code> <code>bool</code> <p>whether to shorten the names of the prefixes of the fields being flattened or not. Default: False.</p> <code>False</code> <code>alias</code> <code>bool</code> <p>whether to define alias for the columns being flattened or not. Default: True.</p> <code>True</code> <code>num_chars</code> <code>int</code> <p>number of characters to consider when shortening the names of the fields. Default: 7.</p> <code>7</code> <code>ignore_cols</code> <code>List</code> <p>columns which you don't want to flatten. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List</code> <p>A function to be called in .transform() spark function.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/schema_utils.py</code> <pre><code>@staticmethod\ndef schema_flattener(\nschema: StructType,\nprefix: str = None,\nlevel: int = 1,\nmax_level: int = None,\nshorten_names: bool = False,\nalias: bool = True,\nnum_chars: int = 7,\nignore_cols: List = None,\n) -&gt; List:\n\"\"\"Recursive method to flatten the schema of the dataframe.\n    Args:\n        schema: schema to be flattened.\n        prefix: prefix of the struct to get the value for. Only relevant\n            for being used in the internal recursive logic.\n        level: level of the depth in the schema being flattened. Only relevant\n            for being used in the internal recursive logic.\n        max_level: level until which you want to flatten the schema. Default: None.\n        shorten_names: whether to shorten the names of the prefixes of the fields\n            being flattened or not. Default: False.\n        alias: whether to define alias for the columns being flattened or\n            not. Default: True.\n        num_chars: number of characters to consider when shortening the names of\n            the fields. Default: 7.\n        ignore_cols: columns which you don't want to flatten. Default: None.\n    Returns:\n        A function to be called in .transform() spark function.\n    \"\"\"\ncols = []\nignore_cols = ignore_cols if ignore_cols else []\nfor field in schema.fields:\nname = prefix + \".\" + field.name if prefix else field.name\nfield_type = field.dataType\nif (\nisinstance(field_type, StructType)\nand name not in ignore_cols\nand (max_level is None or level &lt;= max_level)\n):\ncols += SchemaUtils.schema_flattener(\nschema=field_type,\nprefix=name,\nlevel=level + 1,\nmax_level=max_level,\nshorten_names=shorten_names,\nalias=alias,\nnum_chars=num_chars,\nignore_cols=ignore_cols,\n)\nelse:\nif alias and prefix:\nprefix_alias = SchemaUtils._get_prefix_alias(\nnum_chars, prefix, shorten_names\n)\ncols.append(col(name).alias(f\"{prefix_alias}{field.name}\"))\nelse:\ncols.append(col(name))\nreturn cols\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html","title":"Sharepoint utils","text":"<p>Utilities for sharepoint API operations.</p>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils","title":"<code>SharepointUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Class with methods to connect and extract data from SharePoint.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>class SharepointUtils(object):\n\"\"\"Class with methods to connect and extract data from SharePoint.\"\"\"\ndef __init__(\nself,\nclient_id: str,\ntenant_id: str,\nlocal_path: str,\napi_version: str,\nsite_name: str,\ndrive_name: str,\nfile_name: str,\nsecret: str,\nfolder_relative_path: str = None,\nchunk_size: int = 100,\nlocal_options: dict = None,\nconflict_behaviour: str = \"replace\",\n):\n\"\"\"Instantiate objects of the SharepointUtils class.\n        Args:\n            client_id: application (client) ID of your Azure AD app.\n            tenant_id: tenant ID (directory ID) from Azure AD for authentication.\n            local_path: local directory path (Volume) where the files are temporarily\n            stored.\n            api_version: Graph API version to use.\n            site_name: name of the SharePoint site where the files are stored.\n            drive_name: name of the document library or drive in SharePoint.\n            file_name: name of the file to be stored in sharepoint.\n            secret: client secret for authentication.\n            folder_relative_path: optional; relative path within the\n            drive(drive_name) where the file will be stored.\n            chunk_size: Optional; size of file chunks to be uploaded in\n            bytes (default is 100 bytes).\n            local_options: Optional; additional options for customizing write\n            action to local path.\n            conflict_behaviour: Optional; defines how conflicts in file uploads are\n            handled('replace', 'fail', etc.).\n        Returns:\n            A SharepointUtils object.\n        \"\"\"\nself.client_id = client_id\nself.tenant_id = tenant_id\nself.local_path = local_path\nself.api_version = api_version\nself.site_name = site_name\nself.drive_name = drive_name\nself.file_name = file_name\nself.secret = secret\nself.folder_relative_path = folder_relative_path\nself.chunk_size = chunk_size\nself.local_options = local_options\nself.conflict_behaviour = conflict_behaviour\nself.token = None\nself._create_app()\ndef _get_token(self) -&gt; None:\n\"\"\"Fetch and store a valid access token for SharePoint API.\"\"\"\ntry:\nself.token = self.app.acquire_token_for_client(\nscopes=[f\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}/.default\"]\n)\nexcept Exception as err:\n_logger.error(f\"Token acquisition error: {err}\")\ndef _create_app(self) -&gt; None:\n\"\"\"Create an MSAL (Microsoft Authentication Library) instance.\n        This is used to handle authentication and authorization with Azure AD.\n        \"\"\"\nself.app = msal.ConfidentialClientApplication(\nclient_id=self.client_id,\nauthority=f\"{ExecEnv.ENGINE_CONFIG.sharepoint_authority}/{self.tenant_id}\",\nclient_credential=self.secret,\n)\nself._get_token()\n@retry(\nstop=stop_after_attempt(5),\nwait=wait_exponential(multiplier=30, min=30, max=150),\nretry=retry_if_exception_type(\n(RequestException, SharePointAPIError)\n),  # Retry on these exceptions\n)\ndef _make_request(\nself,\nendpoint: str,\nmethod: str = \"GET\",\nheaders: dict = None,\njson_options: dict = None,\ndata: object = None,\n) -&gt; requests.Response:\n\"\"\"Execute API requests to Microsoft Graph API.\n        !!! note\n            If you try to upload large files sequentially,you may encounter\n            a 503 \"serviceNotAvailable\" error. To mitigate this, consider using\n            coalesce in the Acon transform specification. However, be aware that\n            increasing the number of partitions also increases the likelihood of\n            server throttling\n        Args:\n            endpoint: The API endpoint to call.\n            headers: A dictionary containing the necessary headers.\n            json_options: Optional; JSON data to include in the request body.\n            method: The HTTP method to use ('GET', 'POST', 'PUT', etc.).\n            data: Optional; additional data (e.g., file content) on request body.\n        Returns:\n            A Response object from the request library.\n        Raises:\n            SharePointAPIError: If there is an issue with the SharePoint\n            API request.\n        \"\"\"\nself._get_token()\n# Required to avoid cicd issue\nif not self.token or \"access_token\" not in self.token:\nraise SharePointAPIError(\"Authentication token is missing or invalid.\")\ntry:\nif \"access_token\" in self.token:\nresponse = requests.request(\nmethod=method,\nurl=endpoint,\nheaders=(\nheaders\nif headers\nelse {\"Authorization\": \"Bearer \" + self.token[\"access_token\"]}\n),\njson=json_options,\ndata=data,\n)\nreturn response\nexcept RequestException as error:\nraise SharePointAPIError(f\"{error}\")\ndef _get_site_id(self, site_name: str) -&gt; str:\n\"\"\"Get the site ID from the site name.\n        Args:\n            site_name: The name of the SharePoint site.\n        Returns:\n            The site ID as a string.\n        \"\"\"\nendpoint = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}/{self.api_version}\"\nf\"/sites/{ExecEnv.ENGINE_CONFIG.sharepoint_company_domain}:/\"\nf\"sites/{site_name}\"\n)\nresponse = self._make_request(endpoint=endpoint).json()\ntry:\nif not isinstance(response[\"id\"], str):\nraise TypeError(\"site_id must be a string\")\nreturn response[\"id\"]\nexcept RequestException as error:\nraise SharePointAPIError(f\"{error}\")\ndef _get_drive_id(self, site_name: str, drive_name: str) -&gt; str:\n\"\"\"Get the drive ID from the site name and drive name.\n        Args:\n            site_name: The name of the SharePoint site.\n            drive_name: The name of the drive or document library.\n        Returns:\n            The drive ID as a string.\n        Raises:\n            ValueError: If the drive is not found.\n        \"\"\"\nsite_id = self._get_site_id(site_name)\nendpoint = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}/{self.api_version}\"\nf\"/sites/{site_id}/drives\"\n)\ntry:\nresponse = self._make_request(endpoint=endpoint).json()\nfor drive in response[\"value\"]:\nif drive_name in drive[\"name\"] and isinstance(drive[\"name\"], str):\nreturn str(drive[\"id\"])\nraise ValueError(f\"Requested drive '{drive_name}' could not be found\")\nexcept RequestException as error:\nraise SharePointAPIError(f\"{error}\")\ndef _rename_local_file(self, local_path: str, file_name: str) -&gt; None:\n\"\"\"Rename a local file that starts with 'part-' to the desired file name.\n        Args:\n            local_path: The directory where the file is located.\n            file_name: The new file name for the local file.\n        \"\"\"\nfiles_in_dir = os.listdir(local_path)\npart_file = [f for f in files_in_dir if f.startswith(\"part-\")][0]\ntry:\nos.rename(\nos.path.join(local_path, part_file), os.path.join(local_path, file_name)\n)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\ndef check_if_endpoint_exists(\nself, site_name: str, drive_name: str, folder_root_path: str = None\n) -&gt; bool:\n\"\"\"Check if a specified endpoint exists in SharePoint.\n        This method checks whether a specific endpoint exists within a SharePoint site.\n        If `folder_root_path` is provided, the method checks for the existence of that\n        specific folder within the drive.\n        Args:\n            site_name: Name of the SharePoint site.\n            drive_name: Name of the SharePoint drive (document library)\n            to search within.\n            folder_root_path: Optional; the relative path of the folder within\n            the drive. If not provided, the existence of the drive is checked.\n        Returns:\n            True if the file or folder exists, False otherwise.\n        Raises:\n            SharePointAPIError: If there's an issue with SharePoint API request.\n        \"\"\"\ntry:\nself._get_site_id(site_name)\ndrive_id = self._get_drive_id(site_name, drive_name)\nif folder_root_path:\nquery_body = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}\"\nf\"/{self.api_version}/drives/{drive_id}\"\nf\"/items/root:/{folder_root_path}\"\n)\nself._make_request(endpoint=query_body)\nreturn True\nexcept RequestException as error:\nraise SharePointAPIError(f\"{error}\")\ndef check_if_local_path_exists(self, local_path: str) -&gt; None:\n\"\"\"Check if a specified local path exists.\n        This method checks whether a specific local path exists.\n        Args:\n            local_path: Local path (Volume) where the files are temporarily stored.\n        Returns:\n            True if the folder exists, False otherwise.\n        Raises:\n            IOError: If there is an error when reading from the local path.\n        \"\"\"\ntry:\nos.listdir(local_path)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\ndef write_to_local_path(self, df: DataFrame) -&gt; None:\n\"\"\"Write a Spark DataFrame to a local path (Volume) in CSV format.\n        This method writes the provided Spark DataFrame to a specified local directory,\n        saving it in CSV format. The method renames the output file from its default\n        \"part-*\" naming convention to a specified file name.\n        The dictionary local_options enables the customisation of the write action.\n        The customizable options can be found here:\n        https://spark.apache.org/docs/3.5.1/sql-data-sources-csv.html.\n        Args:\n            df: The Spark DataFrame to write to the local file system.\n        Returns:\n            None.\n        Raises:\n            IOError: If there is an issue during the file writing process.\n        \"\"\"\ntry:\ndf.coalesce(1).write.save(\npath=self.local_path,\nformat=\"csv\",\n**self.local_options if self.local_options else {},\n)\nself._rename_local_file(self.local_path, self.file_name)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\ndef delete_local_path(self) -&gt; None:\n\"\"\"Delete a temporary folder.\n        This method deletes all the files in a given directory.\n        Returns:\n            None.\n        Raises:\n            IOError: If there is an issue during the file writing process.\n        \"\"\"\ntry:\nshutil.rmtree(self.local_path)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\ndef write_to_sharepoint(self) -&gt; None:\n\"\"\"Upload a local file to SharePoint in chunks using the Microsoft Graph API.\n        This method creates an upload session and uploads a local CSV file to a\n        SharePoint document library.\n        The file is divided into chunks (based on the `chunk_size` specified)\n        to handle large file uploads and send sequentially using the upload URL\n        returned from the Graph API.\n        The method uses instance attributes such as `api_domain`, `api_version`,\n        `site_name`, `drive_name`, `folder_relative_path`, and `file_name` to\n        construct the necessary API calls and upload the file to the specified\n        location in SharePoint.\n        Returns:\n            None.\n        Raises:\n            APIError: If an error occurs during any stage of the upload\n            (e.g., failure to create upload session,issues during chunk upload).\n        \"\"\"\ndrive_id = self._get_drive_id(\nsite_name=self.site_name, drive_name=self.drive_name\n)\nif self.folder_relative_path:\nendpoint = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}\"\nf\"/{self.api_version}/drives/{drive_id}/items/root:\"\nf\"/{self.folder_relative_path}/{self.file_name}.csv:\"\nf\"/createUploadSession\"\n)\nelse:\nendpoint = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}\"\nf\"/{self.api_version}/drives/{drive_id}/items/root:\"\nf\"/{self.file_name}.csv:/createUploadSession\"\n)\nresponse = self._make_request(method=\"POST\", endpoint=endpoint)\nresponse.raise_for_status()\nupload_session = response.json()\nupload_url = upload_session[\"uploadUrl\"]\nupload_file = f\"{self.local_path}{self.file_name}\"\nstat = os.stat(upload_file)\nsize = stat.st_size\nwith open(upload_file, \"rb\") as data:\nstart = 0\nwhile start &lt; size:\nchunk = data.read(self.chunk_size)\nbytes_read = len(chunk)\nupload_range = f\"bytes {start}-{start + bytes_read - 1}/{size}\"\nheaders = {\n\"Content-Length\": str(bytes_read),\n\"Content-Range\": upload_range,\n}\nresponse = self._make_request(\nmethod=\"PUT\", endpoint=upload_url, headers=headers, data=chunk\n)\nresponse.raise_for_status()\nstart += bytes_read\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils.__init__","title":"<code>__init__(client_id, tenant_id, local_path, api_version, site_name, drive_name, file_name, secret, folder_relative_path=None, chunk_size=100, local_options=None, conflict_behaviour='replace')</code>","text":"<p>Instantiate objects of the SharepointUtils class.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>str</code> <p>application (client) ID of your Azure AD app.</p> required <code>tenant_id</code> <code>str</code> <p>tenant ID (directory ID) from Azure AD for authentication.</p> required <code>local_path</code> <code>str</code> <p>local directory path (Volume) where the files are temporarily</p> required <code>api_version</code> <code>str</code> <p>Graph API version to use.</p> required <code>site_name</code> <code>str</code> <p>name of the SharePoint site where the files are stored.</p> required <code>drive_name</code> <code>str</code> <p>name of the document library or drive in SharePoint.</p> required <code>file_name</code> <code>str</code> <p>name of the file to be stored in sharepoint.</p> required <code>secret</code> <code>str</code> <p>client secret for authentication.</p> required <code>folder_relative_path</code> <code>str</code> <p>optional; relative path within the</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Optional; size of file chunks to be uploaded in</p> <code>100</code> <code>local_options</code> <code>dict</code> <p>Optional; additional options for customizing write</p> <code>None</code> <code>conflict_behaviour</code> <code>str</code> <p>Optional; defines how conflicts in file uploads are</p> <code>'replace'</code> <p>Returns:</p> Type Description <p>A SharepointUtils object.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>def __init__(\nself,\nclient_id: str,\ntenant_id: str,\nlocal_path: str,\napi_version: str,\nsite_name: str,\ndrive_name: str,\nfile_name: str,\nsecret: str,\nfolder_relative_path: str = None,\nchunk_size: int = 100,\nlocal_options: dict = None,\nconflict_behaviour: str = \"replace\",\n):\n\"\"\"Instantiate objects of the SharepointUtils class.\n    Args:\n        client_id: application (client) ID of your Azure AD app.\n        tenant_id: tenant ID (directory ID) from Azure AD for authentication.\n        local_path: local directory path (Volume) where the files are temporarily\n        stored.\n        api_version: Graph API version to use.\n        site_name: name of the SharePoint site where the files are stored.\n        drive_name: name of the document library or drive in SharePoint.\n        file_name: name of the file to be stored in sharepoint.\n        secret: client secret for authentication.\n        folder_relative_path: optional; relative path within the\n        drive(drive_name) where the file will be stored.\n        chunk_size: Optional; size of file chunks to be uploaded in\n        bytes (default is 100 bytes).\n        local_options: Optional; additional options for customizing write\n        action to local path.\n        conflict_behaviour: Optional; defines how conflicts in file uploads are\n        handled('replace', 'fail', etc.).\n    Returns:\n        A SharepointUtils object.\n    \"\"\"\nself.client_id = client_id\nself.tenant_id = tenant_id\nself.local_path = local_path\nself.api_version = api_version\nself.site_name = site_name\nself.drive_name = drive_name\nself.file_name = file_name\nself.secret = secret\nself.folder_relative_path = folder_relative_path\nself.chunk_size = chunk_size\nself.local_options = local_options\nself.conflict_behaviour = conflict_behaviour\nself.token = None\nself._create_app()\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils.check_if_endpoint_exists","title":"<code>check_if_endpoint_exists(site_name, drive_name, folder_root_path=None)</code>","text":"<p>Check if a specified endpoint exists in SharePoint.</p> <p>This method checks whether a specific endpoint exists within a SharePoint site. If <code>folder_root_path</code> is provided, the method checks for the existence of that specific folder within the drive.</p> <p>Parameters:</p> Name Type Description Default <code>site_name</code> <code>str</code> <p>Name of the SharePoint site.</p> required <code>drive_name</code> <code>str</code> <p>Name of the SharePoint drive (document library)</p> required <code>folder_root_path</code> <code>str</code> <p>Optional; the relative path of the folder within</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file or folder exists, False otherwise.</p> <p>Raises:</p> Type Description <code>SharePointAPIError</code> <p>If there's an issue with SharePoint API request.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>def check_if_endpoint_exists(\nself, site_name: str, drive_name: str, folder_root_path: str = None\n) -&gt; bool:\n\"\"\"Check if a specified endpoint exists in SharePoint.\n    This method checks whether a specific endpoint exists within a SharePoint site.\n    If `folder_root_path` is provided, the method checks for the existence of that\n    specific folder within the drive.\n    Args:\n        site_name: Name of the SharePoint site.\n        drive_name: Name of the SharePoint drive (document library)\n        to search within.\n        folder_root_path: Optional; the relative path of the folder within\n        the drive. If not provided, the existence of the drive is checked.\n    Returns:\n        True if the file or folder exists, False otherwise.\n    Raises:\n        SharePointAPIError: If there's an issue with SharePoint API request.\n    \"\"\"\ntry:\nself._get_site_id(site_name)\ndrive_id = self._get_drive_id(site_name, drive_name)\nif folder_root_path:\nquery_body = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}\"\nf\"/{self.api_version}/drives/{drive_id}\"\nf\"/items/root:/{folder_root_path}\"\n)\nself._make_request(endpoint=query_body)\nreturn True\nexcept RequestException as error:\nraise SharePointAPIError(f\"{error}\")\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils.check_if_local_path_exists","title":"<code>check_if_local_path_exists(local_path)</code>","text":"<p>Check if a specified local path exists.</p> <p>This method checks whether a specific local path exists.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>str</code> <p>Local path (Volume) where the files are temporarily stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>True if the folder exists, False otherwise.</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If there is an error when reading from the local path.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>def check_if_local_path_exists(self, local_path: str) -&gt; None:\n\"\"\"Check if a specified local path exists.\n    This method checks whether a specific local path exists.\n    Args:\n        local_path: Local path (Volume) where the files are temporarily stored.\n    Returns:\n        True if the folder exists, False otherwise.\n    Raises:\n        IOError: If there is an error when reading from the local path.\n    \"\"\"\ntry:\nos.listdir(local_path)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils.delete_local_path","title":"<code>delete_local_path()</code>","text":"<p>Delete a temporary folder.</p> <p>This method deletes all the files in a given directory.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If there is an issue during the file writing process.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>def delete_local_path(self) -&gt; None:\n\"\"\"Delete a temporary folder.\n    This method deletes all the files in a given directory.\n    Returns:\n        None.\n    Raises:\n        IOError: If there is an issue during the file writing process.\n    \"\"\"\ntry:\nshutil.rmtree(self.local_path)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils.write_to_local_path","title":"<code>write_to_local_path(df)</code>","text":"<p>Write a Spark DataFrame to a local path (Volume) in CSV format.</p> <p>This method writes the provided Spark DataFrame to a specified local directory, saving it in CSV format. The method renames the output file from its default \"part-*\" naming convention to a specified file name. The dictionary local_options enables the customisation of the write action. The customizable options can be found here: https://spark.apache.org/docs/3.5.1/sql-data-sources-csv.html.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Spark DataFrame to write to the local file system.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If there is an issue during the file writing process.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>def write_to_local_path(self, df: DataFrame) -&gt; None:\n\"\"\"Write a Spark DataFrame to a local path (Volume) in CSV format.\n    This method writes the provided Spark DataFrame to a specified local directory,\n    saving it in CSV format. The method renames the output file from its default\n    \"part-*\" naming convention to a specified file name.\n    The dictionary local_options enables the customisation of the write action.\n    The customizable options can be found here:\n    https://spark.apache.org/docs/3.5.1/sql-data-sources-csv.html.\n    Args:\n        df: The Spark DataFrame to write to the local file system.\n    Returns:\n        None.\n    Raises:\n        IOError: If there is an issue during the file writing process.\n    \"\"\"\ntry:\ndf.coalesce(1).write.save(\npath=self.local_path,\nformat=\"csv\",\n**self.local_options if self.local_options else {},\n)\nself._rename_local_file(self.local_path, self.file_name)\nexcept IOError as error:\nraise SharePointAPIError(f\"{error}\")\n</code></pre>"},{"location":"reference/packages/utils/sharepoint_utils.html#packages.utils.sharepoint_utils.SharepointUtils.write_to_sharepoint","title":"<code>write_to_sharepoint()</code>","text":"<p>Upload a local file to SharePoint in chunks using the Microsoft Graph API.</p> <p>This method creates an upload session and uploads a local CSV file to a SharePoint document library. The file is divided into chunks (based on the <code>chunk_size</code> specified) to handle large file uploads and send sequentially using the upload URL returned from the Graph API.</p> <p>The method uses instance attributes such as <code>api_domain</code>, <code>api_version</code>, <code>site_name</code>, <code>drive_name</code>, <code>folder_relative_path</code>, and <code>file_name</code> to construct the necessary API calls and upload the file to the specified location in SharePoint.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>APIError</code> <p>If an error occurs during any stage of the upload</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sharepoint_utils.py</code> <pre><code>def write_to_sharepoint(self) -&gt; None:\n\"\"\"Upload a local file to SharePoint in chunks using the Microsoft Graph API.\n    This method creates an upload session and uploads a local CSV file to a\n    SharePoint document library.\n    The file is divided into chunks (based on the `chunk_size` specified)\n    to handle large file uploads and send sequentially using the upload URL\n    returned from the Graph API.\n    The method uses instance attributes such as `api_domain`, `api_version`,\n    `site_name`, `drive_name`, `folder_relative_path`, and `file_name` to\n    construct the necessary API calls and upload the file to the specified\n    location in SharePoint.\n    Returns:\n        None.\n    Raises:\n        APIError: If an error occurs during any stage of the upload\n        (e.g., failure to create upload session,issues during chunk upload).\n    \"\"\"\ndrive_id = self._get_drive_id(\nsite_name=self.site_name, drive_name=self.drive_name\n)\nif self.folder_relative_path:\nendpoint = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}\"\nf\"/{self.api_version}/drives/{drive_id}/items/root:\"\nf\"/{self.folder_relative_path}/{self.file_name}.csv:\"\nf\"/createUploadSession\"\n)\nelse:\nendpoint = (\nf\"{ExecEnv.ENGINE_CONFIG.sharepoint_api_domain}\"\nf\"/{self.api_version}/drives/{drive_id}/items/root:\"\nf\"/{self.file_name}.csv:/createUploadSession\"\n)\nresponse = self._make_request(method=\"POST\", endpoint=endpoint)\nresponse.raise_for_status()\nupload_session = response.json()\nupload_url = upload_session[\"uploadUrl\"]\nupload_file = f\"{self.local_path}{self.file_name}\"\nstat = os.stat(upload_file)\nsize = stat.st_size\nwith open(upload_file, \"rb\") as data:\nstart = 0\nwhile start &lt; size:\nchunk = data.read(self.chunk_size)\nbytes_read = len(chunk)\nupload_range = f\"bytes {start}-{start + bytes_read - 1}/{size}\"\nheaders = {\n\"Content-Length\": str(bytes_read),\n\"Content-Range\": upload_range,\n}\nresponse = self._make_request(\nmethod=\"PUT\", endpoint=upload_url, headers=headers, data=chunk\n)\nresponse.raise_for_status()\nstart += bytes_read\n</code></pre>"},{"location":"reference/packages/utils/sql_parser_utils.html","title":"Sql parser utils","text":"<p>Module to parse sql files.</p>"},{"location":"reference/packages/utils/sql_parser_utils.html#packages.utils.sql_parser_utils.SQLParserUtils","title":"<code>SQLParserUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Parser utilities class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sql_parser_utils.py</code> <pre><code>class SQLParserUtils(object):\n\"\"\"Parser utilities class.\"\"\"\ndef split_sql_commands(\nself,\nsql_commands: str,\ndelimiter: str,\nadvanced_parser: bool,\n) -&gt; list[str]:\n\"\"\"Read the sql commands of a file to choose how to split them.\n        Args:\n            sql_commands: commands to be split.\n            delimiter: delimiter to split the sql commands.\n            advanced_parser: boolean to define if we need to use a complex split.\n        Returns:\n            List with the sql commands.\n        \"\"\"\nif advanced_parser:\nself.sql_commands: str = sql_commands\nself.delimiter: str = delimiter\nself.separated_sql_commands: list[str] = []\nself.split_index: int = 0\nreturn self._split_sql_commands()\nelse:\nreturn sql_commands.split(delimiter)\ndef _split_sql_commands(self) -&gt; list[str]:\n\"\"\"Read the sql commands of a file to split them based on a delimiter.\n        Returns:\n            List with the sql commands.\n        \"\"\"\nsingle_quotes: int = 0\ndouble_quotes: int = 0\none_line_comment: int = 0\nmultiple_line_comment: int = 0\nfor index, char in enumerate(self.sql_commands):\nif char == SQLParser.SINGLE_QUOTES.value and self._character_validation(\nvalue=[double_quotes, one_line_comment, multiple_line_comment]\n):\nsingle_quotes = self._update_value(\nvalue=single_quotes,\ncondition=self._character_validation(\nvalue=self._get_substring(first_char=index - 1, last_char=index)\n),\noperation=\"+-\",\n)\nelif char == SQLParser.DOUBLE_QUOTES.value and self._character_validation(\nvalue=[single_quotes, one_line_comment, multiple_line_comment]\n):\ndouble_quotes = self._update_value(\nvalue=double_quotes,\ncondition=self._character_validation(\nvalue=self._get_substring(first_char=index - 1, last_char=index)\n),\noperation=\"+-\",\n)\nelif char == SQLParser.SINGLE_TRACE.value and self._character_validation(\nvalue=[double_quotes, single_quotes, multiple_line_comment]\n):\none_line_comment = self._update_value(\nvalue=one_line_comment,\ncondition=(\nself._get_substring(first_char=index, last_char=index + 2)\n== SQLParser.DOUBLE_TRACES.value\n),\noperation=\"+\",\n)\nelif (\nchar == SQLParser.SLASH.value or char == SQLParser.STAR.value\n) and self._character_validation(\nvalue=[double_quotes, single_quotes, one_line_comment]\n):\nmultiple_line_comment = self._update_value(\nvalue=multiple_line_comment,\ncondition=self._get_substring(first_char=index, last_char=index + 2)\nin SQLParser.MULTIPLE_LINE_COMMENT.value,\noperation=\"+-\",\n)\none_line_comment = self._update_value(\nvalue=one_line_comment,\ncondition=char == SQLParser.PARAGRAPH.value,\noperation=\"-\",\n)\nself._validate_command_is_closed(\nindex=index,\ndependencies=self._character_validation(\nvalue=[\nsingle_quotes,\ndouble_quotes,\none_line_comment,\nmultiple_line_comment,\n]\n),\n)\nreturn self.separated_sql_commands\ndef _get_substring(self, first_char: int = None, last_char: int = None) -&gt; str:\n\"\"\"Get the substring based on the indexes passed as arguments.\n        Args:\n            first_char: represents the first index of the string.\n            last_char: represents the last index of the string.\n        Returns:\n            The substring based on the indexes passed as arguments.\n        \"\"\"\nreturn self.sql_commands[first_char:last_char]\ndef _validate_command_is_closed(self, index: int, dependencies: int) -&gt; None:\n\"\"\"Validate based on the delimiter if we have the closing of a sql command.\n        Args:\n            index: index of the character in a string.\n            dependencies: represents an int to validate if we are outside of quotes,...\n        \"\"\"\nif (\nself._get_substring(first_char=index, last_char=index + len(self.delimiter))\n== self.delimiter\nand dependencies\n):\nself._add_new_command(\nsql_command=self._get_substring(\nfirst_char=self.split_index, last_char=index\n)\n)\nself.split_index = index + len(self.delimiter)\nif self._get_substring(\nfirst_char=index, last_char=index + len(self.delimiter)\n) != self.delimiter and index + len(self.delimiter) == len(self.sql_commands):\nself._add_new_command(\nsql_command=self._get_substring(\nfirst_char=self.split_index, last_char=len(self.sql_commands)\n)\n)\ndef _character_validation(self, value: Union[str, list]) -&gt; bool:\n\"\"\"Validate if character is the opening/closing/inside of a comment.\n        Args:\n            value: represent the value associated to different validated\n            types or a character to be analyzed.\n        Returns:\n            Boolean that indicates if character found is the opening\n            or closing of a comment, is inside of quotes, comments,...\n        \"\"\"\nif value.__class__.__name__ == \"list\":\nreturn sum(value) == 0\nelse:\nreturn value != SQLParser.BACKSLASH.value\ndef _add_new_command(self, sql_command: str) -&gt; None:\n\"\"\"Add a newly found command to list of sql commands to execute.\n        Args:\n            sql_command: command to be added to list.\n        \"\"\"\nself.separated_sql_commands.append(str(sql_command))\ndef _update_value(self, value: int, operation: str, condition: bool = False) -&gt; int:\n\"\"\"Update value associated to different types of comments or quotes.\n        Args:\n            value: value to be updated\n            operation: operation that we want to perform on the value.\n            condition: validate if we have a condition associated to the value.\n        Returns:\n            A integer that represents the updated value.\n        \"\"\"\nif condition and operation == \"+-\":\nvalue = value + 1 if value == 0 else value - 1\nelif condition and operation == \"+\":\nvalue = value + 1 if value == 0 else value\nelif condition and operation == \"-\":\nvalue = value - 1 if value == 1 else value\nreturn value\n</code></pre>"},{"location":"reference/packages/utils/sql_parser_utils.html#packages.utils.sql_parser_utils.SQLParserUtils.split_sql_commands","title":"<code>split_sql_commands(sql_commands, delimiter, advanced_parser)</code>","text":"<p>Read the sql commands of a file to choose how to split them.</p> <p>Parameters:</p> Name Type Description Default <code>sql_commands</code> <code>str</code> <p>commands to be split.</p> required <code>delimiter</code> <code>str</code> <p>delimiter to split the sql commands.</p> required <code>advanced_parser</code> <code>bool</code> <p>boolean to define if we need to use a complex split.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List with the sql commands.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/sql_parser_utils.py</code> <pre><code>def split_sql_commands(\nself,\nsql_commands: str,\ndelimiter: str,\nadvanced_parser: bool,\n) -&gt; list[str]:\n\"\"\"Read the sql commands of a file to choose how to split them.\n    Args:\n        sql_commands: commands to be split.\n        delimiter: delimiter to split the sql commands.\n        advanced_parser: boolean to define if we need to use a complex split.\n    Returns:\n        List with the sql commands.\n    \"\"\"\nif advanced_parser:\nself.sql_commands: str = sql_commands\nself.delimiter: str = delimiter\nself.separated_sql_commands: list[str] = []\nself.split_index: int = 0\nreturn self._split_sql_commands()\nelse:\nreturn sql_commands.split(delimiter)\n</code></pre>"},{"location":"reference/packages/utils/configs/index.html","title":"Configs","text":"<p>Config utilities package.</p>"},{"location":"reference/packages/utils/configs/config_utils.html","title":"Config utils","text":"<p>Module to read configurations.</p>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils","title":"<code>ConfigUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Config utilities class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>class ConfigUtils(object):\n\"\"\"Config utilities class.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\nSENSITIVE_INFO = [\n\"kafka.ssl.keystore.password\",\n\"kafka.ssl.truststore.password\",\n\"password\",\n\"secret\",\n\"credential\",\n\"credentials\",\n\"pass\",\n\"key\",\n]\n@classmethod\ndef get_acon(\ncls,\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ndisable_dbfs_retry: bool = False,\n) -&gt; dict:\n\"\"\"Get acon based on a filesystem path or on a dict.\n        Args:\n            acon_path: path of the acon (algorithm configuration) file.\n            acon: acon provided directly through python code (e.g., notebooks\n                or other apps).\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            Dict representation of an acon.\n        \"\"\"\nacon = (\nacon if acon else ConfigUtils.read_json_acon(acon_path, disable_dbfs_retry)\n)\nreturn acon\n@staticmethod\ndef get_config(package: str = \"lakehouse_engine.configs\") -&gt; Any:\n\"\"\"Get the lakehouse engine configuration file.\n        Returns:\n            Configuration dictionary\n        \"\"\"\nwith importlib.resources.open_binary(package, \"engine.yaml\") as config:\nconfig = yaml.safe_load(config)\nreturn config\n@staticmethod\ndef get_config_from_file(config_file_path: str) -&gt; Any:\n\"\"\"Get the lakehouse engine configurations using a file path.\n         Args:\n            config_file_path: a string with a path for a yaml file\n            with custom configurations.\n        Returns:\n            Configuration dictionary\n        \"\"\"\nwith open(config_file_path, \"r\") as config:\nconfig = yaml.safe_load(config)\nreturn config\n@classmethod\ndef get_engine_version(cls) -&gt; str:\n\"\"\"Get Lakehouse Engine version from the installed packages.\n        Returns:\n            String of engine version.\n        \"\"\"\ntry:\nversion = pkg_resources.get_distribution(\"lakehouse-engine\").version\nexcept pkg_resources.DistributionNotFound:\ncls._LOGGER.info(\"Could not identify Lakehouse Engine version.\")\nversion = \"\"\nreturn str(version)\n@staticmethod\ndef read_json_acon(path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read an acon (algorithm configuration) file.\n        Args:\n            path: path to the acon file.\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            The acon file content as a dict.\n        \"\"\"\nreturn FileStorageFunctions.read_json(path, disable_dbfs_retry)\n@staticmethod\ndef read_sql(path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read a DDL file in Spark SQL format from a cloud object storage system.\n        Args:\n            path: path to the SQL file.\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            Content of the SQL file.\n        \"\"\"\nreturn FileStorageFunctions.read_sql(path, disable_dbfs_retry)\n@classmethod\ndef remove_sensitive_info(\ncls, dict_to_replace: Union[dict, list]\n) -&gt; Union[dict, list]:\n\"\"\"Remove sensitive info from a dictionary.\n        Args:\n            dict_to_replace: dict where we want to remove sensitive info.\n        Returns:\n            dict without sensitive information.\n        \"\"\"\nif isinstance(dict_to_replace, list):\nreturn [cls.remove_sensitive_info(k) for k in dict_to_replace]\nelif isinstance(dict_to_replace, dict):\nreturn {\nk: \"******\" if k in cls.SENSITIVE_INFO else cls.remove_sensitive_info(v)\nfor k, v in dict_to_replace.items()\n}\nelse:\nreturn dict_to_replace\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.get_acon","title":"<code>get_acon(acon_path=None, acon=None, disable_dbfs_retry=False)</code>  <code>classmethod</code>","text":"<p>Get acon based on a filesystem path or on a dict.</p> <p>Parameters:</p> Name Type Description Default <code>acon_path</code> <code>Optional[str]</code> <p>path of the acon (algorithm configuration) file.</p> <code>None</code> <code>acon</code> <code>Optional[dict]</code> <p>acon provided directly through python code (e.g., notebooks or other apps).</p> <code>None</code> <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict representation of an acon.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@classmethod\ndef get_acon(\ncls,\nacon_path: Optional[str] = None,\nacon: Optional[dict] = None,\ndisable_dbfs_retry: bool = False,\n) -&gt; dict:\n\"\"\"Get acon based on a filesystem path or on a dict.\n    Args:\n        acon_path: path of the acon (algorithm configuration) file.\n        acon: acon provided directly through python code (e.g., notebooks\n            or other apps).\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        Dict representation of an acon.\n    \"\"\"\nacon = (\nacon if acon else ConfigUtils.read_json_acon(acon_path, disable_dbfs_retry)\n)\nreturn acon\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.get_config","title":"<code>get_config(package='lakehouse_engine.configs')</code>  <code>staticmethod</code>","text":"<p>Get the lakehouse engine configuration file.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Configuration dictionary</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@staticmethod\ndef get_config(package: str = \"lakehouse_engine.configs\") -&gt; Any:\n\"\"\"Get the lakehouse engine configuration file.\n    Returns:\n        Configuration dictionary\n    \"\"\"\nwith importlib.resources.open_binary(package, \"engine.yaml\") as config:\nconfig = yaml.safe_load(config)\nreturn config\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.get_config_from_file","title":"<code>get_config_from_file(config_file_path)</code>  <code>staticmethod</code>","text":"<p>Get the lakehouse engine configurations using a file path.</p> <p>Args:     config_file_path: a string with a path for a yaml file     with custom configurations.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Configuration dictionary</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@staticmethod\ndef get_config_from_file(config_file_path: str) -&gt; Any:\n\"\"\"Get the lakehouse engine configurations using a file path.\n     Args:\n        config_file_path: a string with a path for a yaml file\n        with custom configurations.\n    Returns:\n        Configuration dictionary\n    \"\"\"\nwith open(config_file_path, \"r\") as config:\nconfig = yaml.safe_load(config)\nreturn config\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.get_engine_version","title":"<code>get_engine_version()</code>  <code>classmethod</code>","text":"<p>Get Lakehouse Engine version from the installed packages.</p> <p>Returns:</p> Type Description <code>str</code> <p>String of engine version.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@classmethod\ndef get_engine_version(cls) -&gt; str:\n\"\"\"Get Lakehouse Engine version from the installed packages.\n    Returns:\n        String of engine version.\n    \"\"\"\ntry:\nversion = pkg_resources.get_distribution(\"lakehouse-engine\").version\nexcept pkg_resources.DistributionNotFound:\ncls._LOGGER.info(\"Could not identify Lakehouse Engine version.\")\nversion = \"\"\nreturn str(version)\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.read_json_acon","title":"<code>read_json_acon(path, disable_dbfs_retry=False)</code>  <code>staticmethod</code>","text":"<p>Read an acon (algorithm configuration) file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the acon file.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The acon file content as a dict.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@staticmethod\ndef read_json_acon(path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read an acon (algorithm configuration) file.\n    Args:\n        path: path to the acon file.\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        The acon file content as a dict.\n    \"\"\"\nreturn FileStorageFunctions.read_json(path, disable_dbfs_retry)\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.read_sql","title":"<code>read_sql(path, disable_dbfs_retry=False)</code>  <code>staticmethod</code>","text":"<p>Read a DDL file in Spark SQL format from a cloud object storage system.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the SQL file.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Content of the SQL file.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@staticmethod\ndef read_sql(path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read a DDL file in Spark SQL format from a cloud object storage system.\n    Args:\n        path: path to the SQL file.\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        Content of the SQL file.\n    \"\"\"\nreturn FileStorageFunctions.read_sql(path, disable_dbfs_retry)\n</code></pre>"},{"location":"reference/packages/utils/configs/config_utils.html#packages.utils.configs.config_utils.ConfigUtils.remove_sensitive_info","title":"<code>remove_sensitive_info(dict_to_replace)</code>  <code>classmethod</code>","text":"<p>Remove sensitive info from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dict_to_replace</code> <code>Union[dict, list]</code> <p>dict where we want to remove sensitive info.</p> required <p>Returns:</p> Type Description <code>Union[dict, list]</code> <p>dict without sensitive information.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/configs/config_utils.py</code> <pre><code>@classmethod\ndef remove_sensitive_info(\ncls, dict_to_replace: Union[dict, list]\n) -&gt; Union[dict, list]:\n\"\"\"Remove sensitive info from a dictionary.\n    Args:\n        dict_to_replace: dict where we want to remove sensitive info.\n    Returns:\n        dict without sensitive information.\n    \"\"\"\nif isinstance(dict_to_replace, list):\nreturn [cls.remove_sensitive_info(k) for k in dict_to_replace]\nelif isinstance(dict_to_replace, dict):\nreturn {\nk: \"******\" if k in cls.SENSITIVE_INFO else cls.remove_sensitive_info(v)\nfor k, v in dict_to_replace.items()\n}\nelse:\nreturn dict_to_replace\n</code></pre>"},{"location":"reference/packages/utils/extraction/index.html","title":"Extraction","text":"<p>Extraction utilities package.</p>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html","title":"Jdbc extraction utils","text":"<p>Utilities module for JDBC extraction processes.</p>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtraction","title":"<code>JDBCExtraction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>object</code></p> <p>Configurations available for an Extraction from a JDBC source.</p> <p>These configurations cover:</p> <ul> <li>user: username to connect to JDBC source.</li> <li>password: password to connect to JDBC source (always use secrets,     don't use text passwords in your code).</li> <li>url: url to connect to JDBC source.</li> <li>dbtable: <code>database.table</code> to extract data from.</li> <li>calc_upper_bound_schema: custom schema used for the upper bound calculation.</li> <li>changelog_table: table of type changelog from which to extract data,     when the extraction type is delta.</li> <li>partition_column: column used to split the extraction.</li> <li>latest_timestamp_data_location: data location (e.g., s3) containing the data     to get the latest timestamp already loaded into bronze.</li> <li>latest_timestamp_data_format: the format of the dataset in     latest_timestamp_data_location. Default: delta.</li> <li>extraction_type: type of extraction (delta or init). Default: \"delta\".</li> <li>driver: JDBC driver name. Default: \"com.sap.db.jdbc.Driver\".</li> <li>num_partitions: number of Spark partitions to split the extraction.</li> <li>lower_bound: lower bound to decide the partition stride.</li> <li>upper_bound: upper bound to decide the partition stride. If     calculate_upper_bound is True, then upperBound will be     derived by our upper bound optimizer, using the partition column.</li> <li>default_upper_bound: the value to use as default upper bound in case     the result of the upper bound calculation is None. Default: \"1\".</li> <li>fetch_size: how many rows to fetch per round trip. Default: \"100000\".</li> <li>compress: enable network compression. Default: True.</li> <li>custom_schema: specify custom_schema for particular columns of the     returned dataframe in the init/delta extraction of the source table.</li> <li>min_timestamp: min timestamp to consider to filter the changelog data.     Default: None and automatically derived from the location provided.     In case this one is provided it has precedence and the calculation     is not done.</li> <li>max_timestamp: max timestamp to consider to filter the changelog data.     Default: None and automatically derived from the table having information     about the extraction requests, their timestamps and their status.     In case this one is provided it has precedence and the calculation     is not done.</li> <li>generate_predicates: whether to generate predicates automatically or not.     Default: False.</li> <li>predicates: list containing all values to partition (if generate_predicates     is used, the manual values provided are ignored). Default: None.</li> <li>predicates_add_null: whether to consider null on predicates list.     Default: True.</li> <li>extraction_timestamp: the timestamp of the extraction. Default: current time     following the format \"%Y%m%d%H%M%S\".</li> <li>max_timestamp_custom_schema: custom schema used on the max_timestamp derivation     from the table holding the extraction requests information.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>@dataclass\nclass JDBCExtraction(object):\n\"\"\"Configurations available for an Extraction from a JDBC source.\n    These configurations cover:\n    - user: username to connect to JDBC source.\n    - password: password to connect to JDBC source (always use secrets,\n        don't use text passwords in your code).\n    - url: url to connect to JDBC source.\n    - dbtable: `database.table` to extract data from.\n    - calc_upper_bound_schema: custom schema used for the upper bound calculation.\n    - changelog_table: table of type changelog from which to extract data,\n        when the extraction type is delta.\n    - partition_column: column used to split the extraction.\n    - latest_timestamp_data_location: data location (e.g., s3) containing the data\n        to get the latest timestamp already loaded into bronze.\n    - latest_timestamp_data_format: the format of the dataset in\n        latest_timestamp_data_location. Default: delta.\n    - extraction_type: type of extraction (delta or init). Default: \"delta\".\n    - driver: JDBC driver name. Default: \"com.sap.db.jdbc.Driver\".\n    - num_partitions: number of Spark partitions to split the extraction.\n    - lower_bound: lower bound to decide the partition stride.\n    - upper_bound: upper bound to decide the partition stride. If\n        calculate_upper_bound is True, then upperBound will be\n        derived by our upper bound optimizer, using the partition column.\n    - default_upper_bound: the value to use as default upper bound in case\n        the result of the upper bound calculation is None. Default: \"1\".\n    - fetch_size: how many rows to fetch per round trip. Default: \"100000\".\n    - compress: enable network compression. Default: True.\n    - custom_schema: specify custom_schema for particular columns of the\n        returned dataframe in the init/delta extraction of the source table.\n    - min_timestamp: min timestamp to consider to filter the changelog data.\n        Default: None and automatically derived from the location provided.\n        In case this one is provided it has precedence and the calculation\n        is not done.\n    - max_timestamp: max timestamp to consider to filter the changelog data.\n        Default: None and automatically derived from the table having information\n        about the extraction requests, their timestamps and their status.\n        In case this one is provided it has precedence and the calculation\n        is not done.\n    - generate_predicates: whether to generate predicates automatically or not.\n        Default: False.\n    - predicates: list containing all values to partition (if generate_predicates\n        is used, the manual values provided are ignored). Default: None.\n    - predicates_add_null: whether to consider null on predicates list.\n        Default: True.\n    - extraction_timestamp: the timestamp of the extraction. Default: current time\n        following the format \"%Y%m%d%H%M%S\".\n    - max_timestamp_custom_schema: custom schema used on the max_timestamp derivation\n        from the table holding the extraction requests information.\n    \"\"\"\nuser: str\npassword: str\nurl: str\ndbtable: str\ncalc_upper_bound_schema: Optional[str] = None\nchangelog_table: Optional[str] = None\npartition_column: Optional[str] = None\nlatest_timestamp_data_location: Optional[str] = None\nlatest_timestamp_data_format: str = InputFormat.DELTAFILES.value\nextraction_type: str = JDBCExtractionType.DELTA.value\ndriver: str = \"com.sap.db.jdbc.Driver\"\nnum_partitions: Optional[int] = None\nlower_bound: Optional[Union[int, float, str]] = None\nupper_bound: Optional[Union[int, float, str]] = None\ndefault_upper_bound: str = \"1\"\nfetch_size: str = \"100000\"\ncompress: bool = True\ncustom_schema: Optional[str] = None\nmin_timestamp: Optional[str] = None\nmax_timestamp: Optional[str] = None\ngenerate_predicates: bool = False\npredicates: Optional[List] = None\npredicates_add_null: bool = True\nextraction_timestamp: str = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\nmax_timestamp_custom_schema: Optional[str] = None\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionType","title":"<code>JDBCExtractionType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Standardize the types of extractions we can have from a JDBC source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>class JDBCExtractionType(Enum):\n\"\"\"Standardize the types of extractions we can have from a JDBC source.\"\"\"\nINIT = \"init\"\nDELTA = \"delta\"\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils","title":"<code>JDBCExtractionUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Utils for managing data extraction from particularly relevant JDBC sources.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>class JDBCExtractionUtils(object):\n\"\"\"Utils for managing data extraction from particularly relevant JDBC sources.\"\"\"\ndef __init__(self, jdbc_extraction: Any):\n\"\"\"Construct JDBCExtractionUtils.\n        Args:\n            jdbc_extraction: JDBC Extraction configurations. Can be of type:\n                JDBCExtraction, SAPB4Extraction or SAPBWExtraction.\n        \"\"\"\nself._LOGGER: Logger = LoggingHandler(__name__).get_logger()\nself._JDBC_EXTRACTION = jdbc_extraction\n@staticmethod\ndef get_additional_spark_options(\ninput_spec: InputSpec, options: dict, ignore_options: List = None\n) -&gt; dict:\n\"\"\"Helper to get additional Spark Options initially passed.\n        If people provide additional Spark options, not covered by the util function\n        arguments (get_spark_jdbc_options), we need to consider them.\n        Thus, we update the options retrieved by the utils, by checking if there is\n        any Spark option initially provided that is not yet considered in the retrieved\n        options or function arguments and if the value for the key is not None.\n        If these conditions are filled, we add the options and return the complete dict.\n        Args:\n            input_spec: the input specification.\n            options: dict with Spark options.\n            ignore_options: list of options to be ignored by the process.\n                Spark read has two different approaches to parallelize\n                reading process, one of them is using upper/lower bound,\n                another one is using predicates, those process can't be\n                executed at the same time, you must choose one of them.\n                By choosing predicates you can't pass lower and upper bound,\n                also can't pass number of partitions and partition column\n                otherwise spark will interpret the execution partitioned by\n                upper and lower bound and will expect to fill all variables.\n                To avoid fill all predicates hardcoded at the acon, there is\n                a feature that automatically generates all predicates for init\n                or delta load based on input partition column, but at the end\n                of the process, partition column can't be passed to the options,\n                because we are choosing predicates execution, that is why to\n                generate predicates we need to pass some options to ignore.\n        Returns:\n             a dict with all the options passed as argument, plus the options that\n             were initially provided, but were not used in the util\n             (get_spark_jdbc_options).\n        \"\"\"\nfunc_args = JDBCExtractionUtils.get_spark_jdbc_options.__code__.co_varnames\nif ignore_options is None:\nignore_options = []\nignore_options = ignore_options + list(options.keys()) + list(func_args)\nreturn {\nkey: value\nfor key, value in input_spec.options.items()\nif key not in ignore_options and value is not None\n}\ndef get_predicates(self, predicates_query: str) -&gt; List:\n\"\"\"Get the predicates list, based on a predicates query.\n        Args:\n            predicates_query: query to use as the basis to get the distinct values for\n                a specified column, based on which predicates are generated.\n        Returns:\n            List containing the predicates to use to split the extraction from\n            JDBC sources.\n        \"\"\"\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"table\": predicates_query,\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\npredicates_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"get_predicates\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\n)\n)\npredicates_list = [\nf\"{self._JDBC_EXTRACTION.partition_column}='{row[0]}'\"\nfor row in predicates_df.collect()\n]\nif self._JDBC_EXTRACTION.predicates_add_null:\npredicates_list.append(f\"{self._JDBC_EXTRACTION.partition_column} IS NULL\")\nself._LOGGER.info(\nf\"The following predicate list was generated: {predicates_list}\"\n)\nreturn predicates_list\ndef get_spark_jdbc_options(self) -&gt; Tuple[dict, dict]:\n\"\"\"Get the Spark options to extract data from a JDBC source.\n        Returns:\n            The Spark jdbc args dictionary, including the query to submit\n            and also options args dictionary.\n        \"\"\"\noptions_args: Dict[str, Any] = {\n\"fetchSize\": self._JDBC_EXTRACTION.fetch_size,\n\"compress\": self._JDBC_EXTRACTION.compress,\n}\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nif self._JDBC_EXTRACTION.extraction_type == JDBCExtractionType.DELTA.value:\njdbc_args[\"table\"], predicates_query = self._get_delta_query()\nelse:\njdbc_args[\"table\"], predicates_query = self._get_init_query()\nif self._JDBC_EXTRACTION.custom_schema:\noptions_args[\"customSchema\"] = self._JDBC_EXTRACTION.custom_schema\nif self._JDBC_EXTRACTION.generate_predicates:\njdbc_args[\"predicates\"] = self.get_predicates(predicates_query)\nelse:\nif self._JDBC_EXTRACTION.predicates:\njdbc_args[\"predicates\"] = self._JDBC_EXTRACTION.predicates\nelse:\noptions_args = self._get_extraction_partition_opts(\noptions_args,\n)\nreturn options_args, jdbc_args\ndef get_spark_jdbc_optimal_upper_bound(self) -&gt; Any:\n\"\"\"Get an optimal upperBound to properly split a Spark JDBC extraction.\n        Returns:\n             Either an int, date or timestamp to serve as upperBound Spark JDBC option.\n        \"\"\"\noptions = {}\nif self._JDBC_EXTRACTION.calc_upper_bound_schema:\noptions[\"customSchema\"] = self._JDBC_EXTRACTION.calc_upper_bound_schema\ntable = (\nself._JDBC_EXTRACTION.dbtable\nif self._JDBC_EXTRACTION.extraction_type == JDBCExtractionType.INIT.value\nelse self._JDBC_EXTRACTION.changelog_table\n)\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"table\": f\"(SELECT COALESCE(MAX({self._JDBC_EXTRACTION.partition_column}), \"\nf\"{self._JDBC_EXTRACTION.default_upper_bound}) \"\nf\"upper_bound FROM {table})\",  # nosec: B608\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nupper_bound_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"get_optimal_upper_bound\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\noptions=options,\n)\n)\nupper_bound = upper_bound_df.first()[0]\nif upper_bound is not None:\nself._LOGGER.info(\nf\"Upper Bound '{upper_bound}' derived from \"\nf\"'{self._JDBC_EXTRACTION.dbtable}' using the column \"\nf\"'{self._JDBC_EXTRACTION.partition_column}'\"\n)\nreturn upper_bound\nelse:\nraise AttributeError(\nf\"Not able to calculate upper bound from \"\nf\"'{self._JDBC_EXTRACTION.dbtable}' using \"\nf\"the column '{self._JDBC_EXTRACTION.partition_column}'\"\n)\ndef _get_extraction_partition_opts(\nself,\noptions_args: dict,\n) -&gt; dict:\n\"\"\"Get an options dict with custom extraction partition options.\n        Args:\n            options_args: spark jdbc reader options.\n        \"\"\"\nif self._JDBC_EXTRACTION.num_partitions:\noptions_args[\"numPartitions\"] = self._JDBC_EXTRACTION.num_partitions\nif self._JDBC_EXTRACTION.upper_bound:\noptions_args[\"upperBound\"] = self._JDBC_EXTRACTION.upper_bound\nif self._JDBC_EXTRACTION.lower_bound:\noptions_args[\"lowerBound\"] = self._JDBC_EXTRACTION.lower_bound\nif self._JDBC_EXTRACTION.partition_column:\noptions_args[\"partitionColumn\"] = self._JDBC_EXTRACTION.partition_column\nreturn options_args\ndef _get_max_timestamp(self, max_timestamp_query: str) -&gt; str:\n\"\"\"Get the max timestamp, based on the provided query.\n        Args:\n            max_timestamp_query: the query used to derive the max timestamp.\n        Returns:\n            A string having the max timestamp.\n        \"\"\"\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"table\": max_timestamp_query,\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nmax_timestamp_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"get_max_timestamp\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\noptions={\n\"customSchema\": self._JDBC_EXTRACTION.max_timestamp_custom_schema\n},\n)\n)\nmax_timestamp = max_timestamp_df.first()[0]\nself._LOGGER.info(\nf\"Max timestamp {max_timestamp} derived from query: {max_timestamp_query}\"\n)\nreturn str(max_timestamp)\n@abstractmethod\ndef _get_delta_query(self) -&gt; Tuple[str, str]:\n\"\"\"Get a query to extract delta (partially) from a source.\"\"\"\npass\n@abstractmethod\ndef _get_init_query(self) -&gt; Tuple[str, str]:\n\"\"\"Get a query to extract init (fully) from a source.\"\"\"\npass\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.__init__","title":"<code>__init__(jdbc_extraction)</code>","text":"<p>Construct JDBCExtractionUtils.</p> <p>Parameters:</p> Name Type Description Default <code>jdbc_extraction</code> <code>Any</code> <p>JDBC Extraction configurations. Can be of type: JDBCExtraction, SAPB4Extraction or SAPBWExtraction.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>def __init__(self, jdbc_extraction: Any):\n\"\"\"Construct JDBCExtractionUtils.\n    Args:\n        jdbc_extraction: JDBC Extraction configurations. Can be of type:\n            JDBCExtraction, SAPB4Extraction or SAPBWExtraction.\n    \"\"\"\nself._LOGGER: Logger = LoggingHandler(__name__).get_logger()\nself._JDBC_EXTRACTION = jdbc_extraction\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_additional_spark_options","title":"<code>get_additional_spark_options(input_spec, options, ignore_options=None)</code>  <code>staticmethod</code>","text":"<p>Helper to get additional Spark Options initially passed.</p> <p>If people provide additional Spark options, not covered by the util function arguments (get_spark_jdbc_options), we need to consider them. Thus, we update the options retrieved by the utils, by checking if there is any Spark option initially provided that is not yet considered in the retrieved options or function arguments and if the value for the key is not None. If these conditions are filled, we add the options and return the complete dict.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>InputSpec</code> <p>the input specification.</p> required <code>options</code> <code>dict</code> <p>dict with Spark options.</p> required <code>ignore_options</code> <code>List</code> <p>list of options to be ignored by the process. Spark read has two different approaches to parallelize reading process, one of them is using upper/lower bound, another one is using predicates, those process can't be executed at the same time, you must choose one of them. By choosing predicates you can't pass lower and upper bound, also can't pass number of partitions and partition column otherwise spark will interpret the execution partitioned by upper and lower bound and will expect to fill all variables. To avoid fill all predicates hardcoded at the acon, there is a feature that automatically generates all predicates for init or delta load based on input partition column, but at the end of the process, partition column can't be passed to the options, because we are choosing predicates execution, that is why to generate predicates we need to pass some options to ignore.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>a dict with all the options passed as argument, plus the options that</p> <code>dict</code> <p>were initially provided, but were not used in the util</p> <code>dict</code> <p>(get_spark_jdbc_options).</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>@staticmethod\ndef get_additional_spark_options(\ninput_spec: InputSpec, options: dict, ignore_options: List = None\n) -&gt; dict:\n\"\"\"Helper to get additional Spark Options initially passed.\n    If people provide additional Spark options, not covered by the util function\n    arguments (get_spark_jdbc_options), we need to consider them.\n    Thus, we update the options retrieved by the utils, by checking if there is\n    any Spark option initially provided that is not yet considered in the retrieved\n    options or function arguments and if the value for the key is not None.\n    If these conditions are filled, we add the options and return the complete dict.\n    Args:\n        input_spec: the input specification.\n        options: dict with Spark options.\n        ignore_options: list of options to be ignored by the process.\n            Spark read has two different approaches to parallelize\n            reading process, one of them is using upper/lower bound,\n            another one is using predicates, those process can't be\n            executed at the same time, you must choose one of them.\n            By choosing predicates you can't pass lower and upper bound,\n            also can't pass number of partitions and partition column\n            otherwise spark will interpret the execution partitioned by\n            upper and lower bound and will expect to fill all variables.\n            To avoid fill all predicates hardcoded at the acon, there is\n            a feature that automatically generates all predicates for init\n            or delta load based on input partition column, but at the end\n            of the process, partition column can't be passed to the options,\n            because we are choosing predicates execution, that is why to\n            generate predicates we need to pass some options to ignore.\n    Returns:\n         a dict with all the options passed as argument, plus the options that\n         were initially provided, but were not used in the util\n         (get_spark_jdbc_options).\n    \"\"\"\nfunc_args = JDBCExtractionUtils.get_spark_jdbc_options.__code__.co_varnames\nif ignore_options is None:\nignore_options = []\nignore_options = ignore_options + list(options.keys()) + list(func_args)\nreturn {\nkey: value\nfor key, value in input_spec.options.items()\nif key not in ignore_options and value is not None\n}\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_predicates","title":"<code>get_predicates(predicates_query)</code>","text":"<p>Get the predicates list, based on a predicates query.</p> <p>Parameters:</p> Name Type Description Default <code>predicates_query</code> <code>str</code> <p>query to use as the basis to get the distinct values for a specified column, based on which predicates are generated.</p> required <p>Returns:</p> Type Description <code>List</code> <p>List containing the predicates to use to split the extraction from</p> <code>List</code> <p>JDBC sources.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>def get_predicates(self, predicates_query: str) -&gt; List:\n\"\"\"Get the predicates list, based on a predicates query.\n    Args:\n        predicates_query: query to use as the basis to get the distinct values for\n            a specified column, based on which predicates are generated.\n    Returns:\n        List containing the predicates to use to split the extraction from\n        JDBC sources.\n    \"\"\"\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"table\": predicates_query,\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\npredicates_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"get_predicates\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\n)\n)\npredicates_list = [\nf\"{self._JDBC_EXTRACTION.partition_column}='{row[0]}'\"\nfor row in predicates_df.collect()\n]\nif self._JDBC_EXTRACTION.predicates_add_null:\npredicates_list.append(f\"{self._JDBC_EXTRACTION.partition_column} IS NULL\")\nself._LOGGER.info(\nf\"The following predicate list was generated: {predicates_list}\"\n)\nreturn predicates_list\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_spark_jdbc_optimal_upper_bound","title":"<code>get_spark_jdbc_optimal_upper_bound()</code>","text":"<p>Get an optimal upperBound to properly split a Spark JDBC extraction.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Either an int, date or timestamp to serve as upperBound Spark JDBC option.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>def get_spark_jdbc_optimal_upper_bound(self) -&gt; Any:\n\"\"\"Get an optimal upperBound to properly split a Spark JDBC extraction.\n    Returns:\n         Either an int, date or timestamp to serve as upperBound Spark JDBC option.\n    \"\"\"\noptions = {}\nif self._JDBC_EXTRACTION.calc_upper_bound_schema:\noptions[\"customSchema\"] = self._JDBC_EXTRACTION.calc_upper_bound_schema\ntable = (\nself._JDBC_EXTRACTION.dbtable\nif self._JDBC_EXTRACTION.extraction_type == JDBCExtractionType.INIT.value\nelse self._JDBC_EXTRACTION.changelog_table\n)\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"table\": f\"(SELECT COALESCE(MAX({self._JDBC_EXTRACTION.partition_column}), \"\nf\"{self._JDBC_EXTRACTION.default_upper_bound}) \"\nf\"upper_bound FROM {table})\",  # nosec: B608\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nupper_bound_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"get_optimal_upper_bound\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\noptions=options,\n)\n)\nupper_bound = upper_bound_df.first()[0]\nif upper_bound is not None:\nself._LOGGER.info(\nf\"Upper Bound '{upper_bound}' derived from \"\nf\"'{self._JDBC_EXTRACTION.dbtable}' using the column \"\nf\"'{self._JDBC_EXTRACTION.partition_column}'\"\n)\nreturn upper_bound\nelse:\nraise AttributeError(\nf\"Not able to calculate upper bound from \"\nf\"'{self._JDBC_EXTRACTION.dbtable}' using \"\nf\"the column '{self._JDBC_EXTRACTION.partition_column}'\"\n)\n</code></pre>"},{"location":"reference/packages/utils/extraction/jdbc_extraction_utils.html#packages.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_spark_jdbc_options","title":"<code>get_spark_jdbc_options()</code>","text":"<p>Get the Spark options to extract data from a JDBC source.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The Spark jdbc args dictionary, including the query to submit</p> <code>dict</code> <p>and also options args dictionary.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/jdbc_extraction_utils.py</code> <pre><code>def get_spark_jdbc_options(self) -&gt; Tuple[dict, dict]:\n\"\"\"Get the Spark options to extract data from a JDBC source.\n    Returns:\n        The Spark jdbc args dictionary, including the query to submit\n        and also options args dictionary.\n    \"\"\"\noptions_args: Dict[str, Any] = {\n\"fetchSize\": self._JDBC_EXTRACTION.fetch_size,\n\"compress\": self._JDBC_EXTRACTION.compress,\n}\njdbc_args = {\n\"url\": self._JDBC_EXTRACTION.url,\n\"properties\": {\n\"user\": self._JDBC_EXTRACTION.user,\n\"password\": self._JDBC_EXTRACTION.password,\n\"driver\": self._JDBC_EXTRACTION.driver,\n},\n}\nif self._JDBC_EXTRACTION.extraction_type == JDBCExtractionType.DELTA.value:\njdbc_args[\"table\"], predicates_query = self._get_delta_query()\nelse:\njdbc_args[\"table\"], predicates_query = self._get_init_query()\nif self._JDBC_EXTRACTION.custom_schema:\noptions_args[\"customSchema\"] = self._JDBC_EXTRACTION.custom_schema\nif self._JDBC_EXTRACTION.generate_predicates:\njdbc_args[\"predicates\"] = self.get_predicates(predicates_query)\nelse:\nif self._JDBC_EXTRACTION.predicates:\njdbc_args[\"predicates\"] = self._JDBC_EXTRACTION.predicates\nelse:\noptions_args = self._get_extraction_partition_opts(\noptions_args,\n)\nreturn options_args, jdbc_args\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_b4_extraction_utils.html","title":"Sap b4 extraction utils","text":"<p>Utilities module for SAP B4 extraction processes.</p>"},{"location":"reference/packages/utils/extraction/sap_b4_extraction_utils.html#packages.utils.extraction.sap_b4_extraction_utils.ADSOTypes","title":"<code>ADSOTypes</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Standardise the types of ADSOs we can have for Extractions from SAP B4.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_b4_extraction_utils.py</code> <pre><code>class ADSOTypes(Enum):\n\"\"\"Standardise the types of ADSOs we can have for Extractions from SAP B4.\"\"\"\nAQ: str = \"AQ\"\nCL: str = \"CL\"\nSUPPORTED_TYPES: list = [AQ, CL]\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_b4_extraction_utils.html#packages.utils.extraction.sap_b4_extraction_utils.SAPB4Extraction","title":"<code>SAPB4Extraction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>JDBCExtraction</code></p> <p>Configurations available for an Extraction from SAP B4.</p> <p>It inherits from JDBCExtraction configurations, so it can use and/or overwrite those configurations.</p> <p>These configurations cover:</p> <ul> <li>latest_timestamp_input_col: the column containing the request timestamps     in the dataset in latest_timestamp_data_location. Default: REQTSN.</li> <li>request_status_tbl: the name of the SAP B4 table having information     about the extraction requests. Composed of database.table.     Default: SAPHANADB.RSPMREQUEST.</li> <li>request_col_name: name of the column having the request timestamp to join     with the request status table. Default: REQUEST_TSN.</li> <li>data_target: the data target to extract from. User in the join operation with     the request status table.</li> <li>act_req_join_condition: the join condition into activation table     can be changed using this property.     Default: 'tbl.reqtsn = req.request_col_name'.</li> <li>include_changelog_tech_cols: whether to include the technical columns     (usually coming from the changelog) table or not.</li> <li>extra_cols_req_status_tbl: columns to be added from request status table.     It needs to contain the prefix \"req.\". E.g. \"req.col1 as column_one,     req.col2 as column_two\".</li> <li>request_status_tbl_filter: filter to use for filtering the request status table,     influencing the calculation of the max timestamps and the delta extractions.</li> <li>adso_type: the type of ADSO that you are extracting from. Can be \"AQ\" or \"CL\".</li> <li>max_timestamp_custom_schema: the custom schema to apply on the calculation of     the max timestamp to consider for the delta extractions.     Default: timestamp DECIMAL(23,0).</li> <li>default_max_timestamp: the timestamp to use as default, when it is not possible     to derive one.</li> <li>custom_schema: specify custom_schema for particular columns of the     returned dataframe in the init/delta extraction of the source table.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_b4_extraction_utils.py</code> <pre><code>@dataclass\nclass SAPB4Extraction(JDBCExtraction):\n\"\"\"Configurations available for an Extraction from SAP B4.\n    It inherits from JDBCExtraction configurations, so it can use\n    and/or overwrite those configurations.\n    These configurations cover:\n    - latest_timestamp_input_col: the column containing the request timestamps\n        in the dataset in latest_timestamp_data_location. Default: REQTSN.\n    - request_status_tbl: the name of the SAP B4 table having information\n        about the extraction requests. Composed of database.table.\n        Default: SAPHANADB.RSPMREQUEST.\n    - request_col_name: name of the column having the request timestamp to join\n        with the request status table. Default: REQUEST_TSN.\n    - data_target: the data target to extract from. User in the join operation with\n        the request status table.\n    - act_req_join_condition: the join condition into activation table\n        can be changed using this property.\n        Default: 'tbl.reqtsn = req.request_col_name'.\n    - include_changelog_tech_cols: whether to include the technical columns\n        (usually coming from the changelog) table or not.\n    - extra_cols_req_status_tbl: columns to be added from request status table.\n        It needs to contain the prefix \"req.\". E.g. \"req.col1 as column_one,\n        req.col2 as column_two\".\n    - request_status_tbl_filter: filter to use for filtering the request status table,\n        influencing the calculation of the max timestamps and the delta extractions.\n    - adso_type: the type of ADSO that you are extracting from. Can be \"AQ\" or \"CL\".\n    - max_timestamp_custom_schema: the custom schema to apply on the calculation of\n        the max timestamp to consider for the delta extractions.\n        Default: timestamp DECIMAL(23,0).\n    - default_max_timestamp: the timestamp to use as default, when it is not possible\n        to derive one.\n    - custom_schema: specify custom_schema for particular columns of the\n        returned dataframe in the init/delta extraction of the source table.\n    \"\"\"\nlatest_timestamp_input_col: str = \"REQTSN\"\nrequest_status_tbl: str = \"SAPHANADB.RSPMREQUEST\"\nrequest_col_name: str = \"REQUEST_TSN\"\ndata_target: Optional[str] = None\nact_req_join_condition: Optional[str] = None\ninclude_changelog_tech_cols: Optional[bool] = None\nextra_cols_req_status_tbl: Optional[str] = None\nrequest_status_tbl_filter: Optional[str] = None\nadso_type: Optional[str] = None\nmax_timestamp_custom_schema: str = \"timestamp DECIMAL(23,0)\"\ndefault_max_timestamp: str = \"1970000000000000000000\"\ncustom_schema: str = \"REQTSN DECIMAL(23,0)\"\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_b4_extraction_utils.html#packages.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils","title":"<code>SAPB4ExtractionUtils</code>","text":"<p>         Bases: <code>JDBCExtractionUtils</code></p> <p>Utils for managing data extraction from SAP B4.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_b4_extraction_utils.py</code> <pre><code>class SAPB4ExtractionUtils(JDBCExtractionUtils):\n\"\"\"Utils for managing data extraction from SAP B4.\"\"\"\ndef __init__(self, sap_b4_extraction: SAPB4Extraction):\n\"\"\"Construct SAPB4ExtractionUtils.\n        Args:\n            sap_b4_extraction: SAP B4 Extraction configurations.\n        \"\"\"\nself._LOGGER: Logger = LoggingHandler(__name__).get_logger()\nself._B4_EXTRACTION = sap_b4_extraction\nself._B4_EXTRACTION.request_status_tbl_filter = (\nself._get_req_status_tbl_filter()\n)\nself._MAX_TIMESTAMP_QUERY = f\"\"\" --# nosec\n                (SELECT COALESCE(MAX({self._B4_EXTRACTION.request_col_name}),\n{self._B4_EXTRACTION.default_max_timestamp}) as timestamp\n                FROM {self._B4_EXTRACTION.request_status_tbl}\n                WHERE {self._B4_EXTRACTION.request_status_tbl_filter})\n            \"\"\"  # nosec: B608\nsuper().__init__(sap_b4_extraction)\n@staticmethod\ndef get_data_target(input_spec_opt: dict) -&gt; str:\n\"\"\"Get the data_target from the data_target option or derive it.\n        By definition data_target is the same for the table and changelog table and\n        is the same string ignoring everything before / and the first and last\n        character after /. E.g. for a dbtable /BIC/abtable12, the data_target\n        would be btable1.\n        Args:\n            input_spec_opt: options from the input_spec.\n        Returns:\n            A string with the data_target.\n        \"\"\"\nexclude_chars = \"\"\"[\"'\\\\\\\\]\"\"\"\ndata_target: str = input_spec_opt.get(\n\"data_target\",\nre.sub(exclude_chars, \"\", input_spec_opt[\"dbtable\"]).split(\"/\")[-1][1:-1],\n)\nreturn data_target\ndef _get_init_query(self) -&gt; Tuple[str, str]:\n\"\"\"Get a query to do an init load based on a ADSO on a SAP B4 system.\n        Returns:\n            A query to submit to SAP B4 for the initial data extraction. The query\n            is enclosed in parentheses so that Spark treats it as a table and supports\n            it in the dbtable option.\n        \"\"\"\nextraction_query = self._get_init_extraction_query()\npredicates_query = f\"\"\"\n        (SELECT DISTINCT({self._B4_EXTRACTION.partition_column})\n        FROM {self._B4_EXTRACTION.dbtable} t)\n        \"\"\"  # nosec: B608\nreturn extraction_query, predicates_query\ndef _get_init_extraction_query(self) -&gt; str:\n\"\"\"Get the init extraction query based on current timestamp.\n        Returns:\n            A query to submit to SAP B4 for the initial data extraction.\n        \"\"\"\nchangelog_tech_cols = (\nf\"\"\"{self._B4_EXTRACTION.extraction_timestamp}000000000 AS reqtsn,\n                '0' AS datapakid,\n                0 AS record,\"\"\"\nif self._B4_EXTRACTION.include_changelog_tech_cols\nelse \"\"\n)\nextraction_query = f\"\"\"\n                (SELECT t.*, {changelog_tech_cols}\n                    CAST({self._B4_EXTRACTION.extraction_timestamp}\n                        AS DECIMAL(15,0)) AS extraction_start_timestamp\n                FROM {self._B4_EXTRACTION.dbtable} t\n                )\"\"\"  # nosec: B608\nreturn extraction_query\ndef _get_delta_query(self) -&gt; Tuple[str, str]:\n\"\"\"Get a delta query for an SAP B4 ADSO.\n        An SAP B4 ADSO requires a join with a special type of table often called\n        requests status table (RSPMREQUEST), in which B4 tracks down the timestamps,\n        status and metrics associated with the several data loads that were performed\n        into B4. Depending on the type of ADSO (AQ or CL) the join condition and also\n        the ADSO/table to consider to extract from will be different.\n        For AQ types, there is only the active table, from which we extract both inits\n        and deltas and this is also the table used to join with RSPMREQUEST to derive\n        the next portion of the data to extract.\n        For the CL types, we have an active table/adso from which we extract the init\n        and one changelog table from which we extract the delta portions of data.\n        Depending, if it is an init or delta one table or the other is also used to join\n        with RSPMREQUEST.\n        The logic on this function basically ensures that we are reading from the source\n        table considering the data that has arrived between the maximum timestamp that\n        is available in our target destination and the max timestamp of the extractions\n        performed and registered in the RSPMREQUEST table, which follow the filtering\n         criteria.\n        Returns:\n            A query to submit to SAP B4 for the delta data extraction. The query\n            is enclosed in parentheses so that Spark treats it as a table and supports\n            it in the dbtable option.\n        \"\"\"\nif not self._B4_EXTRACTION.min_timestamp:\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nlatest_timestamp_data_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"data_with_latest_timestamp\",\ndata_format=self._B4_EXTRACTION.latest_timestamp_data_format,\nread_type=ReadType.BATCH.value,\nlocation=self._B4_EXTRACTION.latest_timestamp_data_location,\n)\n)\nmin_timestamp = latest_timestamp_data_df.transform(\nAggregators.get_max_value(\nself._B4_EXTRACTION.latest_timestamp_input_col\n)\n).first()[0]\nelse:\nmin_timestamp = self._B4_EXTRACTION.min_timestamp\nmax_timestamp = (\nself._B4_EXTRACTION.max_timestamp\nif self._B4_EXTRACTION.max_timestamp\nelse self._get_max_timestamp(self._MAX_TIMESTAMP_QUERY)\n)\nif self._B4_EXTRACTION.act_req_join_condition:\njoin_condition = f\"{self._B4_EXTRACTION.act_req_join_condition}\"\nelse:\njoin_condition = f\"tbl.reqtsn = req.{self._B4_EXTRACTION.request_col_name}\"\nbase_query = f\"\"\" --# nosec\n        FROM {self._B4_EXTRACTION.changelog_table} AS tbl\n        JOIN {self._B4_EXTRACTION.request_status_tbl} AS req\n            ON {join_condition}\n        WHERE {self._B4_EXTRACTION.request_status_tbl_filter}\n            AND req.{self._B4_EXTRACTION.request_col_name} &gt; {min_timestamp}\n            AND req.{self._B4_EXTRACTION.request_col_name} &lt;= {max_timestamp})\n        \"\"\"\nmain_cols = f\"\"\"\n            (SELECT tbl.*,\n                CAST({self._B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0))\n                    AS extraction_start_timestamp\n            \"\"\"\n# We join the main columns considered for the extraction with\n# extra_cols_act_request that people might want to use, filtering to only\n# add the comma and join the strings, in case extra_cols_act_request is\n# not None or empty.\nextraction_query_cols = \",\".join(\nfilter(None, [main_cols, self._B4_EXTRACTION.extra_cols_req_status_tbl])\n)\nextraction_query = extraction_query_cols + base_query\npredicates_query = f\"\"\"\n        (SELECT DISTINCT({self._B4_EXTRACTION.partition_column})\n{base_query}\n        \"\"\"\nreturn extraction_query, predicates_query\ndef _get_req_status_tbl_filter(self) -&gt; Any:\nif self._B4_EXTRACTION.request_status_tbl_filter:\nreturn self._B4_EXTRACTION.request_status_tbl_filter\nelse:\nif self._B4_EXTRACTION.adso_type == ADSOTypes.AQ.value:\nreturn f\"\"\"\n                    STORAGE = 'AQ' AND REQUEST_IS_IN_PROCESS = 'N' AND\n                    LAST_OPERATION_TYPE IN ('C', 'U') AND REQUEST_STATUS IN ('GG', 'GR')\n                    AND UPPER(DATATARGET) = UPPER('{self._B4_EXTRACTION.data_target}')\n                \"\"\"\nelif self._B4_EXTRACTION.adso_type == ADSOTypes.CL.value:\nreturn f\"\"\"\n                    STORAGE = 'AT' AND REQUEST_IS_IN_PROCESS = 'N' AND\n                    LAST_OPERATION_TYPE IN ('C', 'U') AND REQUEST_STATUS IN ('GG')\n                    AND UPPER(DATATARGET) = UPPER('{self._B4_EXTRACTION.data_target}')\n                \"\"\"\nelse:\nraise NotImplementedError(\nf\"The requested ADSO Type is not fully implemented and/or tested.\"\nf\"Supported ADSO Types: {ADSOTypes.SUPPORTED_TYPES}\"\n)\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_b4_extraction_utils.html#packages.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils.__init__","title":"<code>__init__(sap_b4_extraction)</code>","text":"<p>Construct SAPB4ExtractionUtils.</p> <p>Parameters:</p> Name Type Description Default <code>sap_b4_extraction</code> <code>SAPB4Extraction</code> <p>SAP B4 Extraction configurations.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_b4_extraction_utils.py</code> <pre><code>def __init__(self, sap_b4_extraction: SAPB4Extraction):\n\"\"\"Construct SAPB4ExtractionUtils.\n    Args:\n        sap_b4_extraction: SAP B4 Extraction configurations.\n    \"\"\"\nself._LOGGER: Logger = LoggingHandler(__name__).get_logger()\nself._B4_EXTRACTION = sap_b4_extraction\nself._B4_EXTRACTION.request_status_tbl_filter = (\nself._get_req_status_tbl_filter()\n)\nself._MAX_TIMESTAMP_QUERY = f\"\"\" --# nosec\n            (SELECT COALESCE(MAX({self._B4_EXTRACTION.request_col_name}),\n{self._B4_EXTRACTION.default_max_timestamp}) as timestamp\n            FROM {self._B4_EXTRACTION.request_status_tbl}\n            WHERE {self._B4_EXTRACTION.request_status_tbl_filter})\n        \"\"\"  # nosec: B608\nsuper().__init__(sap_b4_extraction)\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_b4_extraction_utils.html#packages.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils.get_data_target","title":"<code>get_data_target(input_spec_opt)</code>  <code>staticmethod</code>","text":"<p>Get the data_target from the data_target option or derive it.</p> <p>By definition data_target is the same for the table and changelog table and is the same string ignoring everything before / and the first and last character after /. E.g. for a dbtable /BIC/abtable12, the data_target would be btable1.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec_opt</code> <code>dict</code> <p>options from the input_spec.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string with the data_target.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_b4_extraction_utils.py</code> <pre><code>@staticmethod\ndef get_data_target(input_spec_opt: dict) -&gt; str:\n\"\"\"Get the data_target from the data_target option or derive it.\n    By definition data_target is the same for the table and changelog table and\n    is the same string ignoring everything before / and the first and last\n    character after /. E.g. for a dbtable /BIC/abtable12, the data_target\n    would be btable1.\n    Args:\n        input_spec_opt: options from the input_spec.\n    Returns:\n        A string with the data_target.\n    \"\"\"\nexclude_chars = \"\"\"[\"'\\\\\\\\]\"\"\"\ndata_target: str = input_spec_opt.get(\n\"data_target\",\nre.sub(exclude_chars, \"\", input_spec_opt[\"dbtable\"]).split(\"/\")[-1][1:-1],\n)\nreturn data_target\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_bw_extraction_utils.html","title":"Sap bw extraction utils","text":"<p>Utilities module for SAP BW extraction processes.</p>"},{"location":"reference/packages/utils/extraction/sap_bw_extraction_utils.html#packages.utils.extraction.sap_bw_extraction_utils.SAPBWExtraction","title":"<code>SAPBWExtraction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>JDBCExtraction</code></p> <p>Configurations available for an Extraction from SAP BW.</p> <p>It inherits from SAPBWExtraction configurations, so it can use and/or overwrite those configurations.</p> <p>These configurations cover:</p> <ul> <li>latest_timestamp_input_col: the column containing the actrequest timestamp     in the dataset in latest_timestamp_data_location. Default:     \"actrequest_timestamp\".</li> <li>act_request_table: the name of the SAP BW activation requests table.     Composed of database.table. Default: SAPPHA.RSODSACTREQ.</li> <li>request_col_name: name of the column having the request to join     with the activation request table. Default: actrequest.</li> <li>act_req_join_condition: the join condition into activation table     can be changed using this property.     Default: 'changelog_tbl.request = act_req.request_col_name'.</li> <li>odsobject: name of BW Object, used for joining with the activation request     table to get the max actrequest_timestamp to consider while filtering     the changelog table.</li> <li>include_changelog_tech_cols: whether to include the technical columns     (usually coming from the changelog) table or not. Default: True.</li> <li>extra_cols_act_request: list of columns to be added from act request table.     It needs to contain the prefix \"act_req.\". E.g. \"act_req.col1     as column_one, act_req.col2 as column_two\".</li> <li>get_timestamp_from_act_request: whether to get init timestamp     from act request table or assume current/given timestamp.</li> <li>sap_bw_schema: sap bw schema. Default: SAPPHA.</li> <li>max_timestamp_custom_schema: the custom schema to apply on the calculation of     the max timestamp to consider for the delta extractions.     Default: timestamp DECIMAL(23,0).</li> <li>default_max_timestamp: the timestamp to use as default, when it is not possible     to derive one.</li> </ul> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_bw_extraction_utils.py</code> <pre><code>@dataclass\nclass SAPBWExtraction(JDBCExtraction):\n\"\"\"Configurations available for an Extraction from SAP BW.\n    It inherits from SAPBWExtraction configurations, so it can use\n    and/or overwrite those configurations.\n    These configurations cover:\n    - latest_timestamp_input_col: the column containing the actrequest timestamp\n        in the dataset in latest_timestamp_data_location. Default:\n        \"actrequest_timestamp\".\n    - act_request_table: the name of the SAP BW activation requests table.\n        Composed of database.table. Default: SAPPHA.RSODSACTREQ.\n    - request_col_name: name of the column having the request to join\n        with the activation request table. Default: actrequest.\n    - act_req_join_condition: the join condition into activation table\n        can be changed using this property.\n        Default: 'changelog_tbl.request = act_req.request_col_name'.\n    - odsobject: name of BW Object, used for joining with the activation request\n        table to get the max actrequest_timestamp to consider while filtering\n        the changelog table.\n    - include_changelog_tech_cols: whether to include the technical columns\n        (usually coming from the changelog) table or not. Default: True.\n    - extra_cols_act_request: list of columns to be added from act request table.\n        It needs to contain the prefix \"act_req.\". E.g. \"act_req.col1\n        as column_one, act_req.col2 as column_two\".\n    - get_timestamp_from_act_request: whether to get init timestamp\n        from act request table or assume current/given timestamp.\n    - sap_bw_schema: sap bw schema. Default: SAPPHA.\n    - max_timestamp_custom_schema: the custom schema to apply on the calculation of\n        the max timestamp to consider for the delta extractions.\n        Default: timestamp DECIMAL(23,0).\n    - default_max_timestamp: the timestamp to use as default, when it is not possible\n        to derive one.\n    \"\"\"\nlatest_timestamp_input_col: str = \"actrequest_timestamp\"\nact_request_table: str = \"SAPPHA.RSODSACTREQ\"\nrequest_col_name: str = \"actrequest\"\nact_req_join_condition: Optional[str] = None\nodsobject: Optional[str] = None\ninclude_changelog_tech_cols: bool = True\nextra_cols_act_request: Optional[str] = None\nget_timestamp_from_act_request: bool = False\nsap_bw_schema: str = \"SAPPHA\"\nmax_timestamp_custom_schema: str = \"timestamp DECIMAL(15,0)\"\ndefault_max_timestamp: str = \"197000000000000\"\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_bw_extraction_utils.html#packages.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils","title":"<code>SAPBWExtractionUtils</code>","text":"<p>         Bases: <code>JDBCExtractionUtils</code></p> <p>Utils for managing data extraction from particularly relevant JDBC sources.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_bw_extraction_utils.py</code> <pre><code>class SAPBWExtractionUtils(JDBCExtractionUtils):\n\"\"\"Utils for managing data extraction from particularly relevant JDBC sources.\"\"\"\ndef __init__(self, sap_bw_extraction: SAPBWExtraction):\n\"\"\"Construct SAPBWExtractionUtils.\n        Args:\n            sap_bw_extraction: SAP BW Extraction configurations.\n        \"\"\"\nself._LOGGER: Logger = LoggingHandler(__name__).get_logger()\nself._BW_EXTRACTION = sap_bw_extraction\nself._BW_EXTRACTION.changelog_table = self.get_changelog_table()\nself._MAX_TIMESTAMP_QUERY = f\"\"\" --# nosec\n                (SELECT COALESCE(MAX(timestamp),\n{self._BW_EXTRACTION.default_max_timestamp}) as timestamp\n                FROM {self._BW_EXTRACTION.act_request_table}\n                WHERE odsobject = '{self._BW_EXTRACTION.odsobject}'\n                 AND operation = 'A' AND status = '0')\n            \"\"\"  # nosec: B608\nsuper().__init__(sap_bw_extraction)\ndef get_changelog_table(self) -&gt; str:\n\"\"\"Get the changelog table, given an odsobject.\n        Returns:\n             String to use as changelog_table.\n        \"\"\"\nif (\nself._BW_EXTRACTION.odsobject is not None\nand self._BW_EXTRACTION.changelog_table is None\nand self._BW_EXTRACTION.extraction_type != JDBCExtractionType.INIT.value\n):\nescaped_odsobject = copy(self._BW_EXTRACTION.odsobject).replace(\"_\", \"$_\")\nif self._BW_EXTRACTION.sap_bw_schema:\nsystem_table = f\"{self._BW_EXTRACTION.sap_bw_schema}.RSTSODS\"\nelse:\nsystem_table = \"RSTSODS\"\njdbc_args = {\n\"url\": self._BW_EXTRACTION.url,\n\"table\": f\"\"\" -- # nosec\n                    (SELECT ODSNAME_TECH\n                    FROM {system_table}\n                    WHERE ODSNAME LIKE '8{escaped_odsobject}$_%'\n                        ESCAPE '$' AND USERAPP = 'CHANGELOG' AND VERSION = '000')\n                \"\"\",  # nosec: B608\n\"properties\": {\n\"user\": self._BW_EXTRACTION.user,\n\"password\": self._BW_EXTRACTION.password,\n\"driver\": self._BW_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nchangelog_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"changelog_table\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\n)\n)\nchangelog_table = (\nf'{self._BW_EXTRACTION.sap_bw_schema}.\"{changelog_df.first()[0]}\"'\nif self._BW_EXTRACTION.sap_bw_schema\nelse str(changelog_df.first()[0])\n)\nelse:\nchangelog_table = (\nself._BW_EXTRACTION.changelog_table\nif self._BW_EXTRACTION.changelog_table\nelse f\"{self._BW_EXTRACTION.dbtable}_cl\"\n)\nself._LOGGER.info(f\"The changelog table derived is: '{changelog_table}'\")\nreturn changelog_table\n@staticmethod\ndef get_odsobject(input_spec_opt: dict) -&gt; str:\n\"\"\"Get the odsobject based on the provided options.\n        With the table name we may also get the db name, so we need to split.\n        Moreover, there might be the need for people to specify odsobject if\n        it is different from the dbtable.\n        Args:\n            input_spec_opt: options from the input_spec.\n        Returns:\n            A string with the odsobject.\n        \"\"\"\nreturn str(\ninput_spec_opt[\"dbtable\"].split(\".\")[1]\nif len(input_spec_opt[\"dbtable\"].split(\".\")) &gt; 1\nelse input_spec_opt[\"dbtable\"]\n)\ndef _get_init_query(self) -&gt; Tuple[str, str]:\n\"\"\"Get a query to do an init load based on a DSO on a SAP BW system.\n        Returns:\n            A query to submit to SAP BW for the initial data extraction. The query\n            is enclosed in parentheses so that Spark treats it as a table and supports\n            it in the dbtable option.\n        \"\"\"\nif self._BW_EXTRACTION.get_timestamp_from_act_request:\n# check if we are dealing with a DSO of type Write Optimised\nif self._BW_EXTRACTION.dbtable == self._BW_EXTRACTION.changelog_table:\nextraction_query = self._get_init_extraction_query_act_req_timestamp()\nelse:\nraise AttributeError(\n\"Not able to get the extraction query. The option \"\n\"'get_timestamp_from_act_request' is only \"\n\"available/useful for DSOs of type Write Optimised.\"\n)\nelse:\nextraction_query = self._get_init_extraction_query()\npredicates_query = f\"\"\"\n        (SELECT DISTINCT({self._BW_EXTRACTION.partition_column})\n        FROM {self._BW_EXTRACTION.dbtable} t)\n        \"\"\"  # nosec: B608\nreturn extraction_query, predicates_query\ndef _get_init_extraction_query(self) -&gt; str:\n\"\"\"Get extraction query based on given/current timestamp.\n        Returns:\n            A query to submit to SAP BW for the initial data extraction.\n        \"\"\"\nchangelog_tech_cols = (\nf\"\"\"'0' AS request,\n                CAST({self._BW_EXTRACTION.extraction_timestamp} AS DECIMAL(15, 0))\n                 AS actrequest_timestamp,\n                '0' AS datapakid,\n                0 AS partno,\n                0 AS record,\"\"\"\nif self._BW_EXTRACTION.include_changelog_tech_cols\nelse f\"CAST({self._BW_EXTRACTION.extraction_timestamp} \"\nf\"AS DECIMAL(15, 0))\"\nf\" AS actrequest_timestamp,\"\n)\nextraction_query = f\"\"\"\n                (SELECT t.*,\n{changelog_tech_cols}\n                    CAST({self._BW_EXTRACTION.extraction_timestamp}\n                        AS DECIMAL(15, 0)) AS extraction_start_timestamp\n                FROM {self._BW_EXTRACTION.dbtable} t\n                )\"\"\"  # nosec: B608\nreturn extraction_query\ndef _get_init_extraction_query_act_req_timestamp(self) -&gt; str:\n\"\"\"Get extraction query assuming the init timestamp from act_request table.\n        Returns:\n            A query to submit to SAP BW for the initial data extraction from\n            write optimised DSOs, receiving the actrequest_timestamp from\n            the activation requests table.\n        \"\"\"\nextraction_query = f\"\"\"\n            (SELECT t.*,\n                act_req.timestamp as actrequest_timestamp,\n                CAST({self._BW_EXTRACTION.extraction_timestamp} AS DECIMAL(15, 0))\n                 AS extraction_start_timestamp\n            FROM {self._BW_EXTRACTION.dbtable} t\n            JOIN {self._BW_EXTRACTION.act_request_table} AS act_req ON\n                t.request = act_req.{self._BW_EXTRACTION.request_col_name}\n            WHERE act_req.odsobject = '{self._BW_EXTRACTION.odsobject}'\n                AND operation = 'A' AND status = '0'\n            )\"\"\"  # nosec: B608\nreturn extraction_query\ndef _get_delta_query(self) -&gt; Tuple[str, str]:\n\"\"\"Get a delta query for an SAP BW DSO.\n        An SAP BW DSO requires a join with a special type of table often called\n        activation requests table, in which BW tracks down the timestamps associated\n        with the several data loads that were performed into BW. Because the changelog\n        table only contains the active request id, and that cannot be sorted by the\n        downstream consumers to figure out the latest change, we need to join the\n        changelog table with this special table to get the activation requests\n        timestamps to then use them to figure out the latest changes in the delta load\n        logic afterwards.\n        Additionally, we also need to know which was the latest timestamp already loaded\n        into the lakehouse bronze layer. The latest timestamp should always be available\n        in the bronze dataset itself or in a dataset that tracks down all the actrequest\n        timestamps that were already loaded. So we get the max value out of the\n        respective actrequest timestamp column in that dataset.\n        Returns:\n            A query to submit to SAP BW for the delta data extraction. The query\n            is enclosed in parentheses so that Spark treats it as a table and supports\n            it in the dbtable option.\n        \"\"\"\nif not self._BW_EXTRACTION.min_timestamp:\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nlatest_timestamp_data_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"data_with_latest_timestamp\",\ndata_format=self._BW_EXTRACTION.latest_timestamp_data_format,\nread_type=ReadType.BATCH.value,\nlocation=self._BW_EXTRACTION.latest_timestamp_data_location,\n)\n)\nmin_timestamp = latest_timestamp_data_df.transform(\nAggregators.get_max_value(\nself._BW_EXTRACTION.latest_timestamp_input_col\n)\n).first()[0]\nelse:\nmin_timestamp = self._BW_EXTRACTION.min_timestamp\nmax_timestamp = (\nself._BW_EXTRACTION.max_timestamp\nif self._BW_EXTRACTION.max_timestamp\nelse self._get_max_timestamp(self._MAX_TIMESTAMP_QUERY)\n)\nif self._BW_EXTRACTION.act_req_join_condition:\njoin_condition = f\"{self._BW_EXTRACTION.act_req_join_condition}\"\nelse:\njoin_condition = (\nf\"changelog_tbl.request = \"\nf\"act_req.{self._BW_EXTRACTION.request_col_name}\"\n)\nbase_query = f\"\"\" --# nosec\n        FROM {self._BW_EXTRACTION.changelog_table} AS changelog_tbl\n        JOIN {self._BW_EXTRACTION.act_request_table} AS act_req\n            ON {join_condition}\n        WHERE act_req.odsobject = '{self._BW_EXTRACTION.odsobject}'\n            AND act_req.timestamp &gt; {min_timestamp}\n            AND act_req.timestamp &lt;= {max_timestamp}\n            AND operation = 'A' AND status = '0')\n        \"\"\"\nmain_cols = f\"\"\"\n            (SELECT changelog_tbl.*,\n                act_req.TIMESTAMP AS actrequest_timestamp,\n                CAST({self._BW_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0))\n                    AS extraction_start_timestamp\n            \"\"\"\n# We join the main columns considered for the extraction with\n# extra_cols_act_request that people might want to use, filtering to only\n# add the comma and join the strings, in case extra_cols_act_request is\n# not None or empty.\nextraction_query_cols = \",\".join(\nfilter(None, [main_cols, self._BW_EXTRACTION.extra_cols_act_request])\n)\nextraction_query = extraction_query_cols + base_query\npredicates_query = f\"\"\"\n        (SELECT DISTINCT({self._BW_EXTRACTION.partition_column})\n{base_query}\n        \"\"\"\nreturn extraction_query, predicates_query\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_bw_extraction_utils.html#packages.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.__init__","title":"<code>__init__(sap_bw_extraction)</code>","text":"<p>Construct SAPBWExtractionUtils.</p> <p>Parameters:</p> Name Type Description Default <code>sap_bw_extraction</code> <code>SAPBWExtraction</code> <p>SAP BW Extraction configurations.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_bw_extraction_utils.py</code> <pre><code>def __init__(self, sap_bw_extraction: SAPBWExtraction):\n\"\"\"Construct SAPBWExtractionUtils.\n    Args:\n        sap_bw_extraction: SAP BW Extraction configurations.\n    \"\"\"\nself._LOGGER: Logger = LoggingHandler(__name__).get_logger()\nself._BW_EXTRACTION = sap_bw_extraction\nself._BW_EXTRACTION.changelog_table = self.get_changelog_table()\nself._MAX_TIMESTAMP_QUERY = f\"\"\" --# nosec\n            (SELECT COALESCE(MAX(timestamp),\n{self._BW_EXTRACTION.default_max_timestamp}) as timestamp\n            FROM {self._BW_EXTRACTION.act_request_table}\n            WHERE odsobject = '{self._BW_EXTRACTION.odsobject}'\n             AND operation = 'A' AND status = '0')\n        \"\"\"  # nosec: B608\nsuper().__init__(sap_bw_extraction)\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_bw_extraction_utils.html#packages.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.get_changelog_table","title":"<code>get_changelog_table()</code>","text":"<p>Get the changelog table, given an odsobject.</p> <p>Returns:</p> Type Description <code>str</code> <p>String to use as changelog_table.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_bw_extraction_utils.py</code> <pre><code>def get_changelog_table(self) -&gt; str:\n\"\"\"Get the changelog table, given an odsobject.\n    Returns:\n         String to use as changelog_table.\n    \"\"\"\nif (\nself._BW_EXTRACTION.odsobject is not None\nand self._BW_EXTRACTION.changelog_table is None\nand self._BW_EXTRACTION.extraction_type != JDBCExtractionType.INIT.value\n):\nescaped_odsobject = copy(self._BW_EXTRACTION.odsobject).replace(\"_\", \"$_\")\nif self._BW_EXTRACTION.sap_bw_schema:\nsystem_table = f\"{self._BW_EXTRACTION.sap_bw_schema}.RSTSODS\"\nelse:\nsystem_table = \"RSTSODS\"\njdbc_args = {\n\"url\": self._BW_EXTRACTION.url,\n\"table\": f\"\"\" -- # nosec\n                (SELECT ODSNAME_TECH\n                FROM {system_table}\n                WHERE ODSNAME LIKE '8{escaped_odsobject}$_%'\n                    ESCAPE '$' AND USERAPP = 'CHANGELOG' AND VERSION = '000')\n            \"\"\",  # nosec: B608\n\"properties\": {\n\"user\": self._BW_EXTRACTION.user,\n\"password\": self._BW_EXTRACTION.password,\n\"driver\": self._BW_EXTRACTION.driver,\n},\n}\nfrom lakehouse_engine.io.reader_factory import ReaderFactory\nchangelog_df = ReaderFactory.get_data(\nInputSpec(\nspec_id=\"changelog_table\",\ndata_format=InputFormat.JDBC.value,\nread_type=ReadType.BATCH.value,\njdbc_args=jdbc_args,\n)\n)\nchangelog_table = (\nf'{self._BW_EXTRACTION.sap_bw_schema}.\"{changelog_df.first()[0]}\"'\nif self._BW_EXTRACTION.sap_bw_schema\nelse str(changelog_df.first()[0])\n)\nelse:\nchangelog_table = (\nself._BW_EXTRACTION.changelog_table\nif self._BW_EXTRACTION.changelog_table\nelse f\"{self._BW_EXTRACTION.dbtable}_cl\"\n)\nself._LOGGER.info(f\"The changelog table derived is: '{changelog_table}'\")\nreturn changelog_table\n</code></pre>"},{"location":"reference/packages/utils/extraction/sap_bw_extraction_utils.html#packages.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.get_odsobject","title":"<code>get_odsobject(input_spec_opt)</code>  <code>staticmethod</code>","text":"<p>Get the odsobject based on the provided options.</p> <p>With the table name we may also get the db name, so we need to split. Moreover, there might be the need for people to specify odsobject if it is different from the dbtable.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec_opt</code> <code>dict</code> <p>options from the input_spec.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string with the odsobject.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sap_bw_extraction_utils.py</code> <pre><code>@staticmethod\ndef get_odsobject(input_spec_opt: dict) -&gt; str:\n\"\"\"Get the odsobject based on the provided options.\n    With the table name we may also get the db name, so we need to split.\n    Moreover, there might be the need for people to specify odsobject if\n    it is different from the dbtable.\n    Args:\n        input_spec_opt: options from the input_spec.\n    Returns:\n        A string with the odsobject.\n    \"\"\"\nreturn str(\ninput_spec_opt[\"dbtable\"].split(\".\")[1]\nif len(input_spec_opt[\"dbtable\"].split(\".\")) &gt; 1\nelse input_spec_opt[\"dbtable\"]\n)\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html","title":"Sftp extraction utils","text":"<p>Utilities module for SFTP extraction processes.</p>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter","title":"<code>SFTPExtractionFilter</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Standardize the types of filters we can have from a SFTP source.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>class SFTPExtractionFilter(Enum):\n\"\"\"Standardize the types of filters we can have from a SFTP source.\"\"\"\nfile_name_contains = \"file_name_contains\"\nLATEST_FILE = \"latest_file\"\nEARLIEST_FILE = \"earliest_file\"\nGREATER_THAN = \"date_time_gt\"\nLOWER_THAN = \"date_time_lt\"\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils","title":"<code>SFTPExtractionUtils</code>","text":"<p>         Bases: <code>object</code></p> <p>Utils for managing data extraction from particularly relevant SFTP sources.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>class SFTPExtractionUtils(object):\n\"\"\"Utils for managing data extraction from particularly relevant SFTP sources.\"\"\"\n_logger: Logger = LoggingHandler(__name__).get_logger()\n@classmethod\ndef get_files_list(\ncls, sftp: SFTPClient, remote_path: str, options_args: dict\n) -&gt; Set[str]:\n\"\"\"Get a list of files to be extracted from SFTP.\n        The arguments (options_args) to list files are:\n        - date_time_gt(str):\n            Filter the files greater than the string datetime\n            formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".\n        - date_time_lt(str):\n            Filter the files lower than the string datetime\n            formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".\n        - earliest_file(bool):\n            Filter the earliest dated file in the directory.\n        - file_name_contains(str):\n            Filter files when match the pattern.\n        - latest_file(bool):\n            Filter the most recent dated file in the directory.\n        - sub_dir(bool):\n            When true, the engine will search files into subdirectories\n            of the remote_path.\n            It will consider one level below the remote_path.\n            When sub_dir is used with latest_file/earliest_file argument,\n            the engine will retrieve the latest_file/earliest_file\n            for each subdirectory.\n        Args:\n            sftp: the SFTP client object.\n            remote_path: path of files to be filtered.\n            options_args: options from the acon.\n        Returns:\n            A list containing the file names to be passed to Spark.\n        \"\"\"\nall_items, folder_path = cls._get_folder_items(remote_path, sftp, options_args)\nfiltered_files: Set[str] = set()\ntry:\nfor item, folder in zip(all_items, folder_path):\nfile_contains = cls._file_has_pattern(item, options_args)\nfile_in_interval = cls._file_in_date_interval(item, options_args)\nif file_contains and file_in_interval:\nfiltered_files.add(folder + item.filename)\nif (\nSFTPExtractionFilter.EARLIEST_FILE.value in options_args.keys()\nor SFTPExtractionFilter.LATEST_FILE.value in options_args.keys()\n):\nfiltered_files = cls._get_earliest_latest_file(\nsftp, options_args, filtered_files, folder_path\n)\nexcept Exception as e:\ncls._logger.error(f\"SFTP list_files EXCEPTION: - {e}\")\nreturn filtered_files\n@classmethod\ndef get_sftp_client(\ncls,\noptions_args: dict,\n) -&gt; Tuple[SFTPClient, Transport]:\n\"\"\"Get the SFTP client.\n        The SFTP client is used to open an SFTP session across an open\n        SSH Transport and perform remote file operations.\n        Args:\n            options_args: dictionary containing SFTP connection parameters.\n                The Paramiko arguments expected to connect are:\n                - \"hostname\": the server to connect to.\n                - \"port\": the server port to connect to.\n                - \"username\": the username to authenticate as.\n                - \"password\": used for password authentication.\n                - \"pkey\": optional - an optional public key to use for\n                    authentication.\n                - \"passphrase\" \u2013 optional - options used for decrypting private\n                    keys.\n                - \"key_filename\" \u2013 optional - the filename, or list of filenames,\n                    of optional private key(s) and/or certs to try for\n                    authentication.\n                - \"timeout\" \u2013 an optional timeout (in seconds) for the TCP connect.\n                - \"allow_agent\" \u2013 optional - set to False to disable\n                    connecting to the SSH agent.\n                - \"look_for_keys\" \u2013 optional - set to False to disable searching\n                    for discoverable private key files in ~/.ssh/.\n                - \"compress\" \u2013 optional - set to True to turn on compression.\n                - \"sock\" - optional - an open socket or socket-like object\n                    to use for communication to the target host.\n                - \"gss_auth\" \u2013 optional - True if you want to use GSS-API\n                    authentication.\n                - \"gss_kex\" \u2013 optional - Perform GSS-API Key Exchange and\n                    user authentication.\n                - \"gss_deleg_creds\" \u2013 optional - Delegate GSS-API client\n                    credentials or not.\n                - \"gss_host\" \u2013 optional - The targets name in the kerberos database.\n                - \"gss_trust_dns\" \u2013 optional - Indicates whether or\n                    not the DNS is trusted to securely canonicalize the name of the\n                    host being connected to (default True).\n                - \"banner_timeout\" \u2013 an optional timeout (in seconds)\n                    to wait for the SSH banner to be presented.\n                - \"auth_timeout\" \u2013 an optional timeout (in seconds)\n                    to wait for an authentication response.\n                - \"disabled_algorithms\" \u2013 an optional dict passed directly to\n                    Transport and its keyword argument of the same name.\n                - \"transport_factory\" \u2013 an optional callable which is handed a\n                    subset of the constructor arguments (primarily those related\n                    to the socket, GSS functionality, and algorithm selection)\n                    and generates a Transport instance to be used by this client.\n                    Defaults to Transport.__init__.\n                The parameter to specify the private key is expected to be in\n                RSA format. Attempting a connection with a blank host key is\n                not allowed unless the argument \"add_auto_policy\" is explicitly\n                set to True.\n        Returns:\n            sftp -&gt; a new SFTPClient session object.\n            transport -&gt; the Transport for this connection.\n        \"\"\"\nssh_client = p.SSHClient()\ntry:\nif not options_args.get(\"pkey\") and not options_args.get(\"add_auto_policy\"):\nraise WrongArgumentsException(\n\"Get SFTP Client: No host key (pkey) was provided and the \"\n+ \"add_auto_policy property is false.\"\n)\nif options_args.get(\"pkey\") and not options_args.get(\"key_type\"):\nraise WrongArgumentsException(\n\"Get SFTP Client: The key_type must be provided when \"\n+ \"the host key (pkey) is provided.\"\n)\nif options_args.get(\"pkey\", None) and options_args.get(\"key_type\", None):\nkey = cls._get_host_keys(\noptions_args.get(\"pkey\", None), options_args.get(\"key_type\", None)\n)\nssh_client.get_host_keys().add(\nhostname=f\"[{options_args.get('hostname')}]:\"\n+ f\"{options_args.get('port')}\",\nkeytype=\"ssh-rsa\",\nkey=key,\n)\nelif options_args.get(\"add_auto_policy\", None):\nssh_client.load_system_host_keys()\nssh_client.set_missing_host_key_policy(p.WarningPolicy())  # nosec: B507\nelse:\nssh_client.load_system_host_keys()\nssh_client.set_missing_host_key_policy(p.RejectPolicy())\nssh_client.connect(\nhostname=options_args.get(\"hostname\"),\nport=options_args.get(\"port\", 22),\nusername=options_args.get(\"username\", None),\npassword=options_args.get(\"password\", None),\nkey_filename=options_args.get(\"key_filename\", None),\ntimeout=options_args.get(\"timeout\", None),\nallow_agent=options_args.get(\"allow_agent\", True),\nlook_for_keys=options_args.get(\"look_for_keys\", True),\ncompress=options_args.get(\"compress\", False),\nsock=options_args.get(\"sock\", None),\ngss_auth=options_args.get(\"gss_auth\", False),\ngss_kex=options_args.get(\"gss_kex\", False),\ngss_deleg_creds=options_args.get(\"gss_deleg_creds\", False),\ngss_host=options_args.get(\"gss_host\", False),\nbanner_timeout=options_args.get(\"banner_timeout\", None),\nauth_timeout=options_args.get(\"auth_timeout\", None),\ngss_trust_dns=options_args.get(\"gss_trust_dns\", None),\npassphrase=options_args.get(\"passphrase\", None),\ndisabled_algorithms=options_args.get(\"disabled_algorithms\", None),\ntransport_factory=options_args.get(\"transport_factory\", None),\n)\nsftp = ssh_client.open_sftp()\ntransport = ssh_client.get_transport()\nexcept ConnectionError as e:\ncls._logger.error(e)\nraise\nreturn sftp, transport\n@classmethod\ndef validate_format(cls, files_format: str) -&gt; str:\n\"\"\"Validate the file extension based on the format definitions.\n        Args:\n            files_format: a string containing the file extension.\n        Returns:\n            The string validated and formatted.\n        \"\"\"\nformats_allowed = [\nSFTPInputFormat.CSV.value,\nSFTPInputFormat.FWF.value,\nSFTPInputFormat.JSON.value,\nSFTPInputFormat.XML.value,\n]\nif files_format not in formats_allowed:\nraise WrongArgumentsException(\nf\"The formats allowed for SFTP are {formats_allowed}.\"\n)\nreturn files_format\n@classmethod\ndef validate_location(cls, location: str) -&gt; str:\n\"\"\"Validate the location. Add \"/\" in the case it does not exist.\n        Args:\n            location: file path.\n        Returns:\n            The location validated.\n        \"\"\"\nreturn location if location.rfind(\"/\") == len(location) - 1 else location + \"/\"\n@classmethod\ndef _file_has_pattern(cls, item: SFTPAttributes, options_args: dict) -&gt; bool:\n\"\"\"Check if a file follows the pattern used for filtering.\n        Args:\n            item: item available in SFTP directory.\n            options_args: options from the acon.\n        Returns:\n            A boolean telling whether the file contains a pattern or not.\n        \"\"\"\nfile_to_consider = True\nif SFTPExtractionFilter.file_name_contains.value in options_args.keys():\nif not (\noptions_args.get(SFTPExtractionFilter.file_name_contains.value)\nin item.filename\nand (S_ISREG(item.st_mode) or cls._is_compressed(item.filename))\n):\nfile_to_consider = False\nreturn file_to_consider\n@classmethod\ndef _file_in_date_interval(\ncls,\nitem: SFTPAttributes,\noptions_args: dict,\n) -&gt; bool:\n\"\"\"Check if the file is in the expected date interval.\n        The logic is applied based on the arguments greater_than and lower_than.\n        i.e:\n        - if greater_than and lower_than have values,\n        then it performs a between.\n        - if only lower_than has values,\n        then only values lower than the input value will be retrieved.\n        - if only greater_than has values,\n        then only values greater than the input value will be retrieved.\n        Args:\n            item: item available in SFTP directory.\n            options_args: options from the acon.\n        Returns:\n            A boolean telling whether the file is in the expected date interval or not.\n        \"\"\"\nfile_to_consider = True\nif (\nSFTPExtractionFilter.LOWER_THAN.value in options_args.keys()\nor SFTPExtractionFilter.GREATER_THAN.value in options_args.keys()\nand (S_ISREG(item.st_mode) or cls._is_compressed(item.filename))\n):\nlower_than = options_args.get(\nSFTPExtractionFilter.LOWER_THAN.value, \"9999-12-31\"\n)\ngreater_than = options_args.get(\nSFTPExtractionFilter.GREATER_THAN.value, \"1900-01-01\"\n)\nfile_date = datetime.fromtimestamp(item.st_mtime)\nif not (\n(\nlower_than == greater_than\nand cls._validate_date(greater_than)\n&lt;= file_date\n&lt;= cls._validate_date(lower_than)\n)\nor (\ncls._validate_date(greater_than)\n&lt; file_date\n&lt; cls._validate_date(lower_than)\n)\n):\nfile_to_consider = False\nreturn file_to_consider\n@classmethod\ndef _get_earliest_latest_file(\ncls,\nsftp: SFTPClient,\noptions_args: dict,\nlist_filter_files: Set[str],\nfolder_path: List,\n) -&gt; Set[str]:\n\"\"\"Get the earliest or latest file of a directory.\n        Args:\n            sftp: the SFTP client object.\n            options_args: options from the acon.\n            list_filter_files: set of file names to filter from.\n            folder_path: the location of files.\n        Returns:\n            A set containing the earliest/latest file name.\n        \"\"\"\nlist_earl_lat_files: Set[str] = set()\nfor folder in folder_path:\nfile_date = 0\nfile_name = \"\"\nall_items, file_path = cls._get_folder_items(\nf\"{folder}\", sftp, options_args\n)\nfor item in all_items:\nif (\nfolder + item.filename in list_filter_files\nand (S_ISREG(item.st_mode) or cls._is_compressed(item.filename))\nand (\noptions_args.get(\"earliest_file\")\nand (file_date == 0 or item.st_mtime &lt; file_date)\n)\nor (\noptions_args.get(\"latest_file\")\nand (file_date == 0 or item.st_mtime &gt; file_date)\n)\n):\nfile_date = item.st_mtime\nfile_name = folder + item.filename\nlist_earl_lat_files.add(file_name)\nreturn list_earl_lat_files\n@classmethod\ndef _get_folder_items(\ncls, remote_path: str, sftp: SFTPClient, options_args: dict\n) -&gt; Tuple:\n\"\"\"Get the files and the directory to be processed.\n        Args:\n            remote_path: root folder path.\n            sftp: a SFTPClient session object.\n            options_args: options from the acon.\n        Returns:\n            A tuple with a list of items (file object) and a list of directories.\n        \"\"\"\nsub_dir = options_args.get(\"sub_dir\", False)\nall_items: List[SFTPAttributes] = sftp.listdir_attr(remote_path)\nitems: List[SFTPAttributes] = []\nfolders: List = []\nfor item in all_items:\nis_dir = stat.S_ISDIR(item.st_mode)\nif is_dir and sub_dir and not item.filename.endswith((\".gz\", \".zip\")):\ndirs = sftp.listdir_attr(f\"{remote_path}{item.filename}\")\nfor file in dirs:\nitems.append(file)\nfolders.append(f\"{remote_path}{item.filename}/\")\nelse:\nitems.append(item)\nfolders.append(remote_path)\nreturn items, folders\n@classmethod\ndef _get_host_keys(cls, pkey: str, key_type: str) -&gt; PKey:\n\"\"\"Get the pkey that will be added to the server.\n        Args:\n            pkey: a string with a host key value.\n            key_type: the type of key (rsa or ed25519).\n        Returns:\n            A PKey that will be used to authenticate the connection.\n        \"\"\"\nkey: Union[RSAKey, Ed25519Key] = None\nif pkey and key_type.lower() == \"rsa\":\nb_pkey = bytes(pkey, \"UTF-8\")\nkey = p.RSAKey(data=decodebytes(b_pkey))\nelif pkey and key_type.lower() == \"ed25519\":\nb_pkey = bytes(pkey, \"UTF-8\")\nkey = p.Ed25519Key(data=decodebytes(b_pkey))\nreturn key\n@classmethod\ndef _is_compressed(cls, filename: str) -&gt; Any:\n\"\"\"Validate if it is a compressed file.\n        Args:\n            filename: name of the file to be validated.\n        Returns:\n            A boolean with the result.\n        \"\"\"\nreturn filename.endswith((\".gz\", \".zip\"))\n@classmethod\ndef _validate_date(cls, date_text: str) -&gt; datetime:\n\"\"\"Validate the input date format.\n        Args:\n            date_text: a string with the date or datetime value.\n            The expected formats are:\n                YYYY-MM-DD and YYYY-MM-DD HH:MM:SS\n        Returns:\n            The datetime validated and formatted.\n        \"\"\"\nfor fmt in (\"%Y-%m-%d\", \"%Y-%m-%d %H:%M:%S\"):\ntry:\nif date_text is not None:\nreturn datetime.strptime(date_text, fmt)\nexcept ValueError:\npass\nraise ValueError(\n\"Incorrect data format, should be YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\"\n)\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.get_files_list","title":"<code>get_files_list(sftp, remote_path, options_args)</code>  <code>classmethod</code>","text":"<p>Get a list of files to be extracted from SFTP.</p> <p>The arguments (options_args) to list files are:</p> <ul> <li>date_time_gt(str):     Filter the files greater than the string datetime     formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".</li> <li>date_time_lt(str):     Filter the files lower than the string datetime     formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".</li> <li>earliest_file(bool):     Filter the earliest dated file in the directory.</li> <li>file_name_contains(str):     Filter files when match the pattern.</li> <li>latest_file(bool):     Filter the most recent dated file in the directory.</li> <li>sub_dir(bool):     When true, the engine will search files into subdirectories     of the remote_path.     It will consider one level below the remote_path.     When sub_dir is used with latest_file/earliest_file argument,     the engine will retrieve the latest_file/earliest_file     for each subdirectory.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sftp</code> <code>SFTPClient</code> <p>the SFTP client object.</p> required <code>remote_path</code> <code>str</code> <p>path of files to be filtered.</p> required <code>options_args</code> <code>dict</code> <p>options from the acon.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>A list containing the file names to be passed to Spark.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>@classmethod\ndef get_files_list(\ncls, sftp: SFTPClient, remote_path: str, options_args: dict\n) -&gt; Set[str]:\n\"\"\"Get a list of files to be extracted from SFTP.\n    The arguments (options_args) to list files are:\n    - date_time_gt(str):\n        Filter the files greater than the string datetime\n        formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".\n    - date_time_lt(str):\n        Filter the files lower than the string datetime\n        formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".\n    - earliest_file(bool):\n        Filter the earliest dated file in the directory.\n    - file_name_contains(str):\n        Filter files when match the pattern.\n    - latest_file(bool):\n        Filter the most recent dated file in the directory.\n    - sub_dir(bool):\n        When true, the engine will search files into subdirectories\n        of the remote_path.\n        It will consider one level below the remote_path.\n        When sub_dir is used with latest_file/earliest_file argument,\n        the engine will retrieve the latest_file/earliest_file\n        for each subdirectory.\n    Args:\n        sftp: the SFTP client object.\n        remote_path: path of files to be filtered.\n        options_args: options from the acon.\n    Returns:\n        A list containing the file names to be passed to Spark.\n    \"\"\"\nall_items, folder_path = cls._get_folder_items(remote_path, sftp, options_args)\nfiltered_files: Set[str] = set()\ntry:\nfor item, folder in zip(all_items, folder_path):\nfile_contains = cls._file_has_pattern(item, options_args)\nfile_in_interval = cls._file_in_date_interval(item, options_args)\nif file_contains and file_in_interval:\nfiltered_files.add(folder + item.filename)\nif (\nSFTPExtractionFilter.EARLIEST_FILE.value in options_args.keys()\nor SFTPExtractionFilter.LATEST_FILE.value in options_args.keys()\n):\nfiltered_files = cls._get_earliest_latest_file(\nsftp, options_args, filtered_files, folder_path\n)\nexcept Exception as e:\ncls._logger.error(f\"SFTP list_files EXCEPTION: - {e}\")\nreturn filtered_files\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.get_sftp_client","title":"<code>get_sftp_client(options_args)</code>  <code>classmethod</code>","text":"<p>Get the SFTP client.</p> <p>The SFTP client is used to open an SFTP session across an open SSH Transport and perform remote file operations.</p> <p>Parameters:</p> Name Type Description Default <code>options_args</code> <code>dict</code> <p>dictionary containing SFTP connection parameters. The Paramiko arguments expected to connect are:</p> <ul> <li>\"hostname\": the server to connect to.</li> <li>\"port\": the server port to connect to.</li> <li>\"username\": the username to authenticate as.</li> <li>\"password\": used for password authentication.</li> <li>\"pkey\": optional - an optional public key to use for     authentication.</li> <li>\"passphrase\" \u2013 optional - options used for decrypting private     keys.</li> <li>\"key_filename\" \u2013 optional - the filename, or list of filenames,     of optional private key(s) and/or certs to try for     authentication.</li> <li>\"timeout\" \u2013 an optional timeout (in seconds) for the TCP connect.</li> <li>\"allow_agent\" \u2013 optional - set to False to disable     connecting to the SSH agent.</li> <li>\"look_for_keys\" \u2013 optional - set to False to disable searching     for discoverable private key files in ~/.ssh/.</li> <li>\"compress\" \u2013 optional - set to True to turn on compression.</li> <li>\"sock\" - optional - an open socket or socket-like object     to use for communication to the target host.</li> <li>\"gss_auth\" \u2013 optional - True if you want to use GSS-API     authentication.</li> <li>\"gss_kex\" \u2013 optional - Perform GSS-API Key Exchange and     user authentication.</li> <li>\"gss_deleg_creds\" \u2013 optional - Delegate GSS-API client     credentials or not.</li> <li>\"gss_host\" \u2013 optional - The targets name in the kerberos database.</li> <li>\"gss_trust_dns\" \u2013 optional - Indicates whether or     not the DNS is trusted to securely canonicalize the name of the     host being connected to (default True).</li> <li>\"banner_timeout\" \u2013 an optional timeout (in seconds)     to wait for the SSH banner to be presented.</li> <li>\"auth_timeout\" \u2013 an optional timeout (in seconds)     to wait for an authentication response.</li> <li>\"disabled_algorithms\" \u2013 an optional dict passed directly to     Transport and its keyword argument of the same name.</li> <li>\"transport_factory\" \u2013 an optional callable which is handed a     subset of the constructor arguments (primarily those related     to the socket, GSS functionality, and algorithm selection)     and generates a Transport instance to be used by this client.     Defaults to Transport.init.</li> </ul> <p>The parameter to specify the private key is expected to be in RSA format. Attempting a connection with a blank host key is not allowed unless the argument \"add_auto_policy\" is explicitly set to True.</p> required <p>Returns:</p> Type Description <code>SFTPClient</code> <p>sftp -&gt; a new SFTPClient session object.</p> <code>Transport</code> <p>transport -&gt; the Transport for this connection.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>@classmethod\ndef get_sftp_client(\ncls,\noptions_args: dict,\n) -&gt; Tuple[SFTPClient, Transport]:\n\"\"\"Get the SFTP client.\n    The SFTP client is used to open an SFTP session across an open\n    SSH Transport and perform remote file operations.\n    Args:\n        options_args: dictionary containing SFTP connection parameters.\n            The Paramiko arguments expected to connect are:\n            - \"hostname\": the server to connect to.\n            - \"port\": the server port to connect to.\n            - \"username\": the username to authenticate as.\n            - \"password\": used for password authentication.\n            - \"pkey\": optional - an optional public key to use for\n                authentication.\n            - \"passphrase\" \u2013 optional - options used for decrypting private\n                keys.\n            - \"key_filename\" \u2013 optional - the filename, or list of filenames,\n                of optional private key(s) and/or certs to try for\n                authentication.\n            - \"timeout\" \u2013 an optional timeout (in seconds) for the TCP connect.\n            - \"allow_agent\" \u2013 optional - set to False to disable\n                connecting to the SSH agent.\n            - \"look_for_keys\" \u2013 optional - set to False to disable searching\n                for discoverable private key files in ~/.ssh/.\n            - \"compress\" \u2013 optional - set to True to turn on compression.\n            - \"sock\" - optional - an open socket or socket-like object\n                to use for communication to the target host.\n            - \"gss_auth\" \u2013 optional - True if you want to use GSS-API\n                authentication.\n            - \"gss_kex\" \u2013 optional - Perform GSS-API Key Exchange and\n                user authentication.\n            - \"gss_deleg_creds\" \u2013 optional - Delegate GSS-API client\n                credentials or not.\n            - \"gss_host\" \u2013 optional - The targets name in the kerberos database.\n            - \"gss_trust_dns\" \u2013 optional - Indicates whether or\n                not the DNS is trusted to securely canonicalize the name of the\n                host being connected to (default True).\n            - \"banner_timeout\" \u2013 an optional timeout (in seconds)\n                to wait for the SSH banner to be presented.\n            - \"auth_timeout\" \u2013 an optional timeout (in seconds)\n                to wait for an authentication response.\n            - \"disabled_algorithms\" \u2013 an optional dict passed directly to\n                Transport and its keyword argument of the same name.\n            - \"transport_factory\" \u2013 an optional callable which is handed a\n                subset of the constructor arguments (primarily those related\n                to the socket, GSS functionality, and algorithm selection)\n                and generates a Transport instance to be used by this client.\n                Defaults to Transport.__init__.\n            The parameter to specify the private key is expected to be in\n            RSA format. Attempting a connection with a blank host key is\n            not allowed unless the argument \"add_auto_policy\" is explicitly\n            set to True.\n    Returns:\n        sftp -&gt; a new SFTPClient session object.\n        transport -&gt; the Transport for this connection.\n    \"\"\"\nssh_client = p.SSHClient()\ntry:\nif not options_args.get(\"pkey\") and not options_args.get(\"add_auto_policy\"):\nraise WrongArgumentsException(\n\"Get SFTP Client: No host key (pkey) was provided and the \"\n+ \"add_auto_policy property is false.\"\n)\nif options_args.get(\"pkey\") and not options_args.get(\"key_type\"):\nraise WrongArgumentsException(\n\"Get SFTP Client: The key_type must be provided when \"\n+ \"the host key (pkey) is provided.\"\n)\nif options_args.get(\"pkey\", None) and options_args.get(\"key_type\", None):\nkey = cls._get_host_keys(\noptions_args.get(\"pkey\", None), options_args.get(\"key_type\", None)\n)\nssh_client.get_host_keys().add(\nhostname=f\"[{options_args.get('hostname')}]:\"\n+ f\"{options_args.get('port')}\",\nkeytype=\"ssh-rsa\",\nkey=key,\n)\nelif options_args.get(\"add_auto_policy\", None):\nssh_client.load_system_host_keys()\nssh_client.set_missing_host_key_policy(p.WarningPolicy())  # nosec: B507\nelse:\nssh_client.load_system_host_keys()\nssh_client.set_missing_host_key_policy(p.RejectPolicy())\nssh_client.connect(\nhostname=options_args.get(\"hostname\"),\nport=options_args.get(\"port\", 22),\nusername=options_args.get(\"username\", None),\npassword=options_args.get(\"password\", None),\nkey_filename=options_args.get(\"key_filename\", None),\ntimeout=options_args.get(\"timeout\", None),\nallow_agent=options_args.get(\"allow_agent\", True),\nlook_for_keys=options_args.get(\"look_for_keys\", True),\ncompress=options_args.get(\"compress\", False),\nsock=options_args.get(\"sock\", None),\ngss_auth=options_args.get(\"gss_auth\", False),\ngss_kex=options_args.get(\"gss_kex\", False),\ngss_deleg_creds=options_args.get(\"gss_deleg_creds\", False),\ngss_host=options_args.get(\"gss_host\", False),\nbanner_timeout=options_args.get(\"banner_timeout\", None),\nauth_timeout=options_args.get(\"auth_timeout\", None),\ngss_trust_dns=options_args.get(\"gss_trust_dns\", None),\npassphrase=options_args.get(\"passphrase\", None),\ndisabled_algorithms=options_args.get(\"disabled_algorithms\", None),\ntransport_factory=options_args.get(\"transport_factory\", None),\n)\nsftp = ssh_client.open_sftp()\ntransport = ssh_client.get_transport()\nexcept ConnectionError as e:\ncls._logger.error(e)\nraise\nreturn sftp, transport\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.validate_format","title":"<code>validate_format(files_format)</code>  <code>classmethod</code>","text":"<p>Validate the file extension based on the format definitions.</p> <p>Parameters:</p> Name Type Description Default <code>files_format</code> <code>str</code> <p>a string containing the file extension.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string validated and formatted.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>@classmethod\ndef validate_format(cls, files_format: str) -&gt; str:\n\"\"\"Validate the file extension based on the format definitions.\n    Args:\n        files_format: a string containing the file extension.\n    Returns:\n        The string validated and formatted.\n    \"\"\"\nformats_allowed = [\nSFTPInputFormat.CSV.value,\nSFTPInputFormat.FWF.value,\nSFTPInputFormat.JSON.value,\nSFTPInputFormat.XML.value,\n]\nif files_format not in formats_allowed:\nraise WrongArgumentsException(\nf\"The formats allowed for SFTP are {formats_allowed}.\"\n)\nreturn files_format\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.validate_location","title":"<code>validate_location(location)</code>  <code>classmethod</code>","text":"<p>Validate the location. Add \"/\" in the case it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>file path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The location validated.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>@classmethod\ndef validate_location(cls, location: str) -&gt; str:\n\"\"\"Validate the location. Add \"/\" in the case it does not exist.\n    Args:\n        location: file path.\n    Returns:\n        The location validated.\n    \"\"\"\nreturn location if location.rfind(\"/\") == len(location) - 1 else location + \"/\"\n</code></pre>"},{"location":"reference/packages/utils/extraction/sftp_extraction_utils.html#packages.utils.extraction.sftp_extraction_utils.SFTPInputFormat","title":"<code>SFTPInputFormat</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Formats of algorithm input.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/extraction/sftp_extraction_utils.py</code> <pre><code>class SFTPInputFormat(Enum):\n\"\"\"Formats of algorithm input.\"\"\"\nCSV = \"csv\"\nFWF = \"fwf\"\nJSON = \"json\"\nXML = \"xml\"\n</code></pre>"},{"location":"reference/packages/utils/storage/index.html","title":"Storage","text":"<p>Utilities to interact with storage systems.</p>"},{"location":"reference/packages/utils/storage/dbfs_storage.html","title":"Dbfs storage","text":"<p>Module to represent a DBFS file storage system.</p>"},{"location":"reference/packages/utils/storage/dbfs_storage.html#packages.utils.storage.dbfs_storage.DBFSStorage","title":"<code>DBFSStorage</code>","text":"<p>         Bases: <code>FileStorage</code></p> <p>Class to represent a DBFS file storage system.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/dbfs_storage.py</code> <pre><code>class DBFSStorage(FileStorage):\n\"\"\"Class to represent a DBFS file storage system.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n_MAX_INT = 2147483647\n@classmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; Any:\n\"\"\"Get the content of a file.\n        Args:\n            url: url of the file.\n        Returns:\n            File payload/content.\n        \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\nstr_url = urlunparse(url)\ncls._LOGGER.info(f\"Trying with dbfs_storage: Reading from file: {str_url}\")\nreturn DatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.head(\nstr_url, cls._MAX_INT\n)\n@classmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n        Args:\n            url: url of the file.\n            content: content to write into the file.\n        \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\nstr_url = urlunparse(url)\ncls._LOGGER.info(f\"Trying with dbfs_storage: Writing into file: {str_url}\")\nDatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.put(str_url, content, True)\n</code></pre>"},{"location":"reference/packages/utils/storage/dbfs_storage.html#packages.utils.storage.dbfs_storage.DBFSStorage.get_file_payload","title":"<code>get_file_payload(url)</code>  <code>classmethod</code>","text":"<p>Get the content of a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>File payload/content.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/dbfs_storage.py</code> <pre><code>@classmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; Any:\n\"\"\"Get the content of a file.\n    Args:\n        url: url of the file.\n    Returns:\n        File payload/content.\n    \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\nstr_url = urlunparse(url)\ncls._LOGGER.info(f\"Trying with dbfs_storage: Reading from file: {str_url}\")\nreturn DatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.head(\nstr_url, cls._MAX_INT\n)\n</code></pre>"},{"location":"reference/packages/utils/storage/dbfs_storage.html#packages.utils.storage.dbfs_storage.DBFSStorage.write_payload_to_file","title":"<code>write_payload_to_file(url, content)</code>  <code>classmethod</code>","text":"<p>Write payload into a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <code>content</code> <code>str</code> <p>content to write into the file.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/dbfs_storage.py</code> <pre><code>@classmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n    Args:\n        url: url of the file.\n        content: content to write into the file.\n    \"\"\"\nfrom lakehouse_engine.core.exec_env import ExecEnv\nstr_url = urlunparse(url)\ncls._LOGGER.info(f\"Trying with dbfs_storage: Writing into file: {str_url}\")\nDatabricksUtils.get_db_utils(ExecEnv.SESSION).fs.put(str_url, content, True)\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage.html","title":"File storage","text":"<p>Module for abstract representation of a storage system holding files.</p>"},{"location":"reference/packages/utils/storage/file_storage.html#packages.utils.storage.file_storage.FileStorage","title":"<code>FileStorage</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract file storage class.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage.py</code> <pre><code>class FileStorage(ABC):\n\"\"\"Abstract file storage class.\"\"\"\n@classmethod\n@abstractmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; Any:\n\"\"\"Get the payload of a file.\n        Args:\n            url: url of the file.\n        Returns:\n            File payload/content.\n        \"\"\"\npass\n@classmethod\n@abstractmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n        Args:\n            url: url of the file.\n            content: content to write into the file.\n        \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage.html#packages.utils.storage.file_storage.FileStorage.get_file_payload","title":"<code>get_file_payload(url)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Get the payload of a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>File payload/content.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; Any:\n\"\"\"Get the payload of a file.\n    Args:\n        url: url of the file.\n    Returns:\n        File payload/content.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage.html#packages.utils.storage.file_storage.FileStorage.write_payload_to_file","title":"<code>write_payload_to_file(url, content)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Write payload into a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <code>content</code> <code>str</code> <p>content to write into the file.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage.py</code> <pre><code>@classmethod\n@abstractmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n    Args:\n        url: url of the file.\n        content: content to write into the file.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage_functions.html","title":"File storage functions","text":"<p>Module for common file storage functions.</p>"},{"location":"reference/packages/utils/storage/file_storage_functions.html#packages.utils.storage.file_storage_functions.FileStorageFunctions","title":"<code>FileStorageFunctions</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Class for common file storage functions.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage_functions.py</code> <pre><code>class FileStorageFunctions(ABC):  # noqa: B024\n\"\"\"Class for common file storage functions.\"\"\"\n@classmethod\ndef read_json(cls, path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read a json file.\n        The file should be in a supported file system (e.g., s3, dbfs or\n        local filesystem).\n        Args:\n            path: path to the json file.\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            Dict with json file content.\n        \"\"\"\nurl = urlparse(path, allow_fragments=False)\nif disable_dbfs_retry:\nreturn json.load(S3Storage.get_file_payload(url))\nelif url.scheme == \"s3\" and cls.is_boto3_configured():\ntry:\nreturn json.load(S3Storage.get_file_payload(url))\nexcept Exception:\nreturn json.loads(DBFSStorage.get_file_payload(url))\nelif url.scheme == \"file\":\nreturn json.load(LocalFSStorage.get_file_payload(url))\nelif url.scheme in [\"dbfs\", \"s3\"]:\nreturn json.loads(DBFSStorage.get_file_payload(url))\nelse:\nraise NotImplementedError(\nf\"File storage protocol not implemented for {path}.\"\n)\n@classmethod\ndef read_sql(cls, path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read a sql file.\n        The file should be in a supported file system (e.g., s3, dbfs or local\n        filesystem).\n        Args:\n            path: path to the sql file.\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        Returns:\n            Content of the SQL file.\n        \"\"\"\nurl = urlparse(path, allow_fragments=False)\nif disable_dbfs_retry:\nreturn S3Storage.get_file_payload(url).read().decode(\"utf-8\")\nelif url.scheme == \"s3\" and cls.is_boto3_configured():\ntry:\nreturn S3Storage.get_file_payload(url).read().decode(\"utf-8\")\nexcept Exception:\nreturn DBFSStorage.get_file_payload(url)\nelif url.scheme == \"file\":\nreturn LocalFSStorage.get_file_payload(url).read()\nelif url.scheme in [\"dbfs\", \"s3\"]:\nreturn DBFSStorage.get_file_payload(url)\nelse:\nraise NotImplementedError(\nf\"Object storage protocol not implemented for {path}.\"\n)\n@classmethod\ndef write_payload(\ncls, path: str, url: ParseResult, content: str, disable_dbfs_retry: bool = False\n) -&gt; None:\n\"\"\"Write payload into a file.\n        The file should be in a supported file system (e.g., s3, dbfs or local\n        filesystem).\n        Args:\n            path: path to validate the file type.\n            url: url of the file.\n            content: content to write into the file.\n            disable_dbfs_retry: optional flag to disable file storage dbfs.\n        \"\"\"\nif disable_dbfs_retry:\nS3Storage.write_payload_to_file(url, content)\nelif path.startswith(\"s3://\") and cls.is_boto3_configured():\ntry:\nS3Storage.write_payload_to_file(url, content)\nexcept Exception:\nDBFSStorage.write_payload_to_file(url, content)\nelif path.startswith((\"s3://\", \"dbfs:/\")):\nDBFSStorage.write_payload_to_file(url, content)\nelse:\nLocalFSStorage.write_payload_to_file(url, content)\n@staticmethod\ndef is_boto3_configured() -&gt; bool:\n\"\"\"Check if boto3 is able to locate credentials and properly configured.\n        If boto3 is not properly configured, we might want to try a different reader.\n        \"\"\"\ntry:\nboto3.client(\"sts\").get_caller_identity()\nreturn True\nexcept Exception:\nreturn False\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage_functions.html#packages.utils.storage.file_storage_functions.FileStorageFunctions.is_boto3_configured","title":"<code>is_boto3_configured()</code>  <code>staticmethod</code>","text":"<p>Check if boto3 is able to locate credentials and properly configured.</p> <p>If boto3 is not properly configured, we might want to try a different reader.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage_functions.py</code> <pre><code>@staticmethod\ndef is_boto3_configured() -&gt; bool:\n\"\"\"Check if boto3 is able to locate credentials and properly configured.\n    If boto3 is not properly configured, we might want to try a different reader.\n    \"\"\"\ntry:\nboto3.client(\"sts\").get_caller_identity()\nreturn True\nexcept Exception:\nreturn False\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage_functions.html#packages.utils.storage.file_storage_functions.FileStorageFunctions.read_json","title":"<code>read_json(path, disable_dbfs_retry=False)</code>  <code>classmethod</code>","text":"<p>Read a json file.</p> <p>The file should be in a supported file system (e.g., s3, dbfs or local filesystem).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the json file.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Dict with json file content.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage_functions.py</code> <pre><code>@classmethod\ndef read_json(cls, path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read a json file.\n    The file should be in a supported file system (e.g., s3, dbfs or\n    local filesystem).\n    Args:\n        path: path to the json file.\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        Dict with json file content.\n    \"\"\"\nurl = urlparse(path, allow_fragments=False)\nif disable_dbfs_retry:\nreturn json.load(S3Storage.get_file_payload(url))\nelif url.scheme == \"s3\" and cls.is_boto3_configured():\ntry:\nreturn json.load(S3Storage.get_file_payload(url))\nexcept Exception:\nreturn json.loads(DBFSStorage.get_file_payload(url))\nelif url.scheme == \"file\":\nreturn json.load(LocalFSStorage.get_file_payload(url))\nelif url.scheme in [\"dbfs\", \"s3\"]:\nreturn json.loads(DBFSStorage.get_file_payload(url))\nelse:\nraise NotImplementedError(\nf\"File storage protocol not implemented for {path}.\"\n)\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage_functions.html#packages.utils.storage.file_storage_functions.FileStorageFunctions.read_sql","title":"<code>read_sql(path, disable_dbfs_retry=False)</code>  <code>classmethod</code>","text":"<p>Read a sql file.</p> <p>The file should be in a supported file system (e.g., s3, dbfs or local filesystem).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the sql file.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Content of the SQL file.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage_functions.py</code> <pre><code>@classmethod\ndef read_sql(cls, path: str, disable_dbfs_retry: bool = False) -&gt; Any:\n\"\"\"Read a sql file.\n    The file should be in a supported file system (e.g., s3, dbfs or local\n    filesystem).\n    Args:\n        path: path to the sql file.\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    Returns:\n        Content of the SQL file.\n    \"\"\"\nurl = urlparse(path, allow_fragments=False)\nif disable_dbfs_retry:\nreturn S3Storage.get_file_payload(url).read().decode(\"utf-8\")\nelif url.scheme == \"s3\" and cls.is_boto3_configured():\ntry:\nreturn S3Storage.get_file_payload(url).read().decode(\"utf-8\")\nexcept Exception:\nreturn DBFSStorage.get_file_payload(url)\nelif url.scheme == \"file\":\nreturn LocalFSStorage.get_file_payload(url).read()\nelif url.scheme in [\"dbfs\", \"s3\"]:\nreturn DBFSStorage.get_file_payload(url)\nelse:\nraise NotImplementedError(\nf\"Object storage protocol not implemented for {path}.\"\n)\n</code></pre>"},{"location":"reference/packages/utils/storage/file_storage_functions.html#packages.utils.storage.file_storage_functions.FileStorageFunctions.write_payload","title":"<code>write_payload(path, url, content, disable_dbfs_retry=False)</code>  <code>classmethod</code>","text":"<p>Write payload into a file.</p> <p>The file should be in a supported file system (e.g., s3, dbfs or local filesystem).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to validate the file type.</p> required <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <code>content</code> <code>str</code> <p>content to write into the file.</p> required <code>disable_dbfs_retry</code> <code>bool</code> <p>optional flag to disable file storage dbfs.</p> <code>False</code> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/file_storage_functions.py</code> <pre><code>@classmethod\ndef write_payload(\ncls, path: str, url: ParseResult, content: str, disable_dbfs_retry: bool = False\n) -&gt; None:\n\"\"\"Write payload into a file.\n    The file should be in a supported file system (e.g., s3, dbfs or local\n    filesystem).\n    Args:\n        path: path to validate the file type.\n        url: url of the file.\n        content: content to write into the file.\n        disable_dbfs_retry: optional flag to disable file storage dbfs.\n    \"\"\"\nif disable_dbfs_retry:\nS3Storage.write_payload_to_file(url, content)\nelif path.startswith(\"s3://\") and cls.is_boto3_configured():\ntry:\nS3Storage.write_payload_to_file(url, content)\nexcept Exception:\nDBFSStorage.write_payload_to_file(url, content)\nelif path.startswith((\"s3://\", \"dbfs:/\")):\nDBFSStorage.write_payload_to_file(url, content)\nelse:\nLocalFSStorage.write_payload_to_file(url, content)\n</code></pre>"},{"location":"reference/packages/utils/storage/local_fs_storage.html","title":"Local fs storage","text":"<p>Module to represent a local file storage system.</p>"},{"location":"reference/packages/utils/storage/local_fs_storage.html#packages.utils.storage.local_fs_storage.LocalFSStorage","title":"<code>LocalFSStorage</code>","text":"<p>         Bases: <code>FileStorage</code></p> <p>Class to represent a local file storage system.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/local_fs_storage.py</code> <pre><code>class LocalFSStorage(FileStorage):\n\"\"\"Class to represent a local file storage system.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; TextIO:\n\"\"\"Get the payload of a file.\n        Args:\n            url: url of the file.\n        Returns:\n            file payload/content.\n        \"\"\"\ncls._LOGGER.info(f\"Reading from file: {url.scheme}:{url.netloc}/{url.path}\")\nreturn open(f\"{url.netloc}/{url.path}\", \"r\")\n@classmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n        Args:\n            url: url of the file.\n            content: content to write into the file.\n        \"\"\"\ncls._LOGGER.info(f\"Writing into file: {url.scheme}:{url.netloc}/{url.path}\")\nos.makedirs(os.path.dirname(f\"{url.netloc}/{url.path}\"), exist_ok=True)\nwith open(f\"{url.netloc}/{url.path}\", \"w\") as file:\nfile.write(content)\n</code></pre>"},{"location":"reference/packages/utils/storage/local_fs_storage.html#packages.utils.storage.local_fs_storage.LocalFSStorage.get_file_payload","title":"<code>get_file_payload(url)</code>  <code>classmethod</code>","text":"<p>Get the payload of a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <p>Returns:</p> Type Description <code>TextIO</code> <p>file payload/content.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/local_fs_storage.py</code> <pre><code>@classmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; TextIO:\n\"\"\"Get the payload of a file.\n    Args:\n        url: url of the file.\n    Returns:\n        file payload/content.\n    \"\"\"\ncls._LOGGER.info(f\"Reading from file: {url.scheme}:{url.netloc}/{url.path}\")\nreturn open(f\"{url.netloc}/{url.path}\", \"r\")\n</code></pre>"},{"location":"reference/packages/utils/storage/local_fs_storage.html#packages.utils.storage.local_fs_storage.LocalFSStorage.write_payload_to_file","title":"<code>write_payload_to_file(url, content)</code>  <code>classmethod</code>","text":"<p>Write payload into a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <code>content</code> <code>str</code> <p>content to write into the file.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/local_fs_storage.py</code> <pre><code>@classmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n    Args:\n        url: url of the file.\n        content: content to write into the file.\n    \"\"\"\ncls._LOGGER.info(f\"Writing into file: {url.scheme}:{url.netloc}/{url.path}\")\nos.makedirs(os.path.dirname(f\"{url.netloc}/{url.path}\"), exist_ok=True)\nwith open(f\"{url.netloc}/{url.path}\", \"w\") as file:\nfile.write(content)\n</code></pre>"},{"location":"reference/packages/utils/storage/s3_storage.html","title":"S3 storage","text":"<p>Module to represent a s3 file storage system.</p>"},{"location":"reference/packages/utils/storage/s3_storage.html#packages.utils.storage.s3_storage.S3Storage","title":"<code>S3Storage</code>","text":"<p>         Bases: <code>FileStorage</code></p> <p>Class to represent a s3 file storage system.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/s3_storage.py</code> <pre><code>class S3Storage(FileStorage):\n\"\"\"Class to represent a s3 file storage system.\"\"\"\n_LOGGER = LoggingHandler(__name__).get_logger()\n@classmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; Any:\n\"\"\"Get the payload of a config file.\n        Args:\n            url: url of the file.\n        Returns:\n            File payload/content.\n        \"\"\"\ns3 = boto3.resource(\"s3\")\nobj = s3.Object(url.netloc, url.path.lstrip(\"/\"))\ncls._LOGGER.info(\nf\"Trying with s3_storage: \"\nf\"Reading from file: {url.scheme}://{url.netloc}{url.path}\"\n)\nreturn obj.get()[\"Body\"]\n@classmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n        Args:\n            url: url of the file.\n            content: content to write into the file.\n        \"\"\"\ns3 = boto3.resource(\"s3\")\nobj = s3.Object(url.netloc, url.path.lstrip(\"/\"))\ncls._LOGGER.info(\nf\"Trying with s3_storage: \"\nf\"Writing into file: {url.scheme}://{url.netloc}{url.path}\"\n)\nobj.put(Body=content)\n</code></pre>"},{"location":"reference/packages/utils/storage/s3_storage.html#packages.utils.storage.s3_storage.S3Storage.get_file_payload","title":"<code>get_file_payload(url)</code>  <code>classmethod</code>","text":"<p>Get the payload of a config file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>File payload/content.</p> Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/s3_storage.py</code> <pre><code>@classmethod\ndef get_file_payload(cls, url: ParseResult) -&gt; Any:\n\"\"\"Get the payload of a config file.\n    Args:\n        url: url of the file.\n    Returns:\n        File payload/content.\n    \"\"\"\ns3 = boto3.resource(\"s3\")\nobj = s3.Object(url.netloc, url.path.lstrip(\"/\"))\ncls._LOGGER.info(\nf\"Trying with s3_storage: \"\nf\"Reading from file: {url.scheme}://{url.netloc}{url.path}\"\n)\nreturn obj.get()[\"Body\"]\n</code></pre>"},{"location":"reference/packages/utils/storage/s3_storage.html#packages.utils.storage.s3_storage.S3Storage.write_payload_to_file","title":"<code>write_payload_to_file(url, content)</code>  <code>classmethod</code>","text":"<p>Write payload into a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>ParseResult</code> <p>url of the file.</p> required <code>content</code> <code>str</code> <p>content to write into the file.</p> required Source code in <code>mkdocs/lakehouse_engine/packages/utils/storage/s3_storage.py</code> <pre><code>@classmethod\ndef write_payload_to_file(cls, url: ParseResult, content: str) -&gt; None:\n\"\"\"Write payload into a file.\n    Args:\n        url: url of the file.\n        content: content to write into the file.\n    \"\"\"\ns3 = boto3.resource(\"s3\")\nobj = s3.Object(url.netloc, url.path.lstrip(\"/\"))\ncls._LOGGER.info(\nf\"Trying with s3_storage: \"\nf\"Writing into file: {url.scheme}://{url.netloc}{url.path}\"\n)\nobj.put(Body=content)\n</code></pre>"}]}