window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "lakehouse_engine", "modulename": "lakehouse_engine", "kind": "module", "doc": "<p>Lakehouse engine package containing all the system subpackages.</p>\n"}, {"fullname": "lakehouse_engine.algorithms", "modulename": "lakehouse_engine.algorithms", "kind": "module", "doc": "<p>Package containing all the lakehouse engine algorithms.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.algorithm", "modulename": "lakehouse_engine.algorithms.algorithm", "kind": "module", "doc": "<p>Module containing the Algorithm class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.algorithm.Algorithm", "modulename": "lakehouse_engine.algorithms.algorithm", "qualname": "Algorithm", "kind": "class", "doc": "<p>Class to define the behavior of every algorithm based on ACONs.</p>\n", "bases": "lakehouse_engine.core.executable.Executable"}, {"fullname": "lakehouse_engine.algorithms.algorithm.Algorithm.__init__", "modulename": "lakehouse_engine.algorithms.algorithm", "qualname": "Algorithm.__init__", "kind": "function", "doc": "<p>Construct Algorithm instances.</p>\n\n<p>Args:\n    acon: algorithm configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.algorithm.Algorithm.get_dq_spec", "modulename": "lakehouse_engine.algorithms.algorithm", "qualname": "Algorithm.get_dq_spec", "kind": "function", "doc": "<p>Get data quality specification object from acon.</p>\n\n<p>Args:\n    spec: data quality specifications.</p>\n\n<p>Returns:\n    The DQSpec and the List of DQ Functions Specs.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader", "modulename": "lakehouse_engine.algorithms.data_loader", "kind": "module", "doc": "<p>Module to define DataLoader class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader", "kind": "class", "doc": "<p>Load data using an algorithm configuration (ACON represented as dict).</p>\n\n<p>This algorithm focuses on the cases where users will be specifying all the algorithm\nsteps and configurations through a dict based configuration, which we name ACON\nin our framework.</p>\n\n<p>Since an ACON is a dict you can pass a custom transformer through a python function\nand, therefore, the DataLoader can also be used to load data with custom\ntransformations not provided in our transformers package.</p>\n\n<p>As the algorithm base class of the lakehouse-engine framework is based on the\nconcept of ACON, this DataLoader algorithm simply inherits from Algorithm,\nwithout overriding anything. We designed the codebase like this to avoid\ninstantiating the Algorithm class directly, which was always meant to be an\nabstraction for any specific algorithm included in the lakehouse-engine framework.</p>\n", "bases": "lakehouse_engine.algorithms.algorithm.Algorithm"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.__init__", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.__init__", "kind": "function", "doc": "<p>Construct DataLoader algorithm instances.</p>\n\n<p>A data loader needs several specifications to work properly,\nbut some of them might be optional. The available specifications are:</p>\n\n<pre><code>- input specifications (mandatory): specify how to read data.\n- transform specifications (optional): specify how to transform data.\n- data quality specifications (optional): specify how to execute the data\n    quality process.\n- output specifications (mandatory): specify how to write data to the\n    target.\n- terminate specifications (optional): specify what to do after writing into\n    the target (e.g., optimizing target table, vacuum, compute stats, etc).\n</code></pre>\n\n<p>Args:\n    acon: algorithm configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.read", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.read", "kind": "function", "doc": "<p>Read data from an input location into a distributed dataframe.</p>\n\n<p>Returns:\n     An ordered dict with all the dataframes that were read.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.transform", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.transform", "kind": "function", "doc": "<p>Transform (optionally) the data that was read.</p>\n\n<p>If there isn't a transformation specification this step will be skipped, and the\noriginal dataframes that were read will be returned.\nTransformations can have dependency from another transformation result, however\nwe need to keep in mind if we are using streaming source and for some reason we\nneed to enable micro batch processing, this result cannot be used as input to\nanother transformation. Micro batch processing in pyspark streaming is only\navailable in .write(), which means this transformation with micro batch needs\nto be the end of the process.</p>\n\n<p>Args:\n    data: input dataframes in an ordered dict.</p>\n\n<p>Returns:\n    Another ordered dict with the transformed dataframes, according to the\n    transformation specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.process_dq", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.process_dq", "kind": "function", "doc": "<p>Process the data quality tasks for the data that was read and/or transformed.</p>\n\n<p>It supports multiple input dataframes. Although just one is advisable.</p>\n\n<p>It is possible to use data quality validators/expectations that will validate\nyour data and fail the process in case the expectations are not met. The DQ\nprocess also generates and keeps updating a site containing the results of the\nexpectations that were done on your data. The location of the site is\nconfigurable and can either be on file system or S3. If you define it to be\nstored on S3, you can even configure your S3 bucket to serve the site so that\npeople can easily check the quality of your data. Moreover, it is also\npossible to store the result of the DQ process into a defined result sink.</p>\n\n<p>Args:\n    data: dataframes from previous steps of the algorithm that we which to\n        run the DQ process on.</p>\n\n<p>Returns:\n    Another ordered dict with the validated dataframes.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.write", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.write", "kind": "function", "doc": "<p>Write the data that was read and transformed (if applicable).</p>\n\n<p>It supports writing multiple datasets. However, we only recommend to write one\ndataframe. This recommendation is based on easy debugging and reproducibility,\nsince if we start mixing several datasets being fueled by the same algorithm, it\nwould unleash an infinite sea of reproducibility issues plus tight coupling and\ndependencies between datasets. Having said that, there may be cases where\nwriting multiple datasets is desirable according to the use case requirements.\nUse it accordingly.</p>\n\n<p>Args:\n    data: dataframes that were read and transformed (if applicable).</p>\n\n<p>Returns:\n    Dataframes that were written.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.terminate", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.terminate", "kind": "function", "doc": "<p>Terminate the algorithm.</p>\n\n<p>Args:\n    data: dataframes that were written.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.execute", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.execute", "kind": "function", "doc": "<p>Define the algorithm execution behaviour.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.dq_validator", "modulename": "lakehouse_engine.algorithms.dq_validator", "kind": "module", "doc": "<p>Module to define Data Validator class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator", "kind": "class", "doc": "<p>Validate data using an algorithm configuration (ACON represented as dict).</p>\n\n<p>This algorithm focuses on isolate Data Quality Validations from loading,\napplying a set of data quality functions to a specific input dataset,\nwithout the need to define any output specification.\nYou can use any input specification compatible with the lakehouse engine\n(dataframe, table, files, etc).</p>\n", "bases": "lakehouse_engine.algorithms.algorithm.Algorithm"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.__init__", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.__init__", "kind": "function", "doc": "<p>Construct DQValidator algorithm instances.</p>\n\n<p>A data quality validator needs the following specifications to work\nproperly:\n    - input specification (mandatory): specify how and what data to\n    read.\n    - data quality specification (mandatory): specify how to execute\n    the data quality process.\n    - restore_prev_version (optional): specify if, having\n    delta table/files as input, they should be restored to the\n    previous version if the data quality process fails. Note: this\n    is only considered if fail_on_error is kept as True.</p>\n\n<p>Args:\n    acon: algorithm configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.read", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.read", "kind": "function", "doc": "<p>Read data from an input location into a distributed dataframe.</p>\n\n<p>Returns:\n     Dataframe with data that was read.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.process_dq", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.process_dq", "kind": "function", "doc": "<p>Process the data quality tasks for the data that was read.</p>\n\n<p>It supports a single input dataframe.</p>\n\n<p>It is possible to use data quality validators/expectations that will validate\nyour data and fail the process in case the expectations are not met. The DQ\nprocess also generates and keeps updating a site containing the results of the\nexpectations that were done on your data. The location of the site is\nconfigurable and can either be on file system or S3. If you define it to be\nstored on S3, you can even configure your S3 bucket to serve the site so that\npeople can easily check the quality of your data. Moreover, it is also\npossible to store the result of the DQ process into a defined result sink.</p>\n\n<p>Args:\n    data: input dataframe on which to run the DQ process.</p>\n\n<p>Returns:\n    Validated dataframe.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.execute", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.execute", "kind": "function", "doc": "<p>Define the algorithm execution behaviour.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.exceptions", "modulename": "lakehouse_engine.algorithms.exceptions", "kind": "module", "doc": "<p>Package defining all the algorithm custom exceptions.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.exceptions.ReconciliationFailedException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "ReconciliationFailedException", "kind": "class", "doc": "<p>Exception for when the reconciliation process fails.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.exceptions.NoNewDataException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "NoNewDataException", "kind": "class", "doc": "<p>Exception for when no new data is available.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.exceptions.SensorAlreadyExistsException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "SensorAlreadyExistsException", "kind": "class", "doc": "<p>Exception for when a sensor with same sensor id already exists.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.exceptions.RestoreTypeNotFoundException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "RestoreTypeNotFoundException", "kind": "class", "doc": "<p>Exception for when the restore type is not found.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.reconciliator", "modulename": "lakehouse_engine.algorithms.reconciliator", "kind": "module", "doc": "<p>Module containing the Reconciliator class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationType", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationType", "kind": "class", "doc": "<p>Type of Reconciliation.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationType.PCT", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationType.PCT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReconciliationType.PCT: &#x27;percentage&#x27;&gt;"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationType.ABS", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationType.ABS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReconciliationType.ABS: &#x27;absolute&#x27;&gt;"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationTransformers", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationTransformers", "kind": "class", "doc": "<p>Transformers Available for the Reconciliation Algorithm.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationTransformers.AVAILABLE_TRANSFORMERS", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationTransformers.AVAILABLE_TRANSFORMERS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict", "default_value": "&lt;ReconciliationTransformers.AVAILABLE_TRANSFORMERS: {&#x27;cache&#x27;: &lt;bound method Optimizers.cache of &lt;class &#x27;lakehouse_engine.transformers.optimizers.Optimizers&#x27;&gt;&gt;, &#x27;persist&#x27;: &lt;bound method Optimizers.persist of &lt;class &#x27;lakehouse_engine.transformers.optimizers.Optimizers&#x27;&gt;&gt;}&gt;"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator", "kind": "class", "doc": "<p>Class to define the behavior of an algorithm that checks if data reconciles.</p>\n\n<p>Checking if data reconciles, using this algorithm, is a matter of reading the\n'truth' data and the 'current' data. You can use any input specification compatible\nwith the lakehouse engine to read 'truth' or 'current' data. On top of that, you\ncan pass a 'truth_preprocess_query' and a 'current_preprocess_query' so you can\npreprocess the data before it goes into the actual reconciliation process.\nMoreover, you can use the 'truth_preprocess_query_args' and\n'current_preprocess_query_args' to pass additional arguments to be used to apply\nadditional operations on top of the dataframe, resulting from the previous steps.\nWith these arguments you can apply additional operations like caching or persisting\nthe Dataframe. The way to pass the additional arguments for the operations is\nsimilar to the TransformSpec, but only a few operations are allowed. Those are\ndefined in ReconciliationTransformers.AVAILABLE_TRANSFORMERS.</p>\n\n<p>The reconciliation process is focused on joining 'truth' with 'current' by all\nprovided columns except the ones passed as 'metrics'. After that it calculates the\ndifferences in the metrics attributes (either percentage or absolute difference).\nFinally, it aggregates the differences, using the supplied aggregation function\n(e.g., sum, avg, min, max, etc).</p>\n\n<p>All of these configurations are passed via the ACON to instantiate a\nReconciliatorSpec object.</p>\n\n<p>Notes:\n    - It is crucial that both the current and truth datasets have exactly the same\n        structure.\n    - You should not use 0 as yellow or red threshold, as the algorithm will verify\n        if the difference between the truth and current values is bigger\n        or equal than those thresholds.\n    - The reconciliation does not produce any negative values or percentages, as we\n        use the absolute value of the differences. This means that the recon result\n        will not indicate if it was the current values that were bigger or smaller\n        than the truth values, or vice versa.</p>\n", "bases": "lakehouse_engine.core.executable.Executable"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.__init__", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.__init__", "kind": "function", "doc": "<p>Construct Algorithm instances.</p>\n\n<p>Args:\n    acon: algorithm configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.get_source_of_truth", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.get_source_of_truth", "kind": "function", "doc": "<p>Get the source of truth (expected result) for the reconciliation process.</p>\n\n<p>Returns:\n    DataFrame containing the source of truth.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.get_current_results", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.get_current_results", "kind": "function", "doc": "<p>Get the current results from the table that we are checking if it reconciles.</p>\n\n<p>Returns:\n    DataFrame containing the current results.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.execute", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.execute", "kind": "function", "doc": "<p>Reconcile the current results against the truth dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.sensor", "modulename": "lakehouse_engine.algorithms.sensor", "kind": "module", "doc": "<p>Module to define Sensor algorithm behavior.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.sensor.Sensor", "modulename": "lakehouse_engine.algorithms.sensor", "qualname": "Sensor", "kind": "class", "doc": "<p>Class representing a sensor to check if the upstream has new data.</p>\n", "bases": "lakehouse_engine.algorithms.algorithm.Algorithm"}, {"fullname": "lakehouse_engine.algorithms.sensor.Sensor.__init__", "modulename": "lakehouse_engine.algorithms.sensor", "qualname": "Sensor.__init__", "kind": "function", "doc": "<p>Construct Sensor instances.</p>\n\n<p>Args:\n    acon: algorithm configuration.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.sensor.Sensor.execute", "modulename": "lakehouse_engine.algorithms.sensor", "qualname": "Sensor.execute", "kind": "function", "doc": "<p>Execute the sensor.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.configs", "modulename": "lakehouse_engine.configs", "kind": "module", "doc": "<p>This module receives a config file which is included in the wheel.</p>\n"}, {"fullname": "lakehouse_engine.core", "modulename": "lakehouse_engine.core", "kind": "module", "doc": "<p>Package with the core behaviour of the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions", "modulename": "lakehouse_engine.core.definitions", "kind": "module", "doc": "<p>Definitions of standard values and structures for core components.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.EngineConfig", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineConfig", "kind": "class", "doc": "<p>Definitions that can come from the Engine Config file.</p>\n\n<p>dq_bucket: S3 bucket used to store data quality related artifacts.\nnotif_disallowed_email_servers: email servers not allowed to be used\n    for sending notifications.\nengine_usage_path: path where the engine usage stats are stored.\ncollect_engine_usage: whether to enable the collection of lakehouse\n    engine usage stats or not.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.EngineConfig.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dq_bucket</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">notif_disallowed_email_servers</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">engine_usage_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.EngineStats", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineStats", "kind": "class", "doc": "<p>Definitions for collection of Lakehouse Engine Stats.</p>\n\n<p>Note: whenever the value comes from a key inside a Spark Config\nthat returns an array, it can be specified with a '#' so that it\nis adequately processed.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.EngineStats.CLUSTER_USAGE_TAGS", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineStats.CLUSTER_USAGE_TAGS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EngineStats.CLUSTER_USAGE_TAGS: &#x27;spark.databricks.clusterUsageTags&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.EngineStats.DEF_SPARK_CONFS", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineStats.DEF_SPARK_CONFS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EngineStats.DEF_SPARK_CONFS: {&#x27;dp_name&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#accountName&#x27;, &#x27;environment&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#environment&#x27;, &#x27;workspace_id&#x27;: &#x27;spark.databricks.clusterUsageTags.orgId&#x27;, &#x27;job_id&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#JobId&#x27;, &#x27;job_name&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#RunName&#x27;, &#x27;run_id&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#ClusterName&#x27;}&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat", "kind": "class", "doc": "<p>Formats of algorithm input.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.JDBC", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.JDBC", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.JDBC: &#x27;jdbc&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.AVRO", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.AVRO", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.AVRO: &#x27;avro&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.JSON", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.JSON: &#x27;json&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.CSV", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.CSV", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.CSV: &#x27;csv&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.PARQUET", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.PARQUET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.PARQUET: &#x27;parquet&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.DELTAFILES", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.DELTAFILES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.DELTAFILES: &#x27;delta&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.CLOUDFILES", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.CLOUDFILES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.CLOUDFILES: &#x27;cloudfiles&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.KAFKA", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.KAFKA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.KAFKA: &#x27;kafka&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SQL", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SQL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SQL: &#x27;sql&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SAP_BW", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SAP_BW", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SAP_BW: &#x27;sap_bw&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SAP_B4", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SAP_B4", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SAP_B4: &#x27;sap_b4&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.DATAFRAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.DATAFRAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.DATAFRAME: &#x27;dataframe&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SFTP", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SFTP", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SFTP: &#x27;sftp&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.values", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.values", "kind": "function", "doc": "<p>Generates a list containing all enum values.</p>\n\n<p>Return:\n    A list with all enum values.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.exists", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.exists", "kind": "function", "doc": "<p>Checks if the input format exists in the enum values.</p>\n\n<p>Args:\n    input_format: format to check if exists.</p>\n\n<p>Return:\n    If the input format exists in our enum.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">input_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat", "kind": "class", "doc": "<p>Formats of algorithm output.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.JDBC", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.JDBC", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.JDBC: &#x27;jdbc&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.AVRO", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.AVRO", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.AVRO: &#x27;avro&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.JSON", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.JSON: &#x27;json&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.CSV", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.CSV", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.CSV: &#x27;csv&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.PARQUET", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.PARQUET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.PARQUET: &#x27;parquet&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.DELTAFILES", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.DELTAFILES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.DELTAFILES: &#x27;delta&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.KAFKA", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.KAFKA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.KAFKA: &#x27;kafka&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.CONSOLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.CONSOLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.CONSOLE: &#x27;console&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.NOOP", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.NOOP", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.NOOP: &#x27;noop&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.DATAFRAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.DATAFRAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.DATAFRAME: &#x27;dataframe&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.FILE", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.FILE: &#x27;file&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.TABLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.TABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.TABLE: &#x27;table&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.NotifierType", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotifierType", "kind": "class", "doc": "<p>Type of notifier available.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.NotifierType.EMAIL", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotifierType.EMAIL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;NotifierType.EMAIL: &#x27;email&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.NotificationRuntimeParameters", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotificationRuntimeParameters", "kind": "class", "doc": "<p>Parameters to be replaced in runtime.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.NotificationRuntimeParameters.DATABRICKS_JOB_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotificationRuntimeParameters.DATABRICKS_JOB_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;NotificationRuntimeParameters.DATABRICKS_JOB_NAME: &#x27;databricks_job_name&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID: &#x27;databricks_workspace_id&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadType", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadType", "kind": "class", "doc": "<p>Define the types of read operations.</p>\n\n<p>BATCH - read the data in batch mode (e.g., Spark batch).\nSTREAMING - read the data in streaming mode (e.g., Spark streaming).</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.ReadType.BATCH", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadType.BATCH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadType.BATCH: &#x27;batch&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadType.STREAMING", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadType.STREAMING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadType.STREAMING: &#x27;streaming&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode", "kind": "class", "doc": "<p>Different modes that control how we handle compliance to the provided schema.</p>\n\n<p>These read modes map to Spark's read modes at the moment.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode.PERMISSIVE", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode.PERMISSIVE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadMode.PERMISSIVE: &#x27;PERMISSIVE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode.FAILFAST", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode.FAILFAST", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadMode.FAILFAST: &#x27;FAILFAST&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode.DROPMALFORMED", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode.DROPMALFORMED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadMode.DROPMALFORMED: &#x27;DROPMALFORMED&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults", "kind": "class", "doc": "<p>Defaults used on the data quality process.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.FILE_SYSTEM_STORE", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.FILE_SYSTEM_STORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.FILE_SYSTEM_STORE: &#x27;file_system&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.FILE_SYSTEM_S3_STORE", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.FILE_SYSTEM_S3_STORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.FILE_SYSTEM_S3_STORE: &#x27;s3&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DQ_BATCH_IDENTIFIERS", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DQ_BATCH_IDENTIFIERS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DQ_BATCH_IDENTIFIERS: [&#x27;spec_id&#x27;, &#x27;input_id&#x27;, &#x27;timestamp&#x27;]&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATASOURCE_CLASS_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATASOURCE_CLASS_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATASOURCE_CLASS_NAME: &#x27;Datasource&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATASOURCE_EXECUTION_ENGINE", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATASOURCE_EXECUTION_ENGINE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATASOURCE_EXECUTION_ENGINE: &#x27;SparkDFExecutionEngine&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CONNECTORS_CLASS_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CONNECTORS_CLASS_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CONNECTORS_CLASS_NAME: &#x27;RuntimeDataConnector&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CONNECTORS_MODULE_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CONNECTORS_MODULE_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CONNECTORS_MODULE_NAME: &#x27;great_expectations.datasource.data_connector&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CHECKPOINTS_CLASS_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CHECKPOINTS_CLASS_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CHECKPOINTS_CLASS_NAME: &#x27;SimpleCheckpoint&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION: 1.0&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.STORE_BACKEND", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.STORE_BACKEND", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.FILE_SYSTEM_S3_STORE: &#x27;s3&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.EXPECTATIONS_STORE_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.EXPECTATIONS_STORE_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.EXPECTATIONS_STORE_PREFIX: &#x27;dq/expectations/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.VALIDATIONS_STORE_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.VALIDATIONS_STORE_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.VALIDATIONS_STORE_PREFIX: &#x27;dq/validations/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_DOCS_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_DOCS_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_DOCS_PREFIX: &#x27;dq/data_docs/site/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.CHECKPOINT_STORE_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.CHECKPOINT_STORE_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.CHECKPOINT_STORE_PREFIX: &#x27;dq/checkpoints/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.VALIDATION_COLUMN_IDENTIFIER", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.VALIDATION_COLUMN_IDENTIFIER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.VALIDATION_COLUMN_IDENTIFIER: &#x27;validationresultidentifier&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.CUSTOM_EXPECTATION_LIST", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.CUSTOM_EXPECTATION_LIST", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.CUSTOM_EXPECTATION_LIST: [&#x27;expect_column_values_to_be_date_not_older_than&#x27;, &#x27;expect_column_pair_a_to_be_smaller_or_equal_than_b&#x27;, &#x27;expect_multicolumn_column_a_must_equal_b_or_c&#x27;, &#x27;expect_queried_column_agg_value_to_be&#x27;]&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DQ_VALIDATIONS_SCHEMA", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DQ_VALIDATIONS_SCHEMA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DQ_VALIDATIONS_SCHEMA: StructType([StructField(&#x27;dq_validations&#x27;, StructType([StructField(&#x27;run_name&#x27;, StringType(), True), StructField(&#x27;run_success&#x27;, BooleanType(), True), StructField(&#x27;raised_exceptions&#x27;, BooleanType(), True), StructField(&#x27;run_row_success&#x27;, BooleanType(), True), StructField(&#x27;dq_failure_details&#x27;, ArrayType(StructType([StructField(&#x27;expectation_type&#x27;, StringType(), True), StructField(&#x27;kwargs&#x27;, StringType(), True)]), True), True)]), True)])&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType", "kind": "class", "doc": "<p>Types of write operations.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.OVERWRITE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.OVERWRITE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.OVERWRITE: &#x27;overwrite&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.COMPLETE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.COMPLETE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.COMPLETE: &#x27;complete&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.APPEND", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.APPEND", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.APPEND: &#x27;append&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.UPDATE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.UPDATE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.UPDATE: &#x27;update&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.MERGE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.MERGE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.MERGE: &#x27;merge&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.ERROR_IF_EXISTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.ERROR_IF_EXISTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.ERROR_IF_EXISTS: &#x27;error&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.IGNORE_IF_EXISTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.IGNORE_IF_EXISTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.IGNORE_IF_EXISTS: &#x27;ignore&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputSpec", "kind": "class", "doc": "<p>Specification of an algorithm input.</p>\n\n<p>This is very aligned with the way the execution environment connects to the sources\n(e.g., spark sources).</p>\n\n<p>spec_id: spec_id of the input specification read_type: ReadType type of read\noperation.\ndata_format: format of the input.\nsftp_files_format: format of the files (csv, fwf, json, xml...) in a sftp\n    directory.\ndf_name: dataframe name.\ndb_table: table name in the form of <db>.<table>.\nlocation: uri that identifies from where to read data in the specified format.\nenforce_schema_from_table: if we want to enforce the table schema or not, by\n    providing a table name in the form of <db>.<table>.\nquery: sql query to execute and return the dataframe. Use it if you do not want to\n    read from a file system nor from a table, but rather from a sql query instead.\nschema: dict representation of a schema of the input (e.g., Spark struct type\n    schema).\nschema_path: path to a file with a representation of a schema of the input (e.g.,\n    Spark struct type schema).\nwith_filepath: if we want to include the path of the file that is being read. Only\n    works with the file reader (batch and streaming modes are supported).\noptions: dict with other relevant options according to the execution\n    environment (e.g., spark) possible sources.\ncalculate_upper_bound: when to calculate upper bound to extract from SAP BW or not.\ncalc_upper_bound_schema: specific schema for the calculated upper_bound.\ngenerate_predicates: when to generate predicates to extract from SAP BW or not.\npredicates_add_null: if we want to include is null on partition by predicates.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.InputSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">read_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">data_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sftp_files_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">df_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">enforce_schema_from_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">schema_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_filepath</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">jdbc_args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">calculate_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.TransformerSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformerSpec", "kind": "class", "doc": "<p>Transformer Specification, i.e., a single transformation amongst many.</p>\n\n<p>function: name of the function (or callable function) to be executed.\nargs: (not applicable if using a callable function) dict with the arguments to pass\nto the function <k,v> pairs with the name of the parameter of the function and the\nrespective value.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.TransformerSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformerSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">function</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.TransformSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformSpec", "kind": "class", "doc": "<p>Transformation Specification.</p>\n\n<p>I.e., the specification that defines the many transformations to be done to the data\nthat was read.</p>\n\n<p>spec_id: id of the terminate specification input_id: id of the corresponding input\nspecification.\ntransformers: list of transformers to execute.\nforce_streaming_foreach_batch_processing: sometimes, when using streaming, we want\n    to force the transform to be executed in the foreachBatch function to ensure\n    non-supported streaming operations can be properly executed.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.TransformSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">transformers</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TransformerSpec</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">force_streaming_foreach_batch_processing</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.DQType", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQType", "kind": "class", "doc": "<p>Available data quality tasks.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.DQType.VALIDATOR", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQType.VALIDATOR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQType.VALIDATOR: &#x27;validator&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQType.ASSISTANT", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQType.ASSISTANT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQType.ASSISTANT: &#x27;assistant&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQFunctionSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQFunctionSpec", "kind": "class", "doc": "<p>Defines a data quality function specification.</p>\n\n<p>function - name of the data quality function (expectation) to execute.\nIt follows the great_expectations api <a href=\"https://greatexpectations.io/expectations/\">https://greatexpectations.io/expectations/</a>.\nargs - args of the function (expectation). Follow the same api as above.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.DQFunctionSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQFunctionSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">function</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.DQSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQSpec", "kind": "class", "doc": "<p>Data quality overall specification.</p>\n\n<pre><code>spec_id - id of the specification.\ninput_id - id of the input specification.\ndq_type - type of DQ process to execute (e.g. validator).\ndq_functions - list of function specifications to execute.\nunexpected_rows_pk - the list of columns composing the primary key of the\n    source data to identify the rows failing the DQ validations. Note: only one\n    of tbl_to_derive_pk or unexpected_rows_pk arguments need to be provided. It\n    is mandatory to provide one of these arguments when using tag_source_data\n    as True. When tag_source_data is False, this is not mandatory, but still\n    recommended.\ntbl_to_derive_pk - db.table to automatically derive the unexpected_rows_pk from.\n    Note: only one of tbl_to_derive_pk or unexpected_rows_pk arguments need to\n    be provided. It is mandatory to provide one of these arguments when using\n    tag_source_data as True. hen tag_source_data is False, this is not\n    mandatory, but still recommended.\ngx_result_format - great expectations result format. Default: \"COMPLETE\".\n</code></pre>\n\n<p>\u00b4   tag_source_data - when set to true, this will ensure that the DQ process ends by\n        tagging the source data with an additional column with information about the\n        DQ results. This column makes it possible to identify if the DQ run was\n        succeeded in general and, if not, it unlocks the insights to know what\n        specific rows have made the DQ validations fail and why. Default: False.\n        Note: it only works if result_sink_explode is True, gx_result_format is\n        COMPLETE, fail_on_error is False (which is done automatically when\n        you specify tag_source_data as True) and tbl_to_derive_pk or\n        unexpected_rows_pk is configured.\n    store_backend - which store_backend to use (e.g. s3 or file_system).\n    local_fs_root_dir - path of the root directory. Note: only applicable for\n        store_backend file_system.\n    bucket - the bucket name to consider for the store_backend (store DQ artefacts).\n        Note: only applicable for store_backend s3.\n    data_docs_bucket - the bucket name for data docs only. When defined, it will\n        supersede bucket parameter.\n    expectations_store_prefix - prefix where to store expectations' data. Note: only\n        applicable for store_backend s3.\n    validations_store_prefix - prefix where to store validations' data. Note: only\n        applicable for store_backend s3.\n    data_docs_prefix - prefix where to store data_docs' data. Note: only applicable\n        for store_backend s3.\n    checkpoint_store_prefix - prefix where to store checkpoints' data. Note: only\n        applicable for store_backend s3.\n    data_asset_name - name of the data asset to consider when configuring the great\n        expectations' data source.\n    expectation_suite_name - name to consider for great expectations' suite.\n    assistant_options - additional options to pass to the DQ assistant processor.\n    result_sink_db_table - db.table_name indicating the database and table in which\n        to save the results of the DQ process.\n    result_sink_location - file system location in which to save the results of the\n        DQ process.\n    result_sink_partitions - the list of partitions to consider.\n    result_sink_format - format of the result table (e.g. delta, parquet, kafka...).\n    result_sink_options - extra spark options for configuring the result sink.\n        E.g: can be used to configure a Kafka sink if result_sink_format is kafka.\n    result_sink_explode - flag to determine if the output table/location should have\n        the columns exploded (as True) or not (as False). Default: True.\n    result_sink_extra_columns - list of extra columns to be exploded (following\n        the pattern \"<name>.*\") or columns to be selected. It is only used when\n        result_sink_explode is set to True.\n    source - name of data source, to be easier to identify in analysis. If not\n        specified, it is set as default <input_id>. This will be only used\n        when result_sink_explode is set to True.\n    fail_on_error - whether to fail the algorithm if the validations of your data in\n        the DQ process failed.\n    cache_df - whether to cache the dataframe before running the DQ process or not.\n    critical_functions - functions that should not fail. When this argument is\n        defined, fail_on_error is nullified.\n    max_percentage_failure - percentage of failure that should be allowed.\n        This argument has priority over both fail_on_error and critical_functions.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.DQSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dq_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dq_functions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unexpected_rows_pk</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tbl_to_derive_pk</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gx_result_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;COMPLETE&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tag_source_data</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">assistant_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">store_backend</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;s3&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">local_fs_root_dir</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bucket</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">data_docs_bucket</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">expectations_store_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/expectations/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">validations_store_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/validations/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_docs_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/data_docs/site/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_store_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/checkpoints/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_asset_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">expectation_suite_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_explode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_extra_columns</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">source</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fail_on_error</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">cache_df</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">critical_functions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_percentage_failure</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.MergeOptions", "modulename": "lakehouse_engine.core.definitions", "qualname": "MergeOptions", "kind": "class", "doc": "<p>Options for a merge operation.</p>\n\n<p>merge_predicate: predicate to apply to the merge operation so that we can check if a\n    new record corresponds to a record already included in the historical data.\ninsert_only: indicates if the merge should only insert data (e.g., deduplicate\n    scenarios).\ndelete_predicate: predicate to apply to the delete operation.\nupdate_predicate: predicate to apply to the update operation.\ninsert_predicate: predicate to apply to the insert operation.\nupdate_column_set: rules to apply to the update operation which allows to set the\n    value for each column to be updated.\n    (e.g. {\"data\": \"new.data\", \"count\": \"current.count + 1\"} )\ninsert_column_set: rules to apply to the insert operation which allows to set the\n    value for each column to be inserted.\n    (e.g. {\"date\": \"updates.date\", \"count\": \"1\"} )</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.MergeOptions.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "MergeOptions.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">merge_predicate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">insert_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">delete_predicate</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">update_predicate</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">insert_predicate</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">update_column_set</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">insert_column_set</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.OutputSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputSpec", "kind": "class", "doc": "<p>Specification of an algorithm output.</p>\n\n<p>This is very aligned with the way the execution environment connects to the output\nsystems (e.g., spark outputs).</p>\n\n<p>spec_id: id of the output specification.\ninput_id: id of the corresponding input specification.\nwrite_type: type of write operation.\ndata_format: format of the output. Defaults to DELTA.\ndb_table: table name in the form of <db>.<table>.\nlocation: uri that identifies from where to write data in the specified format.\npartitions: list of partition input_col names.\nmerge_opts: options to apply to the merge operation.\nstreaming_micro_batch_transformers: transformers to invoke for each streaming micro\n    batch, before writing (i.e., in Spark's foreachBatch structured\n    streaming function). Note: the lakehouse engine manages this for you, so\n    you don't have to manually specify streaming transformations here, so we don't\n    advise you to manually specify transformations through this parameter. Supply\n    them as regular transformers in the transform_specs sections of an ACON.\nstreaming_once: if the streaming query is to be executed just once, or not,\n    generating just one micro batch.\nstreaming_processing_time: if streaming query is to be kept alive, this indicates\n    the processing time of each micro batch.\nstreaming_available_now: if set to True, set a trigger that processes all available\n    data in multiple batches then terminates the query.\n    When using streaming, this is the default trigger that the lakehouse-engine will\n    use, unless you configure a different one.\nstreaming_continuous: set a trigger that runs a continuous query with a given\n    checkpoint interval.\nstreaming_await_termination: whether to wait (True) for the termination of the\n    streaming query (e.g. timeout or exception) or not (False). Default: True.\nstreaming_await_termination_timeout: a timeout to set to the\n    streaming_await_termination. Default: None.\nwith_batch_id: whether to include the streaming batch id in the final data, or not.\n    It only takes effect in streaming mode.\noptions: dict with other relevant options according to the execution environment\n    (e.g., spark) possible outputs.  E.g.,: JDBC options, checkpoint location for\n    streaming, etc.\nstreaming_micro_batch_dq_processors: similar to streaming_micro_batch_transformers\n    but for the DQ functions to be executed. Used internally by the lakehouse\n    engine, so you don't have to supply DQ functions through this parameter. Use the\n    dq_specs of the acon instead.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.OutputSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">write_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">merge_opts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">MergeOptions</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_micro_batch_transformers</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TransformerSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_once</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_processing_time</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_available_now</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_continuous</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_await_termination</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_await_termination_timeout</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_batch_id</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_micro_batch_dq_processors</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.TerminatorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "TerminatorSpec", "kind": "class", "doc": "<p>Terminator Specification.</p>\n\n<p>I.e., the specification that defines a terminator operation to be executed. Examples\nare compute statistics, vacuum, optimize, etc.</p>\n\n<p>spec_id: id of the terminate specification.\nfunction: terminator function to execute.\nargs: arguments of the terminator function.\ninput_id: id of the corresponding output specification (Optional).</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.TerminatorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "TerminatorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">function</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.ReconciliatorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReconciliatorSpec", "kind": "class", "doc": "<p>Reconciliator Specification.</p>\n\n<p>metrics: list of metrics in the form of:\n    [{\n        metric: name of the column present in both truth and current datasets,\n        aggregation: sum, avg, max, min, ...,\n        type: percentage or absolute,\n        yellow: value,\n        red: value\n    }].\nrecon_type: reconciliation type (percentage or absolute). Percentage calculates\n    the difference between truth and current results as a percentage (x-y/x), and\n    absolute calculates the raw difference (x - y).\ntruth_input_spec: input specification of the truth data.\ncurrent_input_spec: input specification of the current results data\ntruth_preprocess_query: additional query on top of the truth input data to\n    preprocess the truth data before it gets fueled into the reconciliation process.\n    Important note: you need to assume that the data out of\n    the truth_input_spec is referencable by a table called 'truth'.\ntruth_preprocess_query_args: optional dict having the functions/transformations to\n    apply on top of the truth_preprocess_query and respective arguments. Note: cache\n    is being applied on the Dataframe, by default. For turning the default behavior\n    off, pass <code>\"truth_preprocess_query_args\": []</code>.\ncurrent_preprocess_query: additional query on top of the current results input data\n    to preprocess the current results data before it gets fueled into the\n    reconciliation process. Important note: you need to assume that the data out of\n    the current_results_input_spec is referencable by a table called 'current'.\ncurrent_preprocess_query_args: optional dict having the functions/transformations to\n    apply on top of the current_preprocess_query and respective arguments. Note:\n    cache is being applied on the Dataframe, by default. For turning the default\n    behavior off, pass <code>\"current_preprocess_query_args\": []</code>.\nignore_empty_df: optional boolean, to ignore the recon process if source &amp; target\n   dataframes are empty, recon will exit success code (passed)</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.ReconciliatorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReconciliatorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">truth_input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">current_input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">truth_preprocess_query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">truth_preprocess_query_args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">current_preprocess_query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">current_preprocess_query_args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_empty_df</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.DQValidatorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQValidatorSpec", "kind": "class", "doc": "<p>Data Quality Validator Specification.</p>\n\n<p>input_spec: input specification of the data to be checked/validated.\ndq_spec: data quality specification.\nrestore_prev_version: specify if, having\ndelta table/files as input, they should be restored to the\nprevious version if the data quality process fails. Note: this\nis only considered if fail_on_error is kept as True.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.DQValidatorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQValidatorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">dq_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span>,</span><span class=\"param\">\t<span class=\"n\">restore_prev_version</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions", "kind": "class", "doc": "<p>SQL definitions statements.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.compute_table_stats", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.compute_table_stats", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.compute_table_stats: &#x27;ANALYZE TABLE {} COMPUTE STATISTICS&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.drop_table_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.drop_table_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.drop_table_stmt: &#x27;DROP TABLE IF EXISTS&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.drop_view_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.drop_view_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.drop_view_stmt: &#x27;DROP VIEW IF EXISTS&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.truncate_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.truncate_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.truncate_stmt: &#x27;TRUNCATE TABLE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.describe_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.describe_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.describe_stmt: &#x27;DESCRIBE TABLE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.optimize_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.optimize_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.optimize_stmt: &#x27;OPTIMIZE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.show_tbl_props_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.show_tbl_props_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.show_tbl_props_stmt: &#x27;SHOW TBLPROPERTIES&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.delete_where_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.delete_where_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.delete_where_stmt: &#x27;DELETE FROM {} WHERE {}&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys", "kind": "class", "doc": "<p>File Manager s3 api keys.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.CONTENTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.CONTENTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.CONTENTS: &#x27;Contents&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.KEY", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.KEY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.KEY: &#x27;Key&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.CONTINUATION", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.CONTINUATION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.CONTINUATION: &#x27;NextContinuationToken&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.BUCKET", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.BUCKET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.BUCKET: &#x27;Bucket&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.OBJECTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.OBJECTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.OBJECTS: &#x27;Objects&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SensorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorSpec", "kind": "class", "doc": "<p>Sensor Specification.</p>\n\n<p>sensor_id: sensor id.\nassets: a list of assets that are considered as available to\n    consume downstream after this sensor has status\n    PROCESSED_NEW_DATA.\ncontrol_db_table_name: db.table to store sensor metadata.\ninput_spec: input specification of the source to be checked for new data.\npreprocess_query: SQL query to transform/filter the result from the\n    upstream. Consider that we should refer to 'new_data' whenever\n    we are referring to the input of the sensor. E.g.:\n        \"SELECT dummy_col FROM new_data WHERE ...\"\ncheckpoint_location: optional location to store checkpoints to resume\n    from. These checkpoints use the same as Spark checkpoint strategy.\n    For Spark readers that do not support checkpoints, use the\n    preprocess_query parameter to form a SQL query to filter the result\n    from the upstream accordingly.\nfail_on_empty_result: if the sensor should throw an error if there is no new\n    data in the upstream. Default: True.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.SensorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">preprocess_query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fail_on_empty_result</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.SensorSpec.create_from_acon", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorSpec.create_from_acon", "kind": "function", "doc": "<p>Create SensorSpec from acon.</p>\n\n<p>Args:\n    acon: sensor ACON.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.SensorStatus", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorStatus", "kind": "class", "doc": "<p>Status for a sensor.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SensorStatus.ACQUIRED_NEW_DATA", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorStatus.ACQUIRED_NEW_DATA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SensorStatus.ACQUIRED_NEW_DATA: &#x27;ACQUIRED_NEW_DATA&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SensorStatus.PROCESSED_NEW_DATA", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorStatus.PROCESSED_NEW_DATA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SensorStatus.PROCESSED_NEW_DATA: &#x27;PROCESSED_NEW_DATA&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain", "kind": "class", "doc": "<p>Defaults used on consuming data from SAP Logchain.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain.DBTABLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain.DBTABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SAPLogchain.DBTABLE: &#x27;SAPPHA.RSPCLOGCHAIN&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain.GREEN_STATUS", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain.GREEN_STATUS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SAPLogchain.GREEN_STATUS: &#x27;G&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain.ENGINE_TABLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain.ENGINE_TABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SAPLogchain.ENGINE_TABLE: &#x27;sensor_new_data&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType", "kind": "class", "doc": "<p>Archive types.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.BULK", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.BULK", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreType.BULK: &#x27;Bulk&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.STANDARD", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.STANDARD", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreType.STANDARD: &#x27;Standard&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.EXPEDITED", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.EXPEDITED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreType.EXPEDITED: &#x27;Expedited&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.values", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.values", "kind": "function", "doc": "<p>Generates a list containing all enum values.</p>\n\n<p>Return:\n    A list with all enum values.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.exists", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.exists", "kind": "function", "doc": "<p>Checks if the restore type exists in the enum values.</p>\n\n<p>Args:\n    restore_type: restore type to check if exists.</p>\n\n<p>Return:\n    If the restore type exists in our enum.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">restore_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus", "kind": "class", "doc": "<p>Archive types.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus.NOT_STARTED", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus.NOT_STARTED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreStatus.NOT_STARTED: &#x27;not_started&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus.ONGOING", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus.ONGOING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreStatus.ONGOING: &#x27;ongoing&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus.RESTORED", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus.RESTORED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreStatus.RESTORED: &#x27;restored&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.exec_env", "modulename": "lakehouse_engine.core.exec_env", "kind": "module", "doc": "<p>Module to take care of creating a singleton of the execution environment class.</p>\n"}, {"fullname": "lakehouse_engine.core.exec_env.ExecEnv", "modulename": "lakehouse_engine.core.exec_env", "qualname": "ExecEnv", "kind": "class", "doc": "<p>Represents the basic resources regarding the engine execution environment.</p>\n\n<p>Currently, it is used to encapsulate both the logic to get the Spark\nsession and the engine configurations.</p>\n"}, {"fullname": "lakehouse_engine.core.exec_env.ExecEnv.set_default_engine_config", "modulename": "lakehouse_engine.core.exec_env", "qualname": "ExecEnv.set_default_engine_config", "kind": "function", "doc": "<p>Set default engine configurations by reading them from a specified package.</p>\n\n<p>Args:\n    package: package where the engine configurations can be found.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">package</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.exec_env.ExecEnv.get_or_create", "modulename": "lakehouse_engine.core.exec_env", "qualname": "ExecEnv.get_or_create", "kind": "function", "doc": "<p>Get or create an execution environment session (currently Spark).</p>\n\n<p>It instantiates a singleton session that can be accessed anywhere from the\nlakehouse engine.</p>\n\n<p>Args:\n    session: spark session.\n    enable_hive_support: whether to enable hive support or not.\n    app_name: application name.\n    config: extra spark configs to supply to the spark session.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">session</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">SparkSession</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">enable_hive_support</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">app_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.executable", "modulename": "lakehouse_engine.core.executable", "kind": "module", "doc": "<p>Module representing an executable lakehouse engine component.</p>\n"}, {"fullname": "lakehouse_engine.core.executable.Executable", "modulename": "lakehouse_engine.core.executable", "qualname": "Executable", "kind": "class", "doc": "<p>Abstract class defining the behaviour of an executable component.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.core.executable.Executable.execute", "modulename": "lakehouse_engine.core.executable", "qualname": "Executable.execute", "kind": "function", "doc": "<p>Define the executable component behaviour.</p>\n\n<p>E.g., the behaviour of an algorithm inheriting from this.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager", "modulename": "lakehouse_engine.core.file_manager", "kind": "module", "doc": "<p>File manager module.</p>\n"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager", "kind": "class", "doc": "<p>Set of actions to manipulate files in several ways.</p>\n"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.__init__", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.__init__", "kind": "function", "doc": "<p>Construct FileManager algorithm instances.</p>\n\n<p>Args:\n    configs: configurations for the FileManager algorithm.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">configs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.get_function", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.get_function", "kind": "function", "doc": "<p>Get a specific function to execute.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.delete_objects", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.delete_objects", "kind": "function", "doc": "<p>Delete objects and 'directories' in s3.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be deleted based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.copy_objects", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.copy_objects", "kind": "function", "doc": "<p>Copies objects and 'directories' in s3.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.request_restore", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.request_restore", "kind": "function", "doc": "<p>Request the restore of archived data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.check_restore_status", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.check_restore_status", "kind": "function", "doc": "<p>Check the restore status of archived data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.request_restore_to_destination_and_wait", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.request_restore_to_destination_and_wait", "kind": "function", "doc": "<p>Request and wait for the restore to complete, polling the restore status.</p>\n\n<p>After the restore is done, copy the restored files to destination</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.ArchiveFileManager", "modulename": "lakehouse_engine.core.file_manager", "qualname": "ArchiveFileManager", "kind": "class", "doc": "<p>Set of actions to restore archives.</p>\n"}, {"fullname": "lakehouse_engine.core.file_manager.ArchiveFileManager.check_restore_status", "modulename": "lakehouse_engine.core.file_manager", "qualname": "ArchiveFileManager.check_restore_status", "kind": "function", "doc": "<p>Check the restore status of archived data.</p>\n\n<p>Args:\n    source_bucket: name of bucket to check the restore status.\n    source_object: object to check the restore status.</p>\n\n<p>Returns:\n    A dict containing the amount of objects in each status.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">source_bucket</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">source_object</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.ArchiveFileManager.request_restore", "modulename": "lakehouse_engine.core.file_manager", "qualname": "ArchiveFileManager.request_restore", "kind": "function", "doc": "<p>Request the restore of archived data.</p>\n\n<p>Args:\n    source_bucket: name of bucket to perform the restore.\n    source_object: object to be restored.\n    restore_expiration: restore expiration in days.\n    retrieval_tier: type of restore, possible values are:\n        Bulk, Standard or Expedited.\n    dry_run: if dry_run is set to True the function will print a dict with\n        all the paths that would be deleted based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">source_bucket</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">source_object</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">restore_expiration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_tier</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dry_run</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.ArchiveFileManager.request_restore_and_wait", "modulename": "lakehouse_engine.core.file_manager", "qualname": "ArchiveFileManager.request_restore_and_wait", "kind": "function", "doc": "<p>Request and wait for the restore to complete, polling the restore status.</p>\n\n<p>Args:\n    source_bucket: name of bucket to perform the restore.\n    source_object: object to be restored.\n    restore_expiration: restore expiration in days.\n    retrieval_tier: type of restore, possible values are:\n        Bulk, Standard or Expedited.\n    dry_run: if dry_run is set to True the function will print a dict with\n        all the paths that would be deleted based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">source_bucket</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">source_object</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">restore_expiration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_tier</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dry_run</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager", "modulename": "lakehouse_engine.core.sensor_manager", "kind": "module", "doc": "<p>Module to define Sensor Manager classes.</p>\n"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager", "kind": "class", "doc": "<p>Class to control the Sensor execution.</p>\n"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager.check_if_sensor_has_acquired_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager.check_if_sensor_has_acquired_data", "kind": "function", "doc": "<p>Check if sensor has acquired new data.</p>\n\n<p>Args:\n    sensor_id: sensor id.\n    control_db_table_name: db.table to control sensor runs.</p>\n\n<p>Returns:\n    True if acquired new data, otherwise False</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager.update_sensor_status", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager.update_sensor_status", "kind": "function", "doc": "<p>Control sensor execution storing the execution data in a delta table.</p>\n\n<p>Args:\n    sensor_spec: sensor spec containing all sensor\n        information we need to update the control status.\n    status: status of the sensor.\n    upstream_key: upstream key (e.g., used to store an attribute\n        name from the upstream so that new data can be detected\n        automatically).\n    upstream_value: upstream value (e.g., used to store the max\n        attribute value from the upstream so that new data can be\n        detected automatically).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">SensorSpec</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager.read_sensor_table_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager.read_sensor_table_data", "kind": "function", "doc": "<p>Read data from delta table containing sensor status info.</p>\n\n<p>Args:\n    sensor_id: sensor id. If this parameter is defined search occurs\n        only considering this parameter. Otherwise, it considers sensor\n        assets and checkpoint location.\n    control_db_table_name: db.table to control sensor runs.\n    assets: list of assets that are fueled by the pipeline\n        where this sensor is.</p>\n\n<p>Return:\n    Row containing the data for the provided sensor_id.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">Row</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager", "kind": "class", "doc": "<p>Class to deal with Sensor Upstream data.</p>\n"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.generate_filter_exp_query", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.generate_filter_exp_query", "kind": "function", "doc": "<p>Generates a sensor preprocess query based on timestamp logic.</p>\n\n<p>Args:\n    sensor_id: sensor id.\n    filter_exp: expression to filter incoming new data.\n        You can use the placeholder <code>?upstream_value</code> so that\n        it can be replaced by the upstream_value in the\n        control_db_table_name for this specific sensor_id.\n    control_db_table_name: db.table to retrieve the last status change\n        timestamp. This is only relevant for the jdbc sensor.\n    upstream_key: the key of custom sensor information\n        to control how to identify new data from the\n        upstream (e.g., a time column in the upstream).\n    upstream_value: value for custom sensor\n        to identify new data from the upstream\n        (e.g., the value of a time present in the upstream)\n        If none we will set the default value.\n        Note: This parameter is used just to override the\n        default value <code>-2147483647</code>.\n    upstream_table_name: value for custom sensor\n        to query new data from the upstream.\n        If none we will set the default value,\n        our <code>sensor_new_data</code> view.</p>\n\n<p>Return:\n    The query string.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">filter_exp</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.generate_sensor_table_preprocess_query", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.generate_sensor_table_preprocess_query", "kind": "function", "doc": "<p>Generates a query to be used for a sensor having other sensor as upstream.</p>\n\n<p>Args:\n    sensor_id: sensor id.</p>\n\n<p>Return:\n    The query string.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.read_new_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.read_new_data", "kind": "function", "doc": "<p>Read new data from the upstream into the sensor 'new_data_df'.</p>\n\n<p>Args:\n    sensor_spec: sensor spec containing all sensor information.</p>\n\n<p>Return:\n    An empty dataframe if it doesn't have new data otherwise the new data</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">SensorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.get_new_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.get_new_data", "kind": "function", "doc": "<p>Get new data from upstream df if it's present.</p>\n\n<p>Args:\n    new_data_df: DataFrame possibly containing new data.</p>\n\n<p>Return:\n    Optional row, present if there is new data in the upstream,\n    absent otherwise.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">new_data_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">Row</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.generate_sensor_sap_logchain_query", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.generate_sensor_sap_logchain_query", "kind": "function", "doc": "<p>Generates a sensor query based in the SAP Logchain table.</p>\n\n<p>Args:\n    chain_id: chain id to query the status on SAP.\n    dbtable: db.table to retrieve the data to\n        check if the sap chain is already finished.\n    status: db.table to retrieve the last status change\n        timestamp.\n    engine_table_name: table name exposed with the SAP LOGCHAIN data.\n        This table will be used in the jdbc query.</p>\n\n<p>Return:\n    The query string.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">chain_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA.RSPCLOGCHAIN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;G&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">engine_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sensor_new_data&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager", "modulename": "lakehouse_engine.core.table_manager", "kind": "module", "doc": "<p>Table manager module.</p>\n"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager", "kind": "class", "doc": "<p>Set of actions to manipulate tables/views in several ways.</p>\n"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.__init__", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.__init__", "kind": "function", "doc": "<p>Construct TableManager algorithm instances.</p>\n\n<p>Args:\n    configs: configurations for the TableManager algorithm.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">configs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.get_function", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.get_function", "kind": "function", "doc": "<p>Get a specific function to execute.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.create", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.create", "kind": "function", "doc": "<p>Create a new table or view on metastore.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.create_many", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.create_many", "kind": "function", "doc": "<p>Create multiple tables or views on metastore.</p>\n\n<p>In this function the path to the ddl files can be separated by comma.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.compute_table_statistics", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.compute_table_statistics", "kind": "function", "doc": "<p>Compute table statistics.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.drop_table", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.drop_table", "kind": "function", "doc": "<p>Delete table function deletes table from metastore and erases all data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.drop_view", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.drop_view", "kind": "function", "doc": "<p>Delete view function deletes view from metastore and erases all data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.truncate", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.truncate", "kind": "function", "doc": "<p>Truncate function erases all data but keeps metadata.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.vacuum", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.vacuum", "kind": "function", "doc": "<p>Vacuum function erases older versions from Delta Lake tables or locations.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.describe", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.describe", "kind": "function", "doc": "<p>Describe function describes metadata from some table or view.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.optimize", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.optimize", "kind": "function", "doc": "<p>Optimize function optimizes the layout of Delta Lake data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.execute_multiple_sql_files", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.execute_multiple_sql_files", "kind": "function", "doc": "<p>Execute multiple statements in multiple sql files.</p>\n\n<p>In this function the path to the files is separated by comma.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.execute_sql", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.execute_sql", "kind": "function", "doc": "<p>Execute sql commands separated by semicolon (;).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.show_tbl_properties", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.show_tbl_properties", "kind": "function", "doc": "<p>Show Table Properties.</p>\n\n<p>Returns: a dataframe with the table properties.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.get_tbl_pk", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.get_tbl_pk", "kind": "function", "doc": "<p>Get the primary key of a particular table.</p>\n\n<p>Returns: the list of columns that are part of the primary key.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.repair_table", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.repair_table", "kind": "function", "doc": "<p>Run the repair table command.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.delete_where", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.delete_where", "kind": "function", "doc": "<p>Run the delete where command.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors", "modulename": "lakehouse_engine.dq_processors", "kind": "module", "doc": "<p>Package to define data quality processes available in the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.assistant", "modulename": "lakehouse_engine.dq_processors.assistant", "kind": "module", "doc": "<p>Module containing the definition of a data assistant.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.assistant.Assistant", "modulename": "lakehouse_engine.dq_processors.assistant", "qualname": "Assistant", "kind": "class", "doc": "<p>Class containing the data assistant.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.assistant.Assistant.run_data_assistant", "modulename": "lakehouse_engine.dq_processors.assistant", "qualname": "Assistant.run_data_assistant", "kind": "function", "doc": "<p>Entrypoint to run the data assistant.</p>\n\n<p>Based on the data, it uses GE Onboarding Data Assistant to generate expectations\nthat can be applied to the data. Then, it returns the generated expectations\nand, depending on your configuration, it can display plots of the metrics,\nexpectations and also display or store the profiling of the data, for you to get\na better sense of it.</p>\n\n<p>Args:\n    context: the BaseDataContext containing the configurations for the data\n    source and store backend.\n    batch_request: batch request to be able to query underlying data.\n    expectation_suite_name: name of the expectation suite.\n    assistant_options: additional options to pass to the DQ assistant processor.\n    data: the input dataframe for which the DQ is running.\n    profile_file_name: file name for storing the profiling html file.</p>\n\n<p>Returns:\n    The context with the expectation suite stored.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">BaseDataContext</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_request</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"o\">.</span><span class=\"n\">RuntimeBatchRequest</span>,</span><span class=\"param\">\t<span class=\"n\">expectation_suite_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">assistant_options</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">profile_file_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations", "modulename": "lakehouse_engine.dq_processors.custom_expectations", "kind": "module", "doc": "<p>Package containing custom DQ expectations available in the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "kind": "module", "doc": "<p>Expectation to check if column 'a' is lower or equal than column 'b'.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b.ColumnPairCustom", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "qualname": "ColumnPairCustom", "kind": "class", "doc": "<p>Asserts that column 'A' is lower or equal than column 'B'.</p>\n\n<p>Additionally, the 'margin' parameter can be used to add a margin to the\ncheck between column 'A' and 'B': 'A' &lt;= 'B' + 'margin'.</p>\n", "bases": "great_expectations.expectations.metrics.map_metric_provider.column_pair_map_metric_provider.ColumnPairMapMetricProvider"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b.ExpectColumnPairAToBeSmallerOrEqualThanB", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "qualname": "ExpectColumnPairAToBeSmallerOrEqualThanB", "kind": "class", "doc": "<p>Expect values in column A to be lower or equal than column B.</p>\n\n<p>Args:\n    column_A: The first column name.\n    column_B: The second column name.\n    margin: additional approximation to column B value.</p>\n\n<p>Keyword Args:\n    allow_cross_type_comparisons: If True, allow\n        comparisons between types (e.g. integer and string).\n        Otherwise, attempting such comparisons will raise an exception.\n    ignore_row_if: \"both_values_are_missing\",\n        \"either_value_is_missing\", \"neither\" (default).\n    result_format: Which output mode to use:\n        <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.\n    include_config: If True (default), then include the expectation config\n        as part of the result object.\n    catch_exceptions: If True, then catch exceptions and\n        include them as part of the result object. Default: False.\n    meta: A JSON-serializable dictionary (nesting allowed)\n        that will be included in the output without modification.</p>\n\n<p>Returns:\n    An ExpectationSuiteValidationResult.</p>\n", "bases": "great_expectations.expectations.expectation.ColumnPairMapExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "kind": "module", "doc": "<p>Expectation to check if column value is a date within a timeframe.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than.ColumnValuesDateNotOlderThan", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "qualname": "ColumnValuesDateNotOlderThan", "kind": "class", "doc": "<p>Asserts that column values are a date that isn't older than a given date.</p>\n", "bases": "great_expectations.expectations.metrics.map_metric_provider.column_map_metric_provider.ColumnMapMetricProvider"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than.ExpectColumnValuesToBeDateNotOlderThan", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "qualname": "ExpectColumnValuesToBeDateNotOlderThan", "kind": "class", "doc": "<p>Expect value in column to be date that is not older than a given time.</p>\n\n<p>Since timedelta can only define an interval up to weeks, a month is defined\nas 4 weeks and a year is defined as 52 weeks.</p>\n\n<p>Args:\n    column: Name of column to validate\n    Note: Column must be of type Date, Timestamp or String (with Timestamp format).\n    Format: yyyy-MM-ddTHH:mm:ss\n    timeframe: dict with the definition of the timeframe.\n    kwargs: dict with additional parameters.</p>\n\n<p>Keyword Args:\n    allow_cross_type_comparisons: If True, allow\n        comparisons between types (e.g. integer and string).\n        Otherwise, attempting such comparisons will raise an exception.\n    ignore_row_if: \"both_values_are_missing\",\n        \"either_value_is_missing\", \"neither\" (default).\n    result_format: Which output mode to use:\n        <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.\n    include_config: If True (default), then include the expectation config\n        as part of the result object.\n    catch_exceptions: If True, then catch exceptions and\n        include them as part of the result object. Default: False.\n    meta: A JSON-serializable dictionary (nesting allowed)\n        that will be included in the output without modification.</p>\n\n<p>Returns:\n    An ExpectationSuiteValidationResult.</p>\n", "bases": "great_expectations.expectations.expectation.ColumnMapExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "kind": "module", "doc": "<p>Expectation to check if column 'a' equals 'b', or 'c'.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c.MulticolumnCustomMetric", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "qualname": "MulticolumnCustomMetric", "kind": "class", "doc": "<p>Expectation metric definition.</p>\n\n<p>This expectation asserts that column 'a' must equal to column 'b' or column 'c'.\nIn addition to this it is possible to validate that column 'b' or 'c' match a regex.</p>\n", "bases": "great_expectations.expectations.metrics.map_metric_provider.multicolumn_map_metric_provider.MulticolumnMapMetricProvider"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c.ExpectMulticolumnColumnAMustEqualBOrC", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "qualname": "ExpectMulticolumnColumnAMustEqualBOrC", "kind": "class", "doc": "<p>MultiColumn Expectation.</p>\n\n<p>Expect that the column 'a' is equal to 'b' when this is\nnot empty; otherwise 'a' must be equal to 'c'.</p>\n\n<p>Args:\n    column_list: The column names to evaluate.</p>\n\n<p>Keyword Args:\n    ignore_row_if: default to \"never\".\n    result_format:  Which output mode to use:\n       <code>BOOLEAN_ONLY</code>, <code>BASIC</code>, <code>COMPLETE</code>, or <code>SUMMARY</code>.\n       Default set to <code>BASIC</code>.\n    include_config: If True, then include the expectation\n       config as part of the result object.\n       Default set to True.\n    catch_exceptions: If True, then catch exceptions\n       and include them as part of the result object.\n       Default set to False.</p>\n\n<p>Returns:\n    An ExpectationSuiteValidationResult.</p>\n", "bases": "great_expectations.expectations.expectation.MulticolumnMapExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "kind": "module", "doc": "<p>Expectation to check if aggregated column satisfy the condition.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be.ExpectQueriedColumnAggValueToBe", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "qualname": "ExpectQueriedColumnAggValueToBe", "kind": "class", "doc": "<p>Expect agg of column to satisfy the condition specified.</p>\n\n<p>Args:\n    template_dict: dict with the following keys:\n        column (column to check sum).\n        group_column_list (group by column names to be listed).\n        condition (how to validate the aggregated value eg: between,\n        greater, lesser).\n        max_value (maximum allowed value).\n        min_value (minimum allowed value).\n        agg_type (sum/count/max/min).</p>\n", "bases": "great_expectations.expectations.expectation.QueryExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be.ExpectQueriedColumnAggValueToBe.validate_configuration", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "qualname": "ExpectQueriedColumnAggValueToBe.validate_configuration", "kind": "function", "doc": "<p>Validates that a configuration has been set.</p>\n\n<p>Args:\n    configuration (OPTIONAL[ExpectationConfiguration]):\n    An optional Expectation Configuration entry.</p>\n\n<p>Returns:\n    None. Raises InvalidExpectationConfigurationError</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">expectation_configuration</span><span class=\"o\">.</span><span class=\"n\">ExpectationConfiguration</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.dq_factory", "modulename": "lakehouse_engine.dq_processors.dq_factory", "kind": "module", "doc": "<p>Module containing the class definition of the Data Quality Factory.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.dq_factory.DQFactory", "modulename": "lakehouse_engine.dq_processors.dq_factory", "qualname": "DQFactory", "kind": "class", "doc": "<p>Class for the Data Quality Factory.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.dq_factory.DQFactory.run_dq_process", "modulename": "lakehouse_engine.dq_processors.dq_factory", "qualname": "DQFactory.run_dq_process", "kind": "function", "doc": "<p>Run the specified data quality process on a dataframe.</p>\n\n<p>Based on the dq_specs we apply the defined expectations on top of the dataframe\nin order to apply the necessary validations and then output the result of\nthe data quality process.</p>\n\n<p>Args:\n    dq_spec: data quality specification.\n    data: input dataframe to run the dq process on.</p>\n\n<p>Returns:\n    The DataFrame containing the results of the DQ process.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">dq_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.exceptions", "modulename": "lakehouse_engine.dq_processors.exceptions", "kind": "module", "doc": "<p>Package defining all the DQ custom exceptions.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.exceptions.DQValidationsFailedException", "modulename": "lakehouse_engine.dq_processors.exceptions", "qualname": "DQValidationsFailedException", "kind": "class", "doc": "<p>Exception for when the data quality validations fail.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.dq_processors.exceptions.DQCheckpointsResultsException", "modulename": "lakehouse_engine.dq_processors.exceptions", "qualname": "DQCheckpointsResultsException", "kind": "class", "doc": "<p>Exception for when the checkpoint results parsing fail.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.dq_processors.validator", "modulename": "lakehouse_engine.dq_processors.validator", "kind": "module", "doc": "<p>Module containing the definition of a data quality validator.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.validator.Validator", "modulename": "lakehouse_engine.dq_processors.validator", "qualname": "Validator", "kind": "class", "doc": "<p>Class containing the data quality validator.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.validator.Validator.get_dq_validator", "modulename": "lakehouse_engine.dq_processors.validator", "qualname": "Validator.get_dq_validator", "kind": "function", "doc": "<p>Get a validator according to the specification.</p>\n\n<p>We use getattr to dynamically execute any expectation available.\ngetattr(validator, function) is similar to validator.function(). With this\napproach, we can execute any expectation supported.</p>\n\n<p>Args:\n    context: the BaseDataContext containing the configurations for the data\n    source and store backend.\n    batch_request: run time batch request to be able to query underlying data.\n    expectation_suite_name: name of the expectation suite.\n    dq_functions: a list of DQFunctionSpec to consider in the expectation suite.\n    critical_functions: list of critical expectations in the expectation suite.</p>\n\n<p>Returns:\n    The validator with the expectation suite stored.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">BaseDataContext</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_request</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"o\">.</span><span class=\"n\">RuntimeBatchRequest</span>,</span><span class=\"param\">\t<span class=\"n\">expectation_suite_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dq_functions</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">critical_functions</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.validator.Validator.tag_source_with_dq", "modulename": "lakehouse_engine.dq_processors.validator", "qualname": "Validator.tag_source_with_dq", "kind": "function", "doc": "<p>Tags the source dataframe with a new column having the DQ results.</p>\n\n<p>Args:\n    source_pk: the primary key of the source data.\n    source_df: the source dataframe to be tagged with DQ results.\n    results_df: dq results dataframe.</p>\n\n<p>Returns: a dataframe tagged with the DQ results.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">source_pk</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">source_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">results_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine", "modulename": "lakehouse_engine.engine", "kind": "module", "doc": "<p>Contract of the lakehouse engine with all the available functions to be executed.</p>\n"}, {"fullname": "lakehouse_engine.engine.load_data", "modulename": "lakehouse_engine.engine", "qualname": "load_data", "kind": "function", "doc": "<p>Load data using the DataLoader algorithm.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks or other\n        apps).\n    collect_engine_usage: a boolean that enables or disables the collection and\n            storage of Lakehouse usage statistics.\n    spark_confs: optional dictionary with the spark confs to be used when collecting\n        the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.execute_reconciliation", "modulename": "lakehouse_engine.engine", "qualname": "execute_reconciliation", "kind": "function", "doc": "<p>Execute the Reconciliator algorithm.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks or other\n        apps).\n    collect_engine_usage: whether we want to enable the collection and storage of\n        lakehouse engine usage stats or not.\n    spark_confs: optional dictionary with the spark confs to be used when collecting\n        the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.execute_dq_validation", "modulename": "lakehouse_engine.engine", "qualname": "execute_dq_validation", "kind": "function", "doc": "<p>Execute the DQValidator algorithm.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks or other\n        apps).\n    collect_engine_usage: whether we want to enable the collection and storage of\n        lakehouse engine usage stats or not.\n    spark_confs: optional dictionary with the spark confs to be used when collecting\n        the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.manage_table", "modulename": "lakehouse_engine.engine", "qualname": "manage_table", "kind": "function", "doc": "<p>Manipulate tables/views using Table Manager algorithm.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks\n        or other apps).\n    collect_engine_usage: whether we want to enable the collection and storage of\n        lakehouse engine usage stats or not.\n    spark_confs: optional dictionary with the spark confs to be used when collecting\n        the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.manage_files", "modulename": "lakehouse_engine.engine", "qualname": "manage_files", "kind": "function", "doc": "<p>Manipulate s3 files using File Manager algorithm.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks\n        or other apps).\n    collect_engine_usage: whether we want to enable the collection and storage of\n        lakehouse engine usage stats or not.\n    spark_confs: optional dictionary with the spark confs to be used when collecting\n        the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.execute_sensor", "modulename": "lakehouse_engine.engine", "qualname": "execute_sensor", "kind": "function", "doc": "<p>Execute a sensor based on a Sensor Algorithm Configuration.</p>\n\n<p>A sensor is useful to check if an upstream system has new data.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks\n        or other apps).\n    collect_engine_usage: whether we want to enable the collection and storage of\n        lakehouse engine usage stats or not.\n    spark_confs: optional dictionary with the spark confs to be used when collecting\n        the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.update_sensor_status", "modulename": "lakehouse_engine.engine", "qualname": "update_sensor_status", "kind": "function", "doc": "<p>Update internal sensor status.</p>\n\n<p>Update the sensor status in the control table,\nit should be used to tell the system\nthat the sensor has processed all new data that was previously identified,\nhence updating the shifted sensor status.\nUsually used to move from <code>SensorStatus.ACQUIRED_NEW_DATA</code> to\n<code>SensorStatus.PROCESSED_NEW_DATA</code>,\nbut there might be scenarios - still to identify -\nwhere we can update the sensor status from/to different statuses.</p>\n\n<p>Args:\n    sensor_id: sensor id.\n    control_db_table_name: db.table to store sensor checkpoints.\n    status: status of the sensor.\n    assets: a list of assets that are considered as available to\n        consume downstream after this sensor has status\n        PROCESSED_NEW_DATA.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;PROCESSED_NEW_DATA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.generate_sensor_query", "modulename": "lakehouse_engine.engine", "qualname": "generate_sensor_query", "kind": "function", "doc": "<p>Generates a preprocess query to be used in a sensor configuration.</p>\n\n<p>Args:\n    sensor_id: sensor id.\n    filter_exp: expression to filter incoming new data.\n        You can use the placeholder ?default_upstream_key and\n        ?default_upstream_value, so that it can be replaced by the\n        respective values in the control_db_table_name for this specific\n        sensor_id.\n    control_db_table_name: db.table to retrieve the last status change\n        timestamp. This is only relevant for the jdbc sensor.\n    upstream_key: the key of custom sensor information to control how to\n        identify new data from the upstream (e.g., a time column in the\n        upstream).\n    upstream_value: the upstream value\n        to identify new data from the upstream (e.g., the value of a time\n        present in the upstream).\n    upstream_table_name: value for custom sensor\n            to query new data from the upstream\n            If none we will set the default value,\n            our <code>sensor_new_data</code> view.</p>\n\n<p>Return:\n    The query string.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">filter_exp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.generate_sensor_sap_logchain_query", "modulename": "lakehouse_engine.engine", "qualname": "generate_sensor_sap_logchain_query", "kind": "function", "doc": "<p>Generates a sensor query based in the SAP Logchain table.</p>\n\n<p>Args:\n    chain_id: chain id to query the status on SAP.\n    dbtable: db.table to retrieve the data to\n            check if the sap chain is already finished.\n    status: db.table to retrieve the last status change\n            timestamp.\n    engine_table_name: table name exposed with the SAP LOGCHAIN data.\n            This table will be used in the jdbc query.</p>\n\n<p>Return:\n    The query string.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">chain_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA.RSPCLOGCHAIN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;G&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">engine_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sensor_new_data&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.send_notification", "modulename": "lakehouse_engine.engine", "qualname": "send_notification", "kind": "function", "doc": "<p>Send a notification using a notifier.</p>\n\n<p>Args:\n    args: arguments for the notifier.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io", "modulename": "lakehouse_engine.io", "kind": "module", "doc": "<p>Input and Output package responsible for the behaviour of reading and writing.</p>\n"}, {"fullname": "lakehouse_engine.io.exceptions", "modulename": "lakehouse_engine.io.exceptions", "kind": "module", "doc": "<p>Package defining all the io custom exceptions.</p>\n"}, {"fullname": "lakehouse_engine.io.exceptions.IncrementalFilterInputNotFoundException", "modulename": "lakehouse_engine.io.exceptions", "qualname": "IncrementalFilterInputNotFoundException", "kind": "class", "doc": "<p>Exception for when the input of an incremental filter is not found.</p>\n\n<p>This may occur when tables are being loaded in incremental way, taking the increment\ndefinition out of a specific table, but the table still does not exist, mainly\nbecause probably it was not loaded for the first time yet.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.io.exceptions.WrongIOFormatException", "modulename": "lakehouse_engine.io.exceptions", "qualname": "WrongIOFormatException", "kind": "class", "doc": "<p>Exception for when a user provides a wrong I/O format.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.io.exceptions.NotSupportedException", "modulename": "lakehouse_engine.io.exceptions", "qualname": "NotSupportedException", "kind": "class", "doc": "<p>Exception for when a user provides a not supported operation.</p>\n", "bases": "builtins.RuntimeError"}, {"fullname": "lakehouse_engine.io.reader", "modulename": "lakehouse_engine.io.reader", "kind": "module", "doc": "<p>Defines abstract reader behaviour.</p>\n"}, {"fullname": "lakehouse_engine.io.reader.Reader", "modulename": "lakehouse_engine.io.reader", "qualname": "Reader", "kind": "class", "doc": "<p>Abstract Reader class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.reader.Reader.__init__", "modulename": "lakehouse_engine.io.reader", "qualname": "Reader.__init__", "kind": "function", "doc": "<p>Construct Reader instances.</p>\n\n<p>Args:\n    input_spec: input specification for reading data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.reader.Reader.read", "modulename": "lakehouse_engine.io.reader", "qualname": "Reader.read", "kind": "function", "doc": "<p>Abstract read method.</p>\n\n<p>Returns:\n    A dataframe read according to the input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.reader_factory", "modulename": "lakehouse_engine.io.reader_factory", "kind": "module", "doc": "<p>Module for reader factory.</p>\n"}, {"fullname": "lakehouse_engine.io.reader_factory.ReaderFactory", "modulename": "lakehouse_engine.io.reader_factory", "qualname": "ReaderFactory", "kind": "class", "doc": "<p>Class for reader factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.reader_factory.ReaderFactory.get_data", "modulename": "lakehouse_engine.io.reader_factory", "qualname": "ReaderFactory.get_data", "kind": "function", "doc": "<p>Get data according to the input specification following a factory pattern.</p>\n\n<p>Args:\n    spec: input specification to get the data.</p>\n\n<p>Returns:\n    A dataframe containing the data.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers", "modulename": "lakehouse_engine.io.readers", "kind": "module", "doc": "<p>Readers package to define reading behaviour.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from dataframes.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader.DataFrameReader", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "qualname": "DataFrameReader", "kind": "class", "doc": "<p>Class to read data from a dataframe.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader.DataFrameReader.__init__", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "qualname": "DataFrameReader.__init__", "kind": "function", "doc": "<p>Construct DataFrameReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader.DataFrameReader.read", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "qualname": "DataFrameReader.read", "kind": "function", "doc": "<p>Read data from a dataframe.</p>\n\n<p>Returns:\n    A dataframe containing the data from a dataframe previously\n    computed.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.file_reader", "modulename": "lakehouse_engine.io.readers.file_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from files.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.file_reader.FileReader", "modulename": "lakehouse_engine.io.readers.file_reader", "qualname": "FileReader", "kind": "class", "doc": "<p>Class to read from files.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.file_reader.FileReader.__init__", "modulename": "lakehouse_engine.io.readers.file_reader", "qualname": "FileReader.__init__", "kind": "function", "doc": "<p>Construct FileReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.file_reader.FileReader.read", "modulename": "lakehouse_engine.io.readers.file_reader", "qualname": "FileReader.read", "kind": "function", "doc": "<p>Read file data.</p>\n\n<p>Returns:\n    A dataframe containing the data from the files.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from JDBC sources.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader.JDBCReader", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "qualname": "JDBCReader", "kind": "class", "doc": "<p>Class to read from JDBC source.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader.JDBCReader.__init__", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "qualname": "JDBCReader.__init__", "kind": "function", "doc": "<p>Construct JDBCReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader.JDBCReader.read", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "qualname": "JDBCReader.read", "kind": "function", "doc": "<p>Read data from JDBC source.</p>\n\n<p>Returns:\n    A dataframe containing the data from the JDBC source.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader", "modulename": "lakehouse_engine.io.readers.kafka_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from Kafka.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader.KafkaReader", "modulename": "lakehouse_engine.io.readers.kafka_reader", "qualname": "KafkaReader", "kind": "class", "doc": "<p>Class to read from Kafka.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader.KafkaReader.__init__", "modulename": "lakehouse_engine.io.readers.kafka_reader", "qualname": "KafkaReader.__init__", "kind": "function", "doc": "<p>Construct KafkaReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader.KafkaReader.read", "modulename": "lakehouse_engine.io.readers.kafka_reader", "qualname": "KafkaReader.read", "kind": "function", "doc": "<p>Read Kafka data.</p>\n\n<p>Returns:\n    A dataframe containing the data from Kafka.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.query_reader", "modulename": "lakehouse_engine.io.readers.query_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from a query.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.query_reader.QueryReader", "modulename": "lakehouse_engine.io.readers.query_reader", "qualname": "QueryReader", "kind": "class", "doc": "<p>Class to read data from a query.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.query_reader.QueryReader.__init__", "modulename": "lakehouse_engine.io.readers.query_reader", "qualname": "QueryReader.__init__", "kind": "function", "doc": "<p>Construct QueryReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.query_reader.QueryReader.read", "modulename": "lakehouse_engine.io.readers.query_reader", "qualname": "QueryReader.read", "kind": "function", "doc": "<p>Read data from a query.</p>\n\n<p>Returns:\n    A dataframe containing the data from the query.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from SAP B4 sources.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader.SAPB4Reader", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "qualname": "SAPB4Reader", "kind": "class", "doc": "<p>Class to read from SAP B4 source.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader.SAPB4Reader.__init__", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "qualname": "SAPB4Reader.__init__", "kind": "function", "doc": "<p>Construct SAPB4Reader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader.SAPB4Reader.read", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "qualname": "SAPB4Reader.read", "kind": "function", "doc": "<p>Read data from SAP B4 source.</p>\n\n<p>Returns:\n    A dataframe containing the data from the SAP B4 source.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from SAP BW sources.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader.SAPBWReader", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "qualname": "SAPBWReader", "kind": "class", "doc": "<p>Class to read from SAP BW source.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader.SAPBWReader.__init__", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "qualname": "SAPBWReader.__init__", "kind": "function", "doc": "<p>Construct SAPBWReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader.SAPBWReader.read", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "qualname": "SAPBWReader.read", "kind": "function", "doc": "<p>Read data from SAP BW source.</p>\n\n<p>Returns:\n    A dataframe containing the data from the SAP BW source.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader", "modulename": "lakehouse_engine.io.readers.sftp_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from SFTP.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader.SFTPReader", "modulename": "lakehouse_engine.io.readers.sftp_reader", "qualname": "SFTPReader", "kind": "class", "doc": "<p>Class to read from SFTP.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader.SFTPReader.__init__", "modulename": "lakehouse_engine.io.readers.sftp_reader", "qualname": "SFTPReader.__init__", "kind": "function", "doc": "<p>Construct SFTPReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader.SFTPReader.read", "modulename": "lakehouse_engine.io.readers.sftp_reader", "qualname": "SFTPReader.read", "kind": "function", "doc": "<p>Read SFTP data.</p>\n\n<p>Returns:\n    A dataframe containing the data from SFTP.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.table_reader", "modulename": "lakehouse_engine.io.readers.table_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from tables.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.table_reader.TableReader", "modulename": "lakehouse_engine.io.readers.table_reader", "qualname": "TableReader", "kind": "class", "doc": "<p>Class to read data from a table.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.table_reader.TableReader.__init__", "modulename": "lakehouse_engine.io.readers.table_reader", "qualname": "TableReader.__init__", "kind": "function", "doc": "<p>Construct TableReader instances.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.table_reader.TableReader.read", "modulename": "lakehouse_engine.io.readers.table_reader", "qualname": "TableReader.read", "kind": "function", "doc": "<p>Read data from a table.</p>\n\n<p>Returns:\n    A dataframe containing the data from the table.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer", "modulename": "lakehouse_engine.io.writer", "kind": "module", "doc": "<p>Defines abstract writer behaviour.</p>\n"}, {"fullname": "lakehouse_engine.io.writer.Writer", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer", "kind": "class", "doc": "<p>Abstract Writer class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.writer.Writer.__init__", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.__init__", "kind": "function", "doc": "<p>Construct Writer instances.</p>\n\n<p>Args:\n    output_spec: output specification to write data.\n    df: dataframe to write.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writer.Writer.write", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.write", "kind": "function", "doc": "<p>Abstract write method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.write_transformed_micro_batch", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.write_transformed_micro_batch", "kind": "function", "doc": "<p>Define how to write a streaming micro batch after transforming it.</p>\n\n<p>This function must define an inner function that manipulates a streaming batch,\nand then return that function. Look for concrete implementations of this\nfunction for more clarity.</p>\n\n<p>Args:\n    kwargs: any keyword arguments.</p>\n\n<p>Returns:\n    A function to be executed in the foreachBatch spark write method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.get_transformed_micro_batch", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.get_transformed_micro_batch", "kind": "function", "doc": "<p>Get the result of the transformations applied to a micro batch dataframe.</p>\n\n<p>Args:\n    output_spec: output specification associated with the writer.\n    batch_df: batch dataframe (given from streaming foreachBatch).\n    batch_id: if of the batch (given from streaming foreachBatch).\n    data: list of all dfs generated on previous steps before writer\n    to be available on micro batch transforms.</p>\n\n<p>Returns:\n    The transformed dataframe.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">batch_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">batch_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.get_streaming_trigger", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.get_streaming_trigger", "kind": "function", "doc": "<p>Define which streaming trigger will be used.</p>\n\n<p>Args:\n    output_spec: output specification.</p>\n\n<p>Returns:\n    A dict containing streaming trigger.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.run_micro_batch_dq_process", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.run_micro_batch_dq_process", "kind": "function", "doc": "<p>Run the data quality process in a streaming micro batch dataframe.</p>\n\n<p>Iterates over the specs and performs the checks or analysis depending on the\ndata quality specification provided in the configuration.</p>\n\n<p>Args:\n    df: the dataframe in which to run the dq process on.\n    dq_spec: data quality specification.</p>\n\n<p>Returns: the validated dataframe.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">dq_spec</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer_factory", "modulename": "lakehouse_engine.io.writer_factory", "kind": "module", "doc": "<p>Module for writer factory.</p>\n"}, {"fullname": "lakehouse_engine.io.writer_factory.WriterFactory", "modulename": "lakehouse_engine.io.writer_factory", "qualname": "WriterFactory", "kind": "class", "doc": "<p>Class for writer factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.writer_factory.WriterFactory.get_writer", "modulename": "lakehouse_engine.io.writer_factory", "qualname": "WriterFactory.get_writer", "kind": "function", "doc": "<p>Get a writer according to the output specification using a factory pattern.</p>\n\n<p>Args:\n    OutputSpec spec: output specification to write data.\n    DataFrame df: dataframe to be written.\n    OrderedDict data: list of all dfs generated on previous steps before writer.</p>\n\n<p>Returns:\n    Writer: writer that will write the data.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">Writer</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers", "modulename": "lakehouse_engine.io.writers", "kind": "module", "doc": "<p>Package containing the writers responsible for writing data.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.console_writer", "modulename": "lakehouse_engine.io.writers.console_writer", "kind": "module", "doc": "<p>Module to define behaviour to write to console.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.console_writer.ConsoleWriter", "modulename": "lakehouse_engine.io.writers.console_writer", "qualname": "ConsoleWriter", "kind": "class", "doc": "<p>Class to write data to console.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.console_writer.ConsoleWriter.__init__", "modulename": "lakehouse_engine.io.writers.console_writer", "qualname": "ConsoleWriter.__init__", "kind": "function", "doc": "<p>Construct ConsoleWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification\n    df: dataframe to be written.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.console_writer.ConsoleWriter.write", "modulename": "lakehouse_engine.io.writers.console_writer", "qualname": "ConsoleWriter.write", "kind": "function", "doc": "<p>Write data to console.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "kind": "module", "doc": "<p>Module to define behaviour to write to dataframe.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer.DataFrameWriter", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "qualname": "DataFrameWriter", "kind": "class", "doc": "<p>Class to write data to dataframe.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer.DataFrameWriter.__init__", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "qualname": "DataFrameWriter.__init__", "kind": "function", "doc": "<p>Construct DataFrameWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification.\n    df: dataframe to be written.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer.DataFrameWriter.write", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "qualname": "DataFrameWriter.write", "kind": "function", "doc": "<p>Write data to dataframe.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "kind": "module", "doc": "<p>Module to define the behaviour of delta merges.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer.DeltaMergeWriter", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "qualname": "DeltaMergeWriter", "kind": "class", "doc": "<p>Class to merge data using delta lake.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer.DeltaMergeWriter.__init__", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "qualname": "DeltaMergeWriter.__init__", "kind": "function", "doc": "<p>Construct DeltaMergeWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification containing merge options and\n        relevant information.\n    df: the dataframe containing the new data to be merged.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer.DeltaMergeWriter.write", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "qualname": "DeltaMergeWriter.write", "kind": "function", "doc": "<p>Merge new data with current data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.file_writer", "modulename": "lakehouse_engine.io.writers.file_writer", "kind": "module", "doc": "<p>Module to define behaviour to write to files.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.file_writer.FileWriter", "modulename": "lakehouse_engine.io.writers.file_writer", "qualname": "FileWriter", "kind": "class", "doc": "<p>Class to write data to files.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.file_writer.FileWriter.__init__", "modulename": "lakehouse_engine.io.writers.file_writer", "qualname": "FileWriter.__init__", "kind": "function", "doc": "<p>Construct FileWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification\n    df: dataframe to be written.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.file_writer.FileWriter.write", "modulename": "lakehouse_engine.io.writers.file_writer", "qualname": "FileWriter.write", "kind": "function", "doc": "<p>Write data to files.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "kind": "module", "doc": "<p>Module that defines the behaviour to write to JDBC targets.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer.JDBCWriter", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "qualname": "JDBCWriter", "kind": "class", "doc": "<p>Class to write to JDBC targets.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer.JDBCWriter.__init__", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "qualname": "JDBCWriter.__init__", "kind": "function", "doc": "<p>Construct JDBCWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification.\n    df: dataframe to be writen.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer.JDBCWriter.write", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "qualname": "JDBCWriter.write", "kind": "function", "doc": "<p>Write data into JDBC target.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer", "modulename": "lakehouse_engine.io.writers.kafka_writer", "kind": "module", "doc": "<p>Module that defines the behaviour to write to Kafka.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer.KafkaWriter", "modulename": "lakehouse_engine.io.writers.kafka_writer", "qualname": "KafkaWriter", "kind": "class", "doc": "<p>Class to write to a Kafka target.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer.KafkaWriter.__init__", "modulename": "lakehouse_engine.io.writers.kafka_writer", "qualname": "KafkaWriter.__init__", "kind": "function", "doc": "<p>Construct KafkaWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification.\n    df: dataframe to be written.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer.KafkaWriter.write", "modulename": "lakehouse_engine.io.writers.kafka_writer", "qualname": "KafkaWriter.write", "kind": "function", "doc": "<p>Write data to Kafka.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.table_writer", "modulename": "lakehouse_engine.io.writers.table_writer", "kind": "module", "doc": "<p>Module that defines the behaviour to write to tables.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.table_writer.TableWriter", "modulename": "lakehouse_engine.io.writers.table_writer", "qualname": "TableWriter", "kind": "class", "doc": "<p>Class to write to a table.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.table_writer.TableWriter.__init__", "modulename": "lakehouse_engine.io.writers.table_writer", "qualname": "TableWriter.__init__", "kind": "function", "doc": "<p>Construct TableWriter instances.</p>\n\n<p>Args:\n    output_spec: output specification.\n    df: dataframe to be written.\n    data: list of all dfs generated on previous steps before writer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.table_writer.TableWriter.write", "modulename": "lakehouse_engine.io.writers.table_writer", "qualname": "TableWriter.write", "kind": "function", "doc": "<p>Write data to a table.</p>\n\n<p>After the write operation we repair the table (e.g., update partitions).\nHowever, there's a caveat to this, which is the fact that this repair\noperation is not reachable if we are running long-running streaming mode.\nTherefore, we recommend not using the TableWriter with formats other than\ndelta lake for those scenarios (as delta lake does not need msck repair).\nSo, you can: 1) use delta lake format for the table; 2) use the FileWriter\nand run the repair with a certain frequency in a separate task of your\npipeline.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators", "modulename": "lakehouse_engine.terminators", "kind": "module", "doc": "<p>Package to define algorithm terminators (e.g., vacuum, optimize, compute stats).</p>\n"}, {"fullname": "lakehouse_engine.terminators.cdf_processor", "modulename": "lakehouse_engine.terminators.cdf_processor", "kind": "module", "doc": "<p>Defines change data feed processor behaviour.</p>\n"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor", "kind": "class", "doc": "<p>Change data feed processor class.</p>\n"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor.expose_cdf", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor.expose_cdf", "kind": "function", "doc": "<p>Expose CDF to external location.</p>\n\n<p>Args:\n    spec: terminator specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor.delete_old_data", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor.delete_old_data", "kind": "function", "doc": "<p>Delete old data from cdf delta table.</p>\n\n<p>Args:\n    spec: terminator specifications.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor.vacuum_cdf_data", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor.vacuum_cdf_data", "kind": "function", "doc": "<p>Vacuum old data from cdf delta table.</p>\n\n<p>Args:\n    spec: terminator specifications.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.dataset_optimizer", "modulename": "lakehouse_engine.terminators.dataset_optimizer", "kind": "module", "doc": "<p>Module with dataset optimizer terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.dataset_optimizer.DatasetOptimizer", "modulename": "lakehouse_engine.terminators.dataset_optimizer", "qualname": "DatasetOptimizer", "kind": "class", "doc": "<p>Class with dataset optimizer terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.dataset_optimizer.DatasetOptimizer.optimize_dataset", "modulename": "lakehouse_engine.terminators.dataset_optimizer", "qualname": "DatasetOptimizer.optimize_dataset", "kind": "function", "doc": "<p>Optimize a dataset based on a set of pre-conceived optimizations.</p>\n\n<p>Most of the times the dataset is a table, but it can be a file-based one only.</p>\n\n<p>Args:\n    db_table: database_name.table_name.\n    location: dataset/table filesystem location.\n    compute_table_stats: to compute table statistics or not.\n    vacuum: (delta lake tables only) whether to vacuum the delta lake\n        table or not.\n    vacuum_hours: (delta lake tables only) number of hours to consider\n        in vacuum operation.\n    optimize: (delta lake tables only) whether to optimize the table or\n        not. Custom optimize parameters can be supplied through ExecEnv (Spark)\n        configs\n    optimize_where: expression to use in the optimize function.\n    optimize_zorder_col_list: (delta lake tables only) list of\n        columns to consider in the zorder optimization process. Custom optimize\n        parameters can be supplied through ExecEnv (Spark) configs.\n    debug: flag indicating if we are just debugging this for local\n        tests and therefore pass through all the exceptions to perform some\n        assertions in local tests.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">compute_table_stats</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">vacuum</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">vacuum_hours</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">720</span>,</span><span class=\"param\">\t<span class=\"n\">optimize</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_where</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_zorder_col_list</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">debug</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier", "modulename": "lakehouse_engine.terminators.notifier", "kind": "module", "doc": "<p>Module with notification terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier", "kind": "class", "doc": "<p>Abstract Notification class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.__init__", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.__init__", "kind": "function", "doc": "<p>Construct Notification instances.</p>\n\n<p>Args:\n    notification_spec: notification specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">notification_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.create_notification", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.create_notification", "kind": "function", "doc": "<p>Abstract create notification method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.send_notification", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.send_notification", "kind": "function", "doc": "<p>Abstract send notification method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.check_if_notification_is_failure_notification", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.check_if_notification_is_failure_notification", "kind": "function", "doc": "<p>Check if given notification is a failure notification.</p>\n\n<p>Args:\n    spec: spec to validate if it is a failure notification.</p>\n\n<p>Returns:\n    A boolean telling if the notification is a failure notification</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier_factory", "modulename": "lakehouse_engine.terminators.notifier_factory", "kind": "module", "doc": "<p>Module for notifier factory.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifier_factory.NotifierFactory", "modulename": "lakehouse_engine.terminators.notifier_factory", "qualname": "NotifierFactory", "kind": "class", "doc": "<p>Class for notification factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.terminators.notifier_factory.NotifierFactory.get_notifier", "modulename": "lakehouse_engine.terminators.notifier_factory", "qualname": "NotifierFactory.get_notifier", "kind": "function", "doc": "<p>Get a notifier according to the terminator specs using a factory.</p>\n\n<p>Args:\n    spec: terminator specification.</p>\n\n<p>Returns:\n    Notifier: notifier that will handle notifications.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">terminators</span><span class=\"o\">.</span><span class=\"n\">notifier</span><span class=\"o\">.</span><span class=\"n\">Notifier</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier_factory.NotifierFactory.generate_failure_notification", "modulename": "lakehouse_engine.terminators.notifier_factory", "qualname": "NotifierFactory.generate_failure_notification", "kind": "function", "doc": "<p>Check if it is necessary to send a failure notification and generate it.</p>\n\n<p>Args:\n    spec: List of termination specs\n    exception: Exception that caused the failure.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"nb\">list</span>, </span><span class=\"param\"><span class=\"n\">exception</span><span class=\"p\">:</span> <span class=\"ne\">Exception</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifiers", "modulename": "lakehouse_engine.terminators.notifiers", "kind": "module", "doc": "<p>Notifications module.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "kind": "module", "doc": "<p>Module with email notifier.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier", "kind": "class", "doc": "<p>Base Notification class.</p>\n", "bases": "lakehouse_engine.terminators.notifier.Notifier"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier.__init__", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier.__init__", "kind": "function", "doc": "<p>Construct Email Notification instance.</p>\n\n<p>Args:\n    notification_spec: notification specification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">notification_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier.create_notification", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier.create_notification", "kind": "function", "doc": "<p>Creates the notification to be sent.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier.send_notification", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier.send_notification", "kind": "function", "doc": "<p>Sends the notification by using a series of methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifiers.notification_templates", "modulename": "lakehouse_engine.terminators.notifiers.notification_templates", "kind": "module", "doc": "<p>Email notification templates.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifiers.notification_templates.NotificationsTemplates", "modulename": "lakehouse_engine.terminators.notifiers.notification_templates", "qualname": "NotificationsTemplates", "kind": "class", "doc": "<p>Templates for notifications.</p>\n"}, {"fullname": "lakehouse_engine.terminators.sensor_terminator", "modulename": "lakehouse_engine.terminators.sensor_terminator", "kind": "module", "doc": "<p>Defines terminator behaviour.</p>\n"}, {"fullname": "lakehouse_engine.terminators.sensor_terminator.SensorTerminator", "modulename": "lakehouse_engine.terminators.sensor_terminator", "qualname": "SensorTerminator", "kind": "class", "doc": "<p>Sensor Terminator class.</p>\n"}, {"fullname": "lakehouse_engine.terminators.sensor_terminator.SensorTerminator.update_sensor_status", "modulename": "lakehouse_engine.terminators.sensor_terminator", "qualname": "SensorTerminator.update_sensor_status", "kind": "function", "doc": "<p>Update internal sensor status.</p>\n\n<p>Update the sensor status in the control table, it should be used to tell the\nsystem that the sensor has processed all new data that was previously\nidentified, hence updating the shifted sensor status.\nUsually used to move from <code>SensorStatus.ACQUIRED_NEW_DATA</code> to\n<code>SensorStatus.PROCESSED_NEW_DATA</code>, but there might be scenarios - still\nto identify - where we can update the sensor status from/to different statuses.</p>\n\n<p>Args:\n    sensor_id: sensor id.\n    control_db_table_name: db.table to store sensor checkpoints.\n    status: status of the sensor.\n    assets: a list of assets that are considered as available to\n        consume downstream after this sensor has status\n        PROCESSED_NEW_DATA.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;PROCESSED_NEW_DATA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.spark_terminator", "modulename": "lakehouse_engine.terminators.spark_terminator", "kind": "module", "doc": "<p>Defines terminator behaviour.</p>\n"}, {"fullname": "lakehouse_engine.terminators.spark_terminator.SparkTerminator", "modulename": "lakehouse_engine.terminators.spark_terminator", "qualname": "SparkTerminator", "kind": "class", "doc": "<p>Spark Terminator class.</p>\n"}, {"fullname": "lakehouse_engine.terminators.spark_terminator.SparkTerminator.terminate_spark", "modulename": "lakehouse_engine.terminators.spark_terminator", "qualname": "SparkTerminator.terminate_spark", "kind": "function", "doc": "<p>Terminate spark session.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.terminator_factory", "modulename": "lakehouse_engine.terminators.terminator_factory", "kind": "module", "doc": "<p>Module with the factory pattern to return terminators.</p>\n"}, {"fullname": "lakehouse_engine.terminators.terminator_factory.TerminatorFactory", "modulename": "lakehouse_engine.terminators.terminator_factory", "qualname": "TerminatorFactory", "kind": "class", "doc": "<p>TerminatorFactory class following the factory pattern.</p>\n"}, {"fullname": "lakehouse_engine.terminators.terminator_factory.TerminatorFactory.execute_terminator", "modulename": "lakehouse_engine.terminators.terminator_factory", "qualname": "TerminatorFactory.execute_terminator", "kind": "function", "doc": "<p>Execute a terminator following the factory pattern.</p>\n\n<p>Args:\n    spec: terminator specification.\n    df: dataframe to be used in the terminator. Needed when a\n        terminator requires one dataframe as input.</p>\n\n<p>Returns:\n    Transformer function to be executed in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers", "modulename": "lakehouse_engine.transformers", "kind": "module", "doc": "<p>Package to define transformers available in the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.transformers.aggregators", "modulename": "lakehouse_engine.transformers.aggregators", "kind": "module", "doc": "<p>Aggregators module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.aggregators.Aggregators", "modulename": "lakehouse_engine.transformers.aggregators", "qualname": "Aggregators", "kind": "class", "doc": "<p>Class containing all aggregation functions.</p>\n"}, {"fullname": "lakehouse_engine.transformers.aggregators.Aggregators.get_max_value", "modulename": "lakehouse_engine.transformers.aggregators", "qualname": "Aggregators.get_max_value", "kind": "function", "doc": "<p>Get the maximum value of a given column of a dataframe.</p>\n\n<p>Args:\n    input_col: name of the input column.\n    output_col: name of the output column (defaults to \"latest\").</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;latest&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_creators", "modulename": "lakehouse_engine.transformers.column_creators", "kind": "module", "doc": "<p>Column creators transformers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators", "kind": "class", "doc": "<p>Class containing all functions that can create columns to add value.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators.with_row_id", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators.with_row_id", "kind": "function", "doc": "<p>Create a sequential but not consecutive id.</p>\n\n<p>Args:\n    output_col: optional name of the output column.</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lhe_row_id&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators.with_auto_increment_id", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators.with_auto_increment_id", "kind": "function", "doc": "<p>Create a sequential and consecutive id.</p>\n\n<p>Args:\n    output_col: optional name of the output column.</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lhe_row_id&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators.with_literals", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators.with_literals", "kind": "function", "doc": "<p>Create columns given a map of column names and literal values (constants).</p>\n\n<p>Args:\n    Dict[str, Any] literals: map of column names and literal values (constants).</p>\n\n<p>Returns:\n    Callable: A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">literals</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers", "modulename": "lakehouse_engine.transformers.column_reshapers", "kind": "module", "doc": "<p>Module with column reshaping transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers", "kind": "class", "doc": "<p>Class containing column reshaping transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.cast", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.cast", "kind": "function", "doc": "<p>Cast specific columns into the designated type.</p>\n\n<p>Args:\n    cols: dict with columns and respective target types.\n        Target types need to have the exact name of spark types:\n        <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\">https://spark.apache.org/docs/latest/sql-ref-datatypes.html</a></p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.column_selector", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.column_selector", "kind": "function", "doc": "<p>Select specific columns with specific output aliases.</p>\n\n<p>Args:\n    cols: dict with columns to select and respective aliases.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.flatten_schema", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.flatten_schema", "kind": "function", "doc": "<p>Flatten the schema of the dataframe.</p>\n\n<p>Args:\n    max_level: level until which you want to flatten the schema.\n        Default: None.\n    shorten_names: whether to shorten the names of the prefixes\n        of the fields being flattened or not. Default: False.\n    alias: whether to define alias for the columns being flattened\n        or not. Default: True.\n    num_chars: number of characters to consider when shortening\n        the names of the fields. Default: 7.\n    ignore_cols: columns which you don't want to flatten.\n        Default: None.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">max_level</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">shorten_names</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">alias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">num_chars</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.explode_columns", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.explode_columns", "kind": "function", "doc": "<p>Explode columns with types like ArrayType and MapType.</p>\n\n<p>After it can be applied the flatten_schema transformation,\nif we desired for example to explode the map (as we explode a StructType)\nor to explode a StructType inside the array.\nWe recommend you to specify always the columns desired to explode\nand not explode all columns.</p>\n\n<p>Args:\n    explode_arrays: whether you want to explode array columns (True)\n        or not (False). Default: False.\n    array_cols_to_explode: array columns which you want to explode.\n        If you don't specify it will get all array columns and explode them.\n        Default: None.\n    explode_maps: whether you want to explode map columns (True)\n        or not (False). Default: False.\n    map_cols_to_explode: map columns which you want to explode.\n        If you don't specify it will get all map columns and explode them.\n        Default: None.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">explode_arrays</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">array_cols_to_explode</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">explode_maps</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">map_cols_to_explode</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.with_expressions", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.with_expressions", "kind": "function", "doc": "<p>Execute Spark SQL expressions to create the specified columns.</p>\n\n<p>This function uses the Spark expr function:\n<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/\">https://spark.apache.org/docs/latest/api/python/reference/api/</a>\npyspark.sql.functions.expr.html</p>\n\n<p>Args:\n    cols_and_exprs: dict with columns and respective expressions to compute\n        (Spark SQL expressions).</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols_and_exprs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.rename", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.rename", "kind": "function", "doc": "<p>Rename specific columns into the designated name.</p>\n\n<p>Args:\n    cols: dict with columns and respective target names.\n    escape_col_names: whether to escape column names (e.g. <code>/BIC/COL1</code>) or not.\n    If True it creates a column with the new name and drop the old one.\n    If False, uses the native withColumnRenamed Spark function. Default: True.</p>\n\n<p>Returns:\n    Function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">escape_col_names</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.from_avro", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.from_avro", "kind": "function", "doc": "<p>Select all attributes from avro.</p>\n\n<p>Args:\n    schema: the schema string.\n    key_col: the name of the key column.\n    value_col: the name of the value column.\n    options: extra options (e.g., mode: \"PERMISSIVE\").\n    expand_key: whether you want to expand the content inside the key\n    column or not. Default: false.\n    expand_value: whether you want to expand the content inside the value\n    column or not. Default: true.</p>\n\n<p>Returns:\n    Function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;key&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">value_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;value&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">expand_key</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">expand_value</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.from_avro_with_registry", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.from_avro_with_registry", "kind": "function", "doc": "<p>Select all attributes from avro using a schema registry.</p>\n\n<p>Args:\n    schema_registry: the url to the schema registry.\n    value_schema: the name of the value schema entry in the schema registry.\n    value_col: the name of the value column.\n    key_schema: the name of the key schema entry in the schema\n    registry. Default: None.\n    key_col: the name of the key column.\n    expand_key: whether you want to expand the content inside the key\n    column or not. Default: false.\n    expand_value: whether you want to expand the content inside the value\n    column or not. Default: true.</p>\n\n<p>Returns:\n    Function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">schema_registry</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">value_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">value_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;value&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">key_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;key&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">expand_key</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">expand_value</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.from_json", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.from_json", "kind": "function", "doc": "<p>Convert a json string into a json column (struct).</p>\n\n<p>The new json column can be added to the existing columns (default) or it can\nreplace all the others, being the only one to output. The new column gets the\nsame name as the original one suffixed with '_json'.</p>\n\n<p>Args:\n    input_col: dict with columns and respective target names.\n    schema_path: path to the StructType schema (spark schema).\n    schema: dict with the StructType schema (spark schema).\n    json_options: options to parse the json value.\n    drop_all_cols: whether to drop all the input columns or not.\n        Defaults to False.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">schema_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">json_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">drop_all_cols</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.to_json", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.to_json", "kind": "function", "doc": "<p>Convert dataframe columns into a json value.</p>\n\n<p>Args:\n    in_cols: name(s) of the input column(s).\n        Example values:\n        \"*\" - all\n        columns; \"my_col\" - one column named \"my_col\";\n        \"my_col1, my_col2\" - two columns.\n    out_col: name of the output column.\n    json_options: options to parse the json value.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">in_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">out_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">json_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.condensers", "modulename": "lakehouse_engine.transformers.condensers", "kind": "module", "doc": "<p>Condensers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.condensers.Condensers", "modulename": "lakehouse_engine.transformers.condensers", "qualname": "Condensers", "kind": "class", "doc": "<p>Class containing all the functions to condensate data for later merges.</p>\n"}, {"fullname": "lakehouse_engine.transformers.condensers.Condensers.condense_record_mode_cdc", "modulename": "lakehouse_engine.transformers.condensers", "qualname": "Condensers.condense_record_mode_cdc", "kind": "function", "doc": "<p>Condense Change Data Capture (CDC) based on record_mode strategy.</p>\n\n<p>This CDC data is particularly seen in some CDC enabled systems. Other systems\nmay have different CDC strategies.</p>\n\n<p>Args:\n    business_key: The business key (logical primary key) of the data.\n    ranking_key_desc: In this type of CDC condensation the data needs to be\n        ordered descendingly in a certain way, using columns specified in this\n        parameter.\n    ranking_key_asc: In this type of CDC condensation the data needs to be\n        ordered ascendingly in a certain way, using columns specified in\n        this parameter.\n    record_mode_col: Name of the record mode input_col.\n    valid_record_modes: Depending on the context, not all record modes may be\n        considered for condensation. Use this parameter to skip those.</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">business_key</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">record_mode_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">valid_record_modes</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ranking_key_desc</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ranking_key_asc</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.condensers.Condensers.group_and_rank", "modulename": "lakehouse_engine.transformers.condensers", "qualname": "Condensers.group_and_rank", "kind": "function", "doc": "<p>Condense data based on a simple group by + take latest mechanism.</p>\n\n<p>Args:\n    group_key: list of column names to use in the group by.\n    ranking_key: the data needs to be ordered descendingly using columns\n        specified in this parameter.\n    descending: if the ranking considers descending order or not. Defaults to\n        True.</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">group_key</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ranking_key</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">descending</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.custom_transformers", "modulename": "lakehouse_engine.transformers.custom_transformers", "kind": "module", "doc": "<p>Custom transformers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.custom_transformers.CustomTransformers", "modulename": "lakehouse_engine.transformers.custom_transformers", "qualname": "CustomTransformers", "kind": "class", "doc": "<p>Class representing a CustomTransformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.custom_transformers.CustomTransformers.custom_transformation", "modulename": "lakehouse_engine.transformers.custom_transformers", "qualname": "CustomTransformers.custom_transformation", "kind": "function", "doc": "<p>Execute a custom transformation provided by the user.</p>\n\n<p>This transformer can be very useful whenever the user cannot use our provided\ntransformers, or they want to write complex logic in the transform step of the\nalgorithm.</p>\n\n<p>Attention!!! Please bare in mind that the custom_transformer function provided\nas argument needs to receive a DataFrame and return a DataFrame, because it is\nhow Spark's .transform method is able to chain the transformations.\nExample:\n    def my_custom_logic(df: DataFrame) -> DataFrame:</p>\n\n<p>Args:\n    custom_transformer: custom transformer function. A python function with all\n        required pyspark logic provided by the user.</p>\n\n<p>Returns:\n    Callable: the same function provided as parameter, in order to e called\n        later in the TransformerFactory.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">custom_transformer</span><span class=\"p\">:</span> <span class=\"n\">Callable</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.data_maskers", "modulename": "lakehouse_engine.transformers.data_maskers", "kind": "module", "doc": "<p>Module with data masking transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.data_maskers.DataMaskers", "modulename": "lakehouse_engine.transformers.data_maskers", "qualname": "DataMaskers", "kind": "class", "doc": "<p>Class containing data masking transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.data_maskers.DataMaskers.hash_masker", "modulename": "lakehouse_engine.transformers.data_maskers", "qualname": "DataMaskers.hash_masker", "kind": "function", "doc": "<p>Mask specific columns using an hashing approach.</p>\n\n<p>Args:\n    cols: list of column names to mask.\n    approach: hashing approach. Defaults to 'SHA'. There's \"MURMUR3\" as well.\n    num_bits: number of bits of the SHA approach. Only applies to SHA approach.\n    suffix: suffix to apply to new column name. Defaults to \"_hash\".\n        Note: you can pass an empty suffix to have the original column replaced.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">approach</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SHA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_bits</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">suffix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;_hash&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.data_maskers.DataMaskers.column_dropper", "modulename": "lakehouse_engine.transformers.data_maskers", "qualname": "DataMaskers.column_dropper", "kind": "function", "doc": "<p>Drop specific columns.</p>\n\n<p>Args:\n    cols: list of column names to drop.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers", "modulename": "lakehouse_engine.transformers.date_transformers", "kind": "module", "doc": "<p>Module containing date transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers", "kind": "class", "doc": "<p>Class with set of transformers to transform dates in several forms.</p>\n"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.add_current_date", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.add_current_date", "kind": "function", "doc": "<p>Add column with current date.</p>\n\n<p>The current date comes from the driver as a constant, not from every executor.</p>\n\n<p>Args:\n    output_col: name of the output column.</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.convert_to_date", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.convert_to_date", "kind": "function", "doc": "<p>Convert multiple string columns with a source format into dates.</p>\n\n<p>Args:\n    cols: list of names of the string columns to convert.\n    source_format: dates source format (e.g., YYYY-MM-dd). Check here:\n        <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/\">https://docs.oracle.com/javase/10/docs/api/java/time/format/</a>\n        DateTimeFormatter.html</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">source_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.convert_to_timestamp", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.convert_to_timestamp", "kind": "function", "doc": "<p>Convert multiple string columns with a source format into timestamps.</p>\n\n<p>Args:\n    cols: list of names of the string columns to convert.\n    source_format: dates source format (e.g., MM-dd-yyyy HH:mm:ss.SSS). Check\n        here: <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/\">https://docs.oracle.com/javase/10/docs/api/java/time/format/</a>\n        DateTimeFormatter.html</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">source_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.format_date", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.format_date", "kind": "function", "doc": "<p>Convert multiple date/timestamp columns into strings with the target format.</p>\n\n<p>Args:\n    cols: list of names of the string columns to convert.\n    target_format: strings target format (e.g., YYYY-MM-dd). Check here:\n        <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/\">https://docs.oracle.com/javase/10/docs/api/java/time/format/</a>\n        DateTimeFormatter.html</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">target_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.get_date_hierarchy", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.get_date_hierarchy", "kind": "function", "doc": "<p>Create day/month/week/quarter/year hierarchy for the provided date columns.</p>\n\n<p>Uses Spark's extract function.</p>\n\n<p>Args:\n    cols: list of names of the date columns to create the hierarchy.\n    formats: dict with the correspondence between the hierarchy and the format\n        to apply.\n        Example: {\n            \"year\": \"year\",\n            \"month\": \"month\",\n            \"day\": \"day\",\n            \"week\": \"week\",\n            \"quarter\": \"quarter\"\n        }\n        Check here: <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/\">https://docs.oracle.com/javase/10/docs/api/java/time/format/</a>\n        DateTimeFormatter.html</p>\n\n<p>Returns:\n    A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">formats</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.exceptions", "modulename": "lakehouse_engine.transformers.exceptions", "kind": "module", "doc": "<p>Module for all the transformers exceptions.</p>\n"}, {"fullname": "lakehouse_engine.transformers.exceptions.WrongArgumentsException", "modulename": "lakehouse_engine.transformers.exceptions", "qualname": "WrongArgumentsException", "kind": "class", "doc": "<p>Exception for when a user provides wrong arguments to a transformer.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.transformers.exceptions.UnsupportedStreamingTransformerException", "modulename": "lakehouse_engine.transformers.exceptions", "qualname": "UnsupportedStreamingTransformerException", "kind": "class", "doc": "<p>Exception for when a user requests a transformer not supported in streaming.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.transformers.filters", "modulename": "lakehouse_engine.transformers.filters", "kind": "module", "doc": "<p>Module containing the filters transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.filters.Filters", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters", "kind": "class", "doc": "<p>Class containing the filters transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.incremental_filter", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.incremental_filter", "kind": "function", "doc": "<p>Incrementally Filter a certain dataframe given an increment logic.</p>\n\n<p>This logic can either be an increment value or an increment dataframe from which\nthe get the latest value from. By default the operator for the filtering process\nis greater or equal to cover cases where we receive late arriving data not cover\nin a previous load. You can change greater_or_equal to false to use greater,\nwhen you trust the source will never output more data with the increment after\nyou have load the data (e.g., you will never load data until the source is still\ndumping data, which may cause you to get an incomplete picture of the last\narrived data).</p>\n\n<p>Args:\n    input_col: input column name\n    increment_value: value to which to filter the data, considering the\n        provided input_Col.\n    increment_df: a dataframe to get the increment value from.\n        you either specify this or the increment_value (this takes precedence).\n        This is a good approach to get the latest value from a given dataframe\n        that was read and apply that value as filter here. In this way you can\n        perform incremental loads based on the last value of a given dataframe\n        (e.g., table or file based). Can be used together with the\n        get_max_value transformer to accomplish these incremental based loads.\n        See our append load feature tests  to see how to provide an acon for\n        incremental loads, taking advantage of the scenario explained here.\n    increment_col: name of the column from which to get the increment\n        value from from (when using increment_df approach). This assumes there's\n        only one row in the increment_df, reason why is a good idea to use\n        together with the get_max_value transformer. Defaults to \"latest\"\n        because that's the default output column name provided by the\n        get_max_value transformer.\n    greater_or_equal: if filtering should be done by also including the\n        increment value or not (useful for scenarios where you are performing\n        increment loads but still want to include data considering the increment\n        value, and not only values greater than that increment... examples may\n        include scenarios where you already loaded data including those values,\n        but the source produced more data containing those values).\n        Defaults to false.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">increment_value</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">increment_df</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">increment_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;latest&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">greater_or_equal</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.expression_filter", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.expression_filter", "kind": "function", "doc": "<p>Filter a dataframe based on an expression.</p>\n\n<p>Args:\n    exp: filter expression.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">exp</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.column_filter_exp", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.column_filter_exp", "kind": "function", "doc": "<p>Filter a dataframe's columns based on a list of SQL expressions.</p>\n\n<p>Args:\n    exp: column filter expressions.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">exp</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.drop_duplicate_rows", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.drop_duplicate_rows", "kind": "function", "doc": "<p>Drop duplicate rows using spark function dropDuplicates().</p>\n\n<p>This transformer can be used with or without arguments.\nThe provided argument needs to be a list of columns.\nFor example: [\u201cName\u201d,\u201dVAT\u201d] will drop duplicate records within\n\"Name\" and \"VAT\" columns.\nIf the transformer is used without providing any columns list or providing\nan empty list, such as [] the result will be the same as using\nthe distinct() pyspark function. If the watermark dict is present it will\nensure that the drop operation will apply to rows within the watermark timeline\nwindow.</p>\n\n<p>Args:\n    cols: column names.\n    watermarker: properties to apply watermarker to the transformer.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">watermarker</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.joiners", "modulename": "lakehouse_engine.transformers.joiners", "kind": "module", "doc": "<p>Module with join transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.joiners.Joiners", "modulename": "lakehouse_engine.transformers.joiners", "qualname": "Joiners", "kind": "class", "doc": "<p>Class containing join transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.joiners.Joiners.join", "modulename": "lakehouse_engine.transformers.joiners", "qualname": "Joiners.join", "kind": "function", "doc": "<p>Join two dataframes based on specified type and columns.</p>\n\n<p>Some stream to stream joins are only possible if you apply Watermark, so this\nmethod also provides a parameter to enable watermarking specification.</p>\n\n<p>Args:\n    left_df_alias: alias of the first dataframe.\n    join_with: right dataframe.\n    right_df_alias: alias of the second dataframe.\n    join_condition: condition to join dataframes.\n    join_type: type of join. Defaults to inner.\n        Available values: inner, cross, outer, full, full outer,\n        left, left outer, right, right outer, semi,\n        left semi, anti, and left anti.\n    broadcast_join: whether to perform a broadcast join or not.\n    select_cols: list of columns to select at the end.\n    watermarker: properties to apply watermarking.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">join_with</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">join_condition</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">left_df_alias</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;a&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">right_df_alias</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;b&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">join_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;inner&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">broadcast_join</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">select_cols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">watermarker</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.null_handlers", "modulename": "lakehouse_engine.transformers.null_handlers", "kind": "module", "doc": "<p>Module with null handlers transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.null_handlers.NullHandlers", "modulename": "lakehouse_engine.transformers.null_handlers", "qualname": "NullHandlers", "kind": "class", "doc": "<p>Class containing null handler transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.null_handlers.NullHandlers.replace_nulls", "modulename": "lakehouse_engine.transformers.null_handlers", "qualname": "NullHandlers.replace_nulls", "kind": "function", "doc": "<p>Replace nulls in a dataframe.</p>\n\n<p>Args:\n    replace_on_nums: if it is to replace nulls on numeric columns.\n        Applies to ints, longs and floats.\n    default_num_value: default integer value to use as replacement.\n    replace_on_strings: if it is to replace nulls on string columns.\n    default_string_value: default string value to use as replacement.\n    subset_cols: list of columns in which to replace nulls. If not\n        provided, all nulls in all columns will be replaced as specified.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">replace_on_nums</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">default_num_value</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">999</span>,</span><span class=\"param\">\t<span class=\"n\">replace_on_strings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">default_string_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;UNKNOWN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">subset_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.optimizers", "modulename": "lakehouse_engine.transformers.optimizers", "kind": "module", "doc": "<p>Optimizers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers", "kind": "class", "doc": "<p>Class containing all the functions that can provide optimizations.</p>\n"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers.cache", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers.cache", "kind": "function", "doc": "<p>Caches the current dataframe.</p>\n\n<p>The default storage level used is MEMORY_AND_DISK.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers.persist", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers.persist", "kind": "function", "doc": "<p>Caches the current dataframe with a specific StorageLevel.</p>\n\n<p>Args:\n    storage_level: the type of StorageLevel, as default MEMORY_AND_DISK_DESER.\n        More options here: <a href=\"https://spark.apache.org/docs/latest/api/python/\">https://spark.apache.org/docs/latest/api/python/</a>\n        reference/api/pyspark.StorageLevel.html</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">storage_level</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers.unpersist", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers.unpersist", "kind": "function", "doc": "<p>Removes the dataframe from the disk and memory.</p>\n\n<p>Args:\n    blocking: whether to block until all the data blocks are\n        removed from disk/memory or run asynchronously.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">blocking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.regex_transformers", "modulename": "lakehouse_engine.transformers.regex_transformers", "kind": "module", "doc": "<p>Regex transformers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.regex_transformers.RegexTransformers", "modulename": "lakehouse_engine.transformers.regex_transformers", "qualname": "RegexTransformers", "kind": "class", "doc": "<p>Class containing all regex functions.</p>\n"}, {"fullname": "lakehouse_engine.transformers.regex_transformers.RegexTransformers.with_regex_value", "modulename": "lakehouse_engine.transformers.regex_transformers", "qualname": "RegexTransformers.with_regex_value", "kind": "function", "doc": "<p>Get the result of applying a regex to an input column (via regexp_extract).</p>\n\n<p>Args:\n    input_col: name of the input column.\n    output_col: name of the output column.\n    regex: regular expression.\n    drop_input_col: whether to drop input_col or not.\n    idx: index to return.</p>\n\n<p>Returns:\n     A function to be executed in the .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">regex</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">drop_input_col</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.repartitioners", "modulename": "lakehouse_engine.transformers.repartitioners", "kind": "module", "doc": "<p>Module with repartitioners transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.repartitioners.Repartitioners", "modulename": "lakehouse_engine.transformers.repartitioners", "qualname": "Repartitioners", "kind": "class", "doc": "<p>Class containing repartitioners transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.repartitioners.Repartitioners.coalesce", "modulename": "lakehouse_engine.transformers.repartitioners", "qualname": "Repartitioners.coalesce", "kind": "function", "doc": "<p>Coalesce a dataframe into n partitions.</p>\n\n<p>Args:\n    num_partitions: num of partitions to coalesce.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.repartitioners.Repartitioners.repartition", "modulename": "lakehouse_engine.transformers.repartitioners", "qualname": "Repartitioners.repartition", "kind": "function", "doc": "<p>Repartition a dataframe into n partitions.</p>\n\n<p>If num_partitions is provided repartitioning happens based on the provided\nnumber, otherwise it happens based on the values of the provided cols (columns).</p>\n\n<p>Args:\n    num_partitions: num of partitions to repartition.\n    cols: list of columns to use for repartitioning.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.transformer_factory", "modulename": "lakehouse_engine.transformers.transformer_factory", "kind": "module", "doc": "<p>Module with the factory pattern to return transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.transformer_factory.TransformerFactory", "modulename": "lakehouse_engine.transformers.transformer_factory", "qualname": "TransformerFactory", "kind": "class", "doc": "<p>TransformerFactory class following the factory pattern.</p>\n"}, {"fullname": "lakehouse_engine.transformers.transformer_factory.TransformerFactory.get_transformer", "modulename": "lakehouse_engine.transformers.transformer_factory", "qualname": "TransformerFactory.get_transformer", "kind": "function", "doc": "<p>Get a transformer following the factory pattern.</p>\n\n<p>Args:\n    spec: transformer specification (individual transformation... not to be\n        confused with list of all transformations).\n    data: ordered dict of dataframes to be transformed. Needed when a\n        transformer requires more than one dataframe as input.</p>\n\n<p>Returns:\n    Transformer function to be executed in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TransformerSpec</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.unions", "modulename": "lakehouse_engine.transformers.unions", "kind": "module", "doc": "<p>Module with union transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.unions.Unions", "modulename": "lakehouse_engine.transformers.unions", "qualname": "Unions", "kind": "class", "doc": "<p>Class containing union transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.unions.Unions.union", "modulename": "lakehouse_engine.transformers.unions", "qualname": "Unions.union", "kind": "function", "doc": "<p>Union dataframes, resolving columns by position (not by name).</p>\n\n<p>Args:\n    union_with: list of dataframes to union.\n    deduplication: whether to perform deduplication of elements or not.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">union_with</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">deduplication</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.unions.Unions.union_by_name", "modulename": "lakehouse_engine.transformers.unions", "qualname": "Unions.union_by_name", "kind": "function", "doc": "<p>Union dataframes, resolving columns by name (not by position).</p>\n\n<p>Args:\n    union_with: list of dataframes to union.\n    deduplication: whether to perform deduplication of elements or not.\n    allow_missing_columns: allow the union of DataFrames with different\n        schemas.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">union_with</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">deduplication</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">allow_missing_columns</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.watermarker", "modulename": "lakehouse_engine.transformers.watermarker", "kind": "module", "doc": "<p>Watermarker module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.watermarker.Watermarker", "modulename": "lakehouse_engine.transformers.watermarker", "qualname": "Watermarker", "kind": "class", "doc": "<p>Class containing all watermarker transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.watermarker.Watermarker.with_watermark", "modulename": "lakehouse_engine.transformers.watermarker", "qualname": "Watermarker.with_watermark", "kind": "function", "doc": "<p>Get the dataframe with watermarker defined.</p>\n\n<p>Args:\n    watermarker_column: name of the input column to be considered for\n     the watermarking. Note: it must be a timestamp.\n    watermarker_time: time window to define the watermark value.</p>\n\n<p>Returns:\n    A function to be executed on other transformers.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">watermarker_column</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">watermarker_time</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils", "modulename": "lakehouse_engine.utils", "kind": "module", "doc": "<p>Utilities package.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs", "modulename": "lakehouse_engine.utils.configs", "kind": "module", "doc": "<p>Config utilities package.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs.config_utils", "modulename": "lakehouse_engine.utils.configs.config_utils", "kind": "module", "doc": "<p>Module to read configurations.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils", "kind": "class", "doc": "<p>Config utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.get_acon", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.get_acon", "kind": "function", "doc": "<p>Get acon based on a filesystem path or on a dict.</p>\n\n<p>Args:\n    acon_path: path of the acon (algorithm configuration) file.\n    acon: acon provided directly through python code (e.g., notebooks\n        or other apps).</p>\n\n<p>Returns:\n    Dict representation of an acon.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.get_config", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.get_config", "kind": "function", "doc": "<p>Get the lakehouse engine configuration file.</p>\n\n<p>Returns:\n    Configuration dictionary</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">package</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lakehouse_engine.configs&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.get_engine_version", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.get_engine_version", "kind": "function", "doc": "<p>Get Lakehouse Engine version from the installed packages.</p>\n\n<p>Returns:\n    String of engine version.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.read_json_acon", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.read_json_acon", "kind": "function", "doc": "<p>Read an acon (algorithm configuration) file.</p>\n\n<p>Args:\n    path: path to the acon file.</p>\n\n<p>Returns:\n    The acon file content as a dict.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.read_sql", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.read_sql", "kind": "function", "doc": "<p>Read a DDL file in Spark SQL format from a cloud object storage system.</p>\n\n<p>Args:\n    path: path to the acon (algorithm configuration) file.</p>\n\n<p>Returns:\n    Content of the SQL file.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.databricks_utils", "modulename": "lakehouse_engine.utils.databricks_utils", "kind": "module", "doc": "<p>Utilities for databricks operations.</p>\n"}, {"fullname": "lakehouse_engine.utils.databricks_utils.DatabricksUtils", "modulename": "lakehouse_engine.utils.databricks_utils", "qualname": "DatabricksUtils", "kind": "class", "doc": "<p>Databricks utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.databricks_utils.DatabricksUtils.get_db_utils", "modulename": "lakehouse_engine.utils.databricks_utils", "qualname": "DatabricksUtils.get_db_utils", "kind": "function", "doc": "<p>Get db utils on databricks.</p>\n\n<p>Args:\n    spark: spark session.</p>\n\n<p>Returns:\n    Dbutils from databricks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spark</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">SparkSession</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.databricks_utils.DatabricksUtils.get_databricks_job_information", "modulename": "lakehouse_engine.utils.databricks_utils", "qualname": "DatabricksUtils.get_databricks_job_information", "kind": "function", "doc": "<p>Get notebook context from running acon.</p>\n\n<p>Returns:\n    Dict containing databricks notebook context.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats", "modulename": "lakehouse_engine.utils.engine_usage_stats", "kind": "module", "doc": "<p>Utilities for recording the engine activity.</p>\n"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats.EngineUsageStats", "modulename": "lakehouse_engine.utils.engine_usage_stats", "qualname": "EngineUsageStats", "kind": "class", "doc": "<p>Engine Usage utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats.EngineUsageStats.store_engine_usage", "modulename": "lakehouse_engine.utils.engine_usage_stats", "qualname": "EngineUsageStats.store_engine_usage", "kind": "function", "doc": "<p>Collects and store Lakehouse Engine usage statistics.</p>\n\n<p>These statistics include the acon and other relevant information, such as\nthe lakehouse engine version and the functions/algorithms being used.</p>\n\n<p>Args:\n    acon: acon dictionary file.\n    func_name: function name that called this log acon.\n    collect_engine_usage: a boolean that enables or disables the collection and\n        storage of Lakehouse usage statistics.\n    spark_confs: optional dictionary with the spark confs to be used when\n        collecting the engine usage.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">func_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.expectations_utils", "modulename": "lakehouse_engine.utils.expectations_utils", "kind": "module", "doc": "<p>Utilities to be used by custom expectations.</p>\n"}, {"fullname": "lakehouse_engine.utils.expectations_utils.validate_result", "modulename": "lakehouse_engine.utils.expectations_utils", "qualname": "validate_result", "kind": "function", "doc": "<p>Validates the test results of the custom expectations.</p>\n\n<p>If you need to make additional validations on your custom expectation\nand/or require additional fields to be returned you can add them before\ncalling this function. The partial_success and partial_result\noptional parameters can be used to pass the result of additional\nvalidations and add more information to the result key of the\nreturned dict respectively.</p>\n\n<p>Args:\n    expectation: Expectation to validate.\n    configuration: Configuration used in the test.\n    metrics: Test result metrics.\n    partial_success: Result of validations done before calling this method.\n    partial_result: Extra fields to be returned to the user.</p>\n\n<p>Returns:\n     The result of the validation.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">expectation</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">expectations</span><span class=\"o\">.</span><span class=\"n\">expectation</span><span class=\"o\">.</span><span class=\"n\">Expectation</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">expectation_configuration</span><span class=\"o\">.</span><span class=\"n\">ExpectationConfiguration</span>,</span><span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">Dict</span>,</span><span class=\"param\">\t<span class=\"n\">partial_success</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">partial_result</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction", "modulename": "lakehouse_engine.utils.extraction", "kind": "module", "doc": "<p>Extraction utilities package.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "kind": "module", "doc": "<p>Utilities module for JDBC extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionType", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionType", "kind": "class", "doc": "<p>Standardize the types of extractions we can have from a JDBC source.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionType.INIT", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionType.INIT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;JDBCExtractionType.INIT: &#x27;init&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionType.DELTA", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionType.DELTA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;JDBCExtractionType.DELTA: &#x27;delta&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtraction", "kind": "class", "doc": "<p>Configurations available for an Extraction from a JDBC source.</p>\n\n<p>These configurations cover:\n    user: username to connect to JDBC source.\n    password: password to connect to JDBC source (always use secrets,\n        don't use text passwords in your code).\n    url: url to connect to JDBC source.\n    dbtable: database.table to extract data from.\n    calc_upper_bound_schema: custom schema used for the upper bound calculation.\n    changelog_table: table of type changelog from which to extract data,\n        when the extraction type is delta.\n    partition_column: column used to split the extraction.\n    latest_timestamp_data_location: data location (e.g., s3) containing the data\n        to get the latest timestamp already loaded into bronze.\n    latest_timestamp_data_format: the format of the dataset in\n        latest_timestamp_data_location. Default: delta.\n    extraction_type: type of extraction (delta or init). Default: \"delta\".\n    driver: JDBC driver name. Default: \"com.sap.db.jdbc.Driver\".\n    num_partitions: number of Spark partitions to split the extraction.\n    lower_bound: lower bound to decide the partition stride.\n    upper_bound: upper bound to decide the partition stride. If\n        calculate_upper_bound is True, then upperBound will be\n        derived by our upper bound optimizer, using the partition column.\n    default_upper_bound: the value to use as default upper bound in case\n        the result of the upper bound calculation is None. Default: \"1\".\n    fetch_size: how many rows to fetch per round trip. Default: \"100000\".\n    compress: enable network compression. Default: True.\n    custom_schema: specify custom_schema for particular columns of the\n        returned dataframe in the init/delta extraction of the source table.\n    min_timestamp: min timestamp to consider to filter the changelog data.\n        Default: None and automatically derived from the location provided.\n        In case this one is provided it has precedence and the calculation\n        is not done.\n    max_timestamp: max timestamp to consider to filter the changelog data.\n        Default: None and automatically derived from the table having information\n        about the extraction requests, their timestamps and their status.\n        In case this one is provided it has precedence and the calculation\n        is not done.\n    generate_predicates: whether to generate predicates automatically or not.\n        Default: False.\n    predicates: list containing all values to partition (if generate_predicates\n        is used, the manual values provided are ignored). Default: None.\n    predicates_add_null: whether to consider null on predicates list.\n        Default: True.\n    extraction_timestamp: the timestamp of the extraction. Default: current time\n        following the format \"%Y%m%d%H%M%S\".\n    max_timestamp_custom_schema: custom schema used on the max_timestamp derivation\n        from the table holding the extraction requests information.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction.__init__", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtraction.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">user</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">changelog_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partition_column</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">driver</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;com.sap.db.jdbc.Driver&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lower_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upper_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fetch_size</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;100000&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">custom_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;20240111203402&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp_custom_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from particularly relevant JDBC sources.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.__init__", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.__init__", "kind": "function", "doc": "<p>Construct JDBCExtractionUtils.</p>\n\n<p>Args:\n    jdbc_extraction: JDBC Extraction configurations. Can be of type:\n        JDBCExtraction, SAPB4Extraction or SAPBWExtraction.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">jdbc_extraction</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_additional_spark_options", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_additional_spark_options", "kind": "function", "doc": "<p>Helper to get additional Spark Options initially passed.</p>\n\n<p>If people provide additional Spark options, not covered by the util function\narguments (get_spark_jdbc_options), we need to consider them.\nThus, we update the options retrieved by the utils, by checking if there is\nany Spark option initially provided that is not yet considered in the retrieved\noptions or function arguments and if the value for the key is not None.\nIf these conditions are filled, we add the options and return the complete dict.</p>\n\n<p>Args:\n    input_spec: the input specification.\n    options: dict with Spark options.\n    ignore_options: list of options to be ignored by the process.\n        Spark read has two different approaches to parallelize\n        reading process, one of them is using upper/lower bound,\n        another one is using predicates, those process can't be\n        executed at the same time, you must choose one of them.\n        By choosing predicates you can't pass lower and upper bound,\n        also can't pass number of partitions and partition column\n        otherwise spark will interpret the execution partitioned by\n        upper and lower bound and will expect to fill all variables.\n        To avoid fill all predicates hardcoded at the acon, there is\n        a feature that automatically generates all predicates for init\n        or delta load based on input partition column, but at the end\n        of the process, partition column can't be passed to the options,\n        because we are choosing predicates execution, that is why to\n        generate predicates we need to pass some options to ignore.</p>\n\n<p>Returns:\n     a dict with all the options passed as argument, plus the options that\n     were initially provided, but were not used in the util\n     (get_spark_jdbc_options).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_options</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_predicates", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_predicates", "kind": "function", "doc": "<p>Get the predicates list, based on a predicates query.</p>\n\n<p>Args:\n    predicates_query: query to use as the basis to get the distinct values for\n        a specified column, based on which predicates are generated.</p>\n\n<p>Returns:\n    List containing the predicates to use to split the extraction from\n    JDBC sources.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">predicates_query</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_spark_jdbc_options", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_spark_jdbc_options", "kind": "function", "doc": "<p>Get the Spark options to extract data from a JDBC source.</p>\n\n<p>Returns:\n    The Spark jdbc args dictionary, including the query to submit\n    and also options args dictionary.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_spark_jdbc_optimal_upper_bound", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_spark_jdbc_optimal_upper_bound", "kind": "function", "doc": "<p>Get an optimal upperBound to properly split a Spark JDBC extraction.</p>\n\n<p>Returns:\n     Either an int, date or timestamp to serve as upperBound Spark JDBC option.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "kind": "module", "doc": "<p>Utilities module for SAP B4 extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes", "kind": "class", "doc": "<p>Standardise the types of ADSOs we can have for Extractions from SAP B4.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes.AQ", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes.AQ", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&lt;ADSOTypes.AQ: &#x27;AQ&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes.CL", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes.CL", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&lt;ADSOTypes.CL: &#x27;CL&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes.SUPPORTED_TYPES", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes.SUPPORTED_TYPES", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list", "default_value": "&lt;ADSOTypes.SUPPORTED_TYPES: [&#x27;AQ&#x27;, &#x27;CL&#x27;]&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4Extraction", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4Extraction", "kind": "class", "doc": "<p>Configurations available for an Extraction from SAP B4.</p>\n\n<p>It inherits from JDBCExtraction configurations, so it can use\nand/or overwrite those configurations.</p>\n\n<p>These configurations cover:\n    latest_timestamp_input_col: the column containing the request timestamps\n        in the dataset in latest_timestamp_data_location. Default: REQTSN.\n    request_status_tbl: the name of the SAP B4 table having information\n        about the extraction requests. Composed of database.table.\n        Default: SAPHANADB.RSPMREQUEST.\n    request_col_name: name of the column having the request timestamp to join\n        with the request status table. Default: REQUEST_TSN.\n    data_target: the data target to extract from. User in the join operation with\n        the request status table.\n    act_req_join_condition: the join condition into activation table\n        can be changed using this property.\n        Default: 'tbl.reqtsn = req.request_col_name'.\n    include_changelog_tech_cols: whether to include the technical columns\n        (usually coming from the changelog) table or not.\n    extra_cols_req_status_tbl: columns to be added from request status table.\n        It needs to contain the prefix \"req.\". E.g. \"req.col1 as column_one,\n        req.col2 as column_two\".\n    request_status_tbl_filter: filter to use for filtering the request status table,\n        influencing the calculation of the max timestamps and the delta extractions.\n    adso_type: the type of ADSO that you are extracting from. Can be \"AQ\" or \"CL\".\n    max_timestamp_custom_schema: the custom schema to apply on the calculation of\n        the max timestamp to consider for the delta extractions.\n        Default: timestamp DECIMAL(23,0).\n    default_max_timestamp: the timestamp to use as default, when it is not possible\n        to derive one.\n    custom_schema: specify custom_schema for particular columns of the\n        returned dataframe in the init/delta extraction of the source table.</p>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4Extraction.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4Extraction.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">user</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">changelog_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partition_column</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">driver</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;com.sap.db.jdbc.Driver&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lower_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upper_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fetch_size</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;100000&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">custom_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;REQTSN DECIMAL(23,0)&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">min_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;20240111203402&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp_custom_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;timestamp DECIMAL(23,0)&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;REQTSN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">request_status_tbl</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPHANADB.RSPMREQUEST&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">request_col_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;REQUEST_TSN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_target</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">act_req_join_condition</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">include_changelog_tech_cols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">extra_cols_req_status_tbl</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">request_status_tbl_filter</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">adso_type</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_max_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1970000000000000000000&#39;</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4ExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from SAP B4.</p>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4ExtractionUtils.__init__", "kind": "function", "doc": "<p>Construct SAPB4ExtractionUtils.</p>\n\n<p>Args:\n    sap_b4_extraction: SAP B4 Extraction configurations.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sap_b4_extraction</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">extraction</span><span class=\"o\">.</span><span class=\"n\">sap_b4_extraction_utils</span><span class=\"o\">.</span><span class=\"n\">SAPB4Extraction</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils.get_data_target", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4ExtractionUtils.get_data_target", "kind": "function", "doc": "<p>Get the data_target from the data_target option or derive it.</p>\n\n<p>By definition data_target is the same for the table and changelog table and\nis the same string ignoring everything before / and the first and last\ncharacter after /. E.g. for a dbtable /BIC/abtable12, the data_target\nwould be btable1.</p>\n\n<p>Args:\n    input_spec_opt: options from the input_spec.</p>\n\n<p>Returns:\n    A string with the data_target.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec_opt</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "kind": "module", "doc": "<p>Utilities module for SAP BW extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtraction", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtraction", "kind": "class", "doc": "<p>Configurations available for an Extraction from SAP BW.</p>\n\n<p>It inherits from SAPBWExtraction configurations, so it can use\nand/or overwrite those configurations.</p>\n\n<p>These configurations cover:\n    latest_timestamp_input_col: the column containing the actrequest timestamp\n        in the dataset in latest_timestamp_data_location. Default:\n        \"actrequest_timestamp\".\n    act_request_table: the name of the SAP BW activation requests table.\n        Composed of database.table. Default: SAPPHA.RSODSACTREQ.\n    request_col_name: name of the column having the request to join\n        with the activation request table. Default: actrequest.\n    act_req_join_condition: the join condition into activation table\n        can be changed using this property.\n        Default: 'changelog_tbl.request = act_req.request_col_name'.\n    odsobject: name of BW Object, used for joining with the activation request\n        table to get the max actrequest_timestamp to consider while filtering\n        the changelog table.\n    include_changelog_tech_cols: whether to include the technical columns\n        (usually coming from the changelog) table or not. Default: True.\n    extra_cols_act_request: list of columns to be added from act request table.\n        It needs to contain the prefix \"act_req.\". E.g. \"act_req.col1\n        as column_one, act_req.col2 as column_two\".\n    get_timestamp_from_act_request: whether to get init timestamp\n        from act request table or assume current/given timestamp.\n    sap_bw_schema: sap bw schema. Default: SAPPHA.\n    max_timestamp_custom_schema: the custom schema to apply on the calculation of\n        the max timestamp to consider for the delta extractions.\n        Default: timestamp DECIMAL(23,0).\n    default_max_timestamp: the timestamp to use as default, when it is not possible\n        to derive one.</p>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtraction.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtraction.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">user</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">changelog_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partition_column</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">driver</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;com.sap.db.jdbc.Driver&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lower_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upper_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fetch_size</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;100000&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">custom_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;20240111203402&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp_custom_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;timestamp DECIMAL(15,0)&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;actrequest_timestamp&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">act_request_table</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA.RSODSACTREQ&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">request_col_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;actrequest&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">act_req_join_condition</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">odsobject</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">include_changelog_tech_cols</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extra_cols_act_request</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">get_timestamp_from_act_request</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">sap_bw_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">default_max_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;197000000000000&#39;</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from particularly relevant JDBC sources.</p>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils.__init__", "kind": "function", "doc": "<p>Construct SAPBWExtractionUtils.</p>\n\n<p>Args:\n    sap_bw_extraction: SAP BW Extraction configurations.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sap_bw_extraction</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">extraction</span><span class=\"o\">.</span><span class=\"n\">sap_bw_extraction_utils</span><span class=\"o\">.</span><span class=\"n\">SAPBWExtraction</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.get_changelog_table", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils.get_changelog_table", "kind": "function", "doc": "<p>Get the changelog table, given an odsobject.</p>\n\n<p>Returns:\n     String to use as changelog_table.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.get_odsobject", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils.get_odsobject", "kind": "function", "doc": "<p>Get the odsobject based on the provided options.</p>\n\n<p>With the table name we may also get the db name, so we need to split.\nMoreover, there might be the need for people to specify odsobject if\nit is different from the dbtable.</p>\n\n<p>Args:\n    input_spec_opt: options from the input_spec.</p>\n\n<p>Returns:\n    A string with the odsobject.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec_opt</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "kind": "module", "doc": "<p>Utilities module for SFTP extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat", "kind": "class", "doc": "<p>Formats of algorithm input.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.CSV", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.CSV", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.CSV: &#x27;csv&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.FWF", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.FWF", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.FWF: &#x27;fwf&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.JSON", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.JSON: &#x27;json&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.XML", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.XML", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.XML: &#x27;xml&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter", "kind": "class", "doc": "<p>Standardize the types of filters we can have from a SFTP source.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.file_name_contains", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.file_name_contains", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.file_name_contains: &#x27;file_name_contains&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.LATEST_FILE", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.LATEST_FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.LATEST_FILE: &#x27;latest_file&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.EARLIEST_FILE", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.EARLIEST_FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.EARLIEST_FILE: &#x27;earliest_file&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.GREATER_THAN", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.GREATER_THAN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.GREATER_THAN: &#x27;date_time_gt&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.LOWER_THAN", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.LOWER_THAN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.LOWER_THAN: &#x27;date_time_lt&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from particularly relevant SFTP sources.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.get_files_list", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.get_files_list", "kind": "function", "doc": "<p>Get a list of files to be extracted from SFTP.</p>\n\n<p>The arguments (options_args) to list files are:\ndate_time_gt(str):\n    Filter the files greater than the string datetime\n    formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".\ndate_time_lt(str):\n    Filter the files lower than the string datetime\n    formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".\nearliest_file(bool):\n    Filter the earliest dated file in the directory.\nfile_name_contains(str):\n    Filter files when match the pattern.\nlatest_file(bool):\n    Filter the most recent dated file in the directory.\nsub_dir(bool):\n    When true, the engine will search files into subdirectories\n    of the remote_path.\n    It will consider one level below the remote_path.\n    When sub_dir is used with latest_file/earliest_file argument,\n    the engine will retrieve the latest_file/earliest_file\n    for each subdirectory.</p>\n\n<p>Args:\n    sftp: the SFTP client object.\n    remote_path: path of files to be filtered.\n    options_args: options from the acon.</p>\n\n<p>Returns:\n    A list containing the file names to be passed to Spark.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sftp</span><span class=\"p\">:</span> <span class=\"n\">paramiko</span><span class=\"o\">.</span><span class=\"n\">sftp_client</span><span class=\"o\">.</span><span class=\"n\">SFTPClient</span>,</span><span class=\"param\">\t<span class=\"n\">remote_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">options_args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Set</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.get_sftp_client", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.get_sftp_client", "kind": "function", "doc": "<p>Get the SFTP client.</p>\n\n<p>The SFTP client is used to open an SFTP session across an open\nSSH Transport and perform remote file operations.</p>\n\n<p>Args:\n    options_args: dictionary containing SFTP connection parameters.\n    The Paramiko arguments expected to connect are:\n        \"hostname\": the server to connect to.\n        \"port\": the server port to connect to.\n        \"username\": the username to authenticate as.\n        \"password\": used for password authentication.\n        \"pkey\": optional - an optional public key to use for authentication.\n        \"passphrase\" \u2013 optional - options used for decrypting private keys.\n        \"key_filename\" \u2013 optional - the filename, or list of filenames,\n            of optional private key(s) and/or certs to try for authentication.\n        \"timeout\" \u2013 an optional timeout (in seconds) for the TCP connect.\n        \"allow_agent\" \u2013 optional - set to False to disable\n            connecting to the SSH agent.\n        \"look_for_keys\" \u2013 optional - set to False to disable searching\n            for discoverable private key files in ~/.ssh/.\n        \"compress\" \u2013 optional - set to True to turn on compression.\n        \"sock\" - optional - an open socket or socket-like object\n            to use for communication to the target host.\n        \"gss_auth\" \u2013 optional - True if you want to use GSS-API authentication.\n        \"gss_kex\" \u2013 optional - Perform GSS-API Key Exchange and\n            user authentication.\n        \"gss_deleg_creds\" \u2013 optional - Delegate GSS-API client\n            credentials or not.\n        \"gss_host\" \u2013 optional - The targets name in the kerberos database.\n        \"gss_trust_dns\" \u2013 optional - Indicates whether or\n            not the DNS is trusted to securely canonicalize the name of the\n            host being connected to (default True).\n        \"banner_timeout\" \u2013 an optional timeout (in seconds)\n            to wait for the SSH banner to be presented.\n        \"auth_timeout\" \u2013 an optional timeout (in seconds)\n            to wait for an authentication response.\n        \"disabled_algorithms\" \u2013 an optional dict passed directly to Transport\n            and its keyword argument of the same name.\n        \"transport_factory\" \u2013 an optional callable which is handed a subset of\n            the constructor arguments (primarily those related to the socket,\n            GSS functionality, and algorithm selection) and generates a\n            Transport instance to be used by this client.\n            Defaults to Transport.__init__.</p>\n\n<pre><code>The parameter to specify the private key is expected to be in RSA format.\nAttempting a connection with a blank host key is not allowed\nunless the argument \"add_auto_policy\" is explicitly set to True.\n</code></pre>\n\n<p>Returns:\n    sftp -> a new SFTPClient session object.\n    transport -> the Transport for this connection.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">options_args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">paramiko</span><span class=\"o\">.</span><span class=\"n\">sftp_client</span><span class=\"o\">.</span><span class=\"n\">SFTPClient</span><span class=\"p\">,</span> <span class=\"n\">paramiko</span><span class=\"o\">.</span><span class=\"n\">transport</span><span class=\"o\">.</span><span class=\"n\">Transport</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.validate_format", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.validate_format", "kind": "function", "doc": "<p>Validate the file extension based on the format definitions.</p>\n\n<p>Args:\n    files_format: a string containing the file extension.</p>\n\n<p>Returns:\n    The string validated and formatted.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">files_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.validate_location", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.validate_location", "kind": "function", "doc": "<p>Validate the location. Add \"/\" in the case it does not exist.</p>\n\n<p>Args:\n    location: file path.</p>\n\n<p>Returns:\n    The location validated.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.file_utils", "modulename": "lakehouse_engine.utils.file_utils", "kind": "module", "doc": "<p>Utilities for file name based operations.</p>\n"}, {"fullname": "lakehouse_engine.utils.file_utils.get_file_names_without_file_type", "modulename": "lakehouse_engine.utils.file_utils", "qualname": "get_file_names_without_file_type", "kind": "function", "doc": "<p>Function to retrieve list of file names in a folder.</p>\n\n<p>This function filters by file type and removes the extension of the file name\nit returns.</p>\n\n<p>Args:\n    path: path to the folder to list files\n    file_type: type of the file to include in list\n    exclude_regex: regex of file names to exclude</p>\n\n<p>Returns:\n    A list of file names without file type.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">file_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">exclude_regex</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.logging_handler", "modulename": "lakehouse_engine.utils.logging_handler", "kind": "module", "doc": "<p>Module to configure project logging.</p>\n"}, {"fullname": "lakehouse_engine.utils.logging_handler.FilterSensitiveData", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "FilterSensitiveData", "kind": "class", "doc": "<p>Logging filter to hide sensitive data from being shown in the logs.</p>\n", "bases": "logging.Filter"}, {"fullname": "lakehouse_engine.utils.logging_handler.FilterSensitiveData.filter", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "FilterSensitiveData.filter", "kind": "function", "doc": "<p>Hide sensitive information from being shown in the logs.</p>\n\n<p>Based on the configured regex and replace strings, the content of the log\nrecords is replaced and then all the records are allowed to be logged\n(return True).</p>\n\n<p>Args:\n    record: the LogRecord event being logged.</p>\n\n<p>Returns:\n    The transformed record to be logged.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">record</span><span class=\"p\">:</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">LogRecord</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.logging_handler.LoggingHandler", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "LoggingHandler", "kind": "class", "doc": "<p>Handle the logging of the lakehouse engine project.</p>\n"}, {"fullname": "lakehouse_engine.utils.logging_handler.LoggingHandler.__init__", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "LoggingHandler.__init__", "kind": "function", "doc": "<p>Construct a LoggingHandler instance.</p>\n\n<p>Args:\n    class_name: name of the class to be indicated in the logs.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.logging_handler.LoggingHandler.get_logger", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "LoggingHandler.get_logger", "kind": "function", "doc": "<p>Get the _logger instance variable.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the logger object.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Logger</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils", "modulename": "lakehouse_engine.utils.schema_utils", "kind": "module", "doc": "<p>Utilities to facilitate dataframe schema management.</p>\n"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils", "kind": "class", "doc": "<p>Schema utils that help retrieve and manage schemas of dataframes.</p>\n"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_file", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_file", "kind": "function", "doc": "<p>Get a spark schema from a file (spark StructType json file) in a file system.</p>\n\n<p>Args:\n    file_path: path of the file in a file system. Check here:\n        <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/</a>\n        StructType.html</p>\n\n<p>Returns:\n    Spark schema struct type.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_file_to_dict", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_file_to_dict", "kind": "function", "doc": "<p>Get a dict with the spark schema from a file in a file system.</p>\n\n<p>Args:\n    file_path: path of the file in a file system. Check here:\n        <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/</a>\n        StructType.html</p>\n\n<p>Returns:\n     Spark schema in a dict.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_dict", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_dict", "kind": "function", "doc": "<p>Get a spark schema from a dict.</p>\n\n<p>Args:\n    struct_type: dict containing a spark schema structure. Check here:\n        <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/</a>\n        StructType.html</p>\n\n<p>Returns:\n     Spark schema struct type.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">struct_type</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_table_schema", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_table_schema", "kind": "function", "doc": "<p>Get a spark schema from a table.</p>\n\n<p>Args:\n    table: table name from which to inherit the schema.</p>\n\n<p>Returns:\n    Spark schema struct type.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">table</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_input_spec", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_input_spec", "kind": "function", "doc": "<p>Get a spark schema from an input specification.</p>\n\n<p>This covers scenarios where the schema is provided as part of the input\nspecification of the algorithm. Schema can come from the table specified in the\ninput specification (enforce_schema_from_table) or by the dict with the spark\nschema provided there also.</p>\n\n<p>Args:\n    input_spec: input specification.</p>\n\n<p>Returns:\n    spark schema struct type.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.schema_flattener", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.schema_flattener", "kind": "function", "doc": "<p>Recursive method to flatten the schema of the dataframe.</p>\n\n<p>Args:\n    schema: schema to be flattened.\n    prefix: prefix of the struct to get the value for. Only relevant\n    for being used in the internal recursive logic.\n    level: level of the depth in the schema being flattened. Only relevant\n    for being used in the internal recursive logic.\n    max_level: level until which you want to flatten the schema. Default: None.\n    shorten_names: whether to shorten the names of the prefixes of the fields\n    being flattened or not. Default: False.\n    alias: whether to define alias for the columns being flattened or\n    not. Default: True.\n    num_chars: number of characters to consider when shortening the names of\n    the fields. Default: 7.\n    ignore_cols: columns which you don't want to flatten. Default: None.</p>\n\n<p>Returns:\n    A function to be called in .transform() spark function.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>,</span><span class=\"param\">\t<span class=\"n\">prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">level</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">max_level</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">shorten_names</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">alias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">num_chars</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage", "modulename": "lakehouse_engine.utils.storage", "kind": "module", "doc": "<p>Utilities to interact with storage systems.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.file_storage", "modulename": "lakehouse_engine.utils.storage.file_storage", "kind": "module", "doc": "<p>Module for abstract representation of a storage system holding files.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.file_storage.FileStorage", "modulename": "lakehouse_engine.utils.storage.file_storage", "qualname": "FileStorage", "kind": "class", "doc": "<p>Abstract file storage class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.utils.storage.file_storage.FileStorage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.file_storage", "qualname": "FileStorage.get_file_payload", "kind": "function", "doc": "<p>Get the payload of a file.</p>\n\n<p>Args:\n    url: url of the file.</p>\n\n<p>Returns:\n    File payload/content.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage.FileStorage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.file_storage", "qualname": "FileStorage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<p>Args:\n    url: url of the file.\n    content: content to write into the file.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "kind": "module", "doc": "<p>Module for common file storage functions.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions", "kind": "class", "doc": "<p>Class for common file storage functions.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions.read_json", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions.read_json", "kind": "function", "doc": "<p>Read a json file.</p>\n\n<p>The file should be in a supported file system (e.g., s3 or local filesystem -\nfor local tests only).</p>\n\n<p>Args:\n    path: path to the json file.</p>\n\n<p>Returns:\n    Dict with json file content.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "kind": "module", "doc": "<p>Module to represent a local file storage system.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage.LocalFSStorage", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "qualname": "LocalFSStorage", "kind": "class", "doc": "<p>Class to represent a local file storage system.</p>\n", "bases": "lakehouse_engine.utils.storage.file_storage.FileStorage"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage.LocalFSStorage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "qualname": "LocalFSStorage.get_file_payload", "kind": "function", "doc": "<p>Get the payload of a file.</p>\n\n<p>Args:\n    url: url of the file.</p>\n\n<p>Returns:\n    file payload/content.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> &lt;class &#x27;TextIO&#x27;&gt;:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage.LocalFSStorage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "qualname": "LocalFSStorage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<p>Args:\n    url: url of the file.\n    content: content to write into the file.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage", "modulename": "lakehouse_engine.utils.storage.s3_storage", "kind": "module", "doc": "<p>Module to represent a s3 file storage system.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage.S3Storage", "modulename": "lakehouse_engine.utils.storage.s3_storage", "qualname": "S3Storage", "kind": "class", "doc": "<p>Class to represent a s3 file storage system.</p>\n", "bases": "lakehouse_engine.utils.storage.file_storage.FileStorage"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage.S3Storage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.s3_storage", "qualname": "S3Storage.get_file_payload", "kind": "function", "doc": "<p>Get the payload of a config file.</p>\n\n<p>Args:\n    url: url of the file.</p>\n\n<p>Returns:\n    File payload/content.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage.S3Storage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.s3_storage", "qualname": "S3Storage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<p>Args:\n    url: url of the file.\n    content: content to write into the file.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();