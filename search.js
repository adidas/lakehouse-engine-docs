window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "lakehouse_engine", "modulename": "lakehouse_engine", "kind": "module", "doc": "<p>Lakehouse engine package containing all the system subpackages.</p>\n\n<ul>\n<li><a href=\"lakehouse_engine/algorithms.html\">Algorithms</a></li>\n<li><a href=\"lakehouse_engine/configs.html\">Configs</a></li>\n<li><a href=\"lakehouse_engine/core.html\">Core</a></li>\n<li><a href=\"lakehouse_engine/dq_processors.html\">DQ Processors</a></li>\n<li><a href=\"lakehouse_engine/engine.html\">Engine</a></li>\n<li><a href=\"lakehouse_engine/io.html\">IO</a></li>\n<li><a href=\"lakehouse_engine/terminators.html\">Terminators</a></li>\n<li><a href=\"lakehouse_engine/transformers.html\">Transformers</a></li>\n<li><a href=\"lakehouse_engine/utils.html\">Utils</a></li>\n</ul>\n"}, {"fullname": "lakehouse_engine.algorithms", "modulename": "lakehouse_engine.algorithms", "kind": "module", "doc": "<p>Package containing all the lakehouse engine algorithms.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.algorithm", "modulename": "lakehouse_engine.algorithms.algorithm", "kind": "module", "doc": "<p>Module containing the Algorithm class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.algorithm.Algorithm", "modulename": "lakehouse_engine.algorithms.algorithm", "qualname": "Algorithm", "kind": "class", "doc": "<p>Class to define the behavior of every algorithm based on ACONs.</p>\n", "bases": "lakehouse_engine.core.executable.Executable"}, {"fullname": "lakehouse_engine.algorithms.algorithm.Algorithm.__init__", "modulename": "lakehouse_engine.algorithms.algorithm", "qualname": "Algorithm.__init__", "kind": "function", "doc": "<p>Construct Algorithm instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  algorithm configuration.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.algorithm.Algorithm.get_dq_spec", "modulename": "lakehouse_engine.algorithms.algorithm", "qualname": "Algorithm.get_dq_spec", "kind": "function", "doc": "<p>Get data quality specification object from acon.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  data quality specifications.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DQSpec and the List of DQ Functions Specs.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader", "modulename": "lakehouse_engine.algorithms.data_loader", "kind": "module", "doc": "<p>Module to define DataLoader class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader", "kind": "class", "doc": "<p>Load data using an algorithm configuration (ACON represented as dict).</p>\n\n<p>This algorithm focuses on the cases where users will be specifying all the algorithm\nsteps and configurations through a dict based configuration, which we name ACON\nin our framework.</p>\n\n<p>Since an ACON is a dict you can pass a custom transformer through a python function\nand, therefore, the DataLoader can also be used to load data with custom\ntransformations not provided in our transformers package.</p>\n\n<p>As the algorithm base class of the lakehouse-engine framework is based on the\nconcept of ACON, this DataLoader algorithm simply inherits from Algorithm,\nwithout overriding anything. We designed the codebase like this to avoid\ninstantiating the Algorithm class directly, which was always meant to be an\nabstraction for any specific algorithm included in the lakehouse-engine framework.</p>\n", "bases": "lakehouse_engine.algorithms.algorithm.Algorithm"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.__init__", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.__init__", "kind": "function", "doc": "<p>Construct DataLoader algorithm instances.</p>\n\n<p>A data loader needs several specifications to work properly,\nbut some of them might be optional. The available specifications are:</p>\n\n<ul>\n<li>input specifications (mandatory): specify how to read data.</li>\n<li>transform specifications (optional): specify how to transform data.</li>\n<li>data quality specifications (optional): specify how to execute the data\nquality process.</li>\n<li>output specifications (mandatory): specify how to write data to the\ntarget.</li>\n<li>terminate specifications (optional): specify what to do after writing into\nthe target (e.g., optimizing target table, vacuum, compute stats, etc).</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  algorithm configuration.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.read", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.read", "kind": "function", "doc": "<p>Read data from an input location into a distributed dataframe.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>An ordered dict with all the dataframes that were read.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.transform", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.transform", "kind": "function", "doc": "<p>Transform (optionally) the data that was read.</p>\n\n<p>If there isn't a transformation specification this step will be skipped, and the\noriginal dataframes that were read will be returned.\nTransformations can have dependency from another transformation result, however\nwe need to keep in mind if we are using streaming source and for some reason we\nneed to enable micro batch processing, this result cannot be used as input to\nanother transformation. Micro batch processing in pyspark streaming is only\navailable in .write(), which means this transformation with micro batch needs\nto be the end of the process.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  input dataframes in an ordered dict.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Another ordered dict with the transformed dataframes, according to the\n  transformation specification.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.process_dq", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.process_dq", "kind": "function", "doc": "<p>Process the data quality tasks for the data that was read and/or transformed.</p>\n\n<p>It supports multiple input dataframes. Although just one is advisable.</p>\n\n<p>It is possible to use data quality validators/expectations that will validate\nyour data and fail the process in case the expectations are not met. The DQ\nprocess also generates and keeps updating a site containing the results of the\nexpectations that were done on your data. The location of the site is\nconfigurable and can either be on file system or S3. If you define it to be\nstored on S3, you can even configure your S3 bucket to serve the site so that\npeople can easily check the quality of your data. Moreover, it is also\npossible to store the result of the DQ process into a defined result sink.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  dataframes from previous steps of the algorithm that we which to\nrun the DQ process on.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Another ordered dict with the validated dataframes.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.write", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.write", "kind": "function", "doc": "<p>Write the data that was read and transformed (if applicable).</p>\n\n<p>It supports writing multiple datasets. However, we only recommend to write one\ndataframe. This recommendation is based on easy debugging and reproducibility,\nsince if we start mixing several datasets being fueled by the same algorithm, it\nwould unleash an infinite sea of reproducibility issues plus tight coupling and\ndependencies between datasets. Having said that, there may be cases where\nwriting multiple datasets is desirable according to the use case requirements.\nUse it accordingly.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  dataframes that were read and transformed (if applicable).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dataframes that were written.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.terminate", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.terminate", "kind": "function", "doc": "<p>Terminate the algorithm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  dataframes that were written.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.data_loader.DataLoader.execute", "modulename": "lakehouse_engine.algorithms.data_loader", "qualname": "DataLoader.execute", "kind": "function", "doc": "<p>Define the algorithm execution behaviour.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.dq_validator", "modulename": "lakehouse_engine.algorithms.dq_validator", "kind": "module", "doc": "<p>Module to define Data Validator class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator", "kind": "class", "doc": "<p>Validate data using an algorithm configuration (ACON represented as dict).</p>\n\n<p>This algorithm focuses on isolate Data Quality Validations from loading,\napplying a set of data quality functions to a specific input dataset,\nwithout the need to define any output specification.\nYou can use any input specification compatible with the lakehouse engine\n(dataframe, table, files, etc).</p>\n", "bases": "lakehouse_engine.algorithms.algorithm.Algorithm"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.__init__", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.__init__", "kind": "function", "doc": "<p>Construct DQValidator algorithm instances.</p>\n\n<p>A data quality validator needs the following specifications to work\nproperly:\n    - input specification (mandatory): specify how and what data to\n    read.\n    - data quality specification (mandatory): specify how to execute\n    the data quality process.\n    - restore_prev_version (optional): specify if, having\n    delta table/files as input, they should be restored to the\n    previous version if the data quality process fails. Note: this\n    is only considered if fail_on_error is kept as True.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  algorithm configuration.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.read", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.read", "kind": "function", "doc": "<p>Read data from an input location into a distributed dataframe.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dataframe with data that was read.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.process_dq", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.process_dq", "kind": "function", "doc": "<p>Process the data quality tasks for the data that was read.</p>\n\n<p>It supports a single input dataframe.</p>\n\n<p>It is possible to use data quality validators/expectations that will validate\nyour data and fail the process in case the expectations are not met. The DQ\nprocess also generates and keeps updating a site containing the results of the\nexpectations that were done on your data. The location of the site is\nconfigurable and can either be on file system or S3. If you define it to be\nstored on S3, you can even configure your S3 bucket to serve the site so that\npeople can easily check the quality of your data. Moreover, it is also\npossible to store the result of the DQ process into a defined result sink.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  input dataframe on which to run the DQ process.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Validated dataframe.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.dq_validator.DQValidator.execute", "modulename": "lakehouse_engine.algorithms.dq_validator", "qualname": "DQValidator.execute", "kind": "function", "doc": "<p>Define the algorithm execution behaviour.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.exceptions", "modulename": "lakehouse_engine.algorithms.exceptions", "kind": "module", "doc": "<p>Package defining all the algorithm custom exceptions.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.exceptions.ReconciliationFailedException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "ReconciliationFailedException", "kind": "class", "doc": "<p>Exception for when the reconciliation process fails.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.exceptions.NoNewDataException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "NoNewDataException", "kind": "class", "doc": "<p>Exception for when no new data is available.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.exceptions.SensorAlreadyExistsException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "SensorAlreadyExistsException", "kind": "class", "doc": "<p>Exception for when a sensor with same sensor id already exists.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.exceptions.RestoreTypeNotFoundException", "modulename": "lakehouse_engine.algorithms.exceptions", "qualname": "RestoreTypeNotFoundException", "kind": "class", "doc": "<p>Exception for when the restore type is not found.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.algorithms.reconciliator", "modulename": "lakehouse_engine.algorithms.reconciliator", "kind": "module", "doc": "<p>Module containing the Reconciliator class.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationType", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationType", "kind": "class", "doc": "<p>Type of Reconciliation.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationType.PCT", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationType.PCT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReconciliationType.PCT: &#x27;percentage&#x27;&gt;"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationType.ABS", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationType.ABS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReconciliationType.ABS: &#x27;absolute&#x27;&gt;"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationTransformers", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationTransformers", "kind": "class", "doc": "<p>Transformers Available for the Reconciliation Algorithm.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.ReconciliationTransformers.AVAILABLE_TRANSFORMERS", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "ReconciliationTransformers.AVAILABLE_TRANSFORMERS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict", "default_value": "&lt;ReconciliationTransformers.AVAILABLE_TRANSFORMERS: {&#x27;cache&#x27;: &lt;bound method Optimizers.cache of &lt;class &#x27;lakehouse_engine.transformers.optimizers.Optimizers&#x27;&gt;&gt;, &#x27;persist&#x27;: &lt;bound method Optimizers.persist of &lt;class &#x27;lakehouse_engine.transformers.optimizers.Optimizers&#x27;&gt;&gt;}&gt;"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator", "kind": "class", "doc": "<p>Class to define the behavior of an algorithm that checks if data reconciles.</p>\n\n<p>Checking if data reconciles, using this algorithm, is a matter of reading the\n'truth' data and the 'current' data. You can use any input specification compatible\nwith the lakehouse engine to read 'truth' or 'current' data. On top of that, you\ncan pass a 'truth_preprocess_query' and a 'current_preprocess_query' so you can\npreprocess the data before it goes into the actual reconciliation process.\nMoreover, you can use the 'truth_preprocess_query_args' and\n'current_preprocess_query_args' to pass additional arguments to be used to apply\nadditional operations on top of the dataframe, resulting from the previous steps.\nWith these arguments you can apply additional operations like caching or persisting\nthe Dataframe. The way to pass the additional arguments for the operations is\nsimilar to the TransformSpec, but only a few operations are allowed. Those are\ndefined in ReconciliationTransformers.AVAILABLE_TRANSFORMERS.</p>\n\n<p>The reconciliation process is focused on joining 'truth' with 'current' by all\nprovided columns except the ones passed as 'metrics'. After that it calculates the\ndifferences in the metrics attributes (either percentage or absolute difference).\nFinally, it aggregates the differences, using the supplied aggregation function\n(e.g., sum, avg, min, max, etc).</p>\n\n<p>All of these configurations are passed via the ACON to instantiate a\nReconciliatorSpec object.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>It is crucial that both the current and truth datasets have exactly the same\nstructure.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>You should not use 0 as yellow or red threshold, as the algorithm will verify\nif the difference between the truth and current values is bigger\nor equal than those thresholds.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>The reconciliation does not produce any negative values or percentages, as we\nuse the absolute value of the differences. This means that the recon result\nwill not indicate if it was the current values that were bigger or smaller\nthan the truth values, or vice versa.</p>\n\n</div>\n", "bases": "lakehouse_engine.core.executable.Executable"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.__init__", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.__init__", "kind": "function", "doc": "<p>Construct Algorithm instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  algorithm configuration.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.get_source_of_truth", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.get_source_of_truth", "kind": "function", "doc": "<p>Get the source of truth (expected result) for the reconciliation process.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DataFrame containing the source of truth.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.get_current_results", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.get_current_results", "kind": "function", "doc": "<p>Get the current results from the table that we are checking if it reconciles.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>DataFrame containing the current results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.reconciliator.Reconciliator.execute", "modulename": "lakehouse_engine.algorithms.reconciliator", "qualname": "Reconciliator.execute", "kind": "function", "doc": "<p>Reconcile the current results against the truth dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.algorithms.sensor", "modulename": "lakehouse_engine.algorithms.sensor", "kind": "module", "doc": "<p>Module to define Sensor algorithm behavior.</p>\n"}, {"fullname": "lakehouse_engine.algorithms.sensor.Sensor", "modulename": "lakehouse_engine.algorithms.sensor", "qualname": "Sensor", "kind": "class", "doc": "<p>Class representing a sensor to check if the upstream has new data.</p>\n", "bases": "lakehouse_engine.algorithms.algorithm.Algorithm"}, {"fullname": "lakehouse_engine.algorithms.sensor.Sensor.__init__", "modulename": "lakehouse_engine.algorithms.sensor", "qualname": "Sensor.__init__", "kind": "function", "doc": "<p>Construct Sensor instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  algorithm configuration.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.algorithms.sensor.Sensor.execute", "modulename": "lakehouse_engine.algorithms.sensor", "qualname": "Sensor.execute", "kind": "function", "doc": "<p>Execute the sensor.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.configs", "modulename": "lakehouse_engine.configs", "kind": "module", "doc": "<p>This module receives a config file which is included in the wheel.</p>\n"}, {"fullname": "lakehouse_engine.core", "modulename": "lakehouse_engine.core", "kind": "module", "doc": "<p>Package with the core behaviour of the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.core.dbfs_file_manager", "modulename": "lakehouse_engine.core.dbfs_file_manager", "kind": "module", "doc": "<p>File manager module using dbfs.</p>\n"}, {"fullname": "lakehouse_engine.core.dbfs_file_manager.DBFSFileManager", "modulename": "lakehouse_engine.core.dbfs_file_manager", "qualname": "DBFSFileManager", "kind": "class", "doc": "<p>Set of actions to manipulate dbfs files in several ways.</p>\n", "bases": "lakehouse_engine.core.file_manager.FileManager"}, {"fullname": "lakehouse_engine.core.dbfs_file_manager.DBFSFileManager.get_function", "modulename": "lakehouse_engine.core.dbfs_file_manager", "qualname": "DBFSFileManager.get_function", "kind": "function", "doc": "<p>Get a specific function to execute.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.dbfs_file_manager.DBFSFileManager.delete_objects", "modulename": "lakehouse_engine.core.dbfs_file_manager", "qualname": "DBFSFileManager.delete_objects", "kind": "function", "doc": "<p>Delete objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be deleted based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.dbfs_file_manager.DBFSFileManager.copy_objects", "modulename": "lakehouse_engine.core.dbfs_file_manager", "qualname": "DBFSFileManager.copy_objects", "kind": "function", "doc": "<p>Copies objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be copied based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.dbfs_file_manager.DBFSFileManager.move_objects", "modulename": "lakehouse_engine.core.dbfs_file_manager", "qualname": "DBFSFileManager.move_objects", "kind": "function", "doc": "<p>Moves objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be moved based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions", "modulename": "lakehouse_engine.core.definitions", "kind": "module", "doc": "<p>Definitions of standard values and structures for core components.</p>\n"}, {"fullname": "lakehouse_engine.core.definitions.CollectEngineUsage", "modulename": "lakehouse_engine.core.definitions", "qualname": "CollectEngineUsage", "kind": "class", "doc": "<p>Options for collecting engine usage stats.</p>\n\n<ul>\n<li>enabled, enables the collection and storage of Lakehouse Engine\nusage statistics for any environment.</li>\n<li>prod_only, enables the collection and storage of Lakehouse Engine\nusage statistics for production environment only.</li>\n<li>disabled, disables the collection and storage of Lakehouse Engine\nusage statistics, for all environments.</li>\n</ul>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.CollectEngineUsage.ENABLED", "modulename": "lakehouse_engine.core.definitions", "qualname": "CollectEngineUsage.ENABLED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;CollectEngineUsage.ENABLED: &#x27;enabled&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.CollectEngineUsage.PROD_ONLY", "modulename": "lakehouse_engine.core.definitions", "qualname": "CollectEngineUsage.PROD_ONLY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;CollectEngineUsage.PROD_ONLY: &#x27;prod_only&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.CollectEngineUsage.DISABLED", "modulename": "lakehouse_engine.core.definitions", "qualname": "CollectEngineUsage.DISABLED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;CollectEngineUsage.DISABLED: &#x27;disabled&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.EngineConfig", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineConfig", "kind": "class", "doc": "<p>Definitions that can come from the Engine Config file.</p>\n\n<ul>\n<li>dq_bucket: S3 bucket used to store data quality related artifacts.</li>\n<li>notif_disallowed_email_servers: email servers not allowed to be used\nfor sending notifications.</li>\n<li>engine_usage_path: path where the engine prod usage stats are stored.</li>\n<li>engine_dev_usage_path: path where the engine dev usage stats are stored.</li>\n<li>collect_engine_usage: whether to enable the collection of lakehouse\nengine usage stats or not.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.EngineConfig.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dq_bucket</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">notif_disallowed_email_servers</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">engine_usage_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">engine_dev_usage_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;enabled&#39;</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.EngineStats", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineStats", "kind": "class", "doc": "<p>Definitions for collection of Lakehouse Engine Stats.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Note: whenever the value comes from a key inside a Spark Config\nthat returns an array, it can be specified with a '#' so that it\nis adequately processed.</p>\n\n</div>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.EngineStats.CLUSTER_USAGE_TAGS", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineStats.CLUSTER_USAGE_TAGS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EngineStats.CLUSTER_USAGE_TAGS: &#x27;spark.databricks.clusterUsageTags&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.EngineStats.DEF_SPARK_CONFS", "modulename": "lakehouse_engine.core.definitions", "qualname": "EngineStats.DEF_SPARK_CONFS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EngineStats.DEF_SPARK_CONFS: {&#x27;dp_name&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#accountName&#x27;, &#x27;environment&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#environment&#x27;, &#x27;workspace_id&#x27;: &#x27;spark.databricks.clusterUsageTags.orgId&#x27;, &#x27;job_id&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#JobId&#x27;, &#x27;job_name&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#RunName&#x27;, &#x27;run_id&#x27;: &#x27;spark.databricks.clusterUsageTags.clusterAllTags#ClusterName&#x27;}&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat", "kind": "class", "doc": "<p>Formats of algorithm input.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.JDBC", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.JDBC", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.JDBC: &#x27;jdbc&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.AVRO", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.AVRO", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.AVRO: &#x27;avro&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.JSON", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.JSON: &#x27;json&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.CSV", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.CSV", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.CSV: &#x27;csv&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.PARQUET", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.PARQUET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.PARQUET: &#x27;parquet&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.DELTAFILES", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.DELTAFILES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.DELTAFILES: &#x27;delta&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.CLOUDFILES", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.CLOUDFILES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.CLOUDFILES: &#x27;cloudfiles&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.KAFKA", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.KAFKA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.KAFKA: &#x27;kafka&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SQL", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SQL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SQL: &#x27;sql&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SAP_BW", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SAP_BW", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SAP_BW: &#x27;sap_bw&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SAP_B4", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SAP_B4", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SAP_B4: &#x27;sap_b4&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.DATAFRAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.DATAFRAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.DATAFRAME: &#x27;dataframe&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.SFTP", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.SFTP", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;InputFormat.SFTP: &#x27;sftp&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.values", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.values", "kind": "function", "doc": "<p>Generates a list containing all enum values.</p>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>A list with all enum values.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.InputFormat.exists", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputFormat.exists", "kind": "function", "doc": "<p>Checks if the input format exists in the enum values.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_format:</strong>  format to check if exists.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>If the input format exists in our enum.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">input_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat", "kind": "class", "doc": "<p>Formats of algorithm output.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.JDBC", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.JDBC", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.JDBC: &#x27;jdbc&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.AVRO", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.AVRO", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.AVRO: &#x27;avro&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.JSON", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.JSON: &#x27;json&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.CSV", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.CSV", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.CSV: &#x27;csv&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.PARQUET", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.PARQUET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.PARQUET: &#x27;parquet&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.DELTAFILES", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.DELTAFILES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.DELTAFILES: &#x27;delta&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.KAFKA", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.KAFKA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.KAFKA: &#x27;kafka&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.CONSOLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.CONSOLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.CONSOLE: &#x27;console&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.NOOP", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.NOOP", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.NOOP: &#x27;noop&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.DATAFRAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.DATAFRAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.DATAFRAME: &#x27;dataframe&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.FILE", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.FILE: &#x27;file&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.OutputFormat.TABLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputFormat.TABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;OutputFormat.TABLE: &#x27;table&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.NotifierType", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotifierType", "kind": "class", "doc": "<p>Type of notifier available.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.NotifierType.EMAIL", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotifierType.EMAIL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;NotifierType.EMAIL: &#x27;email&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.NotificationRuntimeParameters", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotificationRuntimeParameters", "kind": "class", "doc": "<p>Parameters to be replaced in runtime.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.NotificationRuntimeParameters.DATABRICKS_JOB_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotificationRuntimeParameters.DATABRICKS_JOB_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;NotificationRuntimeParameters.DATABRICKS_JOB_NAME: &#x27;databricks_job_name&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID", "modulename": "lakehouse_engine.core.definitions", "qualname": "NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;NotificationRuntimeParameters.DATABRICKS_WORKSPACE_ID: &#x27;databricks_workspace_id&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadType", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadType", "kind": "class", "doc": "<p>Define the types of read operations.</p>\n\n<ul>\n<li>BATCH - read the data in batch mode (e.g., Spark batch).</li>\n<li>STREAMING - read the data in streaming mode (e.g., Spark streaming).</li>\n</ul>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.ReadType.BATCH", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadType.BATCH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadType.BATCH: &#x27;batch&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadType.STREAMING", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadType.STREAMING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadType.STREAMING: &#x27;streaming&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode", "kind": "class", "doc": "<p>Different modes that control how we handle compliance to the provided schema.</p>\n\n<p>These read modes map to Spark's read modes at the moment.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode.PERMISSIVE", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode.PERMISSIVE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadMode.PERMISSIVE: &#x27;PERMISSIVE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode.FAILFAST", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode.FAILFAST", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadMode.FAILFAST: &#x27;FAILFAST&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.ReadMode.DROPMALFORMED", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReadMode.DROPMALFORMED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ReadMode.DROPMALFORMED: &#x27;DROPMALFORMED&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults", "kind": "class", "doc": "<p>Defaults used on the data quality process.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.FILE_SYSTEM_STORE", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.FILE_SYSTEM_STORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.FILE_SYSTEM_STORE: &#x27;file_system&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.FILE_SYSTEM_S3_STORE", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.FILE_SYSTEM_S3_STORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.FILE_SYSTEM_S3_STORE: &#x27;s3&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DQ_BATCH_IDENTIFIERS", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DQ_BATCH_IDENTIFIERS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DQ_BATCH_IDENTIFIERS: [&#x27;spec_id&#x27;, &#x27;input_id&#x27;, &#x27;timestamp&#x27;]&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATASOURCE_CLASS_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATASOURCE_CLASS_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATASOURCE_CLASS_NAME: &#x27;Datasource&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATASOURCE_EXECUTION_ENGINE", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATASOURCE_EXECUTION_ENGINE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATASOURCE_EXECUTION_ENGINE: &#x27;SparkDFExecutionEngine&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CONNECTORS_CLASS_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CONNECTORS_CLASS_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CONNECTORS_CLASS_NAME: &#x27;RuntimeDataConnector&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CONNECTORS_MODULE_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CONNECTORS_MODULE_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CONNECTORS_MODULE_NAME: &#x27;great_expectations.datasource.data_connector&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CHECKPOINTS_CLASS_NAME", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CHECKPOINTS_CLASS_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CHECKPOINTS_CLASS_NAME: &#x27;SimpleCheckpoint&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_CHECKPOINTS_CONFIG_VERSION: 1.0&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.STORE_BACKEND", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.STORE_BACKEND", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.FILE_SYSTEM_S3_STORE: &#x27;s3&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.EXPECTATIONS_STORE_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.EXPECTATIONS_STORE_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.EXPECTATIONS_STORE_PREFIX: &#x27;dq/expectations/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.VALIDATIONS_STORE_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.VALIDATIONS_STORE_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.VALIDATIONS_STORE_PREFIX: &#x27;dq/validations/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DATA_DOCS_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DATA_DOCS_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DATA_DOCS_PREFIX: &#x27;dq/data_docs/site/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.CHECKPOINT_STORE_PREFIX", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.CHECKPOINT_STORE_PREFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.CHECKPOINT_STORE_PREFIX: &#x27;dq/checkpoints/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.VALIDATION_COLUMN_IDENTIFIER", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.VALIDATION_COLUMN_IDENTIFIER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.VALIDATION_COLUMN_IDENTIFIER: &#x27;validationresultidentifier&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.CUSTOM_EXPECTATION_LIST", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.CUSTOM_EXPECTATION_LIST", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.CUSTOM_EXPECTATION_LIST: [&#x27;expect_column_values_to_be_date_not_older_than&#x27;, &#x27;expect_column_pair_a_to_be_smaller_or_equal_than_b&#x27;, &#x27;expect_multicolumn_column_a_must_equal_b_or_c&#x27;, &#x27;expect_queried_column_agg_value_to_be&#x27;]&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQDefaults.DQ_VALIDATIONS_SCHEMA", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQDefaults.DQ_VALIDATIONS_SCHEMA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQDefaults.DQ_VALIDATIONS_SCHEMA: StructType([StructField(&#x27;dq_validations&#x27;, StructType([StructField(&#x27;run_name&#x27;, StringType(), True), StructField(&#x27;run_success&#x27;, BooleanType(), True), StructField(&#x27;raised_exceptions&#x27;, BooleanType(), True), StructField(&#x27;run_row_success&#x27;, BooleanType(), True), StructField(&#x27;dq_failure_details&#x27;, ArrayType(StructType([StructField(&#x27;expectation_type&#x27;, StringType(), True), StructField(&#x27;kwargs&#x27;, StringType(), True)]), True), True)]), True)])&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType", "kind": "class", "doc": "<p>Types of write operations.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.OVERWRITE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.OVERWRITE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.OVERWRITE: &#x27;overwrite&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.COMPLETE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.COMPLETE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.COMPLETE: &#x27;complete&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.APPEND", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.APPEND", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.APPEND: &#x27;append&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.UPDATE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.UPDATE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.UPDATE: &#x27;update&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.MERGE", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.MERGE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.MERGE: &#x27;merge&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.ERROR_IF_EXISTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.ERROR_IF_EXISTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.ERROR_IF_EXISTS: &#x27;error&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.WriteType.IGNORE_IF_EXISTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "WriteType.IGNORE_IF_EXISTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;WriteType.IGNORE_IF_EXISTS: &#x27;ignore&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.InputSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputSpec", "kind": "class", "doc": "<p>Specification of an algorithm input.</p>\n\n<p>This is very aligned with the way the execution environment connects to the sources\n(e.g., spark sources).</p>\n\n<ul>\n<li>spec_id: spec_id of the input specification read_type: ReadType type of read\noperation.</li>\n<li>data_format: format of the input.</li>\n<li>sftp_files_format: format of the files (csv, fwf, json, xml...) in a sftp\ndirectory.</li>\n<li>df_name: dataframe name.</li>\n<li>db_table: table name in the form of <code>&lt;db&gt;.&lt;table&gt;</code>.</li>\n<li>location: uri that identifies from where to read data in the specified format.</li>\n<li>enforce_schema_from_table: if we want to enforce the table schema or not, by\nproviding a table name in the form of <code>&lt;db&gt;.&lt;table&gt;</code>.</li>\n<li>query: sql query to execute and return the dataframe. Use it if you do not want to\nread from a file system nor from a table, but rather from a sql query instead.</li>\n<li>schema: dict representation of a schema of the input (e.g., Spark struct type\nschema).</li>\n<li>schema_path: path to a file with a representation of a schema of the input (e.g.,\nSpark struct type schema).</li>\n<li>disable_dbfs_retry: optional flag to disable file storage dbfs.</li>\n<li>with_filepath: if we want to include the path of the file that is being read. Only\nworks with the file reader (batch and streaming modes are supported).</li>\n<li>options: dict with other relevant options according to the execution\nenvironment (e.g., spark) possible sources.</li>\n<li>calculate_upper_bound: when to calculate upper bound to extract from SAP BW\nor not.</li>\n<li>calc_upper_bound_schema: specific schema for the calculated upper_bound.</li>\n<li>generate_predicates: when to generate predicates to extract from SAP BW or not.</li>\n<li>predicates_add_null: if we want to include is null on partition by predicates.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.InputSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "InputSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">read_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">data_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sftp_files_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">df_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">enforce_schema_from_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">schema_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">with_filepath</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">jdbc_args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">calculate_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.TransformerSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformerSpec", "kind": "class", "doc": "<p>Transformer Specification, i.e., a single transformation amongst many.</p>\n\n<ul>\n<li>function: name of the function (or callable function) to be executed.</li>\n<li>args: (not applicable if using a callable function) dict with the arguments\nto pass to the function <code>&lt;k,v&gt;</code> pairs with the name of the parameter of\nthe function and the respective value.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.TransformerSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformerSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">function</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.TransformSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformSpec", "kind": "class", "doc": "<p>Transformation Specification.</p>\n\n<p>I.e., the specification that defines the many transformations to be done to the data\nthat was read.</p>\n\n<ul>\n<li>spec_id: id of the terminate specification</li>\n<li>input_id: id of the corresponding input\nspecification.</li>\n<li>transformers: list of transformers to execute.</li>\n<li>force_streaming_foreach_batch_processing: sometimes, when using streaming, we want\nto force the transform to be executed in the foreachBatch function to ensure\nnon-supported streaming operations can be properly executed.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.TransformSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "TransformSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">transformers</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TransformerSpec</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">force_streaming_foreach_batch_processing</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.DQType", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQType", "kind": "class", "doc": "<p>Available data quality tasks.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.DQType.VALIDATOR", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQType.VALIDATOR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQType.VALIDATOR: &#x27;validator&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQType.ASSISTANT", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQType.ASSISTANT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;DQType.ASSISTANT: &#x27;assistant&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.DQFunctionSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQFunctionSpec", "kind": "class", "doc": "<p>Defines a data quality function specification.</p>\n\n<ul>\n<li>function - name of the data quality function (expectation) to execute.\nIt follows the great_expectations api <a href=\"https://greatexpectations.io/expectations/\">https://greatexpectations.io/expectations/</a>.</li>\n<li>args - args of the function (expectation). Follow the same api as above.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.DQFunctionSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQFunctionSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">function</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.DQSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQSpec", "kind": "class", "doc": "<p>Data quality overall specification.</p>\n\n<ul>\n<li>spec_id - id of the specification.</li>\n<li>input_id - id of the input specification.</li>\n<li>dq_type - type of DQ process to execute (e.g. validator).</li>\n<li>dq_functions - list of function specifications to execute.</li>\n<li>unexpected_rows_pk - the list of columns composing the primary key of the\nsource data to identify the rows failing the DQ validations. Note: only one\nof tbl_to_derive_pk or unexpected_rows_pk arguments need to be provided. It\nis mandatory to provide one of these arguments when using tag_source_data\nas True. When tag_source_data is False, this is not mandatory, but still\nrecommended.</li>\n<li>tbl_to_derive_pk - db.table to automatically derive the unexpected_rows_pk from.\nNote: only one of tbl_to_derive_pk or unexpected_rows_pk arguments need to\nbe provided. It is mandatory to provide one of these arguments when using\ntag_source_data as True. hen tag_source_data is False, this is not\nmandatory, but still recommended.</li>\n<li>gx_result_format - great expectations result format. Default: \"COMPLETE\".</li>\n<li>tag_source_data - when set to true, this will ensure that the DQ process ends by\ntagging the source data with an additional column with information about the\nDQ results. This column makes it possible to identify if the DQ run was\nsucceeded in general and, if not, it unlocks the insights to know what\nspecific rows have made the DQ validations fail and why. Default: False.\nNote: it only works if result_sink_explode is True, gx_result_format is\nCOMPLETE, fail_on_error is False (which is done automatically when\nyou specify tag_source_data as True) and tbl_to_derive_pk or\nunexpected_rows_pk is configured.</li>\n<li>store_backend - which store_backend to use (e.g. s3 or file_system).</li>\n<li>local_fs_root_dir - path of the root directory. Note: only applicable for\nstore_backend file_system.</li>\n<li>data_docs_local_fs - the path for data docs only for store_backend\nfile_system.</li>\n<li>bucket - the bucket name to consider for the store_backend (store DQ artefacts).\nNote: only applicable for store_backend s3.</li>\n<li>data_docs_bucket - the bucket name for data docs only. When defined, it will\nsupersede bucket parameter.</li>\n<li>expectations_store_prefix - prefix where to store expectations' data. Note: only\napplicable for store_backend s3.</li>\n<li>validations_store_prefix - prefix where to store validations' data. Note: only\napplicable for store_backend s3.</li>\n<li>data_docs_prefix - prefix where to store data_docs' data. Note: only applicable\nfor store_backend s3.</li>\n<li>checkpoint_store_prefix - prefix where to store checkpoints' data. Note: only\napplicable for store_backend s3.</li>\n<li>data_asset_name - name of the data asset to consider when configuring the great\nexpectations' data source.</li>\n<li>expectation_suite_name - name to consider for great expectations' suite.</li>\n<li>assistant_options - additional options to pass to the DQ assistant processor.</li>\n<li>result_sink_db_table - db.table_name indicating the database and table in which\nto save the results of the DQ process.</li>\n<li>result_sink_location - file system location in which to save the results of the\nDQ process.</li>\n<li>result_sink_partitions - the list of partitions to consider.</li>\n<li>result_sink_format - format of the result table (e.g. delta, parquet, kafka...).</li>\n<li>result_sink_options - extra spark options for configuring the result sink.\nE.g: can be used to configure a Kafka sink if result_sink_format is kafka.</li>\n<li>result_sink_explode - flag to determine if the output table/location should have\nthe columns exploded (as True) or not (as False). Default: True.</li>\n<li>result_sink_extra_columns - list of extra columns to be exploded (following\nthe pattern \"<name>.*\") or columns to be selected. It is only used when\nresult_sink_explode is set to True.</li>\n<li>source - name of data source, to be easier to identify in analysis. If not\nspecified, it is set as default <input_id>. This will be only used\nwhen result_sink_explode is set to True.</li>\n<li>fail_on_error - whether to fail the algorithm if the validations of your data in\nthe DQ process failed.</li>\n<li>cache_df - whether to cache the dataframe before running the DQ process or not.</li>\n<li>critical_functions - functions that should not fail. When this argument is\ndefined, fail_on_error is nullified.</li>\n<li>max_percentage_failure - percentage of failure that should be allowed.\nThis argument has priority over both fail_on_error and critical_functions.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.DQSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dq_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dq_functions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unexpected_rows_pk</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tbl_to_derive_pk</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gx_result_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;COMPLETE&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tag_source_data</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">assistant_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">store_backend</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;s3&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">local_fs_root_dir</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">data_docs_local_fs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bucket</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">data_docs_bucket</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">expectations_store_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/expectations/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">validations_store_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/validations/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_docs_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/data_docs/site/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_store_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dq/checkpoints/&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_asset_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">expectation_suite_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_explode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">result_sink_extra_columns</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">source</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fail_on_error</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">cache_df</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">critical_functions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_percentage_failure</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.MergeOptions", "modulename": "lakehouse_engine.core.definitions", "qualname": "MergeOptions", "kind": "class", "doc": "<p>Options for a merge operation.</p>\n\n<ul>\n<li>merge_predicate: predicate to apply to the merge operation so that we can\ncheck if a new record corresponds to a record already included in the\nhistorical data.</li>\n<li>insert_only: indicates if the merge should only insert data (e.g., deduplicate\nscenarios).</li>\n<li>delete_predicate: predicate to apply to the delete operation.</li>\n<li>update_predicate: predicate to apply to the update operation.</li>\n<li>insert_predicate: predicate to apply to the insert operation.</li>\n<li>update_column_set: rules to apply to the update operation which allows to\nset the value for each column to be updated.\n(e.g. {\"data\": \"new.data\", \"count\": \"current.count + 1\"} )</li>\n<li>insert_column_set: rules to apply to the insert operation which allows to\nset the value for each column to be inserted.\n(e.g. {\"date\": \"updates.date\", \"count\": \"1\"} )</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.MergeOptions.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "MergeOptions.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">merge_predicate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">insert_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">delete_predicate</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">update_predicate</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">insert_predicate</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">update_column_set</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">insert_column_set</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.OutputSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputSpec", "kind": "class", "doc": "<p>Specification of an algorithm output.</p>\n\n<p>This is very aligned with the way the execution environment connects to the output\nsystems (e.g., spark outputs).</p>\n\n<ul>\n<li>spec_id: id of the output specification.</li>\n<li>input_id: id of the corresponding input specification.</li>\n<li>write_type: type of write operation.</li>\n<li>data_format: format of the output. Defaults to DELTA.</li>\n<li>db_table: table name in the form of <code>&lt;db&gt;.&lt;table&gt;</code>.</li>\n<li>location: uri that identifies from where to write data in the specified format.</li>\n<li>partitions: list of partition input_col names.</li>\n<li>merge_opts: options to apply to the merge operation.</li>\n<li>streaming_micro_batch_transformers: transformers to invoke for each streaming\nmicro batch, before writing (i.e., in Spark's foreachBatch structured\nstreaming function). Note: the lakehouse engine manages this for you, so\nyou don't have to manually specify streaming transformations here, so we don't\nadvise you to manually specify transformations through this parameter. Supply\nthem as regular transformers in the transform_specs sections of an ACON.</li>\n<li>streaming_once: if the streaming query is to be executed just once, or not,\ngenerating just one micro batch.</li>\n<li>streaming_processing_time: if streaming query is to be kept alive, this indicates\nthe processing time of each micro batch.</li>\n<li>streaming_available_now: if set to True, set a trigger that processes all\navailable data in multiple batches then terminates the query.\nWhen using streaming, this is the default trigger that the lakehouse-engine will\nuse, unless you configure a different one.</li>\n<li>streaming_continuous: set a trigger that runs a continuous query with a given\ncheckpoint interval.</li>\n<li>streaming_await_termination: whether to wait (True) for the termination of the\nstreaming query (e.g. timeout or exception) or not (False). Default: True.</li>\n<li>streaming_await_termination_timeout: a timeout to set to the\nstreaming_await_termination. Default: None.</li>\n<li>with_batch_id: whether to include the streaming batch id in the final data,\nor not. It only takes effect in streaming mode.</li>\n<li>options: dict with other relevant options according to the execution environment\n(e.g., spark) possible outputs.  E.g.,: JDBC options, checkpoint location for\nstreaming, etc.</li>\n<li>streaming_micro_batch_dq_processors: similar to streaming_micro_batch_transformers\nbut for the DQ functions to be executed. Used internally by the lakehouse\nengine, so you don't have to supply DQ functions through this parameter. Use the\ndq_specs of the acon instead.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.OutputSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "OutputSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">write_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">merge_opts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">MergeOptions</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_micro_batch_transformers</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TransformerSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_once</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_processing_time</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_available_now</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_continuous</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_await_termination</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_await_termination_timeout</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_batch_id</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">streaming_micro_batch_dq_processors</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.TerminatorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "TerminatorSpec", "kind": "class", "doc": "<p>Terminator Specification.</p>\n\n<p>I.e., the specification that defines a terminator operation to be executed. Examples\nare compute statistics, vacuum, optimize, etc.</p>\n\n<ul>\n<li>function: terminator function to execute.</li>\n<li>args: arguments of the terminator function.</li>\n<li>input_id: id of the corresponding output specification (Optional).</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.TerminatorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "TerminatorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">function</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">input_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.ReconciliatorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReconciliatorSpec", "kind": "class", "doc": "<p>Reconciliator Specification.</p>\n\n<ul>\n<li>metrics: list of metrics in the form of:\n[{\n    metric: name of the column present in both truth and current datasets,\n    aggregation: sum, avg, max, min, ...,\n    type: percentage or absolute,\n    yellow: value,\n    red: value\n}].</li>\n<li>recon_type: reconciliation type (percentage or absolute). Percentage calculates\nthe difference between truth and current results as a percentage (x-y/x), and\nabsolute calculates the raw difference (x - y).</li>\n<li>truth_input_spec: input specification of the truth data.</li>\n<li>current_input_spec: input specification of the current results data</li>\n<li>truth_preprocess_query: additional query on top of the truth input data to\npreprocess the truth data before it gets fueled into the reconciliation process.\nImportant note: you need to assume that the data out of\nthe truth_input_spec is referencable by a table called 'truth'.</li>\n<li>truth_preprocess_query_args: optional dict having the functions/transformations to\napply on top of the truth_preprocess_query and respective arguments. Note: cache\nis being applied on the Dataframe, by default. For turning the default behavior\noff, pass <code>\"truth_preprocess_query_args\": []</code>.</li>\n<li>current_preprocess_query: additional query on top of the current results input\ndata to preprocess the current results data before it gets fueled into the\nreconciliation process. Important note: you need to assume that the data out of\nthe current_results_input_spec is referencable by a table called 'current'.</li>\n<li>current_preprocess_query_args: optional dict having the\nfunctions/transformations to apply on top of the current_preprocess_query\nand respective arguments. Note: cache is being applied on the Dataframe,\nby default. For turning the default behavior off, pass\n<code>\"current_preprocess_query_args\": []</code>.</li>\n<li>ignore_empty_df: optional boolean, to ignore the recon process if source &amp; target\ndataframes are empty, recon will exit success code (passed)</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.ReconciliatorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "ReconciliatorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">truth_input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">current_input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">truth_preprocess_query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">truth_preprocess_query_args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">current_preprocess_query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">current_preprocess_query_args</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_empty_df</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.DQValidatorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQValidatorSpec", "kind": "class", "doc": "<p>Data Quality Validator Specification.</p>\n\n<ul>\n<li>input_spec: input specification of the data to be checked/validated.</li>\n<li>dq_spec: data quality specification.</li>\n<li>restore_prev_version: specify if, having\ndelta table/files as input, they should be restored to the\nprevious version if the data quality process fails. Note: this\nis only considered if fail_on_error is kept as True.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.DQValidatorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "DQValidatorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">dq_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span>,</span><span class=\"param\">\t<span class=\"n\">restore_prev_version</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions", "kind": "class", "doc": "<p>SQL definitions statements.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.compute_table_stats", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.compute_table_stats", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.compute_table_stats: &#x27;ANALYZE TABLE {} COMPUTE STATISTICS&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.drop_table_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.drop_table_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.drop_table_stmt: &#x27;DROP TABLE IF EXISTS&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.drop_view_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.drop_view_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.drop_view_stmt: &#x27;DROP VIEW IF EXISTS&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.truncate_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.truncate_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.truncate_stmt: &#x27;TRUNCATE TABLE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.describe_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.describe_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.describe_stmt: &#x27;DESCRIBE TABLE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.optimize_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.optimize_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.optimize_stmt: &#x27;OPTIMIZE&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.show_tbl_props_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.show_tbl_props_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.show_tbl_props_stmt: &#x27;SHOW TBLPROPERTIES&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLDefinitions.delete_where_stmt", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLDefinitions.delete_where_stmt", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLDefinitions.delete_where_stmt: &#x27;DELETE FROM {} WHERE {}&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys", "kind": "class", "doc": "<p>File Manager s3 api keys.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.CONTENTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.CONTENTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.CONTENTS: &#x27;Contents&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.KEY", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.KEY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.KEY: &#x27;Key&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.CONTINUATION", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.CONTINUATION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.CONTINUATION: &#x27;NextContinuationToken&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.BUCKET", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.BUCKET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.BUCKET: &#x27;Bucket&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.FileManagerAPIKeys.OBJECTS", "modulename": "lakehouse_engine.core.definitions", "qualname": "FileManagerAPIKeys.OBJECTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FileManagerAPIKeys.OBJECTS: &#x27;Objects&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SensorSpec", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorSpec", "kind": "class", "doc": "<p>Sensor Specification.</p>\n\n<ul>\n<li>sensor_id: sensor id.</li>\n<li>assets: a list of assets that are considered as available to\nconsume downstream after this sensor has status\nPROCESSED_NEW_DATA.</li>\n<li>control_db_table_name: db.table to store sensor metadata.</li>\n<li>input_spec: input specification of the source to be checked for new data.</li>\n<li>preprocess_query: SQL query to transform/filter the result from the\nupstream. Consider that we should refer to 'new_data' whenever\nwe are referring to the input of the sensor. E.g.:\n    \"SELECT dummy_col FROM new_data WHERE ...\"</li>\n<li>checkpoint_location: optional location to store checkpoints to resume\nfrom. These checkpoints use the same as Spark checkpoint strategy.\nFor Spark readers that do not support checkpoints, use the\npreprocess_query parameter to form a SQL query to filter the result\nfrom the upstream accordingly.</li>\n<li>fail_on_empty_result: if the sensor should throw an error if there is no new\ndata in the upstream. Default: True.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.core.definitions.SensorSpec.__init__", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorSpec.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">preprocess_query</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fail_on_empty_result</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.definitions.SensorSpec.create_from_acon", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorSpec.create_from_acon", "kind": "function", "doc": "<p>Create SensorSpec from acon.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  sensor ACON.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.SensorStatus", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorStatus", "kind": "class", "doc": "<p>Status for a sensor.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SensorStatus.ACQUIRED_NEW_DATA", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorStatus.ACQUIRED_NEW_DATA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SensorStatus.ACQUIRED_NEW_DATA: &#x27;ACQUIRED_NEW_DATA&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SensorStatus.PROCESSED_NEW_DATA", "modulename": "lakehouse_engine.core.definitions", "qualname": "SensorStatus.PROCESSED_NEW_DATA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SensorStatus.PROCESSED_NEW_DATA: &#x27;PROCESSED_NEW_DATA&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain", "kind": "class", "doc": "<p>Defaults used on consuming data from SAP Logchain.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain.DBTABLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain.DBTABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SAPLogchain.DBTABLE: &#x27;SAPPHA.RSPCLOGCHAIN&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain.GREEN_STATUS", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain.GREEN_STATUS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SAPLogchain.GREEN_STATUS: &#x27;G&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SAPLogchain.ENGINE_TABLE", "modulename": "lakehouse_engine.core.definitions", "qualname": "SAPLogchain.ENGINE_TABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SAPLogchain.ENGINE_TABLE: &#x27;sensor_new_data&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType", "kind": "class", "doc": "<p>Archive types.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.BULK", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.BULK", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreType.BULK: &#x27;Bulk&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.STANDARD", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.STANDARD", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreType.STANDARD: &#x27;Standard&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.EXPEDITED", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.EXPEDITED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreType.EXPEDITED: &#x27;Expedited&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.values", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.values", "kind": "function", "doc": "<p>Generates a list containing all enum values.</p>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>A list with all enum values.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.RestoreType.exists", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreType.exists", "kind": "function", "doc": "<p>Checks if the restore type exists in the enum values.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>restore_type:</strong>  restore type to check if exists.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>If the restore type exists in our enum.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">restore_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus", "kind": "class", "doc": "<p>Archive types.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus.NOT_STARTED", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus.NOT_STARTED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreStatus.NOT_STARTED: &#x27;not_started&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus.ONGOING", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus.ONGOING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreStatus.ONGOING: &#x27;ongoing&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.RestoreStatus.RESTORED", "modulename": "lakehouse_engine.core.definitions", "qualname": "RestoreStatus.RESTORED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RestoreStatus.RESTORED: &#x27;restored&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser", "kind": "class", "doc": "<p>Defaults to use for parsing.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.DOUBLE_QUOTES", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.DOUBLE_QUOTES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.DOUBLE_QUOTES: &#x27;&quot;&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.SINGLE_QUOTES", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.SINGLE_QUOTES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.SINGLE_QUOTES: &quot;&#x27;&quot;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.BACKSLASH", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.BACKSLASH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.BACKSLASH: &#x27;\\\\&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.SINGLE_TRACE", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.SINGLE_TRACE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.SINGLE_TRACE: &#x27;-&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.DOUBLE_TRACES", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.DOUBLE_TRACES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.DOUBLE_TRACES: &#x27;--&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.SLASH", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.SLASH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.SLASH: &#x27;/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.OPENING_MULTIPLE_LINE_COMMENT", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.OPENING_MULTIPLE_LINE_COMMENT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.OPENING_MULTIPLE_LINE_COMMENT: &#x27;/*&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.CLOSING_MULTIPLE_LINE_COMMENT", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.CLOSING_MULTIPLE_LINE_COMMENT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.CLOSING_MULTIPLE_LINE_COMMENT: &#x27;*/&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.PARAGRAPH", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.PARAGRAPH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.PARAGRAPH: &#x27;\\n&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.STAR", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.STAR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.STAR: &#x27;*&#x27;&gt;"}, {"fullname": "lakehouse_engine.core.definitions.SQLParser.MULTIPLE_LINE_COMMENT", "modulename": "lakehouse_engine.core.definitions", "qualname": "SQLParser.MULTIPLE_LINE_COMMENT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SQLParser.MULTIPLE_LINE_COMMENT: [&#x27;/*&#x27;, &#x27;*/&#x27;]&gt;"}, {"fullname": "lakehouse_engine.core.exec_env", "modulename": "lakehouse_engine.core.exec_env", "kind": "module", "doc": "<p>Module to take care of creating a singleton of the execution environment class.</p>\n"}, {"fullname": "lakehouse_engine.core.exec_env.ExecEnv", "modulename": "lakehouse_engine.core.exec_env", "qualname": "ExecEnv", "kind": "class", "doc": "<p>Represents the basic resources regarding the engine execution environment.</p>\n\n<p>Currently, it is used to encapsulate both the logic to get the Spark\nsession and the engine configurations.</p>\n"}, {"fullname": "lakehouse_engine.core.exec_env.ExecEnv.set_default_engine_config", "modulename": "lakehouse_engine.core.exec_env", "qualname": "ExecEnv.set_default_engine_config", "kind": "function", "doc": "<p>Set default engine configurations by reading them from a specified package.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>package:</strong>  package where the engine configurations can be found.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">package</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.exec_env.ExecEnv.get_or_create", "modulename": "lakehouse_engine.core.exec_env", "qualname": "ExecEnv.get_or_create", "kind": "function", "doc": "<p>Get or create an execution environment session (currently Spark).</p>\n\n<p>It instantiates a singleton session that can be accessed anywhere from the\nlakehouse engine.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>session:</strong>  spark session.</li>\n<li><strong>enable_hive_support:</strong>  whether to enable hive support or not.</li>\n<li><strong>app_name:</strong>  application name.</li>\n<li><strong>config:</strong>  extra spark configs to supply to the spark session.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">session</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">SparkSession</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">enable_hive_support</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">app_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.executable", "modulename": "lakehouse_engine.core.executable", "kind": "module", "doc": "<p>Module representing an executable lakehouse engine component.</p>\n"}, {"fullname": "lakehouse_engine.core.executable.Executable", "modulename": "lakehouse_engine.core.executable", "qualname": "Executable", "kind": "class", "doc": "<p>Abstract class defining the behaviour of an executable component.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.core.executable.Executable.execute", "modulename": "lakehouse_engine.core.executable", "qualname": "Executable.execute", "kind": "function", "doc": "<p>Define the executable component behaviour.</p>\n\n<p>E.g., the behaviour of an algorithm inheriting from this.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager", "modulename": "lakehouse_engine.core.file_manager", "kind": "module", "doc": "<p>Module for abstract representation of a file manager system.</p>\n"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager", "kind": "class", "doc": "<p>Abstract file manager class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.__init__", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.__init__", "kind": "function", "doc": "<p>Construct FileManager algorithm instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>configs:</strong>  configurations for the FileManager algorithm.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">configs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.delete_objects", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.delete_objects", "kind": "function", "doc": "<p>Delete objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be deleted based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.copy_objects", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.copy_objects", "kind": "function", "doc": "<p>Copies objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be copied based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManager.move_objects", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManager.move_objects", "kind": "function", "doc": "<p>Moves objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be moved based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.file_manager.FileManagerFactory", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManagerFactory", "kind": "class", "doc": "<p>Class for file manager factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.core.file_manager.FileManagerFactory.execute_function", "modulename": "lakehouse_engine.core.file_manager", "qualname": "FileManagerFactory.execute_function", "kind": "function", "doc": "<p>Get a specific File Manager and function to execute.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">configs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager", "modulename": "lakehouse_engine.core.s3_file_manager", "kind": "module", "doc": "<p>File manager module using boto3.</p>\n"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager", "kind": "class", "doc": "<p>Set of actions to manipulate s3 files in several ways.</p>\n", "bases": "lakehouse_engine.core.file_manager.FileManager"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.get_function", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.get_function", "kind": "function", "doc": "<p>Get a specific function to execute.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.delete_objects", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.delete_objects", "kind": "function", "doc": "<p>Delete objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be deleted based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.copy_objects", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.copy_objects", "kind": "function", "doc": "<p>Copies objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be copied based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.move_objects", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.move_objects", "kind": "function", "doc": "<p>Moves objects and 'directories'.</p>\n\n<p>If dry_run is set to True the function will print a dict with all the\npaths that would be moved based on the given keys.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.request_restore", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.request_restore", "kind": "function", "doc": "<p>Request the restore of archived data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.check_restore_status", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.check_restore_status", "kind": "function", "doc": "<p>Check the restore status of archived data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.S3FileManager.request_restore_to_destination_and_wait", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "S3FileManager.request_restore_to_destination_and_wait", "kind": "function", "doc": "<p>Request and wait for the restore to complete, polling the restore status.</p>\n\n<p>After the restore is done, copy the restored files to destination</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.ArchiveFileManager", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "ArchiveFileManager", "kind": "class", "doc": "<p>Set of actions to restore archives.</p>\n"}, {"fullname": "lakehouse_engine.core.s3_file_manager.ArchiveFileManager.check_restore_status", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "ArchiveFileManager.check_restore_status", "kind": "function", "doc": "<p>Check the restore status of archived data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>source_bucket:</strong>  name of bucket to check the restore status.</li>\n<li><strong>source_object:</strong>  object to check the restore status.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dict containing the amount of objects in each status.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">source_bucket</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">source_object</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.ArchiveFileManager.request_restore", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "ArchiveFileManager.request_restore", "kind": "function", "doc": "<p>Request the restore of archived data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>source_bucket:</strong>  name of bucket to perform the restore.</li>\n<li><strong>source_object:</strong>  object to be restored.</li>\n<li><strong>restore_expiration:</strong>  restore expiration in days.</li>\n<li><strong>retrieval_tier:</strong>  type of restore, possible values are:\nBulk, Standard or Expedited.</li>\n<li><strong>dry_run:</strong>  if dry_run is set to True the function will print a dict with\nall the paths that would be deleted based on the given keys.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">source_bucket</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">source_object</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">restore_expiration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_tier</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dry_run</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.s3_file_manager.ArchiveFileManager.request_restore_and_wait", "modulename": "lakehouse_engine.core.s3_file_manager", "qualname": "ArchiveFileManager.request_restore_and_wait", "kind": "function", "doc": "<p>Request and wait for the restore to complete, polling the restore status.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>source_bucket:</strong>  name of bucket to perform the restore.</li>\n<li><strong>source_object:</strong>  object to be restored.</li>\n<li><strong>restore_expiration:</strong>  restore expiration in days.</li>\n<li><strong>retrieval_tier:</strong>  type of restore, possible values are:\nBulk, Standard or Expedited.</li>\n<li><strong>dry_run:</strong>  if dry_run is set to True the function will print a dict with\nall the paths that would be deleted based on the given keys.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">source_bucket</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">source_object</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">restore_expiration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_tier</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dry_run</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager", "modulename": "lakehouse_engine.core.sensor_manager", "kind": "module", "doc": "<p>Module to define Sensor Manager classes.</p>\n"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager", "kind": "class", "doc": "<p>Class to control the Sensor execution.</p>\n"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager.check_if_sensor_has_acquired_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager.check_if_sensor_has_acquired_data", "kind": "function", "doc": "<p>Check if sensor has acquired new data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id.</li>\n<li><strong>control_db_table_name:</strong>  <code>db.table</code> to control sensor runs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>True if acquired new data, otherwise False</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager.update_sensor_status", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager.update_sensor_status", "kind": "function", "doc": "<p>Control sensor execution storing the execution data in a delta table.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_spec:</strong>  sensor spec containing all sensor\ninformation we need to update the control status.</li>\n<li><strong>status:</strong>  status of the sensor.</li>\n<li><strong>upstream_key:</strong>  upstream key (e.g., used to store an attribute\nname from the upstream so that new data can be detected\nautomatically).</li>\n<li><strong>upstream_value:</strong>  upstream value (e.g., used to store the max\nattribute value from the upstream so that new data can be\ndetected automatically).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">SensorSpec</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorControlTableManager.read_sensor_table_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorControlTableManager.read_sensor_table_data", "kind": "function", "doc": "<p>Read data from delta table containing sensor status info.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id. If this parameter is defined search occurs\nonly considering this parameter. Otherwise, it considers sensor\nassets and checkpoint location.</li>\n<li><strong>control_db_table_name:</strong>  db.table to control sensor runs.</li>\n<li><strong>assets:</strong>  list of assets that are fueled by the pipeline\nwhere this sensor is.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>Row containing the data for the provided sensor_id.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">Row</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager", "kind": "class", "doc": "<p>Class to deal with Sensor Upstream data.</p>\n"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.generate_filter_exp_query", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.generate_filter_exp_query", "kind": "function", "doc": "<p>Generates a sensor preprocess query based on timestamp logic.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id.</li>\n<li><strong>filter_exp:</strong>  expression to filter incoming new data.\nYou can use the placeholder <code>?upstream_value</code> so that\nit can be replaced by the upstream_value in the\ncontrol_db_table_name for this specific sensor_id.</li>\n<li><strong>control_db_table_name:</strong>  db.table to retrieve the last status change\ntimestamp. This is only relevant for the jdbc sensor.</li>\n<li><strong>upstream_key:</strong>  the key of custom sensor information\nto control how to identify new data from the\nupstream (e.g., a time column in the upstream).</li>\n<li><strong>upstream_value:</strong>  value for custom sensor\nto identify new data from the upstream\n(e.g., the value of a time present in the upstream)\nIf none we will set the default value.\nNote: This parameter is used just to override the\ndefault value <code>-2147483647</code>.</li>\n<li><strong>upstream_table_name:</strong>  value for custom sensor\nto query new data from the upstream.\nIf none we will set the default value,\nour <code>sensor_new_data</code> view.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>The query string.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">filter_exp</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.generate_sensor_table_preprocess_query", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.generate_sensor_table_preprocess_query", "kind": "function", "doc": "<p>Generates a query to be used for a sensor having other sensor as upstream.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>The query string.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.read_new_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.read_new_data", "kind": "function", "doc": "<p>Read new data from the upstream into the sensor 'new_data_df'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_spec:</strong>  sensor spec containing all sensor information.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>An empty dataframe if it doesn't have new data otherwise the new data</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">SensorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.get_new_data", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.get_new_data", "kind": "function", "doc": "<p>Get new data from upstream df if it's present.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>new_data_df:</strong>  DataFrame possibly containing new data.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>Optional row, present if there is new data in the upstream,\n  absent otherwise.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">new_data_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">Row</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.sensor_manager.SensorUpstreamManager.generate_sensor_sap_logchain_query", "modulename": "lakehouse_engine.core.sensor_manager", "qualname": "SensorUpstreamManager.generate_sensor_sap_logchain_query", "kind": "function", "doc": "<p>Generates a sensor query based in the SAP Logchain table.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>chain_id:</strong>  chain id to query the status on SAP.</li>\n<li><strong>dbtable:</strong>  db.table to retrieve the data to\ncheck if the sap chain is already finished.</li>\n<li><strong>status:</strong>  db.table to retrieve the last status change\ntimestamp.</li>\n<li><strong>engine_table_name:</strong>  table name exposed with the SAP LOGCHAIN data.\nThis table will be used in the jdbc query.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>The query string.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">chain_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA.RSPCLOGCHAIN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;G&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">engine_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sensor_new_data&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager", "modulename": "lakehouse_engine.core.table_manager", "kind": "module", "doc": "<p>Table manager module.</p>\n"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager", "kind": "class", "doc": "<p>Set of actions to manipulate tables/views in several ways.</p>\n"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.__init__", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.__init__", "kind": "function", "doc": "<p>Construct TableManager algorithm instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>configs:</strong>  configurations for the TableManager algorithm.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">configs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span>)</span>"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.get_function", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.get_function", "kind": "function", "doc": "<p>Get a specific function to execute.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.create", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.create", "kind": "function", "doc": "<p>Create a new table or view on metastore.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.create_many", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.create_many", "kind": "function", "doc": "<p>Create multiple tables or views on metastore.</p>\n\n<p>In this function the path to the ddl files can be separated by comma.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.compute_table_statistics", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.compute_table_statistics", "kind": "function", "doc": "<p>Compute table statistics.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.drop_table", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.drop_table", "kind": "function", "doc": "<p>Delete table function deletes table from metastore and erases all data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.drop_view", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.drop_view", "kind": "function", "doc": "<p>Delete view function deletes view from metastore and erases all data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.truncate", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.truncate", "kind": "function", "doc": "<p>Truncate function erases all data but keeps metadata.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.vacuum", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.vacuum", "kind": "function", "doc": "<p>Vacuum function erases older versions from Delta Lake tables or locations.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.describe", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.describe", "kind": "function", "doc": "<p>Describe function describes metadata from some table or view.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.optimize", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.optimize", "kind": "function", "doc": "<p>Optimize function optimizes the layout of Delta Lake data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.execute_multiple_sql_files", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.execute_multiple_sql_files", "kind": "function", "doc": "<p>Execute multiple statements in multiple sql files.</p>\n\n<p>In this function the path to the files is separated by comma.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.execute_sql", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.execute_sql", "kind": "function", "doc": "<p>Execute sql commands separated by semicolon (;).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.show_tbl_properties", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.show_tbl_properties", "kind": "function", "doc": "<p>Show Table Properties.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe with the table properties.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.get_tbl_pk", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.get_tbl_pk", "kind": "function", "doc": "<p>Get the primary key of a particular table.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of columns that are part of the primary key.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.repair_table", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.repair_table", "kind": "function", "doc": "<p>Run the repair table command.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.core.table_manager.TableManager.delete_where", "modulename": "lakehouse_engine.core.table_manager", "qualname": "TableManager.delete_where", "kind": "function", "doc": "<p>Run the delete where command.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors", "modulename": "lakehouse_engine.dq_processors", "kind": "module", "doc": "<p>Package to define data quality processes available in the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.assistant", "modulename": "lakehouse_engine.dq_processors.assistant", "kind": "module", "doc": "<p>Module containing the definition of a data assistant.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.assistant.Assistant", "modulename": "lakehouse_engine.dq_processors.assistant", "qualname": "Assistant", "kind": "class", "doc": "<p>Class containing the data assistant.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.assistant.Assistant.run_data_assistant", "modulename": "lakehouse_engine.dq_processors.assistant", "qualname": "Assistant.run_data_assistant", "kind": "function", "doc": "<p>Entrypoint to run the data assistant.</p>\n\n<p>Based on the data, it uses GX Onboarding Data Assistant to generate expectations\nthat can be applied to the data. Then, it returns the generated expectations\nand, depending on your configuration, it can display plots of the metrics,\nexpectations and also display or store the profiling of the data, for you to get\na better sense of it.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>context:</strong>  the BaseDataContext containing the configurations for the data\nsource and store backend.</li>\n<li><strong>batch_request:</strong>  batch request to be able to query underlying data.</li>\n<li><strong>expectation_suite_name:</strong>  name of the expectation suite.</li>\n<li><strong>assistant_options:</strong>  additional options to pass to the DQ assistant processor.</li>\n<li><strong>data:</strong>  the input dataframe for which the DQ is running.</li>\n<li><strong>profile_file_name:</strong>  file name for storing the profiling html file.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The context with the expectation suite stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">BaseDataContext</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_request</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"o\">.</span><span class=\"n\">RuntimeBatchRequest</span>,</span><span class=\"param\">\t<span class=\"n\">expectation_suite_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">assistant_options</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">profile_file_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations", "modulename": "lakehouse_engine.dq_processors.custom_expectations", "kind": "module", "doc": "<p>Package containing custom DQ expectations available in the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "kind": "module", "doc": "<p>Expectation to check if column 'a' is lower or equal than column 'b'.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b.ColumnPairCustom", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "qualname": "ColumnPairCustom", "kind": "class", "doc": "<p>Asserts that column 'A' is lower or equal than column 'B'.</p>\n\n<p>Additionally, the 'margin' parameter can be used to add a margin to the\ncheck between column 'A' and 'B': 'A' &lt;= 'B' + 'margin'.</p>\n", "bases": "great_expectations.expectations.metrics.map_metric_provider.column_pair_map_metric_provider.ColumnPairMapMetricProvider"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b.ExpectColumnPairAToBeSmallerOrEqualThanB", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_pair_a_to_be_smaller_or_equal_than_b", "qualname": "ExpectColumnPairAToBeSmallerOrEqualThanB", "kind": "class", "doc": "<p>Expect values in column A to be lower or equal than column B.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>column_A:</strong>  The first column name.</li>\n<li><strong>column_B:</strong>  The second column name.</li>\n<li><strong>margin:</strong>  additional approximation to column B value.</li>\n</ul>\n\n<h6 id=\"keyword-args\">Keyword Args:</h6>\n\n<blockquote>\n  <ul>\n  <li>allow_cross_type_comparisons: If True, allow\n  comparisons between types (e.g. integer and string).\n  Otherwise, attempting such comparisons will raise an exception.</li>\n  <li>ignore_row_if: \"both_values_are_missing\",\n  \"either_value_is_missing\", \"neither\" (default).</li>\n  <li>result_format: Which output mode to use:\n  <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</li>\n  <li>include_config: If True (default), then include the expectation config\n  as part of the result object.</li>\n  <li>catch_exceptions: If True, then catch exceptions and\n  include them as part of the result object. Default: False.</li>\n  <li>meta: A JSON-serializable dictionary (nesting allowed)\n  that will be included in the output without modification.</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>An ExpectationSuiteValidationResult.</p>\n</blockquote>\n", "bases": "great_expectations.expectations.expectation.ColumnPairMapExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "kind": "module", "doc": "<p>Expectation to check if column value is a date within a timeframe.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than.ColumnValuesDateNotOlderThan", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "qualname": "ColumnValuesDateNotOlderThan", "kind": "class", "doc": "<p>Asserts that column values are a date that isn't older than a given date.</p>\n", "bases": "great_expectations.expectations.metrics.map_metric_provider.column_map_metric_provider.ColumnMapMetricProvider"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than.ExpectColumnValuesToBeDateNotOlderThan", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_column_values_to_be_date_not_older_than", "qualname": "ExpectColumnValuesToBeDateNotOlderThan", "kind": "class", "doc": "<p>Expect value in column to be date that is not older than a given time.</p>\n\n<p>Since timedelta can only define an interval up to weeks, a month is defined\nas 4 weeks and a year is defined as 52 weeks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>column:</strong>  Name of column to validate</li>\n<li><strong>Note:</strong>  Column must be of type Date, Timestamp or String (with Timestamp format).\nFormat: yyyy-MM-ddTHH:mm:ss</li>\n<li><strong>timeframe:</strong>  dict with the definition of the timeframe.</li>\n<li><strong>kwargs:</strong>  dict with additional parameters.</li>\n</ul>\n\n<h6 id=\"keyword-args\">Keyword Args:</h6>\n\n<blockquote>\n  <ul>\n  <li>allow_cross_type_comparisons: If True, allow\n  comparisons between types (e.g. integer and string).\n  Otherwise, attempting such comparisons will raise an exception.</li>\n  <li>ignore_row_if: \"both_values_are_missing\",\n  \"either_value_is_missing\", \"neither\" (default).</li>\n  <li>result_format: Which output mode to use:\n  <code>BOOLEAN_ONLY</code>, <code>BASIC</code> (default), <code>COMPLETE</code>, or <code>SUMMARY</code>.</li>\n  <li>include_config: If True (default), then include the expectation config\n  as part of the result object.</li>\n  <li>catch_exceptions: If True, then catch exceptions and\n  include them as part of the result object. Default: False.</li>\n  <li>meta: A JSON-serializable dictionary (nesting allowed)\n  that will be included in the output without modification.</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>An ExpectationSuiteValidationResult.</p>\n</blockquote>\n", "bases": "great_expectations.expectations.expectation.ColumnMapExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "kind": "module", "doc": "<p>Expectation to check if column 'a' equals 'b', or 'c'.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c.MulticolumnCustomMetric", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "qualname": "MulticolumnCustomMetric", "kind": "class", "doc": "<p>Expectation metric definition.</p>\n\n<p>This expectation asserts that column 'a' must equal to column 'b' or column 'c'.\nIn addition to this it is possible to validate that column 'b' or 'c' match a regex.</p>\n", "bases": "great_expectations.expectations.metrics.map_metric_provider.multicolumn_map_metric_provider.MulticolumnMapMetricProvider"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c.ExpectMulticolumnColumnAMustEqualBOrC", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_multicolumn_column_a_must_equal_b_or_c", "qualname": "ExpectMulticolumnColumnAMustEqualBOrC", "kind": "class", "doc": "<p>MultiColumn Expectation.</p>\n\n<p>Expect that the column 'a' is equal to 'b' when this is\nnot empty; otherwise 'a' must be equal to 'c'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>column_list:</strong>  The column names to evaluate.</li>\n</ul>\n\n<h6 id=\"keyword-args\">Keyword Args:</h6>\n\n<blockquote>\n  <ul>\n  <li>ignore_row_if: default to \"never\".</li>\n  <li>result_format:  Which output mode to use:\n  <code>BOOLEAN_ONLY</code>, <code>BASIC</code>, <code>COMPLETE</code>, or <code>SUMMARY</code>.\n  Default set to <code>BASIC</code>.</li>\n  <li>include_config: If True, then include the expectation\n  config as part of the result object.\n  Default set to True.</li>\n  <li>catch_exceptions: If True, then catch exceptions\n  and include them as part of the result object.\n  Default set to False.</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>An ExpectationSuiteValidationResult.</p>\n</blockquote>\n", "bases": "great_expectations.expectations.expectation.MulticolumnMapExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "kind": "module", "doc": "<p>Expectation to check if aggregated column satisfy the condition.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be.ExpectQueriedColumnAggValueToBe", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "qualname": "ExpectQueriedColumnAggValueToBe", "kind": "class", "doc": "<p>Expect agg of column to satisfy the condition specified.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>template_dict:</strong>  dict with the following keys:\n<ul>\n<li>column (column to check sum).</li>\n<li>group_column_list (group by column names to be listed).</li>\n<li>condition (how to validate the aggregated value eg: between,\ngreater, lesser).</li>\n<li>max_value (maximum allowed value).</li>\n<li>min_value (minimum allowed value).</li>\n<li>agg_type (sum/count/max/min).</li>\n</ul></li>\n</ul>\n", "bases": "great_expectations.expectations.expectation.QueryExpectation"}, {"fullname": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be.ExpectQueriedColumnAggValueToBe.validate_configuration", "modulename": "lakehouse_engine.dq_processors.custom_expectations.expect_queried_column_agg_value_to_be", "qualname": "ExpectQueriedColumnAggValueToBe.validate_configuration", "kind": "function", "doc": "<p>Validates that a configuration has been set.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>configuration (OPTIONAL[ExpectationConfiguration]):</strong>  An optional Expectation Configuration entry.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None. Raises InvalidExpectationConfigurationError</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">expectation_configuration</span><span class=\"o\">.</span><span class=\"n\">ExpectationConfiguration</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.dq_factory", "modulename": "lakehouse_engine.dq_processors.dq_factory", "kind": "module", "doc": "<p>Module containing the class definition of the Data Quality Factory.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.dq_factory.DQFactory", "modulename": "lakehouse_engine.dq_processors.dq_factory", "qualname": "DQFactory", "kind": "class", "doc": "<p>Class for the Data Quality Factory.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.dq_factory.DQFactory.run_dq_process", "modulename": "lakehouse_engine.dq_processors.dq_factory", "qualname": "DQFactory.run_dq_process", "kind": "function", "doc": "<p>Run the specified data quality process on a dataframe.</p>\n\n<p>Based on the dq_specs we apply the defined expectations on top of the dataframe\nin order to apply the necessary validations and then output the result of\nthe data quality process.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dq_spec:</strong>  data quality specification.</li>\n<li><strong>data:</strong>  input dataframe to run the dq process on.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataFrame containing the results of the DQ process.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">dq_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.exceptions", "modulename": "lakehouse_engine.dq_processors.exceptions", "kind": "module", "doc": "<p>Package defining all the DQ custom exceptions.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.exceptions.DQValidationsFailedException", "modulename": "lakehouse_engine.dq_processors.exceptions", "qualname": "DQValidationsFailedException", "kind": "class", "doc": "<p>Exception for when the data quality validations fail.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.dq_processors.exceptions.DQCheckpointsResultsException", "modulename": "lakehouse_engine.dq_processors.exceptions", "qualname": "DQCheckpointsResultsException", "kind": "class", "doc": "<p>Exception for when the checkpoint results parsing fail.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.dq_processors.validator", "modulename": "lakehouse_engine.dq_processors.validator", "kind": "module", "doc": "<p>Module containing the definition of a data quality validator.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.validator.Validator", "modulename": "lakehouse_engine.dq_processors.validator", "qualname": "Validator", "kind": "class", "doc": "<p>Class containing the data quality validator.</p>\n"}, {"fullname": "lakehouse_engine.dq_processors.validator.Validator.get_dq_validator", "modulename": "lakehouse_engine.dq_processors.validator", "qualname": "Validator.get_dq_validator", "kind": "function", "doc": "<p>Get a validator according to the specification.</p>\n\n<p>We use getattr to dynamically execute any expectation available.\ngetattr(validator, function) is similar to validator.function(). With this\napproach, we can execute any expectation supported.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>context:</strong>  the BaseDataContext containing the configurations for the data\nsource and store backend.</li>\n<li><strong>batch_request:</strong>  run time batch request to be able to query underlying data.</li>\n<li><strong>expectation_suite_name:</strong>  name of the expectation suite.</li>\n<li><strong>dq_functions:</strong>  a list of DQFunctionSpec to consider in the expectation suite.</li>\n<li><strong>critical_functions:</strong>  list of critical expectations in the expectation suite.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The validator with the expectation suite stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">BaseDataContext</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_request</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"o\">.</span><span class=\"n\">RuntimeBatchRequest</span>,</span><span class=\"param\">\t<span class=\"n\">expectation_suite_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dq_functions</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">critical_functions</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQFunctionSpec</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.dq_processors.validator.Validator.tag_source_with_dq", "modulename": "lakehouse_engine.dq_processors.validator", "qualname": "Validator.tag_source_with_dq", "kind": "function", "doc": "<p>Tags the source dataframe with a new column having the DQ results.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>source_pk:</strong>  the primary key of the source data.</li>\n<li><strong>source_df:</strong>  the source dataframe to be tagged with DQ results.</li>\n<li><strong>results_df:</strong>  dq results dataframe.</li>\n</ul>\n\n<p>Returns: a dataframe tagged with the DQ results.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">source_pk</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">source_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">results_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine", "modulename": "lakehouse_engine.engine", "kind": "module", "doc": "<p>Contract of the lakehouse engine with all the available functions to be executed.</p>\n"}, {"fullname": "lakehouse_engine.engine.load_data", "modulename": "lakehouse_engine.engine", "qualname": "load_data", "kind": "function", "doc": "<p>Load data using the DataLoader algorithm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks or other\napps).</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when collecting\nthe engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;prod_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.execute_reconciliation", "modulename": "lakehouse_engine.engine", "qualname": "execute_reconciliation", "kind": "function", "doc": "<p>Execute the Reconciliator algorithm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks or other\napps).</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when collecting\nthe engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;prod_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.execute_dq_validation", "modulename": "lakehouse_engine.engine", "qualname": "execute_dq_validation", "kind": "function", "doc": "<p>Execute the DQValidator algorithm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks or other\napps).</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when collecting\nthe engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;prod_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.manage_table", "modulename": "lakehouse_engine.engine", "qualname": "manage_table", "kind": "function", "doc": "<p>Manipulate tables/views using Table Manager algorithm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks\nor other apps).</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when collecting\nthe engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;prod_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.manage_files", "modulename": "lakehouse_engine.engine", "qualname": "manage_files", "kind": "function", "doc": "<p>Manipulate s3 files using File Manager algorithm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks\nor other apps).</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when collecting\nthe engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;prod_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.execute_sensor", "modulename": "lakehouse_engine.engine", "qualname": "execute_sensor", "kind": "function", "doc": "<p>Execute a sensor based on a Sensor Algorithm Configuration.</p>\n\n<p>A sensor is useful to check if an upstream system has new data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks\nor other apps).</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when collecting\nthe engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;prod_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.update_sensor_status", "modulename": "lakehouse_engine.engine", "qualname": "update_sensor_status", "kind": "function", "doc": "<p>Update internal sensor status.</p>\n\n<p>Update the sensor status in the control table,\nit should be used to tell the system\nthat the sensor has processed all new data that was previously identified,\nhence updating the shifted sensor status.\nUsually used to move from <code>SensorStatus.ACQUIRED_NEW_DATA</code> to\n<code>SensorStatus.PROCESSED_NEW_DATA</code>,\nbut there might be scenarios - still to identify -\nwhere we can update the sensor status from/to different statuses.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id.</li>\n<li><strong>control_db_table_name:</strong>  <code>db.table</code> to store sensor checkpoints.</li>\n<li><strong>status:</strong>  status of the sensor.</li>\n<li><strong>assets:</strong>  a list of assets that are considered as available to\nconsume downstream after this sensor has status\nPROCESSED_NEW_DATA.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;PROCESSED_NEW_DATA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.generate_sensor_query", "modulename": "lakehouse_engine.engine", "qualname": "generate_sensor_query", "kind": "function", "doc": "<p>Generates a preprocess query to be used in a sensor configuration.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id.</li>\n<li><strong>filter_exp:</strong>  expression to filter incoming new data.\nYou can use the placeholder ?default_upstream_key and\n?default_upstream_value, so that it can be replaced by the\nrespective values in the control_db_table_name for this specific\nsensor_id.</li>\n<li><strong>control_db_table_name:</strong>  <code>db.table</code> to retrieve the last status change\ntimestamp. This is only relevant for the jdbc sensor.</li>\n<li><strong>upstream_key:</strong>  the key of custom sensor information to control how to\nidentify new data from the upstream (e.g., a time column in the\nupstream).</li>\n<li><strong>upstream_value:</strong>  the upstream value\nto identify new data from the upstream (e.g., the value of a time\npresent in the upstream).</li>\n<li><strong>upstream_table_name:</strong>  value for custom sensor\nto query new data from the upstream\nIf none we will set the default value,\nour <code>sensor_new_data</code> view.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>The query string.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">filter_exp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upstream_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.generate_sensor_sap_logchain_query", "modulename": "lakehouse_engine.engine", "qualname": "generate_sensor_sap_logchain_query", "kind": "function", "doc": "<p>Generates a sensor query based in the SAP Logchain table.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>chain_id:</strong>  chain id to query the status on SAP.</li>\n<li><strong>dbtable:</strong>  <code>db.table</code> to retrieve the data to\ncheck if the sap chain is already finished.</li>\n<li><strong>status:</strong>  <code>db.table</code> to retrieve the last status change\ntimestamp.</li>\n<li><strong>engine_table_name:</strong>  table name exposed with the SAP LOGCHAIN data.\nThis table will be used in the jdbc query.</li>\n</ul>\n\n<h6 id=\"return\">Return:</h6>\n\n<blockquote>\n  <p>The query string.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">chain_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA.RSPCLOGCHAIN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;G&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">engine_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sensor_new_data&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.engine.send_notification", "modulename": "lakehouse_engine.engine", "qualname": "send_notification", "kind": "function", "doc": "<p>Send a notification using a notifier.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>args:</strong>  arguments for the notifier.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io", "modulename": "lakehouse_engine.io", "kind": "module", "doc": "<p>Input and Output package responsible for the behaviour of reading and writing.</p>\n"}, {"fullname": "lakehouse_engine.io.exceptions", "modulename": "lakehouse_engine.io.exceptions", "kind": "module", "doc": "<p>Package defining all the io custom exceptions.</p>\n"}, {"fullname": "lakehouse_engine.io.exceptions.IncrementalFilterInputNotFoundException", "modulename": "lakehouse_engine.io.exceptions", "qualname": "IncrementalFilterInputNotFoundException", "kind": "class", "doc": "<p>Exception for when the input of an incremental filter is not found.</p>\n\n<p>This may occur when tables are being loaded in incremental way, taking the increment\ndefinition out of a specific table, but the table still does not exist, mainly\nbecause probably it was not loaded for the first time yet.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.io.exceptions.WrongIOFormatException", "modulename": "lakehouse_engine.io.exceptions", "qualname": "WrongIOFormatException", "kind": "class", "doc": "<p>Exception for when a user provides a wrong I/O format.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.io.exceptions.NotSupportedException", "modulename": "lakehouse_engine.io.exceptions", "qualname": "NotSupportedException", "kind": "class", "doc": "<p>Exception for when a user provides a not supported operation.</p>\n", "bases": "builtins.RuntimeError"}, {"fullname": "lakehouse_engine.io.reader", "modulename": "lakehouse_engine.io.reader", "kind": "module", "doc": "<p>Defines abstract reader behaviour.</p>\n"}, {"fullname": "lakehouse_engine.io.reader.Reader", "modulename": "lakehouse_engine.io.reader", "qualname": "Reader", "kind": "class", "doc": "<p>Abstract Reader class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.reader.Reader.__init__", "modulename": "lakehouse_engine.io.reader", "qualname": "Reader.__init__", "kind": "function", "doc": "<p>Construct Reader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification for reading data.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.reader.Reader.read", "modulename": "lakehouse_engine.io.reader", "qualname": "Reader.read", "kind": "function", "doc": "<p>Abstract read method.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe read according to the input specification.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.reader_factory", "modulename": "lakehouse_engine.io.reader_factory", "kind": "module", "doc": "<p>Module for reader factory.</p>\n"}, {"fullname": "lakehouse_engine.io.reader_factory.ReaderFactory", "modulename": "lakehouse_engine.io.reader_factory", "qualname": "ReaderFactory", "kind": "class", "doc": "<p>Class for reader factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.reader_factory.ReaderFactory.get_data", "modulename": "lakehouse_engine.io.reader_factory", "qualname": "ReaderFactory.get_data", "kind": "function", "doc": "<p>Get data according to the input specification following a factory pattern.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  input specification to get the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers", "modulename": "lakehouse_engine.io.readers", "kind": "module", "doc": "<p>Readers package to define reading behaviour.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from dataframes.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader.DataFrameReader", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "qualname": "DataFrameReader", "kind": "class", "doc": "<p>Class to read data from a dataframe.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader.DataFrameReader.__init__", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "qualname": "DataFrameReader.__init__", "kind": "function", "doc": "<p>Construct DataFrameReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.dataframe_reader.DataFrameReader.read", "modulename": "lakehouse_engine.io.readers.dataframe_reader", "qualname": "DataFrameReader.read", "kind": "function", "doc": "<p>Read data from a dataframe.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from a dataframe previously\n  computed.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.file_reader", "modulename": "lakehouse_engine.io.readers.file_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from files.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.file_reader.FileReader", "modulename": "lakehouse_engine.io.readers.file_reader", "qualname": "FileReader", "kind": "class", "doc": "<p>Class to read from files.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.file_reader.FileReader.__init__", "modulename": "lakehouse_engine.io.readers.file_reader", "qualname": "FileReader.__init__", "kind": "function", "doc": "<p>Construct FileReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.file_reader.FileReader.read", "modulename": "lakehouse_engine.io.readers.file_reader", "qualname": "FileReader.read", "kind": "function", "doc": "<p>Read file data.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from the files.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from JDBC sources.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader.JDBCReader", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "qualname": "JDBCReader", "kind": "class", "doc": "<p>Class to read from JDBC source.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader.JDBCReader.__init__", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "qualname": "JDBCReader.__init__", "kind": "function", "doc": "<p>Construct JDBCReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.jdbc_reader.JDBCReader.read", "modulename": "lakehouse_engine.io.readers.jdbc_reader", "qualname": "JDBCReader.read", "kind": "function", "doc": "<p>Read data from JDBC source.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from the JDBC source.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader", "modulename": "lakehouse_engine.io.readers.kafka_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from Kafka.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader.KafkaReader", "modulename": "lakehouse_engine.io.readers.kafka_reader", "qualname": "KafkaReader", "kind": "class", "doc": "<p>Class to read from Kafka.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader.KafkaReader.__init__", "modulename": "lakehouse_engine.io.readers.kafka_reader", "qualname": "KafkaReader.__init__", "kind": "function", "doc": "<p>Construct KafkaReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.kafka_reader.KafkaReader.read", "modulename": "lakehouse_engine.io.readers.kafka_reader", "qualname": "KafkaReader.read", "kind": "function", "doc": "<p>Read Kafka data.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from Kafka.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.query_reader", "modulename": "lakehouse_engine.io.readers.query_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from a query.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.query_reader.QueryReader", "modulename": "lakehouse_engine.io.readers.query_reader", "qualname": "QueryReader", "kind": "class", "doc": "<p>Class to read data from a query.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.query_reader.QueryReader.__init__", "modulename": "lakehouse_engine.io.readers.query_reader", "qualname": "QueryReader.__init__", "kind": "function", "doc": "<p>Construct QueryReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.query_reader.QueryReader.read", "modulename": "lakehouse_engine.io.readers.query_reader", "qualname": "QueryReader.read", "kind": "function", "doc": "<p>Read data from a query.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from the query.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from SAP B4 sources.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader.SAPB4Reader", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "qualname": "SAPB4Reader", "kind": "class", "doc": "<p>Class to read from SAP B4 source.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader.SAPB4Reader.__init__", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "qualname": "SAPB4Reader.__init__", "kind": "function", "doc": "<p>Construct SAPB4Reader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.sap_b4_reader.SAPB4Reader.read", "modulename": "lakehouse_engine.io.readers.sap_b4_reader", "qualname": "SAPB4Reader.read", "kind": "function", "doc": "<p>Read data from SAP B4 source.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from the SAP B4 source.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from SAP BW sources.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader.SAPBWReader", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "qualname": "SAPBWReader", "kind": "class", "doc": "<p>Class to read from SAP BW source.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader.SAPBWReader.__init__", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "qualname": "SAPBWReader.__init__", "kind": "function", "doc": "<p>Construct SAPBWReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.sap_bw_reader.SAPBWReader.read", "modulename": "lakehouse_engine.io.readers.sap_bw_reader", "qualname": "SAPBWReader.read", "kind": "function", "doc": "<p>Read data from SAP BW source.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from the SAP BW source.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader", "modulename": "lakehouse_engine.io.readers.sftp_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from SFTP.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader.SFTPReader", "modulename": "lakehouse_engine.io.readers.sftp_reader", "qualname": "SFTPReader", "kind": "class", "doc": "<p>Class to read from SFTP.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader.SFTPReader.__init__", "modulename": "lakehouse_engine.io.readers.sftp_reader", "qualname": "SFTPReader.__init__", "kind": "function", "doc": "<p>Construct SFTPReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.sftp_reader.SFTPReader.read", "modulename": "lakehouse_engine.io.readers.sftp_reader", "qualname": "SFTPReader.read", "kind": "function", "doc": "<p>Read SFTP data.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from SFTP.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.readers.table_reader", "modulename": "lakehouse_engine.io.readers.table_reader", "kind": "module", "doc": "<p>Module to define behaviour to read from tables.</p>\n"}, {"fullname": "lakehouse_engine.io.readers.table_reader.TableReader", "modulename": "lakehouse_engine.io.readers.table_reader", "qualname": "TableReader", "kind": "class", "doc": "<p>Class to read data from a table.</p>\n", "bases": "lakehouse_engine.io.reader.Reader"}, {"fullname": "lakehouse_engine.io.readers.table_reader.TableReader.__init__", "modulename": "lakehouse_engine.io.readers.table_reader", "qualname": "TableReader.__init__", "kind": "function", "doc": "<p>Construct TableReader instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.readers.table_reader.TableReader.read", "modulename": "lakehouse_engine.io.readers.table_reader", "qualname": "TableReader.read", "kind": "function", "doc": "<p>Read data from a table.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dataframe containing the data from the table.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer", "modulename": "lakehouse_engine.io.writer", "kind": "module", "doc": "<p>Defines abstract writer behaviour.</p>\n"}, {"fullname": "lakehouse_engine.io.writer.Writer", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer", "kind": "class", "doc": "<p>Abstract Writer class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.writer.Writer.__init__", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.__init__", "kind": "function", "doc": "<p>Construct Writer instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification to write data.</li>\n<li><strong>df:</strong>  dataframe to write.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writer.Writer.write", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.write", "kind": "function", "doc": "<p>Abstract write method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.write_transformed_micro_batch", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.write_transformed_micro_batch", "kind": "function", "doc": "<p>Define how to write a streaming micro batch after transforming it.</p>\n\n<p>This function must define an inner function that manipulates a streaming batch,\nand then return that function. Look for concrete implementations of this\nfunction for more clarity.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kwargs:</strong>  any keyword arguments.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the foreachBatch spark write method.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.get_transformed_micro_batch", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.get_transformed_micro_batch", "kind": "function", "doc": "<p>Get the result of the transformations applied to a micro batch dataframe.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification associated with the writer.</li>\n<li><strong>batch_df:</strong>  batch dataframe (given from streaming foreachBatch).</li>\n<li><strong>batch_id:</strong>  if of the batch (given from streaming foreachBatch).</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer\nto be available on micro batch transforms.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The transformed dataframe.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">batch_df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">batch_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.get_streaming_trigger", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.get_streaming_trigger", "kind": "function", "doc": "<p>Define which streaming trigger will be used.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dict containing streaming trigger.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer.Writer.run_micro_batch_dq_process", "modulename": "lakehouse_engine.io.writer", "qualname": "Writer.run_micro_batch_dq_process", "kind": "function", "doc": "<p>Run the data quality process in a streaming micro batch dataframe.</p>\n\n<p>Iterates over the specs and performs the checks or analysis depending on the\ndata quality specification provided in the configuration.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>df:</strong>  the dataframe in which to run the dq process on.</li>\n<li><strong>dq_spec:</strong>  data quality specification.</li>\n</ul>\n\n<p>Returns: the validated dataframe.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">dq_spec</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">DQSpec</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writer_factory", "modulename": "lakehouse_engine.io.writer_factory", "kind": "module", "doc": "<p>Module for writer factory.</p>\n"}, {"fullname": "lakehouse_engine.io.writer_factory.WriterFactory", "modulename": "lakehouse_engine.io.writer_factory", "qualname": "WriterFactory", "kind": "class", "doc": "<p>Class for writer factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.io.writer_factory.WriterFactory.get_writer", "modulename": "lakehouse_engine.io.writer_factory", "qualname": "WriterFactory.get_writer", "kind": "function", "doc": "<p>Get a writer according to the output specification using a factory pattern.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  output specification to write data.</li>\n<li><strong>df:</strong>  dataframe to be written.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Writer: writer that will write the data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">Writer</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers", "modulename": "lakehouse_engine.io.writers", "kind": "module", "doc": "<p>Package containing the writers responsible for writing data.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.console_writer", "modulename": "lakehouse_engine.io.writers.console_writer", "kind": "module", "doc": "<p>Module to define behaviour to write to console.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.console_writer.ConsoleWriter", "modulename": "lakehouse_engine.io.writers.console_writer", "qualname": "ConsoleWriter", "kind": "class", "doc": "<p>Class to write data to console.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.console_writer.ConsoleWriter.__init__", "modulename": "lakehouse_engine.io.writers.console_writer", "qualname": "ConsoleWriter.__init__", "kind": "function", "doc": "<p>Construct ConsoleWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification</li>\n<li><strong>df:</strong>  dataframe to be written.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.console_writer.ConsoleWriter.write", "modulename": "lakehouse_engine.io.writers.console_writer", "qualname": "ConsoleWriter.write", "kind": "function", "doc": "<p>Write data to console.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "kind": "module", "doc": "<p>Module to define behaviour to write to dataframe.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer.DataFrameWriter", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "qualname": "DataFrameWriter", "kind": "class", "doc": "<p>Class to write data to dataframe.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer.DataFrameWriter.__init__", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "qualname": "DataFrameWriter.__init__", "kind": "function", "doc": "<p>Construct DataFrameWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification.</li>\n<li><strong>df:</strong>  dataframe to be written.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.dataframe_writer.DataFrameWriter.write", "modulename": "lakehouse_engine.io.writers.dataframe_writer", "qualname": "DataFrameWriter.write", "kind": "function", "doc": "<p>Write data to dataframe.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">OrderedDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "kind": "module", "doc": "<p>Module to define the behaviour of delta merges.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer.DeltaMergeWriter", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "qualname": "DeltaMergeWriter", "kind": "class", "doc": "<p>Class to merge data using delta lake.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer.DeltaMergeWriter.__init__", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "qualname": "DeltaMergeWriter.__init__", "kind": "function", "doc": "<p>Construct DeltaMergeWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification containing merge options and\nrelevant information.</li>\n<li><strong>df:</strong>  the dataframe containing the new data to be merged.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.delta_merge_writer.DeltaMergeWriter.write", "modulename": "lakehouse_engine.io.writers.delta_merge_writer", "qualname": "DeltaMergeWriter.write", "kind": "function", "doc": "<p>Merge new data with current data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.file_writer", "modulename": "lakehouse_engine.io.writers.file_writer", "kind": "module", "doc": "<p>Module to define behaviour to write to files.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.file_writer.FileWriter", "modulename": "lakehouse_engine.io.writers.file_writer", "qualname": "FileWriter", "kind": "class", "doc": "<p>Class to write data to files.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.file_writer.FileWriter.__init__", "modulename": "lakehouse_engine.io.writers.file_writer", "qualname": "FileWriter.__init__", "kind": "function", "doc": "<p>Construct FileWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification</li>\n<li><strong>df:</strong>  dataframe to be written.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.file_writer.FileWriter.write", "modulename": "lakehouse_engine.io.writers.file_writer", "qualname": "FileWriter.write", "kind": "function", "doc": "<p>Write data to files.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "kind": "module", "doc": "<p>Module that defines the behaviour to write to JDBC targets.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer.JDBCWriter", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "qualname": "JDBCWriter", "kind": "class", "doc": "<p>Class to write to JDBC targets.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer.JDBCWriter.__init__", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "qualname": "JDBCWriter.__init__", "kind": "function", "doc": "<p>Construct JDBCWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification.</li>\n<li><strong>df:</strong>  dataframe to be writen.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.jdbc_writer.JDBCWriter.write", "modulename": "lakehouse_engine.io.writers.jdbc_writer", "qualname": "JDBCWriter.write", "kind": "function", "doc": "<p>Write data into JDBC target.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer", "modulename": "lakehouse_engine.io.writers.kafka_writer", "kind": "module", "doc": "<p>Module that defines the behaviour to write to Kafka.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer.KafkaWriter", "modulename": "lakehouse_engine.io.writers.kafka_writer", "qualname": "KafkaWriter", "kind": "class", "doc": "<p>Class to write to a Kafka target.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer.KafkaWriter.__init__", "modulename": "lakehouse_engine.io.writers.kafka_writer", "qualname": "KafkaWriter.__init__", "kind": "function", "doc": "<p>Construct KafkaWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification.</li>\n<li><strong>df:</strong>  dataframe to be written.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.kafka_writer.KafkaWriter.write", "modulename": "lakehouse_engine.io.writers.kafka_writer", "qualname": "KafkaWriter.write", "kind": "function", "doc": "<p>Write data to Kafka.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.io.writers.table_writer", "modulename": "lakehouse_engine.io.writers.table_writer", "kind": "module", "doc": "<p>Module that defines the behaviour to write to tables.</p>\n"}, {"fullname": "lakehouse_engine.io.writers.table_writer.TableWriter", "modulename": "lakehouse_engine.io.writers.table_writer", "qualname": "TableWriter", "kind": "class", "doc": "<p>Class to write to a table.</p>\n", "bases": "lakehouse_engine.io.writer.Writer"}, {"fullname": "lakehouse_engine.io.writers.table_writer.TableWriter.__init__", "modulename": "lakehouse_engine.io.writers.table_writer", "qualname": "TableWriter.__init__", "kind": "function", "doc": "<p>Construct TableWriter instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_spec:</strong>  output specification.</li>\n<li><strong>df:</strong>  dataframe to be written.</li>\n<li><strong>data:</strong>  list of all dfs generated on previous steps before writer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">OutputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span></span>)</span>"}, {"fullname": "lakehouse_engine.io.writers.table_writer.TableWriter.write", "modulename": "lakehouse_engine.io.writers.table_writer", "qualname": "TableWriter.write", "kind": "function", "doc": "<p>Write data to a table.</p>\n\n<p>After the write operation we repair the table (e.g., update partitions).\nHowever, there's a caveat to this, which is the fact that this repair\noperation is not reachable if we are running long-running streaming mode.\nTherefore, we recommend not using the TableWriter with formats other than\ndelta lake for those scenarios (as delta lake does not need msck repair).\nSo, you can: 1) use delta lake format for the table; 2) use the FileWriter\nand run the repair with a certain frequency in a separate task of your\npipeline.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators", "modulename": "lakehouse_engine.terminators", "kind": "module", "doc": "<p>Package to define algorithm terminators (e.g., vacuum, optimize, compute stats).</p>\n"}, {"fullname": "lakehouse_engine.terminators.cdf_processor", "modulename": "lakehouse_engine.terminators.cdf_processor", "kind": "module", "doc": "<p>Defines change data feed processor behaviour.</p>\n"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor", "kind": "class", "doc": "<p>Change data feed processor class.</p>\n"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor.expose_cdf", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor.expose_cdf", "kind": "function", "doc": "<p>Expose CDF to external location.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  terminator specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor.delete_old_data", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor.delete_old_data", "kind": "function", "doc": "<p>Delete old data from cdf delta table.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  terminator specifications.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.cdf_processor.CDFProcessor.vacuum_cdf_data", "modulename": "lakehouse_engine.terminators.cdf_processor", "qualname": "CDFProcessor.vacuum_cdf_data", "kind": "function", "doc": "<p>Vacuum old data from cdf delta table.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  terminator specifications.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.dataset_optimizer", "modulename": "lakehouse_engine.terminators.dataset_optimizer", "kind": "module", "doc": "<p>Module with dataset optimizer terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.dataset_optimizer.DatasetOptimizer", "modulename": "lakehouse_engine.terminators.dataset_optimizer", "qualname": "DatasetOptimizer", "kind": "class", "doc": "<p>Class with dataset optimizer terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.dataset_optimizer.DatasetOptimizer.optimize_dataset", "modulename": "lakehouse_engine.terminators.dataset_optimizer", "qualname": "DatasetOptimizer.optimize_dataset", "kind": "function", "doc": "<p>Optimize a dataset based on a set of pre-conceived optimizations.</p>\n\n<p>Most of the time the dataset is a table, but it can be a file-based one only.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>db_table:</strong>  <code>database_name.table_name</code>.</li>\n<li><strong>location:</strong>  dataset/table filesystem location.</li>\n<li><strong>compute_table_stats:</strong>  to compute table statistics or not.</li>\n<li><strong>vacuum:</strong>  (delta lake tables only) whether to vacuum the delta lake\ntable or not.</li>\n<li><strong>vacuum_hours:</strong>  (delta lake tables only) number of hours to consider\nin vacuum operation.</li>\n<li><strong>optimize:</strong>  (delta lake tables only) whether to optimize the table or\nnot. Custom optimize parameters can be supplied through ExecEnv (Spark)\nconfigs</li>\n<li><strong>optimize_where:</strong>  expression to use in the optimize function.</li>\n<li><strong>optimize_zorder_col_list:</strong>  (delta lake tables only) list of\ncolumns to consider in the zorder optimization process. Custom optimize\nparameters can be supplied through ExecEnv (Spark) configs.</li>\n<li><strong>debug:</strong>  flag indicating if we are just debugging this for local\ntests and therefore pass through all the exceptions to perform some\nassertions in local tests.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">db_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">compute_table_stats</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">vacuum</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">vacuum_hours</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">720</span>,</span><span class=\"param\">\t<span class=\"n\">optimize</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_where</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optimize_zorder_col_list</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">debug</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier", "modulename": "lakehouse_engine.terminators.notifier", "kind": "module", "doc": "<p>Module with notification terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier", "kind": "class", "doc": "<p>Abstract Notification class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.__init__", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.__init__", "kind": "function", "doc": "<p>Construct Notification instances.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>notification_spec:</strong>  notification specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">notification_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.create_notification", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.create_notification", "kind": "function", "doc": "<p>Abstract create notification method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.send_notification", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.send_notification", "kind": "function", "doc": "<p>Abstract send notification method.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier.Notifier.check_if_notification_is_failure_notification", "modulename": "lakehouse_engine.terminators.notifier", "qualname": "Notifier.check_if_notification_is_failure_notification", "kind": "function", "doc": "<p>Check if given notification is a failure notification.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  spec to validate if it is a failure notification.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A boolean telling if the notification is a failure notification</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier_factory", "modulename": "lakehouse_engine.terminators.notifier_factory", "kind": "module", "doc": "<p>Module for notifier factory.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifier_factory.NotifierFactory", "modulename": "lakehouse_engine.terminators.notifier_factory", "qualname": "NotifierFactory", "kind": "class", "doc": "<p>Class for notification factory.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.terminators.notifier_factory.NotifierFactory.get_notifier", "modulename": "lakehouse_engine.terminators.notifier_factory", "qualname": "NotifierFactory.get_notifier", "kind": "function", "doc": "<p>Get a notifier according to the terminator specs using a factory.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  terminator specification.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Notifier: notifier that will handle notifications.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">terminators</span><span class=\"o\">.</span><span class=\"n\">notifier</span><span class=\"o\">.</span><span class=\"n\">Notifier</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifier_factory.NotifierFactory.generate_failure_notification", "modulename": "lakehouse_engine.terminators.notifier_factory", "qualname": "NotifierFactory.generate_failure_notification", "kind": "function", "doc": "<p>Check if it is necessary to send a failure notification and generate it.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  List of termination specs</li>\n<li><strong>exception:</strong>  Exception that caused the failure.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"nb\">list</span>, </span><span class=\"param\"><span class=\"n\">exception</span><span class=\"p\">:</span> <span class=\"ne\">Exception</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifiers", "modulename": "lakehouse_engine.terminators.notifiers", "kind": "module", "doc": "<p>Notifications module.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "kind": "module", "doc": "<p>Module with email notifier.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier", "kind": "class", "doc": "<p>Base Notification class.</p>\n", "bases": "lakehouse_engine.terminators.notifier.Notifier"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier.__init__", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier.__init__", "kind": "function", "doc": "<p>Construct Email Notification instance.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>notification_spec:</strong>  notification specification.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">notification_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span></span>)</span>"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier.create_notification", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier.create_notification", "kind": "function", "doc": "<p>Creates the notification to be sent.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifiers.email_notifier.EmailNotifier.send_notification", "modulename": "lakehouse_engine.terminators.notifiers.email_notifier", "qualname": "EmailNotifier.send_notification", "kind": "function", "doc": "<p>Sends the notification by using a series of methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.notifiers.notification_templates", "modulename": "lakehouse_engine.terminators.notifiers.notification_templates", "kind": "module", "doc": "<p>Email notification templates.</p>\n"}, {"fullname": "lakehouse_engine.terminators.notifiers.notification_templates.NotificationsTemplates", "modulename": "lakehouse_engine.terminators.notifiers.notification_templates", "qualname": "NotificationsTemplates", "kind": "class", "doc": "<p>Templates for notifications.</p>\n"}, {"fullname": "lakehouse_engine.terminators.sensor_terminator", "modulename": "lakehouse_engine.terminators.sensor_terminator", "kind": "module", "doc": "<p>Module with sensor terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.sensor_terminator.SensorTerminator", "modulename": "lakehouse_engine.terminators.sensor_terminator", "qualname": "SensorTerminator", "kind": "class", "doc": "<p>Sensor Terminator class.</p>\n"}, {"fullname": "lakehouse_engine.terminators.sensor_terminator.SensorTerminator.update_sensor_status", "modulename": "lakehouse_engine.terminators.sensor_terminator", "qualname": "SensorTerminator.update_sensor_status", "kind": "function", "doc": "<p>Update internal sensor status.</p>\n\n<p>Update the sensor status in the control table, it should be used to tell the\nsystem that the sensor has processed all new data that was previously\nidentified, hence updating the shifted sensor status.\nUsually used to move from <code>SensorStatus.ACQUIRED_NEW_DATA</code> to\n<code>SensorStatus.PROCESSED_NEW_DATA</code>, but there might be scenarios - still\nto identify - where we can update the sensor status from/to different statuses.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sensor_id:</strong>  sensor id.</li>\n<li><strong>control_db_table_name:</strong>  <code>db.table</code> to store sensor checkpoints.</li>\n<li><strong>status:</strong>  status of the sensor.</li>\n<li><strong>assets:</strong>  a list of assets that are considered as available to\nconsume downstream after this sensor has status\nPROCESSED_NEW_DATA.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sensor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">control_db_table_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;PROCESSED_NEW_DATA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">assets</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.spark_terminator", "modulename": "lakehouse_engine.terminators.spark_terminator", "kind": "module", "doc": "<p>Module with spark terminator.</p>\n"}, {"fullname": "lakehouse_engine.terminators.spark_terminator.SparkTerminator", "modulename": "lakehouse_engine.terminators.spark_terminator", "qualname": "SparkTerminator", "kind": "class", "doc": "<p>Spark Terminator class.</p>\n"}, {"fullname": "lakehouse_engine.terminators.spark_terminator.SparkTerminator.terminate_spark", "modulename": "lakehouse_engine.terminators.spark_terminator", "qualname": "SparkTerminator.terminate_spark", "kind": "function", "doc": "<p>Terminate spark session.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.terminators.terminator_factory", "modulename": "lakehouse_engine.terminators.terminator_factory", "kind": "module", "doc": "<p>Module with the factory pattern to return terminators.</p>\n"}, {"fullname": "lakehouse_engine.terminators.terminator_factory.TerminatorFactory", "modulename": "lakehouse_engine.terminators.terminator_factory", "qualname": "TerminatorFactory", "kind": "class", "doc": "<p>TerminatorFactory class following the factory pattern.</p>\n"}, {"fullname": "lakehouse_engine.terminators.terminator_factory.TerminatorFactory.execute_terminator", "modulename": "lakehouse_engine.terminators.terminator_factory", "qualname": "TerminatorFactory.execute_terminator", "kind": "function", "doc": "<p>Execute a terminator following the factory pattern.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  terminator specification.</li>\n<li><strong>df:</strong>  dataframe to be used in the terminator. Needed when a\nterminator requires one dataframe as input.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Transformer function to be executed in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TerminatorSpec</span>,</span><span class=\"param\">\t<span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers", "modulename": "lakehouse_engine.transformers", "kind": "module", "doc": "<p>Package to define transformers available in the lakehouse engine.</p>\n"}, {"fullname": "lakehouse_engine.transformers.aggregators", "modulename": "lakehouse_engine.transformers.aggregators", "kind": "module", "doc": "<p>Aggregators module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.aggregators.Aggregators", "modulename": "lakehouse_engine.transformers.aggregators", "qualname": "Aggregators", "kind": "class", "doc": "<p>Class containing all aggregation functions.</p>\n"}, {"fullname": "lakehouse_engine.transformers.aggregators.Aggregators.get_max_value", "modulename": "lakehouse_engine.transformers.aggregators", "qualname": "Aggregators.get_max_value", "kind": "function", "doc": "<p>Get the maximum value of a given column of a dataframe.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_col:</strong>  name of the input column.</li>\n<li><strong>output_col:</strong>  name of the output column (defaults to \"latest\").</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;latest&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_creators", "modulename": "lakehouse_engine.transformers.column_creators", "kind": "module", "doc": "<p>Column creators transformers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators", "kind": "class", "doc": "<p>Class containing all functions that can create columns to add value.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators.with_row_id", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators.with_row_id", "kind": "function", "doc": "<p>Create a sequential but not consecutive id.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_col:</strong>  optional name of the output column.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lhe_row_id&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators.with_auto_increment_id", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators.with_auto_increment_id", "kind": "function", "doc": "<p>Create a sequential and consecutive id.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_col:</strong>  optional name of the output column.</li>\n<li><strong>rdd:</strong>  optional parameter to use spark rdd.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lhe_row_id&#39;</span>, </span><span class=\"param\"><span class=\"n\">rdd</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_creators.ColumnCreators.with_literals", "modulename": "lakehouse_engine.transformers.column_creators", "qualname": "ColumnCreators.with_literals", "kind": "function", "doc": "<p>Create columns given a map of column names and literal values (constants).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>Dict[str, Any] literals:</strong>  map of column names and literal values (constants).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Callable: A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">literals</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers", "modulename": "lakehouse_engine.transformers.column_reshapers", "kind": "module", "doc": "<p>Module with column reshaping transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers", "kind": "class", "doc": "<p>Class containing column reshaping transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.cast", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.cast", "kind": "function", "doc": "<p>Cast specific columns into the designated type.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  dict with columns and respective target types.\nTarget types need to have the exact name of spark types:\n<a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\">https://spark.apache.org/docs/latest/sql-ref-datatypes.html</a></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.column_selector", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.column_selector", "kind": "function", "doc": "<p>Select specific columns with specific output aliases.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  dict with columns to select and respective aliases.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.flatten_schema", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.flatten_schema", "kind": "function", "doc": "<p>Flatten the schema of the dataframe.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>max_level:</strong>  level until which you want to flatten the schema.\nDefault: None.</li>\n<li><strong>shorten_names:</strong>  whether to shorten the names of the prefixes\nof the fields being flattened or not. Default: False.</li>\n<li><strong>alias:</strong>  whether to define alias for the columns being flattened\nor not. Default: True.</li>\n<li><strong>num_chars:</strong>  number of characters to consider when shortening\nthe names of the fields. Default: 7.</li>\n<li><strong>ignore_cols:</strong>  columns which you don't want to flatten.\nDefault: None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">max_level</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">shorten_names</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">alias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">num_chars</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.explode_columns", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.explode_columns", "kind": "function", "doc": "<p>Explode columns with types like ArrayType and MapType.</p>\n\n<p>After it can be applied the flatten_schema transformation,\nif we desired for example to explode the map (as we explode a StructType)\nor to explode a StructType inside the array.\nWe recommend you to specify always the columns desired to explode\nand not explode all columns.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>explode_arrays:</strong>  whether you want to explode array columns (True)\nor not (False). Default: False.</li>\n<li><strong>array_cols_to_explode:</strong>  array columns which you want to explode.\nIf you don't specify it will get all array columns and explode them.\nDefault: None.</li>\n<li><strong>explode_maps:</strong>  whether you want to explode map columns (True)\nor not (False). Default: False.</li>\n<li><strong>map_cols_to_explode:</strong>  map columns which you want to explode.\nIf you don't specify it will get all map columns and explode them.\nDefault: None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">explode_arrays</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">array_cols_to_explode</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">explode_maps</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">map_cols_to_explode</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.with_expressions", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.with_expressions", "kind": "function", "doc": "<p>Execute Spark SQL expressions to create the specified columns.</p>\n\n<p>This function uses the Spark expr function. <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html\">Check here</a>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols_and_exprs:</strong>  dict with columns and respective expressions to compute\n(Spark SQL expressions).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols_and_exprs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.rename", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.rename", "kind": "function", "doc": "<p>Rename specific columns into the designated name.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  dict with columns and respective target names.</li>\n<li><strong>escape_col_names:</strong>  whether to escape column names (e.g. <code>/BIC/COL1</code>) or not.\nIf True it creates a column with the new name and drop the old one.\nIf False, uses the native withColumnRenamed Spark function.\nDefault: True.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">escape_col_names</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.from_avro", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.from_avro", "kind": "function", "doc": "<p>Select all attributes from avro.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>schema:</strong>  the schema string.</li>\n<li><strong>key_col:</strong>  the name of the key column.</li>\n<li><strong>value_col:</strong>  the name of the value column.</li>\n<li><strong>options:</strong>  extra options (e.g., mode: \"PERMISSIVE\").</li>\n<li><strong>expand_key:</strong>  whether you want to expand the content inside the key\ncolumn or not. Default: false.</li>\n<li><strong>expand_value:</strong>  whether you want to expand the content inside the value\ncolumn or not. Default: true.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;key&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">value_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;value&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">expand_key</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">expand_value</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.from_avro_with_registry", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.from_avro_with_registry", "kind": "function", "doc": "<p>Select all attributes from avro using a schema registry.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>schema_registry:</strong>  the url to the schema registry.</li>\n<li><strong>value_schema:</strong>  the name of the value schema entry in the schema registry.</li>\n<li><strong>value_col:</strong>  the name of the value column.</li>\n<li><strong>key_schema:</strong>  the name of the key schema entry in the schema\nregistry. Default: None.</li>\n<li><strong>key_col:</strong>  the name of the key column.</li>\n<li><strong>expand_key:</strong>  whether you want to expand the content inside the key\ncolumn or not. Default: false.</li>\n<li><strong>expand_value:</strong>  whether you want to expand the content inside the value\ncolumn or not. Default: true.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">schema_registry</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">value_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">value_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;value&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">key_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;key&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">expand_key</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">expand_value</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.from_json", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.from_json", "kind": "function", "doc": "<p>Convert a json string into a json column (struct).</p>\n\n<p>The new json column can be added to the existing columns (default) or it can\nreplace all the others, being the only one to output. The new column gets the\nsame name as the original one suffixed with '_json'.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_col:</strong>  dict with columns and respective target names.</li>\n<li><strong>schema_path:</strong>  path to the StructType schema (spark schema).</li>\n<li><strong>schema:</strong>  dict with the StructType schema (spark schema).</li>\n<li><strong>json_options:</strong>  options to parse the json value.</li>\n<li><strong>drop_all_cols:</strong>  whether to drop all the input columns or not.\nDefaults to False.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">schema_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">json_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">drop_all_cols</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.column_reshapers.ColumnReshapers.to_json", "modulename": "lakehouse_engine.transformers.column_reshapers", "qualname": "ColumnReshapers.to_json", "kind": "function", "doc": "<p>Convert dataframe columns into a json value.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>in_cols:</strong>  name(s) of the input column(s).\nExample values:\n\"*\" - all\ncolumns; \"my_col\" - one column named \"my_col\";\n\"my_col1, my_col2\" - two columns.</li>\n<li><strong>out_col:</strong>  name of the output column.</li>\n<li><strong>json_options:</strong>  options to parse the json value.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">in_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">out_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">json_options</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.condensers", "modulename": "lakehouse_engine.transformers.condensers", "kind": "module", "doc": "<p>Condensers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.condensers.Condensers", "modulename": "lakehouse_engine.transformers.condensers", "qualname": "Condensers", "kind": "class", "doc": "<p>Class containing all the functions to condensate data for later merges.</p>\n"}, {"fullname": "lakehouse_engine.transformers.condensers.Condensers.condense_record_mode_cdc", "modulename": "lakehouse_engine.transformers.condensers", "qualname": "Condensers.condense_record_mode_cdc", "kind": "function", "doc": "<p>Condense Change Data Capture (CDC) based on record_mode strategy.</p>\n\n<p>This CDC data is particularly seen in some CDC enabled systems. Other systems\nmay have different CDC strategies.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>business_key:</strong>  The business key (logical primary key) of the data.</li>\n<li><strong>ranking_key_desc:</strong>  In this type of CDC condensation the data needs to be\nin descending order in a certain way, using columns specified in this\nparameter.</li>\n<li><strong>ranking_key_asc:</strong>  In this type of CDC condensation the data needs to be\nin ascending order in a certain way, using columns specified in\nthis parameter.</li>\n<li><strong>record_mode_col:</strong>  Name of the record mode input_col.</li>\n<li><strong>valid_record_modes:</strong>  Depending on the context, not all record modes may be\nconsidered for condensation. Use this parameter to skip those.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">business_key</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">record_mode_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">valid_record_modes</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ranking_key_desc</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ranking_key_asc</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.condensers.Condensers.group_and_rank", "modulename": "lakehouse_engine.transformers.condensers", "qualname": "Condensers.group_and_rank", "kind": "function", "doc": "<p>Condense data based on a simple group by + take latest mechanism.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>group_key:</strong>  list of column names to use in the group by.</li>\n<li><strong>ranking_key:</strong>  the data needs to be in descending order using columns\nspecified in this parameter.</li>\n<li><strong>descending:</strong>  if the ranking considers descending order or not. Defaults to\nTrue.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">group_key</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ranking_key</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">descending</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.custom_transformers", "modulename": "lakehouse_engine.transformers.custom_transformers", "kind": "module", "doc": "<p>Custom transformers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.custom_transformers.CustomTransformers", "modulename": "lakehouse_engine.transformers.custom_transformers", "qualname": "CustomTransformers", "kind": "class", "doc": "<p>Class representing a CustomTransformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.custom_transformers.CustomTransformers.custom_transformation", "modulename": "lakehouse_engine.transformers.custom_transformers", "qualname": "CustomTransformers.custom_transformation", "kind": "function", "doc": "<p>Execute a custom transformation provided by the user.</p>\n\n<p>This transformer can be very useful whenever the user cannot use our provided\ntransformers, or they want to write complex logic in the transform step of the\nalgorithm.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<h6 id=\"attention\">Attention!</h6>\n\n<p>Please bear in mind that the custom_transformer function provided\nas argument needs to receive a DataFrame and return a DataFrame,\nbecause it is how Spark's .transform method is able to chain the\ntransformations.</p>\n\n</div>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">my_custom_logic</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">DataFrame</span><span class=\"p\">:</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>custom_transformer:</strong>  custom transformer function. A python function with all\nrequired pyspark logic provided by the user.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Callable: the same function provided as parameter, in order to e called\n      later in the TransformerFactory.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">custom_transformer</span><span class=\"p\">:</span> <span class=\"n\">Callable</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.data_maskers", "modulename": "lakehouse_engine.transformers.data_maskers", "kind": "module", "doc": "<p>Module with data masking transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.data_maskers.DataMaskers", "modulename": "lakehouse_engine.transformers.data_maskers", "qualname": "DataMaskers", "kind": "class", "doc": "<p>Class containing data masking transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.data_maskers.DataMaskers.hash_masker", "modulename": "lakehouse_engine.transformers.data_maskers", "qualname": "DataMaskers.hash_masker", "kind": "function", "doc": "<p>Mask specific columns using an hashing approach.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  list of column names to mask.</li>\n<li><strong>approach:</strong>  hashing approach. Defaults to 'SHA'. There's \"MURMUR3\" as well.</li>\n<li><strong>num_bits:</strong>  number of bits of the SHA approach. Only applies to SHA approach.</li>\n<li><strong>suffix:</strong>  suffix to apply to new column name. Defaults to \"_hash\".\nNote: you can pass an empty suffix to have the original column replaced.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">approach</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SHA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_bits</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">suffix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;_hash&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.data_maskers.DataMaskers.column_dropper", "modulename": "lakehouse_engine.transformers.data_maskers", "qualname": "DataMaskers.column_dropper", "kind": "function", "doc": "<p>Drop specific columns.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  list of column names to drop.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers", "modulename": "lakehouse_engine.transformers.date_transformers", "kind": "module", "doc": "<p>Module containing date transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers", "kind": "class", "doc": "<p>Class with set of transformers to transform dates in several forms.</p>\n"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.add_current_date", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.add_current_date", "kind": "function", "doc": "<p>Add column with current date.</p>\n\n<p>The current date comes from the driver as a constant, not from every executor.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_col:</strong>  name of the output column.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.convert_to_date", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.convert_to_date", "kind": "function", "doc": "<p>Convert multiple string columns with a source format into dates.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  list of names of the string columns to convert.</li>\n<li><strong>source_format:</strong>  dates source format (e.g., YYYY-MM-dd). <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html\">Check here</a>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">source_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.convert_to_timestamp", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.convert_to_timestamp", "kind": "function", "doc": "<p>Convert multiple string columns with a source format into timestamps.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  list of names of the string columns to convert.</li>\n<li><strong>source_format:</strong>  dates source format (e.g., MM-dd-yyyy HH:mm:ss.SSS).\n<a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html\">Check here</a>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">source_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.format_date", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.format_date", "kind": "function", "doc": "<p>Convert multiple date/timestamp columns into strings with the target format.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  list of names of the string columns to convert.</li>\n<li><strong>target_format:</strong>  strings target format (e.g., YYYY-MM-dd). <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html\">Check here</a>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">target_format</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.date_transformers.DateTransformers.get_date_hierarchy", "modulename": "lakehouse_engine.transformers.date_transformers", "qualname": "DateTransformers.get_date_hierarchy", "kind": "function", "doc": "<p>Create day/month/week/quarter/year hierarchy for the provided date columns.</p>\n\n<p>Uses Spark's extract function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  list of names of the date columns to create the hierarchy.</li>\n<li><strong>formats:</strong>  dict with the correspondence between the hierarchy and the format\nto apply. <a href=\"https://docs.oracle.com/javase/10/docs/api/java/time/format/DateTimeFormatter.html\">Check here</a>.\nExample: {\n    \"year\": \"year\",\n    \"month\": \"month\",\n    \"day\": \"day\",\n    \"week\": \"week\",\n    \"quarter\": \"quarter\"\n}</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">formats</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.exceptions", "modulename": "lakehouse_engine.transformers.exceptions", "kind": "module", "doc": "<p>Module for all the transformers exceptions.</p>\n"}, {"fullname": "lakehouse_engine.transformers.exceptions.WrongArgumentsException", "modulename": "lakehouse_engine.transformers.exceptions", "qualname": "WrongArgumentsException", "kind": "class", "doc": "<p>Exception for when a user provides wrong arguments to a transformer.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.transformers.exceptions.UnsupportedStreamingTransformerException", "modulename": "lakehouse_engine.transformers.exceptions", "qualname": "UnsupportedStreamingTransformerException", "kind": "class", "doc": "<p>Exception for when a user requests a transformer not supported in streaming.</p>\n", "bases": "builtins.Exception"}, {"fullname": "lakehouse_engine.transformers.filters", "modulename": "lakehouse_engine.transformers.filters", "kind": "module", "doc": "<p>Module containing the filters transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.filters.Filters", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters", "kind": "class", "doc": "<p>Class containing the filters transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.incremental_filter", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.incremental_filter", "kind": "function", "doc": "<p>Incrementally Filter a certain dataframe given an increment logic.</p>\n\n<p>This logic can either be an increment value or an increment dataframe from\nwhich the get the latest value from. By default, the operator for the\nfiltering process is greater or equal to cover cases where we receive late\narriving data not cover in a previous load. You can change greater_or_equal\nto false to use greater, when you trust the source will never output more data\nwith the increment after you have load the data (e.g., you will never load\ndata until the source is still dumping data, which may cause you to get an\nincomplete picture of the last arrived data).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_col:</strong>  input column name</li>\n<li><strong>increment_value:</strong>  value to which to filter the data, considering the\nprovided input_Col.</li>\n<li><strong>increment_df:</strong>  a dataframe to get the increment value from.\nyou either specify this or the increment_value (this takes precedence).\nThis is a good approach to get the latest value from a given dataframe\nthat was read and apply that value as filter here. In this way you can\nperform incremental loads based on the last value of a given dataframe\n(e.g., table or file based). Can be used together with the\nget_max_value transformer to accomplish these incremental based loads.\nSee our append load feature tests  to see how to provide an acon for\nincremental loads, taking advantage of the scenario explained here.</li>\n<li><strong>increment_col:</strong>  name of the column from which to get the increment\nvalue from (when using increment_df approach). This assumes there's\nonly one row in the increment_df, reason why is a good idea to use\ntogether with the get_max_value transformer. Defaults to \"latest\"\nbecause that's the default output column name provided by the\nget_max_value transformer.</li>\n<li><strong>greater_or_equal:</strong>  if filtering should be done by also including the\nincrement value or not (useful for scenarios where you are performing\nincrement loads but still want to include data considering the increment\nvalue, and not only values greater than that increment... examples may\ninclude scenarios where you already loaded data including those values,\nbut the source produced more data containing those values).\nDefaults to false.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">increment_value</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">increment_df</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">increment_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;latest&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">greater_or_equal</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.expression_filter", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.expression_filter", "kind": "function", "doc": "<p>Filter a dataframe based on an expression.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>exp:</strong>  filter expression.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">exp</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.column_filter_exp", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.column_filter_exp", "kind": "function", "doc": "<p>Filter a dataframe's columns based on a list of SQL expressions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>exp:</strong>  column filter expressions.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">exp</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.filters.Filters.drop_duplicate_rows", "modulename": "lakehouse_engine.transformers.filters", "qualname": "Filters.drop_duplicate_rows", "kind": "function", "doc": "<p>Drop duplicate rows using spark function dropDuplicates().</p>\n\n<p>This transformer can be used with or without arguments.\nThe provided argument needs to be a list of columns.\nFor example: [\u201cName\u201d,\u201dVAT\u201d] will drop duplicate records within\n\"Name\" and \"VAT\" columns.\nIf the transformer is used without providing any columns list or providing\nan empty list, such as [] the result will be the same as using\nthe distinct() pyspark function. If the watermark dict is present it will\nensure that the drop operation will apply to rows within the watermark timeline\nwindow.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>cols:</strong>  column names.</li>\n<li><strong>watermarker:</strong>  properties to apply watermarker to the transformer.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">watermarker</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.joiners", "modulename": "lakehouse_engine.transformers.joiners", "kind": "module", "doc": "<p>Module with join transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.joiners.Joiners", "modulename": "lakehouse_engine.transformers.joiners", "qualname": "Joiners", "kind": "class", "doc": "<p>Class containing join transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.joiners.Joiners.join", "modulename": "lakehouse_engine.transformers.joiners", "qualname": "Joiners.join", "kind": "function", "doc": "<p>Join two dataframes based on specified type and columns.</p>\n\n<p>Some stream to stream joins are only possible if you apply Watermark, so this\nmethod also provides a parameter to enable watermarking specification.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>left_df_alias:</strong>  alias of the first dataframe.</li>\n<li><strong>join_with:</strong>  right dataframe.</li>\n<li><strong>right_df_alias:</strong>  alias of the second dataframe.</li>\n<li><strong>join_condition:</strong>  condition to join dataframes.</li>\n<li><strong>join_type:</strong>  type of join. Defaults to inner.\nAvailable values: inner, cross, outer, full, full outer,\nleft, left outer, right, right outer, semi,\nleft semi, anti, and left anti.</li>\n<li><strong>broadcast_join:</strong>  whether to perform a broadcast join or not.</li>\n<li><strong>select_cols:</strong>  list of columns to select at the end.</li>\n<li><strong>watermarker:</strong>  properties to apply watermarking.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">join_with</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">join_condition</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">left_df_alias</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;a&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">right_df_alias</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;b&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">join_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;inner&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">broadcast_join</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">select_cols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">watermarker</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.null_handlers", "modulename": "lakehouse_engine.transformers.null_handlers", "kind": "module", "doc": "<p>Module with null handlers transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.null_handlers.NullHandlers", "modulename": "lakehouse_engine.transformers.null_handlers", "qualname": "NullHandlers", "kind": "class", "doc": "<p>Class containing null handler transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.null_handlers.NullHandlers.replace_nulls", "modulename": "lakehouse_engine.transformers.null_handlers", "qualname": "NullHandlers.replace_nulls", "kind": "function", "doc": "<p>Replace nulls in a dataframe.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>replace_on_nums:</strong>  if it is to replace nulls on numeric columns.\nApplies to ints, longs and floats.</li>\n<li><strong>default_num_value:</strong>  default integer value to use as replacement.</li>\n<li><strong>replace_on_strings:</strong>  if it is to replace nulls on string columns.</li>\n<li><strong>default_string_value:</strong>  default string value to use as replacement.</li>\n<li><strong>subset_cols:</strong>  list of columns in which to replace nulls. If not\nprovided, all nulls in all columns will be replaced as specified.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">replace_on_nums</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">default_num_value</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">999</span>,</span><span class=\"param\">\t<span class=\"n\">replace_on_strings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">default_string_value</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;UNKNOWN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">subset_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.optimizers", "modulename": "lakehouse_engine.transformers.optimizers", "kind": "module", "doc": "<p>Optimizers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers", "kind": "class", "doc": "<p>Class containing all the functions that can provide optimizations.</p>\n"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers.cache", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers.cache", "kind": "function", "doc": "<p>Caches the current dataframe.</p>\n\n<p>The default storage level used is MEMORY_AND_DISK.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers.persist", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers.persist", "kind": "function", "doc": "<p>Caches the current dataframe with a specific StorageLevel.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>storage_level:</strong>  the type of StorageLevel, as default MEMORY_AND_DISK_DESER.\n<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html\">More options here</a>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">storage_level</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.optimizers.Optimizers.unpersist", "modulename": "lakehouse_engine.transformers.optimizers", "qualname": "Optimizers.unpersist", "kind": "function", "doc": "<p>Removes the dataframe from the disk and memory.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>blocking:</strong>  whether to block until all the data blocks are\nremoved from disk/memory or run asynchronously.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">blocking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.regex_transformers", "modulename": "lakehouse_engine.transformers.regex_transformers", "kind": "module", "doc": "<p>Regex transformers module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.regex_transformers.RegexTransformers", "modulename": "lakehouse_engine.transformers.regex_transformers", "qualname": "RegexTransformers", "kind": "class", "doc": "<p>Class containing all regex functions.</p>\n"}, {"fullname": "lakehouse_engine.transformers.regex_transformers.RegexTransformers.with_regex_value", "modulename": "lakehouse_engine.transformers.regex_transformers", "qualname": "RegexTransformers.with_regex_value", "kind": "function", "doc": "<p>Get the result of applying a regex to an input column (via regexp_extract).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_col:</strong>  name of the input column.</li>\n<li><strong>output_col:</strong>  name of the output column.</li>\n<li><strong>regex:</strong>  regular expression.</li>\n<li><strong>drop_input_col:</strong>  whether to drop input_col or not.</li>\n<li><strong>idx:</strong>  index to return.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed in the .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">regex</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">drop_input_col</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.repartitioners", "modulename": "lakehouse_engine.transformers.repartitioners", "kind": "module", "doc": "<p>Module with repartitioners transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.repartitioners.Repartitioners", "modulename": "lakehouse_engine.transformers.repartitioners", "qualname": "Repartitioners", "kind": "class", "doc": "<p>Class containing repartitioners transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.repartitioners.Repartitioners.coalesce", "modulename": "lakehouse_engine.transformers.repartitioners", "qualname": "Repartitioners.coalesce", "kind": "function", "doc": "<p>Coalesce a dataframe into n partitions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_partitions:</strong>  num of partitions to coalesce.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.repartitioners.Repartitioners.repartition", "modulename": "lakehouse_engine.transformers.repartitioners", "qualname": "Repartitioners.repartition", "kind": "function", "doc": "<p>Repartition a dataframe into n partitions.</p>\n\n<p>If num_partitions is provided repartitioning happens based on the provided\nnumber, otherwise it happens based on the values of the provided cols (columns).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_partitions:</strong>  num of partitions to repartition.</li>\n<li><strong>cols:</strong>  list of columns to use for repartitioning.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.transformer_factory", "modulename": "lakehouse_engine.transformers.transformer_factory", "kind": "module", "doc": "<p>Module with the factory pattern to return transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.transformer_factory.TransformerFactory", "modulename": "lakehouse_engine.transformers.transformer_factory", "qualname": "TransformerFactory", "kind": "class", "doc": "<p>TransformerFactory class following the factory pattern.</p>\n"}, {"fullname": "lakehouse_engine.transformers.transformer_factory.TransformerFactory.get_transformer", "modulename": "lakehouse_engine.transformers.transformer_factory", "qualname": "TransformerFactory.get_transformer", "kind": "function", "doc": "<p>Get a transformer following the factory pattern.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spec:</strong>  transformer specification (individual transformation... not to be\nconfused with list of all transformations).</li>\n<li><strong>data:</strong>  ordered dict of dataframes to be transformed. Needed when a\ntransformer requires more than one dataframe as input.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Transformer function to be executed in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">TransformerSpec</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">OrderedDict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.unions", "modulename": "lakehouse_engine.transformers.unions", "kind": "module", "doc": "<p>Module with union transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.unions.Unions", "modulename": "lakehouse_engine.transformers.unions", "qualname": "Unions", "kind": "class", "doc": "<p>Class containing union transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.unions.Unions.union", "modulename": "lakehouse_engine.transformers.unions", "qualname": "Unions.union", "kind": "function", "doc": "<p>Union dataframes, resolving columns by position (not by name).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>union_with:</strong>  list of dataframes to union.</li>\n<li><strong>deduplication:</strong>  whether to perform deduplication of elements or not.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">union_with</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">deduplication</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.unions.Unions.union_by_name", "modulename": "lakehouse_engine.transformers.unions", "qualname": "Unions.union_by_name", "kind": "function", "doc": "<p>Union dataframes, resolving columns by name (not by position).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>union_with:</strong>  list of dataframes to union.</li>\n<li><strong>deduplication:</strong>  whether to perform deduplication of elements or not.</li>\n<li><strong>allow_missing_columns:</strong>  allow the union of DataFrames with different\nschemas.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">union_with</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">deduplication</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">allow_missing_columns</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.transformers.watermarker", "modulename": "lakehouse_engine.transformers.watermarker", "kind": "module", "doc": "<p>Watermarker module.</p>\n"}, {"fullname": "lakehouse_engine.transformers.watermarker.Watermarker", "modulename": "lakehouse_engine.transformers.watermarker", "qualname": "Watermarker", "kind": "class", "doc": "<p>Class containing all watermarker transformers.</p>\n"}, {"fullname": "lakehouse_engine.transformers.watermarker.Watermarker.with_watermark", "modulename": "lakehouse_engine.transformers.watermarker", "qualname": "Watermarker.with_watermark", "kind": "function", "doc": "<p>Get the dataframe with watermarker defined.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>watermarker_column:</strong>  name of the input column to be considered for\nthe watermarking. Note: it must be a timestamp.</li>\n<li><strong>watermarker_time:</strong>  time window to define the watermark value.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be executed on other transformers.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">watermarker_column</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">watermarker_time</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils", "modulename": "lakehouse_engine.utils", "kind": "module", "doc": "<p>Utilities package.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs", "modulename": "lakehouse_engine.utils.configs", "kind": "module", "doc": "<p>Config utilities package.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs.config_utils", "modulename": "lakehouse_engine.utils.configs.config_utils", "kind": "module", "doc": "<p>Module to read configurations.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils", "kind": "class", "doc": "<p>Config utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.get_acon", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.get_acon", "kind": "function", "doc": "<p>Get acon based on a filesystem path or on a dict.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon_path:</strong>  path of the acon (algorithm configuration) file.</li>\n<li><strong>acon:</strong>  acon provided directly through python code (e.g., notebooks\nor other apps).</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dict representation of an acon.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">acon_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.get_config", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.get_config", "kind": "function", "doc": "<p>Get the lakehouse engine configuration file.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Configuration dictionary</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">package</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;lakehouse_engine.configs&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.get_engine_version", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.get_engine_version", "kind": "function", "doc": "<p>Get Lakehouse Engine version from the installed packages.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>String of engine version.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.read_json_acon", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.read_json_acon", "kind": "function", "doc": "<p>Read an acon (algorithm configuration) file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  path to the acon file.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The acon file content as a dict.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.read_sql", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.read_sql", "kind": "function", "doc": "<p>Read a DDL file in Spark SQL format from a cloud object storage system.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  path to the SQL file.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Content of the SQL file.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.configs.config_utils.ConfigUtils.remove_sensitive_info", "modulename": "lakehouse_engine.utils.configs.config_utils", "qualname": "ConfigUtils.remove_sensitive_info", "kind": "function", "doc": "<p>Remove sensitive info from a dictionary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dict_to_replace:</strong>  dict where we want to remove sensitive info.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dict without sensitive information.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">dict_to_replace</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.databricks_utils", "modulename": "lakehouse_engine.utils.databricks_utils", "kind": "module", "doc": "<p>Utilities for databricks operations.</p>\n"}, {"fullname": "lakehouse_engine.utils.databricks_utils.DatabricksUtils", "modulename": "lakehouse_engine.utils.databricks_utils", "qualname": "DatabricksUtils", "kind": "class", "doc": "<p>Databricks utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.databricks_utils.DatabricksUtils.get_db_utils", "modulename": "lakehouse_engine.utils.databricks_utils", "qualname": "DatabricksUtils.get_db_utils", "kind": "function", "doc": "<p>Get db utils on databricks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spark:</strong>  spark session.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dbutils from databricks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spark</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">SparkSession</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.databricks_utils.DatabricksUtils.get_databricks_job_information", "modulename": "lakehouse_engine.utils.databricks_utils", "qualname": "DatabricksUtils.get_databricks_job_information", "kind": "function", "doc": "<p>Get notebook context from running acon.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>spark:</strong>  spark session.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dict containing databricks notebook context.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spark</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">SparkSession</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats", "modulename": "lakehouse_engine.utils.engine_usage_stats", "kind": "module", "doc": "<p>Utilities for recording the engine activity.</p>\n"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats.EngineUsageStats", "modulename": "lakehouse_engine.utils.engine_usage_stats", "qualname": "EngineUsageStats", "kind": "class", "doc": "<p>Engine Usage utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats.EngineUsageStats.store_engine_usage", "modulename": "lakehouse_engine.utils.engine_usage_stats", "qualname": "EngineUsageStats.store_engine_usage", "kind": "function", "doc": "<p>Collects and store Lakehouse Engine usage statistics.</p>\n\n<p>These statistics include the acon and other relevant information, such as\nthe lakehouse engine version and the functions/algorithms being used.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>acon:</strong>  acon dictionary file.</li>\n<li><strong>func_name:</strong>  function name that called this log acon.</li>\n<li><strong>collect_engine_usage:</strong>  Lakehouse usage statistics collection strategy.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark confs to be used when\ncollecting the engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">acon</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">func_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">collect_engine_usage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.engine_usage_stats.EngineUsageStats.get_spark_conf_values", "modulename": "lakehouse_engine.utils.engine_usage_stats", "qualname": "EngineUsageStats.get_spark_conf_values", "kind": "function", "doc": "<p>Get information from spark session configurations.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>usage_stats:</strong>  usage_stats dictionary file.</li>\n<li><strong>spark_confs:</strong>  optional dictionary with the spark tags to be used when\ncollecting the engine usage.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">usage_stats</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>, </span><span class=\"param\"><span class=\"n\">spark_confs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.expectations_utils", "modulename": "lakehouse_engine.utils.expectations_utils", "kind": "module", "doc": "<p>Utilities to be used by custom expectations.</p>\n"}, {"fullname": "lakehouse_engine.utils.expectations_utils.validate_result", "modulename": "lakehouse_engine.utils.expectations_utils", "qualname": "validate_result", "kind": "function", "doc": "<p>Validates the test results of the custom expectations.</p>\n\n<p>If you need to make additional validations on your custom expectation\nand/or require additional fields to be returned you can add them before\ncalling this function. The partial_success and partial_result\noptional parameters can be used to pass the result of additional\nvalidations and add more information to the result key of the\nreturned dict respectively.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>expectation:</strong>  Expectation to validate.</li>\n<li><strong>configuration:</strong>  Configuration used in the test.</li>\n<li><strong>metrics:</strong>  Test result metrics.</li>\n<li><strong>partial_success:</strong>  Result of validations done before calling this method.</li>\n<li><strong>partial_result:</strong>  Extra fields to be returned to the user.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The result of the validation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">expectation</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">expectations</span><span class=\"o\">.</span><span class=\"n\">expectation</span><span class=\"o\">.</span><span class=\"n\">Expectation</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"n\">great_expectations</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">expectation_configuration</span><span class=\"o\">.</span><span class=\"n\">ExpectationConfiguration</span>,</span><span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">Dict</span>,</span><span class=\"param\">\t<span class=\"n\">partial_success</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">partial_result</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction", "modulename": "lakehouse_engine.utils.extraction", "kind": "module", "doc": "<p>Extraction utilities package.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "kind": "module", "doc": "<p>Utilities module for JDBC extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionType", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionType", "kind": "class", "doc": "<p>Standardize the types of extractions we can have from a JDBC source.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionType.INIT", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionType.INIT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;JDBCExtractionType.INIT: &#x27;init&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionType.DELTA", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionType.DELTA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;JDBCExtractionType.DELTA: &#x27;delta&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtraction", "kind": "class", "doc": "<p>Configurations available for an Extraction from a JDBC source.</p>\n\n<p>These configurations cover:</p>\n\n<ul>\n<li>user: username to connect to JDBC source.</li>\n<li>password: password to connect to JDBC source (always use secrets,\ndon't use text passwords in your code).</li>\n<li>url: url to connect to JDBC source.</li>\n<li>dbtable: <code>database.table</code> to extract data from.</li>\n<li>calc_upper_bound_schema: custom schema used for the upper bound calculation.</li>\n<li>changelog_table: table of type changelog from which to extract data,\nwhen the extraction type is delta.</li>\n<li>partition_column: column used to split the extraction.</li>\n<li>latest_timestamp_data_location: data location (e.g., s3) containing the data\nto get the latest timestamp already loaded into bronze.</li>\n<li>latest_timestamp_data_format: the format of the dataset in\nlatest_timestamp_data_location. Default: delta.</li>\n<li>extraction_type: type of extraction (delta or init). Default: \"delta\".</li>\n<li>driver: JDBC driver name. Default: \"com.sap.db.jdbc.Driver\".</li>\n<li>num_partitions: number of Spark partitions to split the extraction.</li>\n<li>lower_bound: lower bound to decide the partition stride.</li>\n<li>upper_bound: upper bound to decide the partition stride. If\ncalculate_upper_bound is True, then upperBound will be\nderived by our upper bound optimizer, using the partition column.</li>\n<li>default_upper_bound: the value to use as default upper bound in case\nthe result of the upper bound calculation is None. Default: \"1\".</li>\n<li>fetch_size: how many rows to fetch per round trip. Default: \"100000\".</li>\n<li>compress: enable network compression. Default: True.</li>\n<li>custom_schema: specify custom_schema for particular columns of the\nreturned dataframe in the init/delta extraction of the source table.</li>\n<li>min_timestamp: min timestamp to consider to filter the changelog data.\nDefault: None and automatically derived from the location provided.\nIn case this one is provided it has precedence and the calculation\nis not done.</li>\n<li>max_timestamp: max timestamp to consider to filter the changelog data.\nDefault: None and automatically derived from the table having information\nabout the extraction requests, their timestamps and their status.\nIn case this one is provided it has precedence and the calculation\nis not done.</li>\n<li>generate_predicates: whether to generate predicates automatically or not.\nDefault: False.</li>\n<li>predicates: list containing all values to partition (if generate_predicates\nis used, the manual values provided are ignored). Default: None.</li>\n<li>predicates_add_null: whether to consider null on predicates list.\nDefault: True.</li>\n<li>extraction_timestamp: the timestamp of the extraction. Default: current time\nfollowing the format \"%Y%m%d%H%M%S\".</li>\n<li>max_timestamp_custom_schema: custom schema used on the max_timestamp derivation\nfrom the table holding the extraction requests information.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction.__init__", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtraction.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">user</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">changelog_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partition_column</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">driver</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;com.sap.db.jdbc.Driver&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lower_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upper_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fetch_size</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;100000&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">custom_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;20240403105354&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp_custom_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from particularly relevant JDBC sources.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.__init__", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.__init__", "kind": "function", "doc": "<p>Construct JDBCExtractionUtils.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>jdbc_extraction:</strong>  JDBC Extraction configurations. Can be of type:\nJDBCExtraction, SAPB4Extraction or SAPBWExtraction.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">jdbc_extraction</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_additional_spark_options", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_additional_spark_options", "kind": "function", "doc": "<p>Helper to get additional Spark Options initially passed.</p>\n\n<p>If people provide additional Spark options, not covered by the util function\narguments (get_spark_jdbc_options), we need to consider them.\nThus, we update the options retrieved by the utils, by checking if there is\nany Spark option initially provided that is not yet considered in the retrieved\noptions or function arguments and if the value for the key is not None.\nIf these conditions are filled, we add the options and return the complete dict.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  the input specification.</li>\n<li><strong>options:</strong>  dict with Spark options.</li>\n<li><strong>ignore_options:</strong>  list of options to be ignored by the process.\nSpark read has two different approaches to parallelize\nreading process, one of them is using upper/lower bound,\nanother one is using predicates, those process can't be\nexecuted at the same time, you must choose one of them.\nBy choosing predicates you can't pass lower and upper bound,\nalso can't pass number of partitions and partition column\notherwise spark will interpret the execution partitioned by\nupper and lower bound and will expect to fill all variables.\nTo avoid fill all predicates hardcoded at the acon, there is\na feature that automatically generates all predicates for init\nor delta load based on input partition column, but at the end\nof the process, partition column can't be passed to the options,\nbecause we are choosing predicates execution, that is why to\ngenerate predicates we need to pass some options to ignore.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a dict with all the options passed as argument, plus the options that\n  were initially provided, but were not used in the util\n  (get_spark_jdbc_options).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span>,</span><span class=\"param\">\t<span class=\"n\">options</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_options</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_predicates", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_predicates", "kind": "function", "doc": "<p>Get the predicates list, based on a predicates query.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predicates_query:</strong>  query to use as the basis to get the distinct values for\na specified column, based on which predicates are generated.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>List containing the predicates to use to split the extraction from\n  JDBC sources.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">predicates_query</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_spark_jdbc_options", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_spark_jdbc_options", "kind": "function", "doc": "<p>Get the Spark options to extract data from a JDBC source.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The Spark jdbc args dictionary, including the query to submit\n  and also options args dictionary.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils.get_spark_jdbc_optimal_upper_bound", "modulename": "lakehouse_engine.utils.extraction.jdbc_extraction_utils", "qualname": "JDBCExtractionUtils.get_spark_jdbc_optimal_upper_bound", "kind": "function", "doc": "<p>Get an optimal upperBound to properly split a Spark JDBC extraction.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Either an int, date or timestamp to serve as upperBound Spark JDBC option.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "kind": "module", "doc": "<p>Utilities module for SAP B4 extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes", "kind": "class", "doc": "<p>Standardise the types of ADSOs we can have for Extractions from SAP B4.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes.AQ", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes.AQ", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&lt;ADSOTypes.AQ: &#x27;AQ&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes.CL", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes.CL", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&lt;ADSOTypes.CL: &#x27;CL&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.ADSOTypes.SUPPORTED_TYPES", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "ADSOTypes.SUPPORTED_TYPES", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list", "default_value": "&lt;ADSOTypes.SUPPORTED_TYPES: [&#x27;AQ&#x27;, &#x27;CL&#x27;]&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4Extraction", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4Extraction", "kind": "class", "doc": "<p>Configurations available for an Extraction from SAP B4.</p>\n\n<p>It inherits from JDBCExtraction configurations, so it can use\nand/or overwrite those configurations.</p>\n\n<p>These configurations cover:</p>\n\n<ul>\n<li>latest_timestamp_input_col: the column containing the request timestamps\nin the dataset in latest_timestamp_data_location. Default: REQTSN.</li>\n<li>request_status_tbl: the name of the SAP B4 table having information\nabout the extraction requests. Composed of database.table.\nDefault: SAPHANADB.RSPMREQUEST.</li>\n<li>request_col_name: name of the column having the request timestamp to join\nwith the request status table. Default: REQUEST_TSN.</li>\n<li>data_target: the data target to extract from. User in the join operation with\nthe request status table.</li>\n<li>act_req_join_condition: the join condition into activation table\ncan be changed using this property.\nDefault: 'tbl.reqtsn = req.request_col_name'.</li>\n<li>include_changelog_tech_cols: whether to include the technical columns\n(usually coming from the changelog) table or not.</li>\n<li>extra_cols_req_status_tbl: columns to be added from request status table.\nIt needs to contain the prefix \"req.\". E.g. \"req.col1 as column_one,\nreq.col2 as column_two\".</li>\n<li>request_status_tbl_filter: filter to use for filtering the request status table,\ninfluencing the calculation of the max timestamps and the delta extractions.</li>\n<li>adso_type: the type of ADSO that you are extracting from. Can be \"AQ\" or \"CL\".</li>\n<li>max_timestamp_custom_schema: the custom schema to apply on the calculation of\nthe max timestamp to consider for the delta extractions.\nDefault: timestamp DECIMAL(23,0).</li>\n<li>default_max_timestamp: the timestamp to use as default, when it is not possible\nto derive one.</li>\n<li>custom_schema: specify custom_schema for particular columns of the\nreturned dataframe in the init/delta extraction of the source table.</li>\n</ul>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4Extraction.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4Extraction.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">user</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">changelog_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partition_column</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">driver</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;com.sap.db.jdbc.Driver&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lower_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upper_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fetch_size</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;100000&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">custom_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;REQTSN DECIMAL(23,0)&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">min_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;20240403105354&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp_custom_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;timestamp DECIMAL(23,0)&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;REQTSN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">request_status_tbl</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPHANADB.RSPMREQUEST&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">request_col_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;REQUEST_TSN&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_target</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">act_req_join_condition</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">include_changelog_tech_cols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">extra_cols_req_status_tbl</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">request_status_tbl_filter</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">adso_type</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_max_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1970000000000000000000&#39;</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4ExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from SAP B4.</p>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4ExtractionUtils.__init__", "kind": "function", "doc": "<p>Construct SAPB4ExtractionUtils.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sap_b4_extraction:</strong>  SAP B4 Extraction configurations.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sap_b4_extraction</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">extraction</span><span class=\"o\">.</span><span class=\"n\">sap_b4_extraction_utils</span><span class=\"o\">.</span><span class=\"n\">SAPB4Extraction</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils.SAPB4ExtractionUtils.get_data_target", "modulename": "lakehouse_engine.utils.extraction.sap_b4_extraction_utils", "qualname": "SAPB4ExtractionUtils.get_data_target", "kind": "function", "doc": "<p>Get the data_target from the data_target option or derive it.</p>\n\n<p>By definition data_target is the same for the table and changelog table and\nis the same string ignoring everything before / and the first and last\ncharacter after /. E.g. for a dbtable /BIC/abtable12, the data_target\nwould be btable1.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec_opt:</strong>  options from the input_spec.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A string with the data_target.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec_opt</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "kind": "module", "doc": "<p>Utilities module for SAP BW extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtraction", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtraction", "kind": "class", "doc": "<p>Configurations available for an Extraction from SAP BW.</p>\n\n<p>It inherits from SAPBWExtraction configurations, so it can use\nand/or overwrite those configurations.</p>\n\n<p>These configurations cover:</p>\n\n<ul>\n<li>latest_timestamp_input_col: the column containing the actrequest timestamp\nin the dataset in latest_timestamp_data_location. Default:\n\"actrequest_timestamp\".</li>\n<li>act_request_table: the name of the SAP BW activation requests table.\nComposed of database.table. Default: SAPPHA.RSODSACTREQ.</li>\n<li>request_col_name: name of the column having the request to join\nwith the activation request table. Default: actrequest.</li>\n<li>act_req_join_condition: the join condition into activation table\ncan be changed using this property.\nDefault: 'changelog_tbl.request = act_req.request_col_name'.</li>\n<li>odsobject: name of BW Object, used for joining with the activation request\ntable to get the max actrequest_timestamp to consider while filtering\nthe changelog table.</li>\n<li>include_changelog_tech_cols: whether to include the technical columns\n(usually coming from the changelog) table or not. Default: True.</li>\n<li>extra_cols_act_request: list of columns to be added from act request table.\nIt needs to contain the prefix \"act_req.\". E.g. \"act_req.col1\nas column_one, act_req.col2 as column_two\".</li>\n<li>get_timestamp_from_act_request: whether to get init timestamp\nfrom act request table or assume current/given timestamp.</li>\n<li>sap_bw_schema: sap bw schema. Default: SAPPHA.</li>\n<li>max_timestamp_custom_schema: the custom schema to apply on the calculation of\nthe max timestamp to consider for the delta extractions.\nDefault: timestamp DECIMAL(23,0).</li>\n<li>default_max_timestamp: the timestamp to use as default, when it is not possible\nto derive one.</li>\n</ul>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtraction"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtraction.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtraction.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">user</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dbtable</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">calc_upper_bound_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">changelog_table</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">partition_column</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_data_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;delta&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">driver</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;com.sap.db.jdbc.Driver&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_partitions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lower_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">upper_bound</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">default_upper_bound</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fetch_size</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;100000&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">custom_schema</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generate_predicates</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">predicates</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predicates_add_null</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extraction_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;20240403105354&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">max_timestamp_custom_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;timestamp DECIMAL(15,0)&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">latest_timestamp_input_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;actrequest_timestamp&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">act_request_table</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA.RSODSACTREQ&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">request_col_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;actrequest&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">act_req_join_condition</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">odsobject</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">include_changelog_tech_cols</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">extra_cols_act_request</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">get_timestamp_from_act_request</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">sap_bw_schema</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;SAPPHA&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">default_max_timestamp</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;197000000000000&#39;</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from particularly relevant JDBC sources.</p>\n", "bases": "lakehouse_engine.utils.extraction.jdbc_extraction_utils.JDBCExtractionUtils"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.__init__", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils.__init__", "kind": "function", "doc": "<p>Construct SAPBWExtractionUtils.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sap_bw_extraction:</strong>  SAP BW Extraction configurations.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sap_bw_extraction</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">extraction</span><span class=\"o\">.</span><span class=\"n\">sap_bw_extraction_utils</span><span class=\"o\">.</span><span class=\"n\">SAPBWExtraction</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.get_changelog_table", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils.get_changelog_table", "kind": "function", "doc": "<p>Get the changelog table, given an odsobject.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>String to use as changelog_table.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils.SAPBWExtractionUtils.get_odsobject", "modulename": "lakehouse_engine.utils.extraction.sap_bw_extraction_utils", "qualname": "SAPBWExtractionUtils.get_odsobject", "kind": "function", "doc": "<p>Get the odsobject based on the provided options.</p>\n\n<p>With the table name we may also get the db name, so we need to split.\nMoreover, there might be the need for people to specify odsobject if\nit is different from the dbtable.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec_opt:</strong>  options from the input_spec.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A string with the odsobject.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_spec_opt</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "kind": "module", "doc": "<p>Utilities module for SFTP extraction processes.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat", "kind": "class", "doc": "<p>Formats of algorithm input.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.CSV", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.CSV", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.CSV: &#x27;csv&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.FWF", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.FWF", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.FWF: &#x27;fwf&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.JSON", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.JSON: &#x27;json&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPInputFormat.XML", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPInputFormat.XML", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPInputFormat.XML: &#x27;xml&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter", "kind": "class", "doc": "<p>Standardize the types of filters we can have from a SFTP source.</p>\n", "bases": "enum.Enum"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.file_name_contains", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.file_name_contains", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.file_name_contains: &#x27;file_name_contains&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.LATEST_FILE", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.LATEST_FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.LATEST_FILE: &#x27;latest_file&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.EARLIEST_FILE", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.EARLIEST_FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.EARLIEST_FILE: &#x27;earliest_file&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.GREATER_THAN", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.GREATER_THAN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.GREATER_THAN: &#x27;date_time_gt&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionFilter.LOWER_THAN", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionFilter.LOWER_THAN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;SFTPExtractionFilter.LOWER_THAN: &#x27;date_time_lt&#x27;&gt;"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils", "kind": "class", "doc": "<p>Utils for managing data extraction from particularly relevant SFTP sources.</p>\n"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.get_files_list", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.get_files_list", "kind": "function", "doc": "<p>Get a list of files to be extracted from SFTP.</p>\n\n<p>The arguments (options_args) to list files are:</p>\n\n<ul>\n<li>date_time_gt(str):\nFilter the files greater than the string datetime\nformatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".</li>\n<li>date_time_lt(str):\nFilter the files lower than the string datetime\nformatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\".</li>\n<li>earliest_file(bool):\nFilter the earliest dated file in the directory.</li>\n<li>file_name_contains(str):\nFilter files when match the pattern.</li>\n<li>latest_file(bool):\nFilter the most recent dated file in the directory.</li>\n<li>sub_dir(bool):\nWhen true, the engine will search files into subdirectories\nof the remote_path.\nIt will consider one level below the remote_path.\nWhen sub_dir is used with latest_file/earliest_file argument,\nthe engine will retrieve the latest_file/earliest_file\nfor each subdirectory.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sftp:</strong>  the SFTP client object.</li>\n<li><strong>remote_path:</strong>  path of files to be filtered.</li>\n<li><strong>options_args:</strong>  options from the acon.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A list containing the file names to be passed to Spark.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">sftp</span><span class=\"p\">:</span> <span class=\"n\">paramiko</span><span class=\"o\">.</span><span class=\"n\">sftp_client</span><span class=\"o\">.</span><span class=\"n\">SFTPClient</span>,</span><span class=\"param\">\t<span class=\"n\">remote_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">options_args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Set</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.get_sftp_client", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.get_sftp_client", "kind": "function", "doc": "<p>Get the SFTP client.</p>\n\n<p>The SFTP client is used to open an SFTP session across an open\nSSH Transport and perform remote file operations.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><p><strong>options_args:</strong>  dictionary containing SFTP connection parameters.\nThe Paramiko arguments expected to connect are:</p>\n\n<ul>\n<li>\"hostname\": the server to connect to.</li>\n<li>\"port\": the server port to connect to.</li>\n<li>\"username\": the username to authenticate as.</li>\n<li>\"password\": used for password authentication.</li>\n<li>\"pkey\": optional - an optional public key to use for\nauthentication.</li>\n<li>\"passphrase\" \u2013 optional - options used for decrypting private\nkeys.</li>\n<li>\"key_filename\" \u2013 optional - the filename, or list of filenames,\nof optional private key(s) and/or certs to try for\nauthentication.</li>\n<li>\"timeout\" \u2013 an optional timeout (in seconds) for the TCP connect.</li>\n<li>\"allow_agent\" \u2013 optional - set to False to disable\nconnecting to the SSH agent.</li>\n<li>\"look_for_keys\" \u2013 optional - set to False to disable searching\nfor discoverable private key files in ~/.ssh/.</li>\n<li>\"compress\" \u2013 optional - set to True to turn on compression.</li>\n<li>\"sock\" - optional - an open socket or socket-like object\nto use for communication to the target host.</li>\n<li>\"gss_auth\" \u2013 optional - True if you want to use GSS-API\nauthentication.</li>\n<li>\"gss_kex\" \u2013 optional - Perform GSS-API Key Exchange and\nuser authentication.</li>\n<li>\"gss_deleg_creds\" \u2013 optional - Delegate GSS-API client\ncredentials or not.</li>\n<li>\"gss_host\" \u2013 optional - The targets name in the kerberos database.</li>\n<li>\"gss_trust_dns\" \u2013 optional - Indicates whether or\nnot the DNS is trusted to securely canonicalize the name of the\nhost being connected to (default True).</li>\n<li>\"banner_timeout\" \u2013 an optional timeout (in seconds)\nto wait for the SSH banner to be presented.</li>\n<li>\"auth_timeout\" \u2013 an optional timeout (in seconds)\nto wait for an authentication response.</li>\n<li>\"disabled_algorithms\" \u2013 an optional dict passed directly to\nTransport and its keyword argument of the same name.</li>\n<li>\"transport_factory\" \u2013 an optional callable which is handed a\nsubset of the constructor arguments (primarily those related\nto the socket, GSS functionality, and algorithm selection)\nand generates a Transport instance to be used by this client.\nDefaults to Transport.__init__.</li>\n</ul>\n\n<p>The parameter to specify the private key is expected to be in\nRSA format. Attempting a connection with a blank host key is\nnot allowed unless the argument \"add_auto_policy\" is explicitly\nset to True.</p></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>sftp -> a new SFTPClient session object.\n  transport -> the Transport for this connection.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">options_args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">paramiko</span><span class=\"o\">.</span><span class=\"n\">sftp_client</span><span class=\"o\">.</span><span class=\"n\">SFTPClient</span><span class=\"p\">,</span> <span class=\"n\">paramiko</span><span class=\"o\">.</span><span class=\"n\">transport</span><span class=\"o\">.</span><span class=\"n\">Transport</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.validate_format", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.validate_format", "kind": "function", "doc": "<p>Validate the file extension based on the format definitions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>files_format:</strong>  a string containing the file extension.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The string validated and formatted.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">files_format</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.extraction.sftp_extraction_utils.SFTPExtractionUtils.validate_location", "modulename": "lakehouse_engine.utils.extraction.sftp_extraction_utils", "qualname": "SFTPExtractionUtils.validate_location", "kind": "function", "doc": "<p>Validate the location. Add \"/\" in the case it does not exist.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>location:</strong>  file path.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The location validated.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.file_utils", "modulename": "lakehouse_engine.utils.file_utils", "kind": "module", "doc": "<p>Utilities for file name based operations.</p>\n"}, {"fullname": "lakehouse_engine.utils.file_utils.get_file_names_without_file_type", "modulename": "lakehouse_engine.utils.file_utils", "qualname": "get_file_names_without_file_type", "kind": "function", "doc": "<p>Function to retrieve list of file names in a folder.</p>\n\n<p>This function filters by file type and removes the extension of the file name\nit returns.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  path to the folder to list files</li>\n<li><strong>file_type:</strong>  type of the file to include in list</li>\n<li><strong>exclude_regex:</strong>  regex of file names to exclude</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A list of file names without file type.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">file_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">exclude_regex</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.file_utils.get_directory_path", "modulename": "lakehouse_engine.utils.file_utils", "qualname": "get_directory_path", "kind": "function", "doc": "<p>Add '/' to the end of the path of a directory.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  directory to be processed</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Directory path stripped and with '/' at the end.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.logging_handler", "modulename": "lakehouse_engine.utils.logging_handler", "kind": "module", "doc": "<p>Module to configure project logging.</p>\n"}, {"fullname": "lakehouse_engine.utils.logging_handler.FilterSensitiveData", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "FilterSensitiveData", "kind": "class", "doc": "<p>Logging filter to hide sensitive data from being shown in the logs.</p>\n", "bases": "logging.Filter"}, {"fullname": "lakehouse_engine.utils.logging_handler.FilterSensitiveData.filter", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "FilterSensitiveData.filter", "kind": "function", "doc": "<p>Hide sensitive information from being shown in the logs.</p>\n\n<p>Based on the configured regex and replace strings, the content of the log\nrecords is replaced and then all the records are allowed to be logged\n(return True).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>record:</strong>  the LogRecord event being logged.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The transformed record to be logged.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">record</span><span class=\"p\">:</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">LogRecord</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.logging_handler.LoggingHandler", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "LoggingHandler", "kind": "class", "doc": "<p>Handle the logging of the lakehouse engine project.</p>\n"}, {"fullname": "lakehouse_engine.utils.logging_handler.LoggingHandler.__init__", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "LoggingHandler.__init__", "kind": "function", "doc": "<p>Construct a LoggingHandler instance.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>class_name:</strong>  name of the class to be indicated in the logs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "lakehouse_engine.utils.logging_handler.LoggingHandler.get_logger", "modulename": "lakehouse_engine.utils.logging_handler", "qualname": "LoggingHandler.get_logger", "kind": "function", "doc": "<p>Get the _logger instance variable.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the logger object.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Logger</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils", "modulename": "lakehouse_engine.utils.schema_utils", "kind": "module", "doc": "<p>Utilities to facilitate dataframe schema management.</p>\n"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils", "kind": "class", "doc": "<p>Schema utils that help retrieve and manage schemas of dataframes.</p>\n"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_file", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_file", "kind": "function", "doc": "<p>Get a spark schema from a file (spark StructType json file) in a file system.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>file_path:</strong>  path of the file in a file system. <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html\">Check here</a>.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Spark schema struct type.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_file_to_dict", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_file_to_dict", "kind": "function", "doc": "<p>Get a dict with the spark schema from a file in a file system.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>file_path:</strong>  path of the file in a file system. <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html\">Check here</a>.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Spark schema in a dict.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_dict", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_dict", "kind": "function", "doc": "<p>Get a spark schema from a dict.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>struct_type:</strong>  dict containing a spark schema structure. <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html\">Check here</a>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Spark schema struct type.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">struct_type</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_table_schema", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_table_schema", "kind": "function", "doc": "<p>Get a spark schema from a table.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>table:</strong>  table name from which to inherit the schema.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Spark schema struct type.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">table</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.from_input_spec", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.from_input_spec", "kind": "function", "doc": "<p>Get a spark schema from an input specification.</p>\n\n<p>This covers scenarios where the schema is provided as part of the input\nspecification of the algorithm. Schema can come from the table specified in the\ninput specification (enforce_schema_from_table) or by the dict with the spark\nschema provided there also.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_spec:</strong>  input specification.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>spark schema struct type.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">input_spec</span><span class=\"p\">:</span> <span class=\"n\">lakehouse_engine</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">definitions</span><span class=\"o\">.</span><span class=\"n\">InputSpec</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.schema_utils.SchemaUtils.schema_flattener", "modulename": "lakehouse_engine.utils.schema_utils", "qualname": "SchemaUtils.schema_flattener", "kind": "function", "doc": "<p>Recursive method to flatten the schema of the dataframe.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>schema:</strong>  schema to be flattened.</li>\n<li><strong>prefix:</strong>  prefix of the struct to get the value for. Only relevant\nfor being used in the internal recursive logic.</li>\n<li><strong>level:</strong>  level of the depth in the schema being flattened. Only relevant\nfor being used in the internal recursive logic.</li>\n<li><strong>max_level:</strong>  level until which you want to flatten the schema. Default: None.</li>\n<li><strong>shorten_names:</strong>  whether to shorten the names of the prefixes of the fields\nbeing flattened or not. Default: False.</li>\n<li><strong>alias:</strong>  whether to define alias for the columns being flattened or\nnot. Default: True.</li>\n<li><strong>num_chars:</strong>  number of characters to consider when shortening the names of\nthe fields. Default: 7.</li>\n<li><strong>ignore_cols:</strong>  columns which you don't want to flatten. Default: None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A function to be called in .transform() spark function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">schema</span><span class=\"p\">:</span> <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">StructType</span>,</span><span class=\"param\">\t<span class=\"n\">prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">level</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">max_level</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">shorten_names</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">alias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">num_chars</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_cols</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.sql_parser_utils", "modulename": "lakehouse_engine.utils.sql_parser_utils", "kind": "module", "doc": "<p>Module to parse sql files.</p>\n"}, {"fullname": "lakehouse_engine.utils.sql_parser_utils.SQLParserUtils", "modulename": "lakehouse_engine.utils.sql_parser_utils", "qualname": "SQLParserUtils", "kind": "class", "doc": "<p>Parser utilities class.</p>\n"}, {"fullname": "lakehouse_engine.utils.sql_parser_utils.SQLParserUtils.split_sql_commands", "modulename": "lakehouse_engine.utils.sql_parser_utils", "qualname": "SQLParserUtils.split_sql_commands", "kind": "function", "doc": "<p>Read the sql commands of a file to choose how to split them.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sql_commands:</strong>  commands to be split.</li>\n<li><strong>delimiter:</strong>  delimiter to split the sql commands.</li>\n<li><strong>advanced_parser:</strong>  boolean to define if we need to use a complex split.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>List with the sql commands.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">sql_commands</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">delimiter</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">advanced_parser</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage", "modulename": "lakehouse_engine.utils.storage", "kind": "module", "doc": "<p>Utilities to interact with storage systems.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.dbfs_storage", "modulename": "lakehouse_engine.utils.storage.dbfs_storage", "kind": "module", "doc": "<p>Module to represent a DBFS file storage system.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.dbfs_storage.DBFSStorage", "modulename": "lakehouse_engine.utils.storage.dbfs_storage", "qualname": "DBFSStorage", "kind": "class", "doc": "<p>Class to represent a DBFS file storage system.</p>\n", "bases": "lakehouse_engine.utils.storage.file_storage.FileStorage"}, {"fullname": "lakehouse_engine.utils.storage.dbfs_storage.DBFSStorage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.dbfs_storage", "qualname": "DBFSStorage.get_file_payload", "kind": "function", "doc": "<p>Get the content of a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>File payload/content.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.dbfs_storage.DBFSStorage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.dbfs_storage", "qualname": "DBFSStorage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n<li><strong>content:</strong>  content to write into the file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage", "modulename": "lakehouse_engine.utils.storage.file_storage", "kind": "module", "doc": "<p>Module for abstract representation of a storage system holding files.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.file_storage.FileStorage", "modulename": "lakehouse_engine.utils.storage.file_storage", "qualname": "FileStorage", "kind": "class", "doc": "<p>Abstract file storage class.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.utils.storage.file_storage.FileStorage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.file_storage", "qualname": "FileStorage.get_file_payload", "kind": "function", "doc": "<p>Get the payload of a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>File payload/content.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage.FileStorage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.file_storage", "qualname": "FileStorage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n<li><strong>content:</strong>  content to write into the file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "kind": "module", "doc": "<p>Module for common file storage functions.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions", "kind": "class", "doc": "<p>Class for common file storage functions.</p>\n", "bases": "abc.ABC"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions.read_json", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions.read_json", "kind": "function", "doc": "<p>Read a json file.</p>\n\n<p>The file should be in a supported file system (e.g., s3, dbfs or\nlocal filesystem).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  path to the json file.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Dict with json file content.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions.read_sql", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions.read_sql", "kind": "function", "doc": "<p>Read a sql file.</p>\n\n<p>The file should be in a supported file system (e.g., s3, dbfs or local\nfilesystem).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  path to the sql file.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Content of the SQL file.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions.write_payload", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions.write_payload", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<p>The file should be in a supported file system (e.g., s3, dbfs or local\nfilesystem).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  path to validate the file type.</li>\n<li><strong>url:</strong>  url of the file.</li>\n<li><strong>content:</strong>  content to write into the file.</li>\n<li><strong>disable_dbfs_retry:</strong>  optional flag to disable file storage dbfs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>,</span><span class=\"param\">\t<span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">disable_dbfs_retry</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.file_storage_functions.FileStorageFunctions.is_boto3_configured", "modulename": "lakehouse_engine.utils.storage.file_storage_functions", "qualname": "FileStorageFunctions.is_boto3_configured", "kind": "function", "doc": "<p>Check if boto3 is able to locate credentials and properly configured.</p>\n\n<p>If boto3 is not properly configured, we might want to try a different reader.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "kind": "module", "doc": "<p>Module to represent a local file storage system.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage.LocalFSStorage", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "qualname": "LocalFSStorage", "kind": "class", "doc": "<p>Class to represent a local file storage system.</p>\n", "bases": "lakehouse_engine.utils.storage.file_storage.FileStorage"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage.LocalFSStorage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "qualname": "LocalFSStorage.get_file_payload", "kind": "function", "doc": "<p>Get the payload of a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>file payload/content.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> &lt;class &#x27;TextIO&#x27;&gt;:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.local_fs_storage.LocalFSStorage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.local_fs_storage", "qualname": "LocalFSStorage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n<li><strong>content:</strong>  content to write into the file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage", "modulename": "lakehouse_engine.utils.storage.s3_storage", "kind": "module", "doc": "<p>Module to represent a s3 file storage system.</p>\n"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage.S3Storage", "modulename": "lakehouse_engine.utils.storage.s3_storage", "qualname": "S3Storage", "kind": "class", "doc": "<p>Class to represent a s3 file storage system.</p>\n", "bases": "lakehouse_engine.utils.storage.file_storage.FileStorage"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage.S3Storage.get_file_payload", "modulename": "lakehouse_engine.utils.storage.s3_storage", "qualname": "S3Storage.get_file_payload", "kind": "function", "doc": "<p>Get the payload of a config file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>File payload/content.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine.utils.storage.s3_storage.S3Storage.write_payload_to_file", "modulename": "lakehouse_engine.utils.storage.s3_storage", "qualname": "S3Storage.write_payload_to_file", "kind": "function", "doc": "<p>Write payload into a file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>url:</strong>  url of the file.</li>\n<li><strong>content:</strong>  content to write into the file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"o\">.</span><span class=\"n\">ParseResult</span>, </span><span class=\"param\"><span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "lakehouse_engine_usage", "modulename": "lakehouse_engine_usage", "kind": "module", "doc": "<h1 id=\"how-to-use-the-lakehouse-engine\">How to use the Lakehouse Engine?</h1>\n\n<p>Lakehouse engine usage examples for all the algorithms and other core functionalities.</p>\n\n<ul>\n<li><a href=\"lakehouse_engine_usage/data_loader.html\">Data Loader</a></li>\n<li><a href=\"lakehouse_engine_usage/data_quality.html\">Data Quality</a></li>\n<li><a href=\"lakehouse_engine_usage/reconciliator.html\">Reconciliator</a></li>\n<li><a href=\"lakehouse_engine_usage/sensor.html\">Sensor</a></li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader", "modulename": "lakehouse_engine_usage.data_loader", "kind": "module", "doc": "<h1 id=\"data-loader\">Data Loader</h1>\n\n<h2 id=\"how-to-configure-a-dataloader-algorithm-in-the-lakehouse-engine-by-using-an-acon-file\">How to configure a DataLoader algorithm in the lakehouse-engine by using an ACON file?</h2>\n\n<p>An algorithm (e.g., data load) in the lakehouse-engine is configured using an ACON. The lakehouse-engine is a\nconfiguration-driven framework, so people don't have to write code to execute a Spark algorithm. In contrast, the\nalgorithm is written in pyspark and accepts configurations through a JSON file (an ACON - algorithm configuration). The\nACON is the configuration providing the behaviour of a lakehouse engine algorithm. <a href=\"../lakehouse_engine/algorithms/algorithm.html\">You can check the algorithm code, and\nhow it interprets the ACON here</a>.\nIn this page we will go through the structure of an ACON file and what are the most suitable ACON files for common data\nengineering scenarios.\nCheck the underneath pages to find several <strong>ACON examples</strong> that cover many data extraction, transformation and loading scenarios.</p>\n\n<h2 id=\"overview-of-the-structure-of-the-acon-file-for-dataloads\">Overview of the Structure of the ACON file for DataLoads</h2>\n\n<p>An ACON-based algorithm needs several specifications to work properly, but some of them might be optional. The available\nspecifications are:</p>\n\n<ul>\n<li><strong>Input specifications (input_specs)</strong>: specify how to read data. This is a <strong>mandatory</strong> keyword.</li>\n<li><strong>Transform specifications (transform_specs)</strong>: specify how to transform data.</li>\n<li><strong>Data quality specifications (dq_specs)</strong>: specify how to execute the data quality process.</li>\n<li><strong>Output specifications (output_specs)</strong>: specify how to write data to the target. This is a <strong>mandatory</strong> keyword.</li>\n<li><strong>Terminate specifications (terminate_specs)</strong>: specify what to do after writing into the target (e.g., optimising target table, vacuum, compute stats, expose change data feed to external location, etc.).</li>\n<li><strong>Execution environment (exec_env)</strong>: custom Spark session configurations to be provided for your algorithm (configurations can also be provided from your job/cluster configuration, which we highly advise you to do instead of passing performance related configs here for example).</li>\n</ul>\n\n<p>Below is an example of a complete ACON file that reads from a s3 folder with CSVs and incrementally loads that data (using a merge) into a delta lake table.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p><strong>spec_id</strong> is one of the main concepts to ensure you can chain the steps of the algorithm, so, for example, you can specify the transformations (in transform_specs) of a DataFrame that was read in the input_specs. Check ACON below to see how the spec_id of the input_specs is used as input_id in one transform specification.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;orders_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;schema_path&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my-data-product-bucket/artefacts/metadata/bronze/schemas/orders.json&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;with_filepath&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;badRecordsPath&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my-data-product-bucket/badrecords/order_events_with_dq/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;</span><span class=\"se\">\\u005E</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dateFormat&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;yyyyMMdd&quot;</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my-data-product-bucket/bronze/orders/&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;orders_bronze_with_extraction_date&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;orders_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;with_row_id&quot;</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;with_regex_value&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;lhe_extraction_filepath&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;output_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;extraction_date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;drop_input_col&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;regex&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;.*WE_SO_SCL_(</span><span class=\"se\">\\\\</span><span class=\"s2\">d+).csv&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;check_orders_bronze_with_extraction_date&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;orders_bronze_with_extraction_date&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_dq_checks&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;dq_function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;omnihub_locale_code&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;dq_function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_unique_value_count_to_be_between&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;product_division&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;dq_function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_max_to_be_between&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;so_net_value&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1000</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;dq_function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_value_lengths_to_be_between&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;omnihub_locale_code&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;dq_function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_mean_to_be_between&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;coupon_code&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">20</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;orders_silver&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;check_orders_bronze_with_extraction_date&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">&quot;order_date_header&quot;</span>\n      <span class=\"p\">],</span>\n      <span class=\"s2\">&quot;merge_opts&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">            new.sales_order_header = current.sales_order_header</span>\n<span class=\"s2\">            and new.sales_order_schedule = current.sales_order_schedule</span>\n<span class=\"s2\">            and new.sales_order_item=current.sales_order_item</span>\n<span class=\"s2\">            and new.epoch_status=current.epoch_status</span>\n<span class=\"s2\">            and new.changed_on=current.changed_on</span>\n<span class=\"s2\">            and new.extraction_date=current.extraction_date</span>\n<span class=\"s2\">            and new.lhe_batch_id=current.lhe_batch_id</span>\n<span class=\"s2\">            and new.lhe_row_id=current.lhe_row_id</span>\n<span class=\"s2\">        &quot;&quot;&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;insert_only&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_with_dq&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my-data-product-bucket/silver/order_events_with_dq/&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;with_batch_id&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my-data-product-bucket/checkpoints/order_events_with_dq/&quot;</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;terminate_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;optimize_dataset&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_with_dq&quot;</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h2 id=\"input-specifications\">Input Specifications</h2>\n\n<p>You specify how to read the data by providing a list of Input Specifications. Usually there's just one element in that\nlist, as, in the lakehouse, you are generally focused on reading data from one layer (e.g., source, bronze, silver,\ngold) and put it on the next layer. However, there may be scenarios where you would like to combine two datasets (e.g.,\njoins or incremental filtering on one dataset based on the values of another\none), therefore you can use one or more elements.\n<a href=\"../lakehouse_engine/core/definitions.html#InputSpec\">More information about InputSpecs</a>.</p>\n\n<h5 id=\"relevant-notes\">Relevant notes</h5>\n\n<ul>\n<li>A spec id is fundamental, so you can use the input data later on in any step of the algorithm (transform, write, dq process, terminate).</li>\n<li>You don't have to specify <code>db_table</code> and <code>location</code> at the same time. Depending on the data_format sometimes you read from a table (e.g., jdbc or deltalake table) sometimes you read from a location (e.g., files like deltalake, parquet, json, avro... or kafka topic).</li>\n</ul>\n\n<h2 id=\"transform-specifications\">Transform Specifications</h2>\n\n<p>In the lakehouse engine, you transform data by providing a transform specification, which contains a list of transform functions (transformers). So the transform specification acts upon on input, and it can execute multiple lakehouse engine transformation functions (transformers) upon that input.</p>\n\n<p>If you look into the example above we ask the lakehouse engine to execute two functions on the <code>orders_bronze</code> input\ndata: <code>with_row_id</code> and <code>with_regex_value</code>. Those functions can of course receive arguments. You can see a list of all\navailable transformation functions (transformers) here <code>lakehouse_engine.transformers</code>. Then, you just invoke them in\nyour ACON as demonstrated above, following exactly the same function name and parameters name as described in the code\ndocumentation. \n<a href=\"../lakehouse_engine/core/definitions.html#TransformSpec\">More information about TransformSpec</a>.</p>\n\n<h5 id=\"relevant-notes-2\">Relevant notes</h5>\n\n<ul>\n<li>This stage is fully optional, you can omit it from the ACON.</li>\n<li>There is one relevant option <code>force_streaming_foreach_batch_processing</code> that can be used to force the transform to be\nexecuted in the foreachBatch function to ensure non-supported streaming operations can be properly executed. You don't\nhave to worry about this if you are using regular lakehouse engine transformers. But if you are providing your custom\nlogic in pyspark code via our lakehouse engine\ncustom_transformation (<code>lakehouse_engine.transformers.custom_transformers</code>) then sometimes your logic may contain\nSpark functions that are not compatible with Spark Streaming, and therefore this flag can enable all of your\ncomputation to be streaming-compatible by pushing down all the logic into the foreachBatch() function.</li>\n</ul>\n\n<h2 id=\"data-quality-specifications\">Data Quality Specifications</h2>\n\n<p>One of the most relevant features of the lakehouse engine is that you can have data quality guardrails that prevent you\nfrom loading bad data into your target layer (e.g., bronze, silver or gold). The lakehouse engine data quality process\nincludes two main features at the moment:</p>\n\n<ul>\n<li><strong>Assistant</strong>: The capability to profile data out of a read (<code>input_spec</code>) or transform (<code>transform_spec</code>) stage and based\non the profiling, display it and generate expectations that can be used for the validator.</li>\n<li><strong>Validator</strong>: The capability to perform data quality checks on that data (e.g., is the max value of a column bigger\nthan x?) and even tag your data with the results of the DQ checks.</li>\n</ul>\n\n<p>The output of the data quality process can be written into a <a href=\"data_quality/result_sink.html\"><strong>Result Sink</strong></a> target (e.g. table or files) and is integrated with a <a href=\"data_quality.html#3-data-docs-website\">Data Docs website</a>, which can be a company-wide available website for people to check the quality of their data and share with others.</p>\n\n<p>To achieve all of this functionality the lakehouse engine uses <a href=\"https://greatexpectations.io/\">Great Expectations</a> internally. To hide the Great Expectations internals from our user base and provide friendlier abstractions using the ACON, we have developed the concept of DQSpec that can contain many DQFunctionSpec objects, which is very similar to the relationship between the TransformSpec and TransformerSpec, which means you can have multiple Great Expectations functions executed inside a single data quality specification (as in the ACON above).</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>The names of the functions and args are a 1 to 1 match of <a href=\"https://greatexpectations.io/expectations/\">Great Expectations API</a>.</p>\n\n</div>\n\n<p><a href=\"../lakehouse_engine/core/definitions.html#DQSpec\">More information about DQSpec</a>.</p>\n\n<h5 id=\"relevant-notes-3\">Relevant notes</h5>\n\n<ul>\n<li>You can write the outputs of the DQ process to a sink through the result_sink* parameters of the\nDQSpec. <code>result_sink_options</code> takes any Spark options for a DataFrame writer, which means you can specify the options\naccording to your sink format (e.g., delta, parquet, json, etc.). We usually recommend using <code>\"delta\"</code> as format.</li>\n<li>You can use the results of the DQ checks to tag the data that you are validating. When configured, these details will\nappear as a new column (like any other), as part of the tables of your Data Product.</li>\n<li>To be able to make an analysis with the data of <code>result_sink*</code>, we have available an approach in which you\nset <code>result_sink_explode</code> as true (which is the default) and then you have some columns expanded. Those are:\n<ul>\n<li>General columns: Those are columns that have the basic information regarding <code>dq_specs</code> and will have always values\nand does not depend on the expectation types chosen.\n  -\nColumns: <code>checkpoint_config</code>, <code>run_name</code>, <code>run_time</code>, <code>run_results</code>, <code>success</code>, <code>validation_result_identifier</code>, <code>spec_id</code>, <code>input_id</code>, <code>validation_results</code>, <code>run_time_year</code>, <code>run_time_month</code>, <code>run_time_day</code>.</li>\n<li>Statistics columns: Those are columns that have information about the runs of expectations, being those values for\nthe run and not for each expectation. Those columns come from <code>run_results.validation_result.statistics.*</code>.\n<ul>\n<li>Columns: <code>evaluated_expectations</code>, <code>success_percent</code>, <code>successful_expectations</code>, <code>unsuccessful_expectations</code>.</li>\n</ul></li>\n<li>Expectations columns: Those are columns that have information about the expectation executed.\n<ul>\n<li>Columns: <code>expectation_type</code>, <code>batch_id</code>, <code>expectation_success</code>, <code>exception_info</code>. Those columns are exploded\nfrom <code>run_results.validation_result.results</code>\ninside <code>expectation_config.expectation_type</code>, <code>expectation_config.kwargs.batch_id</code>, <code>success as expectation_success</code>,\nand <code>exception_info</code>. Moreover, we also include <code>unexpected_index_list</code>, <code>observed_value</code> and <code>kwargs</code>.</li>\n</ul></li>\n<li>Arguments of Expectations columns: Those are columns that will depend on the expectation_type selected. Those\ncolumns are exploded from <code>run_results.validation_result.results</code> inside <code>expectation_config.kwargs.*</code>.\n<ul>\n<li>We can have for\nexample: <code>column</code>, <code>column_A</code>, <code>column_B</code>, <code>max_value</code>, <code>min_value</code>, <code>value</code>, <code>value_pairs_set</code>, <code>value_set</code>,\nand others.</li>\n</ul></li>\n<li>More columns desired? Those can be added, using <code>result_sink_extra_columns</code> in which you can select columns\nlike <code>&lt;name&gt;</code> and/or explode columns like <code>&lt;name&gt;.*</code>.</li>\n</ul></li>\n<li>Use the parameter <code>\"source\"</code> to identify the data used for an easier analysis.</li>\n<li>By default, Great Expectation will also provide a site presenting the history of the DQ validations that you have performed on your data.</li>\n<li>You can make an analysis of all your expectations and create a dashboard aggregating all that information.</li>\n<li>This stage is fully optional, you can omit it from the ACON.</li>\n</ul>\n\n<h2 id=\"output-specifications\">Output Specifications</h2>\n\n<p>The output_specs section of an ACON is relatively similar to the input_specs section, but of course focusing on how to write the results of the algorithm, instead of specifying the input for the algorithm, hence the name output_specs (output specifications). <a href=\"../lakehouse_engine/core/definitions.html#OutputSpec\">More information about OutputSpec</a>.</p>\n\n<h5 id=\"relevant-notes-4\">Relevant notes</h5>\n\n<ul>\n<li>Respect the supported write types and output formats.</li>\n<li>One of the most relevant options to specify in the options parameter is the <code>checkpoint_location</code> when in streaming\nread mode, because that location will be responsible for storing which data you already read and transformed from the\nsource, <strong>when the source is a Spark Streaming compatible source (e.g., Kafka or S3 files)</strong>.</li>\n</ul>\n\n<h2 id=\"terminate-specifications\">Terminate Specifications</h2>\n\n<p>The terminate_specs section of the ACON is responsible for some \"wrapping up\" activities like optimising a table,\nvacuuming old files in a delta table, etc. With time the list of available terminators will likely increase (e.g.,\nreconciliation processes), but for now we have the <a href=\"../lakehouse_engine/terminators.html\">following terminators</a>.\nThis stage is fully optional, you can omit it from the ACON.\nThe most relevant now in the context of the lakehouse initiative are the following:</p>\n\n<ul>\n<li><a href=\"../lakehouse_engine/terminators/dataset_optimizer.html\">dataset_optimizer</a></li>\n<li><a href=\"../lakehouse_engine/terminators/cdf_processor.html\">cdf_processor</a></li>\n<li><a href=\"../lakehouse_engine/terminators/sensor_terminator.html\">sensor_terminator</a></li>\n<li><a href=\"../lakehouse_engine/terminators/notifiers/email_notifier.html\">notifier_terminator</a></li>\n</ul>\n\n<p><a href=\"../lakehouse_engine/core/definitions.html#TerminatorSpec\">More information about TerminatorSpec</a>.</p>\n\n<h2 id=\"execution-environment\">Execution Environment</h2>\n\n<p>In the exec_env section of the ACON you can pass any Spark Session configuration that you want to define for the\nexecution of your algorithm. This is basically just a JSON structure that takes in any Spark Session property, so no\ncustom lakehouse engine logic. This stage is fully optional, you can omit it from the ACON.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Please be aware that Spark Session configurations that are not allowed to be changed when the Spark cluster is already\nrunning need to be passed in the configuration of the job/cluster that runs this algorithm, not here in this section.\nThis section only accepts Spark Session configs that can be changed in runtime. Whenever you introduce an option make\nsure that it takes effect during runtime, as to the best of our knowledge there's no list of allowed Spark properties\nto be changed after the cluster is already running. Moreover, typically Spark algorithms fail if you try to modify a\nconfig that can only be set up before the cluster is running.</p>\n\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.append_load_from_jdbc_with_permissive_mode", "modulename": "lakehouse_engine_usage.data_loader.append_load_from_jdbc_with_permissive_mode", "kind": "module", "doc": "<h1 id=\"append-load-from-jdbc-with-permissive-mode-default\">Append Load from JDBC with PERMISSIVE mode (default)</h1>\n\n<p>This scenario is an append load from a JDBC source (e.g., SAP BW, Oracle Database, SQL Server Database...).</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc:sqlite:/app/tests/lakehouse/in/feature/append_load/jdbc_permissive/tests.db&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc_permissive&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;org.sqlite.JDBC&quot;</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;test_db.jdbc_permissive_table&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_date&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;date&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_date&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;test_db.jdbc_permissive_table&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">&quot;date&quot;</span>\n      <span class=\"p\">],</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/append_load/jdbc_permissive/data&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes</h5>\n\n<ul>\n<li>The <strong>ReadMode</strong> is <strong>PERMISSIVE</strong> in this scenario, which <strong>is the default in Spark</strong>, hence we <strong>don't need to specify it</strong>. Permissive means don't enforce any schema on the input data. </li>\n<li>From a JDBC source the ReadType needs to be \"batch\" always as \"streaming\" is not available for a JDBC source.</li>\n<li>In this scenario we do an append load by getting the max date (transformer_spec <a href=\"../../lakehouse_engine/transformers/aggregators.html#Aggregators.get_max_value\">\"get_max_value\"</a>) on bronze and use that date to filter the source to only get data with a date greater than that max date on bronze (transformer_spec <a href=\"../../lakehouse_engine/transformers/filters.html#Filters.incremental_filter\">\"incremental_filter\"</a>). <strong>That is the standard way we do incremental batch loads in the lakehouse engine.</strong> For streaming incremental loads we rely on Spark Streaming checkpoint feature <a href=\"../../lakehouse_engine_usage/data_loader/streaming_append_load_with_terminator.html\">(check a streaming append load ACON example)</a>.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.append_load_with_failfast", "modulename": "lakehouse_engine_usage.data_loader.append_load_with_failfast", "kind": "module", "doc": "<h1 id=\"append-load-with-failfast\">Append Load with FAILFAST</h1>\n\n<p>This scenario is an append load enforcing the schema (using the schema of the target table to enforce the schema of the source, i.e., the schema of the source needs to exactly match the schema of the target table) and FAILFASTING if the schema of the input data does not match the one we specified.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;enforce_schema_from_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;test_db.failfast_table&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;mode&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;FAILFAST&quot;</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/append_load/failfast/data&quot;</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;test_db.failfast_table&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_date&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;date&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_date&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;test_db.failfast_table&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">&quot;date&quot;</span>\n      <span class=\"p\">],</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/append_load/failfast/data&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes</h5>\n\n<ul>\n<li>The <strong>ReadMode</strong> is <strong>FAILFAST</strong> in this scenario, i.e., fail the algorithm if the schema of the input data does not match the one we specified via schema_path, read_schema_from_table or schema Input_specs variables.</li>\n<li>In this scenario we do an append load by getting the max date (transformer_spec <a href=\"../../lakehouse_engine/transformers/aggregators.html#Aggregators.get_max_value\">\"get_max_value\"</a>) on bronze and use that date to filter the source to only get data with a date greater than that max date on bronze (transformer_spec <a href=\"../../lakehouse_engine/transformers/filters.html#Filters.incremental_filter\">\"incremental_filter\"</a>). <strong>That is the standard way we do incremental batch loads in the lakehouse engine.</strong> For streaming incremental loads we rely on Spark Streaming checkpoint feature <a href=\"../../lakehouse_engine_usage/data_loader/streaming_append_load_with_terminator.html\">(check a streaming append load ACON example)</a>.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.batch_delta_load_init_delta_backfill_with_merge", "modulename": "lakehouse_engine_usage.data_loader.batch_delta_load_init_delta_backfill_with_merge", "kind": "module", "doc": "<h1 id=\"batch-delta-load-init-delta-and-backfill-with-merge\">Batch Delta Load Init, Delta and Backfill with Merge</h1>\n\n<p>This scenario illustrates the process of implementing a delta load algorithm by first using an ACON to perform an initial load, then another one to perform the regular deltas that will be triggered on a recurrent basis, and finally an ACON for backfilling specific parcels if ever needed.</p>\n\n<h2 id=\"init-load\">Init Load</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/backfill/data&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condensed_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condense_record_mode_cdc&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;business_key&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;item&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;ranking_key_desc&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;extraction_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;datapakid&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;partno&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;record&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;record_mode_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;recordmode&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;valid_record_modes&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;N&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;R&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;D&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;X&quot;</span>\n            <span class=\"p\">]</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condensed_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;merge_opts&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date&quot;</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant Notes</h5>\n\n<ul>\n<li>We can see that even though this is an init load we still have chosen to condense the records through our <a href=\"../../lakehouse_engine/transformers/condensers.html#Condensers.condense_record_mode_cdc\">\"condense_record_mode_cdc\"</a> transformer. This is a condensation step capable of handling SAP BW style changelogs based on actrequest_timestamps, datapakid, record_mode, etc...</li>\n<li>In the init load we actually did a merge in this case because we wanted to test locally if a merge with an empty target table works, but you don't have to do it, as an init load usually can be just a full load. If a merge of init data with an empty table has any performance implications when compared to a regular insert remains to be tested, but we don't have any reason to recommend a merge over an insert for an init load, and as said, this was done solely for local testing purposes, you can just use <code>write_type: \"overwrite\"</code></li>\n</ul>\n\n<h2 id=\"delta-load\">Delta Load</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/backfill/data&quot;</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_timestamp&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;actrequest_timestamp&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condensed_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_timestamp&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condense_record_mode_cdc&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;business_key&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;item&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;ranking_key_desc&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;extraction_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;datapakid&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;partno&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;record&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;record_mode_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;recordmode&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;valid_record_modes&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;N&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;R&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;D&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;X&quot;</span>\n            <span class=\"p\">]</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condensed_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;merge_opts&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delete_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.recordmode in (&#39;R&#39;,&#39;D&#39;,&#39;X&#39;)&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;insert_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.recordmode is null or new.recordmode not in (&#39;R&#39;,&#39;D&#39;,&#39;X&#39;)&quot;</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes-2\">Relevant Notes</h5>\n\n<ul>\n<li><p>The merge predicate and the insert, delete or update predicates should reflect the reality of your data, and it's up to each data product to figure out which predicates better match their reality:</p>\n\n<ul>\n<li>The merge predicate usually involves making sure that the \"primary key\" for your data matches.\n<div class=\"pdoc-alert pdoc-alert-note\" markdown=\"1\">\n<h6 id=\"performance-tip-ideally-in-order-to-get-a-performance-boost-in-your-merges-you-should-also-place-a-filter-in-your-merge-predicate-eg-certain-technical-or-business-date-in-the-target-table-x-days-ago-based-on-the-assumption-that-the-rows-in-that-specified-interval-will-never-change-in-the-future-this-can-drastically-decrease-the-merge-times-of-big-tables\"><strong>Performance Tip!!!</strong> Ideally, in order to get a performance boost in your merges, you should also place a filter in your merge predicate (e.g., certain technical or business date in the target table &gt;= x days ago), based on the assumption that the rows in that specified interval will never change in the future. This can drastically decrease the merge times of big tables.</h6></li>\n</ul>\n\n</div>\n\n<ul>\n\n<p><li>The insert, delete and update predicates will always depend on the structure of your changelog, and also how you expect your updates to arrive (e.g., in certain data products you know that you will never get out of order data or late arriving data, while in other you can never ensure that). These predicates should reflect that in order to prevent you from doing unwanted changes to the target delta lake table.</p>\n\n<ul>\n<li>For example, in this scenario, we delete rows that have the R, D or X record_mode values, because we know that if after condensing the rows that is the latest status of that row from the changelog, they should be deleted, and we never insert rows with those status (<strong>note</strong>: we use this guardrail in the insert to prevent out of order changes, which is likely not the case in SAP BW).</li>\n<li>Because the <code>insert_predicate</code> is fully optional, in your scenario you may not require that.</li>\n</ul></li>\n\n<p><li>In this scenario, we don't pass an <code>update_predicate</code> in the ACON, because both <code>insert_predicate</code> and update_predicate are fully optional, i.e., if you don't pass them the algorithm will update any data that matches the <code>merge_predicate</code> and insert any data that does not match it. The predicates in these cases just make sure the algorithm does not insert or update any data that you don't want, as in the late arriving changes scenario where a deleted row may arrive first from the changelog then the update row, and to prevent your target table to have inconsistent data for a certain period of time (it will eventually get consistent when you receive the latest correct status from the changelog though) you can have this guardrail in the insert or update predicates. Again, for most sources this will not happen but sources like Kafka for example cannot 100% ensure order, for example.</li>\n<li>In order to understand how we can cover different scenarios (e.g., late arriving changes, out of order changes, etc.), please go <a href=\"../../lakehouse_engine_usage/data_loader/streaming_delta_with_late_arriving_and_out_of_order_events.html\">here</a>.</li>\n</ul></p></li>\n<li>The order of the predicates in the ACON does not matter, is the logic in the lakehouse engine <a href=\"../../lakehouse_engine/io/writers/delta_merge_writer.html#DeltaMergeWriter.__init__\">DeltaMergeWriter's \"_merge\" function</a> that matters.</li>\n<li>Notice the \"&lt;=&gt;\" operator? In Spark SQL that's the null safe equal.</li>\n</ul>\n\n<h2 id=\"backfilling\">Backfilling</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/backfill/data&quot;</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_sales_bronze_timestamp&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;actrequest_timestamp&quot;</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condensed_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;increment_value&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;20180110120052t&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;greater_or_equal&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condense_record_mode_cdc&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;business_key&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;item&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;ranking_key_desc&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;extraction_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;datapakid&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;partno&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;record&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;record_mode_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;recordmode&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;valid_record_modes&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;N&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;R&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;D&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;X&quot;</span>\n            <span class=\"p\">]</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condensed_sales&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/backfill/data&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;merge_opts&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delete_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.recordmode in (&#39;R&#39;,&#39;D&#39;,&#39;X&#39;)&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;insert_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.recordmode is null or new.recordmode not in (&#39;R&#39;,&#39;D&#39;,&#39;X&#39;)&quot;</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes-3\">Relevant Notes</h5>\n\n<ul>\n<li>The backfilling process depicted here is fairly similar to the init load, but it is relevant to highlight  by using a static value (that can be modified accordingly to the backfilling needs) in the <a href=\"../../lakehouse_engine/transformers/filters.html#Filters.incremental_filter\">incremental_filter</a> function.</li>\n<li>Other relevant functions for backfilling may include the <a href=\"../../lakehouse_engine/transformers/filters.html#Filters.expression_filter\">expression_filter</a> function, where you can use a custom SQL filter to filter the input data.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.custom_transformer", "modulename": "lakehouse_engine_usage.data_loader.custom_transformer", "kind": "module", "doc": "<h1 id=\"custom-transformer\">Custom Transformer</h1>\n\n<p>There may appear a scenario where the data product dev team faces the need to perform complex data transformations that are either not yet available in the lakehouse engine or the logic is just too complex to chain in an ACON file. In the context of the lakehouse, the only layers that usually can impose that complexity is silver+ and gold. This page targets exactly those cases.</p>\n\n<p>Below you'll find a notebook where you can pass your own PySpark or Spark SQL logic into the ACON, by dynamically injecting a python function into the ACON dictionary. The lakehouse engine will take care of executing those transformations in the transformation step of the data loader algorithm. Please read the notebook's comments carefully to understand how it works, or simply open it in your notebook environment, which will make the notebook's code and comments more readable.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<h6 id=\"force-streaming-micro-batch-processing\">Force Streaming Micro Batch Processing.</h6>\n\n<p>When you use streaming mode, with a custom transformer, it\u2019s\nhighly advisable that you set the <code>force_streaming_microbatch_processing</code> flag to <code>True</code> in the transform specification, as\nexplained above!</p>\n\n</div>\n\n<h2 id=\"what-is-a-custom-transformer-in-the-lakehouse-engine-and-how-you-can-use-it-to-write-your-own-pyspark-logic\">What is a custom transformer in the Lakehouse Engine and how you can use it to write your own pyspark logic?</h2>\n\n<p>We highly promote the Lakehouse Engine for creating Data Products aligned with the data source (bronze/silver layer), pumping data into silver so our Data Scientists and Analysts can leverage the value of the data in silver, as close as it comes from the source.\nThe low-code and configuration-driven nature of the lakehouse engine makes it a compelling framework to use in such cases, where the transformations that are done from bronze to silver are not that many, as we want to keep the data close to the source.</p>\n\n<p>However, when it comes to Data Products enriched in some way or for insights (silver+, gold), they are typically heavy\non transformations (they are the T of the overall ELT process), so the nature of the lakehouse engine may would have\nget into the way of adequately building it. Considering this, and considering our user base that prefers an ACON-based\napproach and all the nice off-the-shelf features of the lakehouse engine, we have developed a feature that\nallows us to <strong>pass custom transformers where you put your entire pyspark logic and can pass it as an argument\nin the ACON</strong> (the configuration file that configures every lakehouse engine algorithm).</p>\n\n<h3 id=\"motivation\">Motivation:</h3>\n\n<p>Doing that, you let the ACON guide your read, data quality, write and terminate processes, and you just focus on transforming data :)</p>\n\n<h2 id=\"custom-transformation-function\">Custom transformation Function</h2>\n\n<p>The function below is the one that encapsulates all your defined pyspark logic and sends it as a python function to the lakehouse engine. This function will then be invoked internally in the lakehouse engine via a df.transform() function. If you are interested in checking the internals of the lakehouse engine, our codebase is openly available here: <a href=\"https://github.com/adidas/lakehouse-engine\">https://github.com/adidas/lakehouse-engine</a></p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<h6 id=\"attention\">Attention!!!</h6>\n\n<p>For this process to work, your function defined below needs to receive a DataFrame and return a DataFrame. Attempting any other method signature (e.g., defining more parameters) will not work, unless you use something like <a href=\"https://docs.python.org/3/library/functools.html#functools.partial\">python partials</a>, for example.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">get_new_data</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">DataFrame</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Get the new data from the lakehouse engine reader and prepare it.&quot;&quot;&quot;</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span>\n        <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">,</span> <span class=\"n\">when</span><span class=\"p\">(</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;_change_type&quot;</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;delete&quot;</span><span class=\"p\">,</span> <span class=\"n\">lit</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">otherwise</span><span class=\"p\">(</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">)))</span>\n        <span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;order_date&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">groupBy</span><span class=\"p\">(</span><span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;order_date&quot;</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">))</span>\n    <span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">get_joined_data</span><span class=\"p\">(</span><span class=\"n\">new_data_df</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">,</span> <span class=\"n\">current_data_df</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">DataFrame</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Join the new data with the current data already existing in the target dataset.&quot;&quot;&quot;</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span>\n        <span class=\"n\">new_data_df</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">&quot;new_data&quot;</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span>\n            <span class=\"n\">current_data_df</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">&quot;current_data&quot;</span><span class=\"p\">),</span>\n            <span class=\"p\">[</span>\n                <span class=\"n\">new_data_df</span><span class=\"o\">.</span><span class=\"n\">article_id</span> <span class=\"o\">==</span> <span class=\"n\">current_data_df</span><span class=\"o\">.</span><span class=\"n\">article_id</span><span class=\"p\">,</span>\n                <span class=\"n\">new_data_df</span><span class=\"o\">.</span><span class=\"n\">order_date</span> <span class=\"o\">==</span> <span class=\"n\">current_data_df</span><span class=\"o\">.</span><span class=\"n\">order_date</span><span class=\"p\">,</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;left_outer&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span>\n            <span class=\"s2\">&quot;current_amount&quot;</span><span class=\"p\">,</span> <span class=\"n\">when</span><span class=\"p\">(</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;current_data.amount&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">isNull</span><span class=\"p\">(),</span> <span class=\"n\">lit</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">otherwise</span><span class=\"p\">(</span><span class=\"s2\">&quot;current_data.amount&quot;</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s2\">&quot;final_amount&quot;</span><span class=\"p\">,</span> <span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;current_amount&quot;</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;new_data.amount&quot;</span><span class=\"p\">))</span>\n        <span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;new_data.article_id&quot;</span><span class=\"p\">),</span> <span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;new_data.order_date&quot;</span><span class=\"p\">),</span> <span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">&quot;final_amount&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">))</span>\n    <span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">calculate_kpi</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">DataFrame</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Calculate KPI through a custom transformer that will be provided in the ACON.</span>\n\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        df: DataFrame passed as input.</span>\n\n<span class=\"sd\">    Returns:</span>\n<span class=\"sd\">        DataFrame: the transformed DataFrame.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">new_data_df</span> <span class=\"o\">=</span> <span class=\"n\">get_new_data</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># we prefer if you use &#39;ExecEnv.SESSION&#39; instead of &#39;spark&#39;, because is the internal object the</span>\n    <span class=\"c1\"># lakehouse engine uses to refer to the spark session. But if you use &#39;spark&#39; should also be fine.</span>\n    <span class=\"n\">current_data_df</span> <span class=\"o\">=</span> <span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">SESSION</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"p\">(</span>\n        <span class=\"s2\">&quot;my_database.my_table&quot;</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">transformed_df</span> <span class=\"o\">=</span> <span class=\"n\">get_joined_data</span><span class=\"p\">(</span><span class=\"n\">new_data_df</span><span class=\"p\">,</span> <span class=\"n\">current_data_df</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">transformed_df</span>\n</code></pre>\n</div>\n\n<h3 id=\"dont-like-pyspark-api-write-sql\">Don't like pyspark API? Write SQL</h3>\n\n<p>You don't have to comply to the pyspark API if you prefer SQL. Inside the function above (or any of\nthe auxiliary functions you decide to develop) you can write something like:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">calculate_kpi</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">:</span> <span class=\"n\">DataFrame</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">DataFrame</span><span class=\"p\">:</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">createOrReplaceTempView</span><span class=\"p\">(</span><span class=\"s2\">&quot;new_data&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># we prefer if you use &#39;ExecEnv.SESSION&#39; instead of &#39;spark&#39;, because is the internal object the</span>\n    <span class=\"c1\"># lakehouse engine uses to refer to the spark session. But if you use &#39;spark&#39; should also be fine.</span>\n    <span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">SESSION</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">          CREATE OR REPLACE TEMP VIEW my_kpi AS</span>\n<span class=\"sd\">          SELECT ... FROM new_data ...</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">SESSION</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"s2\">&quot;my_kpi&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h2 id=\"just-your-regular-acon\">Just your regular ACON</h2>\n\n<p>If you notice the ACON below, everything is the same as you would do in a Data Product, but the <code>transform_specs</code> section of the ACON has a difference, which is a function called <code>\"custom_transformation\"</code> where we supply as argument the function defined above with the pyspark code.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<h6 id=\"attention-2\">Attention!!!</h6>\n\n<p>Do not pass the function as calculate_kpi(), but as calculate_kpi, otherwise you are telling python to invoke the function right away, as opposed to pass it as argument to be invoked later by the lakehouse engine.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_sales&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;readChangeFeed&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;true&quot;</span><span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;transformed_sales_kpi&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales&quot;</span><span class=\"p\">,</span>\n            <span class=\"c1\"># because we are using streaming, this allows us to make sure that</span>\n            <span class=\"c1\"># all the computation in our custom transformer gets pushed to</span>\n            <span class=\"c1\"># Spark&#39;s foreachBatch method in a stream, which allows us to</span>\n            <span class=\"c1\"># run all Spark functions in a micro batch DataFrame, as there</span>\n            <span class=\"c1\"># are some Spark functions that are not supported in streaming.</span>\n            <span class=\"s2\">&quot;force_streaming_foreach_batch_processing&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;custom_transformation&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;custom_transformer&quot;</span><span class=\"p\">:</span> <span class=\"n\">calculate_kpi</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_table_quality&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;transformed_sales_kpi&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;expectations_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/expectations/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;validations_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/validations/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;checkpoint_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/checkpoints/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_table&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_kpi&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;transformed_sales_kpi&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/gold/my_table&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;merge_opts&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.article_id = current.article_id AND new.order_date = current.order_date&quot;</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.extract_from_sap_b4_adso", "modulename": "lakehouse_engine_usage.data_loader.extract_from_sap_b4_adso", "kind": "module", "doc": "<h1 id=\"extract-from-sap-b4-adsos\">Extract from SAP B4 ADSOs</h1>\n\n<p>A custom sap_b4 reader and a few utils are offered in the lakehouse-engine framework so that consumption of data from\nSAP B4 DSOs can be easily created. The framework abstracts all the logic behind the init/delta extractions\n(AQ vs CL, active table, changelog table, requests status table, how to identify the next delta timestamp...),\nonly requiring a few parameters that are explained and exemplified in the\n<a href=\"#extraction-from-sap-b4-adsos-template\">template</a> scenarios that we have created.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-custom-reader-is-very-similar-and-uses-most-features-from-the-sap_bw-reader-so-if-you-were-using-specific-filtersparameters-with-the-sap_bw-reader-there-is-a-high-chance-you-can-keep-using-it-in-a-very-similar-way-with-the-sap_b4-reader-the-main-concepts-are-applied-to-both-readers-as-the-strategies-on-how-to-parallelize-the-extractions-for-example\">This custom reader is very similar and uses most features from the sap_bw reader, so if you were using specific filters/parameters with the sap_bw reader, there is a high chance you can keep using it in a very similar way with the sap_b4 reader. The main concepts are applied to both readers, as the strategies on how to parallelize the extractions, for example.</h6>\n\n</div>\n\n<p>How can I find a good candidate column for <a href=\"../../lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso.html#how-can-we-decide-the-partitionColumn\">partitioning the extraction from S4Hana?</a></p>\n\n<div class=\"pdoc-alert pdoc-alert-danger\">\n\n<h6 id=\"parallelization-limitations\"><strong>Parallelization Limitations</strong></h6>\n\n<p>There are no limits imposed by the Lakehouse-Engine framework, but you need to consider that there might be differences imposed by the source.</p>\n\n<p>E.g. Each User might be restricted on utilisation of about 100GB memory at a time from the source.</p>\n\n<p>Parallel extractions <strong><em>can bring a jdbc source down</em></strong> if a lot of stress is put on the system. Be careful choosing the number of partitions. Spark is a distributed system and can lead to many connections.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-danger\">\n\n<h6 id=\"in-case-you-want-to-perform-further-filtering-in-the-reqtsn-field-please-be-aware-that-it-is-not-being-pushed-down-to-sap-b4-by-default-meaning-it-will-have-bad-performance\"><strong>In case you want to perform further filtering in the REQTSN field, please be aware that it is not being pushed down to SAP B4 by default (meaning it will have bad performance).</strong></h6>\n\n<p>In that case, you will need to use customSchema option while reading, so that you are able to enable filter push down for those.</p>\n\n</div>\n\n<p>You can check the code documentation of the reader below:</p>\n\n<p><a href=\"../../lakehouse_engine/io/readers/sap_b4_reader.html\"><strong>SAP B4 Reader</strong></a></p>\n\n<p><a href=\"../../lakehouse_engine/utils/extraction/jdbc_extraction_utils.html#JDBCExtraction.__init__\"><strong>JDBC Extractions arguments</strong></a></p>\n\n<p><a href=\"../../lakehouse_engine/utils/extraction/sap_b4_extraction_utils.html#SAPB4Extraction.__init__\"><strong>SAP B4 Extractions arguments</strong></a></p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"for-extractions-using-the-sap-b4-reader-you-can-use-the-arguments-listed-in-the-sap-b4-arguments-but-also-the-ones-listed-in-the-jdbc-extractions-as-those-are-inherited-as-well\">For extractions using the SAP B4 reader, you can use the arguments listed in the SAP B4 arguments, but also the ones listed in the JDBC extractions, as those are inherited as well.</h6>\n\n</div>\n\n<h2 id=\"extraction-from-sap-b4-adsos-template\">Extraction from SAP B4 ADSOs Template</h2>\n\n<p>This template covers the following scenarios of extractions from the SAP B4Hana ADSOs:</p>\n\n<ul>\n<li>1 - The Simplest Scenario (Not parallel - Not Recommended)</li>\n<li>2 - Parallel extraction\n<ul>\n<li>2.1 - Simplest Scenario</li>\n<li>2.2 - Provide upperBound (Recommended)</li>\n<li>2.3 - Automatic upperBound (Recommended)</li>\n<li>2.4 - Provide predicates (Recommended)</li>\n<li>2.5 - Generate predicates (Recommended)</li>\n</ul></li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Note: the template will cover two ADSO Types:</p>\n\n<ul>\n<li><strong>AQ</strong>: ADSO which is of append type and for which a single ADSO/tables holds all the information, like an\nevent table. For this type, the same ADSO is used for reading data both for the inits and deltas. Usually, these\nADSOs end with the digit \"6\".</li>\n<li><strong>CL</strong>: ADSO which is split into two ADSOs, one holding the change log events, the other having the active\ndata (current version of the truth for a particular source). For this type, the ADSO having the active data\nis used for the first extraction (init) and the change log ADSO is used for the subsequent extractions (deltas).\nUsually, these ADSOs are split into active table ending with the digit \"2\" and changelog table ending with digit \"3\".</li>\n</ul>\n\n</div>\n\n<p>For each of these ADSO types, the lakehouse-engine abstracts the logic to get the delta extractions. This logic\nbasically consists of joining the <code>db_table</code> (for <code>AQ</code>) or the <code>changelog_table</code> (for <code>CL</code>) with the table\nhaving the requests status (<code>my_database.requests_status_table</code>).\nOne of the fields used for this joining is the <code>data_target</code>, which has a relationship with the ADSO\n(<code>db_table</code>/<code>changelog_table</code>), being basically the same identifier without considering parts of it.</p>\n\n<p>Based on the previous insights, the queries that the lakehouse-engine generates under the hood translate to\n(this is a simplified version, for more details please refer to the lakehouse-engine code documentation):\n<strong>AQ Init Extraction:</strong>\n<code>SELECT t.*, CAST({self._SAP_B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp\nFROM my_database.my_table t</code></p>\n\n<p><strong>AQ Delta Extraction:</strong>\n<code>SELECT tbl.*, CAST({self._B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp\nFROM my_database.my_table AS tbl\nJOIN my_database.requests_status_table AS req\nWHERE STORAGE = 'AQ' AND REQUEST_IS_IN_PROCESS = 'N' AND LAST_OPERATION_TYPE IN ('C', 'U')\nAND REQUEST_STATUS IN ('GG', 'GR') AND UPPER(DATATARGET) = UPPER('my_identifier')\nAND req.REQUEST_TSN &gt; max_timestamp_in_bronze AND req.REQUEST_TSN &lt;= max_timestamp_in_requests_status_table</code></p>\n\n<p><strong>CL Init Extraction:</strong>\n<code>SELECT t.*,\n    {self._SAP_B4_EXTRACTION.extraction_timestamp}000000000 AS reqtsn,\n    '0' AS datapakid,\n    0 AS record,\n    CAST({self._SAP_B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp\nFROM my_database.my_table_2 t</code></p>\n\n<p><strong>CL Delta Extraction:</strong>\n<code>SELECT tbl.*,\nCAST({self._SAP_B4_EXTRACTION.extraction_timestamp} AS DECIMAL(15,0)) AS extraction_start_timestamp</code>\nFROM my_database.my_table_3 AS tbl\nJOIN my_database.requests_status_table AS req\nWHERE STORAGE = 'AT' AND REQUEST_IS_IN_PROCESS = 'N' AND LAST_OPERATION_TYPE IN ('C', 'U')\nAND REQUEST_STATUS IN ('GG') AND UPPER(DATATARGET) = UPPER('my_data_target')\nAND req.REQUEST_TSN &gt; max_timestamp_in_bronze AND req.REQUEST_TSN &lt;= max_timestamp_in_requests_status_table`</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Introductory Notes:If you want to have a better understanding about JDBC Spark optimizations, here you have a few useful links:</p>\n\n<ul>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></li>\n<li><a href=\"https://docs.databricks.com/en/connect/external-systems/jdbc.html\">https://docs.databricks.com/en/connect/external-systems/jdbc.html</a></li>\n<li><a href=\"https://bit.ly/3x2eCEm\">https://bit.ly/3x2eCEm</a></li>\n<li><a href=\"https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source\">https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source</a></li>\n</ul>\n\n</div>\n\n<h3 id=\"1-the-simplest-scenario-not-parallel-not-recommended\">1 - The Simplest Scenario (Not parallel - Not Recommended)</h3>\n\n<p>This scenario is the simplest one, not taking any advantage of Spark JDBC optimisation techniques\nand using a single connection to retrieve all the data from the source. It should only be used in case the ADSO\nyou want to extract from SAP B4Hana is a small one, with no big requirements in terms of performance to fulfill.\nWhen extracting from the source ADSO, there are two options:</p>\n\n<ul>\n<li><strong>Delta Init</strong> - full extraction of the source ADSO. You should use it in the first time you extract from the\nADSO or any time you want to re-extract completely. Similar to a so-called full load.</li>\n<li><strong>Delta</strong> - extracts the portion of the data that is new or has changed in the source, since the last\nextraction (using the <code>max_timestamp</code> value in the location of the data already extracted\n<code>latest_timestamp_data_location</code>).</li>\n</ul>\n\n<p>Below example is composed of two cells.</p>\n\n<ul>\n<li>The first cell is only responsible to define the variables <code>extraction_type</code> and <code>write_type</code>,\ndepending on the extraction type: <strong>Delta Init</strong> (<code>load_type = \"init\"</code>) or a <strong>Delta</strong> (<code>load_type = \"delta\"</code>).\nThe variables in this cell will also be referenced by other acons/examples in this notebook, similar to what\nyou would do in your pipelines/jobs, defining this centrally and then re-using it.</li>\n<li>The second cell is where the acon to be used is defined (which uses the two variables <code>extraction_type</code> and\n<code>write_type</code> defined) and the <code>load_data</code> algorithm is executed to perform the extraction.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>There may be cases where you might want to always extract fully from the source ADSO. In these cases,\nyou only need to use a Delta Init every time, meaning you would use <code>\"extraction_type\": \"init\"</code> and\n<code>\"write_type\": \"overwrite\"</code> as it is shown below. The explanation about what it is a Delta Init/Delta is\napplicable for all the scenarios presented in this notebook.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_b4&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;adso_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;AQ&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"2-parallel-extraction\">2 - Parallel extraction</h3>\n\n<p>In this section, 5 possible scenarios for parallel extractions from SAP B4Hana ADSOs are presented.</p>\n\n<h4 id=\"21-parallel-extraction-simplest-scenario\">2.1 - Parallel Extraction, Simplest Scenario</h4>\n\n<p>This scenario provides the simplest example you can have for a parallel extraction from SAP B4Hana, only using\nthe property <code>numPartitions</code>. The goal of the scenario is to cover the case in which people do not have\nmuch knowledge around how to optimize the extraction from JDBC sources or cannot identify a column that can\nbe used to split the extraction in several tasks. This scenario can also be used if the use case does not\nhave big performance requirements/concerns, meaning you do not feel the need to optimize the performance of\nthe extraction to its maximum potential.</p>\n\n<p>On the example below, <code>\"numPartitions\": 10</code> is specified, meaning that Spark will open 10 parallel connections\nto the source ADSO and automatically decide how to parallelize the extraction upon that requirement. This is the\nonly change compared to the example provided in the scenario 1.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_b4&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_par_simple/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;adso_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;AQ&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_par_simple/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"22-parallel-extraction-provide-upper_bound-recommended\">2.2 - Parallel Extraction, Provide upper_bound (Recommended)</h4>\n\n<p>This scenario performs the extraction from the SAP B4 ADSO in parallel, but is more concerned with trying to\noptimize and have more control (compared to 2.1 example) on how the extraction is split and performed,\nusing the following options:</p>\n\n<ul>\n<li><code>numPartitions</code> - number of Spark partitions to split the extraction.</li>\n<li><code>partitionColumn</code> - column used to split the extraction. It must be a numeric, date, or timestamp.\nIt should be a column that is able to split the extraction evenly in several tasks. An auto-increment\ncolumn is usually a very good candidate.</li>\n<li><code>lowerBound</code> - lower bound to decide the partition stride.</li>\n<li><code>upperBound</code> - upper bound to decide the partition stride.</li>\n</ul>\n\n<p>This is an adequate example for you to follow if you have/know a column in the ADSO that is good to be used as\nthe <code>partitionColumn</code>. If you compare with the previous example, you'll notice that now <code>numPartitions</code> and\nthree additional options are provided to fine tune the extraction (<code>partitionColumn</code>, <code>lowerBound</code>,\n<code>upperBound</code>).</p>\n\n<p>When these 4 properties are used, Spark will use them to build several queries to split the extraction.</p>\n\n<p><strong>Example:</strong> for <code>\"numPartitions\": 10</code>, <code>\"partitionColumn\": \"record\"</code>, <code>\"lowerBound: 1\"</code>, <code>\"upperBound: 100\"</code>,\nSpark will generate 10 queries like this:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &lt; 10 OR RECORD IS NULL</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 10 AND RECORD &lt; 20</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 20 AND RECORD &lt; 30</code></li>\n<li>...</li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 100</code></li>\n</ul>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_b4&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_par_prov_upper/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;adso_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;AQ&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;upperBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1000000</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_par_prov_upper/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"23-parallel-extraction-automatic-upper_bound-recommended\">2.3 - Parallel Extraction, Automatic upper_bound (Recommended)</h4>\n\n<p>This scenario is very similar to 2.2, the only difference being that <strong><code>upperBound</code>\nis not provided</strong>. Instead, the property <code>calculate_upper_bound</code> equals to true is used to benefit\nfrom the automatic calculation of the <code>upperBound</code> (derived from the <code>partitionColumn</code>) offered by the\nlakehouse-engine framework, which is useful, as in most of the cases you will probably not be aware of\nthe max value for the column. The only thing you need to consider is that if you use this automatic\ncalculation of the upperBound you will be doing an initial query to the SAP B4 ADSO to retrieve the max\nvalue for the <code>partitionColumn</code>, before doing the actual query to perform the extraction.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_b4&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;calculate_upper_bound&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_par_calc_upper/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;adso_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;AQ&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_par_calc_upper/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"24-parallel-extraction-provide-predicates-recommended\">2.4 - Parallel Extraction, Provide Predicates (Recommended)</h4>\n\n<p>This scenario performs the extraction from SAP B4 ADSO in parallel, useful in contexts in which there is no\nnumeric, date or timestamp column to parallelize the extraction (e.g. when extracting from ADSO of Type <code>CL</code>,\nthe active table does not have the <code>RECORD</code> column, which is usually a good option for scenarios 2.2 and 2.3):</p>\n\n<ul>\n<li><code>partitionColumn</code> - column used to split the extraction. It can be of any type.</li>\n</ul>\n\n<p>This is an adequate example for you to follow if you have/know a column in the ADSO that is good to be used as\nthe <code>partitionColumn</code>, specially if these columns are not complying with the scenario 2.2 or 2.3.</p>\n\n<p><strong>When this property is used all predicates need to be provided to Spark, otherwise it will leave data behind.</strong></p>\n\n<p>Below the lakehouse function to generate predicate list automatically is presented.</p>\n\n<p>This function needs to be used carefully, specially on predicates_query and predicates_add_null variables.</p>\n\n<p><strong>predicates_query:</strong> At the sample below the whole table is being considered (<code>select distinct(x) from table</code>),\nbut it is possible to filter predicates list here, specially if you are applying filter on transformations spec,\nand you know entire table won't be necessary, so you can change it to something like this: <code>select distinct(x)\nfrom table where x &gt; y</code>.</p>\n\n<p><strong>predicates_add_null:</strong> You can decide if you want to consider null on predicates list or not, by default\nthis property is <code>True</code>.</p>\n\n<p><strong>Example:</strong> for <code>\"partition_column\": \"CALMONTH\"</code></p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"c1\"># import the lakehouse_engine ExecEnv class, so that you can use the functions it offers</span>\n<span class=\"c1\"># import the lakehouse_engine extraction utils, so that you can use the JDBCExtractionUtils offered functions</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.core.exec_env</span> <span class=\"kn\">import</span> <span class=\"n\">ExecEnv</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.utils.extraction.jdbc_extraction_utils</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">,</span>\n    <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">get_or_create</span><span class=\"p\">()</span>\n\n<span class=\"n\">partition_column</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;CALMONTH&quot;</span>\n<span class=\"n\">dbtable</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_database.my_table_3&quot;</span>\n\n<span class=\"n\">predicates_query</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;(SELECT DISTINCT(</span><span class=\"si\">{</span><span class=\"n\">partition_column</span><span class=\"si\">}</span><span class=\"s2\">) FROM </span><span class=\"si\">{</span><span class=\"n\">dbtable</span><span class=\"si\">}</span><span class=\"s2\">)&quot;&quot;&quot;</span>\n<span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_user&quot;</span>\n<span class=\"n\">password</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span>\n<span class=\"n\">predicates_add_null</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n<span class=\"n\">jdbc_util</span> <span class=\"o\">=</span> <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">(</span>\n        <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">user</span><span class=\"p\">,</span>\n        <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span><span class=\"p\">,</span>\n        <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">url</span><span class=\"p\">,</span>\n        <span class=\"n\">predicates_add_null</span><span class=\"o\">=</span><span class=\"n\">predicates_add_null</span><span class=\"p\">,</span>\n        <span class=\"n\">partition_column</span><span class=\"o\">=</span><span class=\"n\">partition_column</span><span class=\"p\">,</span>\n        <span class=\"n\">dbtable</span><span class=\"o\">=</span><span class=\"n\">dbtable</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">predicates</span> <span class=\"o\">=</span> <span class=\"n\">jdbc_util</span><span class=\"o\">.</span><span class=\"n\">get_predicates</span><span class=\"p\">(</span><span class=\"n\">predicates_query</span><span class=\"p\">)</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_2_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_b4&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_2&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_3&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_2_prov_predicates/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;adso_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;CL&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;predicates&quot;</span><span class=\"p\">:</span> <span class=\"n\">predicates</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_2_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_2_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_2_prov_predicates/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"25-parallel-extraction-generate-predicates\">2.5 - Parallel Extraction, Generate Predicates</h4>\n\n<p>This scenario is very similar to the scenario 2.4, with the only difference that it automatically\ngenerates the predicates (<code>\"generate_predicates\": True</code>).</p>\n\n<p>This is an adequate example for you to follow if you have/know a column in the ADSO that is good to be used as\nthe <code>partitionColumn</code>, specially if these columns are not complying with the scenarios 2.2 and 2.3 (otherwise \nthose would probably be recommended).</p>\n\n<p>When this property is used, the lakehouse engine will generate the predicates to be used to extract data from\nthe source. What the lakehouse engine does is to check for the init/delta portion of the data,\nwhat are the distinct values of the <code>partitionColumn</code> serving that data. Then, these values will be used by\nSpark to generate several queries to extract from the source in a parallel fashion.\nEach distinct value of the <code>partitionColumn</code> will be a query, meaning that you will not have control over the\nnumber of partitions used for the extraction. For example, if you face a scenario in which you\nare using a <code>partitionColumn</code> <code>LOAD_DATE</code> and for today's delta, all the data (let's suppose 2 million rows) is\nserved by a single <code>LOAD_DATE = 20200101</code>, that would mean Spark would use a single partition\nto extract everything. In this extreme case you would probably need to change your <code>partitionColumn</code>. <strong>Note:</strong>\nthese extreme cases are harder to happen when you use the strategy of the scenarios 2.2/2.3.</p>\n\n<p><strong>Example:</strong> for <code>\"partitionColumn\": \"record\"</code>\nGenerate predicates:</p>\n\n<ul>\n<li><code>SELECT DISTINCT(RECORD) as RECORD FROM dummy_table</code></li>\n<li><code>1</code></li>\n<li><code>2</code></li>\n<li><code>3</code></li>\n<li>...</li>\n<li><code>100</code></li>\n<li>Predicates List: ['RECORD=1','RECORD=2','RECORD=3',...,'RECORD=100']</li>\n</ul>\n\n<p>Spark will generate 100 queries like this:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 1</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 2</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 3</code></li>\n<li>...</li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 100</code></li>\n</ul>\n\n<p>Generate predicates will also consider null by default:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD IS NULL</code></li>\n</ul>\n\n<p>To disable this behaviour the following variable value should be changed to false: <code>\"predicates_add_null\": False</code></p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_2_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_b4&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;generate_predicates&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_2&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table_3&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_2_gen_predicates/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;adso_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;CL&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;CALMONTH&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_2_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_2_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_2_gen_predicates/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.extract_from_sap_bw_dso", "modulename": "lakehouse_engine_usage.data_loader.extract_from_sap_bw_dso", "kind": "module", "doc": "<h1 id=\"extract-from-sap-bw-dsos\">Extract from SAP BW DSOs</h1>\n\n<div class=\"pdoc-alert pdoc-alert-danger\">\n\n<h6 id=\"parallelization-limitations\"><strong>Parallelization Limitations</strong></h6>\n\n<p>Parallel extractions <strong>can bring a jdbc source down</strong> if a lot of stress is put on the system. Be careful choosing the number of partitions. Spark is a distributed system and can lead to many connections.</p>\n\n</div>\n\n<p>A custom sap_bw reader and a few utils are offered in the lakehouse-engine framework so that consumption of data from \nSAP BW DSOs can be easily created. The framework abstracts all the logic behind the init/delta extractions \n(active table, changelog table, activation requests table, how to identify the next delta timestamp...), \nonly requiring a few parameters that are explained and exemplified in the \n<a href=\"#extraction-from-sap-bw-template\">template</a> scenarios that we have created.</p>\n\n<p>This page also provides you a section to help you figure out a good candidate for <a href=\"#how-can-we-decide-the-partitionColumn\">partitioning the extraction from SAP BW</a>.</p>\n\n<p>You can check the code documentation of the reader below:</p>\n\n<p><a href=\"../../lakehouse_engine/io/readers/sap_bw_reader.html\"><strong>SAP BW Reader</strong></a></p>\n\n<p><a href=\"../../lakehouse_engine/utils/extraction/jdbc_extraction_utils.html#JDBCExtraction.__init__\"><strong>JDBC Extractions arguments</strong></a></p>\n\n<p><a href=\"../../lakehouse_engine/utils/extraction/sap_bw_extraction_utils.html#SAPBWExtraction.__init__\"><strong>SAP BW Extractions arguments</strong></a></p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>For extractions using the SAP BW reader, you can use the arguments listed in the SAP BW arguments, but also \nthe ones listed in the JDBC extractions, as those are inherited as well.</p>\n\n</div>\n\n<h2 id=\"extraction-from-sap-bw-template\">Extraction from SAP-BW template</h2>\n\n<p>This template covers the following scenarios of extractions from the SAP BW DSOs:</p>\n\n<ul>\n<li>1 - The Simplest Scenario (Not parallel - Not Recommended)</li>\n<li>2 - Parallel extraction\n<ul>\n<li>2.1 - Simplest Scenario</li>\n<li>2.2 - Provide upperBound (Recommended)</li>\n<li>2.3 - Automatic upperBound (Recommended)</li>\n<li>2.4 - Backfilling</li>\n<li>2.5 - Provide predicates (Recommended)</li>\n<li>2.6 - Generate predicates (Recommended)</li>\n</ul></li>\n<li>3 - Extraction from Write Optimized DSO\n<ul>\n<li>3.1 - Get initial actrequest_timestamp from Activation Requests Table</li>\n</ul></li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Introductory Notes: if you want to have a better understanding about JDBC Spark optimizations, \nhere you have a few useful links:</p>\n\n<ul>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></li>\n<li><a href=\"https://docs.databricks.com/en/connect/external-systems/jdbc.html\">https://docs.databricks.com/en/connect/external-systems/jdbc.html</a></li>\n<li><a href=\"https://bit.ly/3x2eCEm\">https://bit.ly/3x2eCEm</a></li>\n<li><a href=\"https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source\">https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source</a></li>\n</ul>\n\n</div>\n\n<h3 id=\"1-the-simplest-scenario-not-parallel-not-recommended\">1 - The Simplest Scenario (Not parallel - Not Recommended)</h3>\n\n<p>This scenario is the simplest one, not taking any advantage of Spark JDBC optimisation techniques <br />\nand using a single connection to retrieve all the data from the source. It should only be used in case the DSO \nyou want to extract from SAP BW is a small one, with no big requirements in terms of performance to fulfill.\nWhen extracting from the source DSO, there are two options:</p>\n\n<ul>\n<li><strong>Delta Init</strong> - full extraction of the source DSO. You should use it in the first time you extract from the \nDSO or any time you want to re-extract completely. Similar to a so-called full load.</li>\n<li><strong>Delta</strong> - extracts the portion of the data that is new or has changed in the source, since the last\nextraction (using the max <code>actrequest_timestamp</code> value in the location of the data already extracted,\nby default).</li>\n</ul>\n\n<p>Below example is composed of two cells.</p>\n\n<ul>\n<li>The first cell is only responsible to define the variables <code>extraction_type</code> and <code>write_type</code>,\ndepending on the extraction type <strong>Delta Init</strong> (<code>LOAD_TYPE = INIT</code>) or a <strong>Delta</strong> (<code>LOAD_TYPE = DELTA</code>).\nThe variables in this cell will also be referenced by other acons/examples in this notebook, similar to what\nyou would do in your pipelines/jobs, defining this centrally and then re-using it.</li>\n<li>The second cell is where the acon to be used is defined (which uses the two variables <code>extraction_type</code> and\n<code>write_type</code> defined) and the <code>load_data</code> algorithm is executed to perform the extraction.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>There may be cases where you might want to always extract fully from the source DSO. In these cases,\nyou only need to use a Delta Init every time, meaning you would use <code>\"extraction_type\": \"init\"</code> and\n<code>\"write_type\": \"overwrite\"</code> as it is shown below. The explanation about what it is a Delta Init/Delta is\napplicable for all the scenarios presented in this notebook.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"c1\"># You should use this custom reader to benefit from the lakehouse-engine utils for extractions from SAP BW</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_changelog_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"2-parallel-extraction\">2 - Parallel extraction</h3>\n\n<p>In this section, 6 possible scenarios for parallel extractions from SAP BW DSOs.</p>\n\n<h4 id=\"21-parallel-extraction-simplest-scenario\">2.1 - Parallel Extraction, Simplest Scenario</h4>\n\n<p>This scenario provides the simplest example you can have for a parallel extraction from SAP BW, only using\nthe property <code>numPartitions</code>. The goal of the scenario is to cover the case in which people does not have\nmuch knowledge around how to optimize the extraction from JDBC sources or cannot identify a column that can\nbe used to split the extraction in several tasks. This scenario can also be used if the use case does not\nhave big performance requirements/concerns, meaning you do not feel the need to optimize the performance of\nthe extraction to its maximum potential. \nOn the example below, <code>\"numPartitions\": 10</code> is specified, meaning that Spark will open 10 parallel connections\nto the source DSO and automatically decide how to parallelize the extraction upon that requirement. This is the\nonly change compared to the example provided in the example 1.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_changelog_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"22-parallel-extraction-provide-upper_bound-recommended\">2.2 - Parallel Extraction, Provide upper_bound (Recommended)</h4>\n\n<p>This scenario performs the extraction from the SAP BW DSO in parallel, but is more concerned with trying to\noptimize and have more control (compared to 2.1 example) on how the extraction is split and performed, using\nthe following options:</p>\n\n<ul>\n<li><code>numPartitions</code> - number of Spark partitions to split the extraction.</li>\n<li><code>partitionColumn</code> - column used to split the extraction. It must be a numeric, date, or timestamp.\nIt should be a column that is able to split the extraction evenly in several tasks. An auto-increment\ncolumn is usually a very good candidate.</li>\n<li><code>lowerBound</code> - lower bound to decide the partition stride.</li>\n<li><code>upperBound</code> - upper bound to decide the partition stride. It can either be <strong>provided (as it is done in\nthis example)</strong> or derived automatically by our upperBound optimizer (example 2.3).</li>\n</ul>\n\n<p>This is an adequate example for you to follow if you have/know a column in the DSO that is good to be used as\nthe <code>partitionColumn</code>. If you compare with the previous example, you'll notice that now <code>numPartitions</code> and\nthree additional options are provided to fine tune the extraction (<code>partitionColumn</code>, <code>lowerBound</code>,\n<code>upperBound</code>).</p>\n\n<p>When these 4 properties are used, Spark will use them to build several queries to split the extraction.</p>\n\n<p><strong>Example:</strong> for <code>\"numPartitions\": 10</code>, <code>\"partitionColumn\": \"record\"</code>, <code>\"lowerBound: 1\"</code>, <code>\"upperBound: 100\"</code>,\nSpark will generate 10 queries like this:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &lt; 10 OR RECORD IS NULL</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 10 AND RECORD &lt; 20</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 20 AND RECORD &lt; 30</code></li>\n<li>...</li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 100</code></li>\n</ul>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_changelog_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_partition_col&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;upperBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">42</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"23-parallel-extraction-automatic-upper_bound-recommended\">2.3 - Parallel Extraction, Automatic upper_bound (Recommended)</h4>\n\n<p>This scenario is very similar to 2.2, the only difference being that <strong>upper_bound\nis not provided</strong>. Instead, the property <code>calculate_upper_bound</code> equals to true is used to benefit\nfrom the automatic calculation of the upperBound (derived from the <code>partitionColumn</code>) offered by the\nlakehouse-engine framework, which is useful, as in most of the cases you will probably not be aware of\nthe max value for the column. The only thing you need to consider is that if you use this automatic\ncalculation of the upperBound you will be doing an initial query to the SAP BW DSO to retrieve the max\nvalue for the <code>partitionColumn</code>, before doing the actual query to perform the extraction.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;calculate_upper_bound&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_changelog_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_partition_col&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"24-parallel-extraction-backfilling\">2.4 - Parallel Extraction, Backfilling</h4>\n\n<p>This scenario covers the case, in which you might want to backfill the data extracted from a SAP BW DSO and\nmade available in the bronze layer. By default, the delta extraction considers the max value of the column\n<code>actrequest_timestamp</code> on the data already extracted. However, there might be cases, in which you might want\nto extract a delta from a particular timestamp onwards or for a particular interval of time. For this, you\ncan use the properties <code>min_timestamp</code> and <code>max_timestamp</code>.</p>\n\n<p>Below, a very similar example to the previous one is provided, the only differences being that\nthe properties <code>\"min_timestamp\": \"20210910000000\"</code> and <code>\"max_timestamp\": \"20210913235959\"</code> are not provided,\nmeaning it will extract the data from the changelog table, using a filter\n<code>\"20210910000000\" &gt; actrequest_timestamp &lt;= \"20210913235959\"</code>, ignoring if some of the data is already\navailable in the destination or not. Moreover, note that the property <code>latest_timestamp_data_location</code>\ndoes not need to be provided, as the timestamps to be considered are being directly provided (if both\nthe timestamps and the <code>latest_timestamp_data_location</code> are provided, the last parameter will have no effect).\nAdditionally, <code>\"extraction_type\": \"delta\"</code> and <code>\"write_type\": \"append\"</code> is forced, instead of using the\nvariables as in the other  examples, because the backfilling scenario only makes sense for delta extractions.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Note: be aware that the backfilling example being shown has no mechanism to enforce that\nyou don't generate duplicated data in bronze. For your scenarios, you can either use this example and solve\nany duplication in the silver layer or extract the delta with a merge strategy while writing to bronze,\ninstead of appending.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;calculate_upper_bound&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_changelog_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_partition_col&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;min_timestamp&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;20210910000000&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;max_timestamp&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;20210913235959&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"25-parallel-extraction-provide-predicates-recommended\">2.5 - Parallel Extraction, Provide Predicates (Recommended)</h4>\n\n<p>This scenario performs the extraction from SAP BW DSO in parallel, useful in contexts in which there is no\nnumeric, date or timestamp column to parallelize the extraction:</p>\n\n<ul>\n<li><code>partitionColumn</code> - column used to split the extraction. It can be of any type. </li>\n</ul>\n\n<p>This is an adequate example for you to follow if you have/know a column in the DSO that is good to be used as\nthe <code>partitionColumn</code>, specially if these columns are not complying with the scenarios 2.2 and 2.3 (otherwise\nthose would probably be recommended).</p>\n\n<p><strong>When this property is used all predicates need to be provided to Spark, otherwise it will leave data behind.</strong></p>\n\n<p>Below the lakehouse function to generate predicate list automatically is presented.</p>\n\n<p>This function needs to be used carefully, specially on predicates_query and predicates_add_null variables.</p>\n\n<p><strong>predicates_query:</strong> At the sample below the whole table is being considered (<code>select distinct(x) from table</code>),\nbut it is possible to filter predicates list here,\nspecially if you are applying filter on transformations spec, and you know entire table won't be necessary, so\nyou can change it to something like this: <code>select distinct(x) from table where x &gt; y</code>.</p>\n\n<p><strong>predicates_add_null:</strong> You can decide if you want to consider null on predicates list or not, by default this\nproperty is True.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"c1\"># import the lakehouse_engine ExecEnv class, so that you can use the functions it offers</span>\n<span class=\"c1\"># import the lakehouse_engine extraction utils, so that you can use the JDBCExtractionUtils offered functions</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.core.exec_env</span> <span class=\"kn\">import</span> <span class=\"n\">ExecEnv</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.utils.extraction.jdbc_extraction_utils</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">,</span>\n    <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">get_or_create</span><span class=\"p\">()</span>\n\n<span class=\"n\">partition_column</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_partition_column&quot;</span>\n<span class=\"n\">dbtable</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span>\n\n<span class=\"n\">predicates_query</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;(SELECT DISTINCT(</span><span class=\"si\">{</span><span class=\"n\">partition_column</span><span class=\"si\">}</span><span class=\"s2\">) FROM </span><span class=\"si\">{</span><span class=\"n\">dbtable</span><span class=\"si\">}</span><span class=\"s2\">)&quot;&quot;&quot;</span>\n<span class=\"n\">column_for_predicates</span> <span class=\"o\">=</span> <span class=\"n\">partition_column</span>\n<span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_user&quot;</span>\n<span class=\"n\">password</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_bw_url&quot;</span>\n<span class=\"n\">predicates_add_null</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n<span class=\"n\">jdbc_util</span> <span class=\"o\">=</span> <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">(</span>\n        <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">user</span><span class=\"p\">,</span>\n        <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span><span class=\"p\">,</span>\n        <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">url</span><span class=\"p\">,</span>\n        <span class=\"n\">dbtable</span><span class=\"o\">=</span><span class=\"n\">dbtable</span><span class=\"p\">,</span>\n        <span class=\"n\">partition_column</span><span class=\"o\">=</span><span class=\"n\">partition_column</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">predicates</span> <span class=\"o\">=</span> <span class=\"n\">jdbc_util</span><span class=\"o\">.</span><span class=\"n\">get_predicates</span><span class=\"p\">(</span><span class=\"n\">predicates_query</span><span class=\"p\">)</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;predicates&quot;</span><span class=\"p\">:</span> <span class=\"n\">predicates</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"26-parallel-extraction-generate-predicates-recommended\">2.6 - Parallel Extraction, Generate Predicates (Recommended)</h4>\n\n<p>This scenario performs the extraction from SAP BW DSO in parallel, useful in contexts in which there is no\nnumeric, date or timestamp column to parallelize the extraction:</p>\n\n<ul>\n<li><code>partitionColumn</code> - column used to split the extraction. It can be of any type.</li>\n</ul>\n\n<p>This is an adequate example for you to follow if you have/know a column in the DSO that is good to be used as\nthe <code>partitionColumn</code>, specially if these columns are not complying with the scenarios 2.2 and 2.3 (otherwise\nthose would probably be recommended).</p>\n\n<p>When this property is used, the lakehouse engine will generate the predicates to be used to extract data from\nthe source. What the lakehouse engine does is to check for the init/delta portion of the data,\nwhat are the distinct values of the <code>partitionColumn</code> serving that data. Then, these values will be used by\nSpark to generate several queries to extract from the source in a parallel fashion.\nEach distinct value of the <code>partitionColumn</code> will be a query, meaning that you will not have control over the\nnumber of partitions used for the extraction. For example, if you face a scenario in which you\nare using a <code>partitionColumn</code> <code>LOAD_DATE</code> and for today's delta, all the data (let's suppose 2 million rows) is\nserved by a single <code>LOAD_DATE = 20200101</code>, that would mean Spark would use a single partition\nto extract everything. In this extreme case you would probably need to change your <code>partitionColumn</code>. <strong>Note:</strong>\nthese extreme cases are harder to happen when you use the strategy of the scenarios 2.2/2.3.</p>\n\n<p><strong>Example:</strong> for <code>\"partitionColumn\": \"record\"</code>\nGenerate predicates:</p>\n\n<ul>\n<li><code>SELECT DISTINCT(RECORD) as RECORD FROM dummy_table</code></li>\n<li><code>1</code></li>\n<li><code>2</code></li>\n<li><code>3</code></li>\n<li>...</li>\n<li><code>100</code></li>\n<li>Predicates List: ['RECORD=1','RECORD=2','RECORD=3',...,'RECORD=100']</li>\n</ul>\n\n<p>Spark will generate 100 queries like this:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 1</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 2</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 3</code></li>\n<li>...</li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD = 100</code></li>\n</ul>\n\n<p>Generate predicates will also consider null by default:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD IS NULL</code></li>\n</ul>\n\n<p>To disable this behaviour the following variable value should be changed to false: <code>\"predicates_add_null\": False</code></p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;generate_predicates&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_partition_col&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"3-extraction-from-write-optimized-dsos\">3 - Extraction from Write Optimized DSOs</h3>\n\n<p>This scenario is based on the best practices of the scenario 2.2, but it is ready to extract data from\nWrite Optimized DSOs, which have the changelog embedded in the active table, instead of having a separate\nchangelog table. Due to this reason, you need to specify that the <code>changelog_table</code> parameter value is equal\nto the <code>dbtable</code> parameter value.\nMoreover, these tables usually already include the changelog technical columns\nlike <code>RECORD</code> and <code>DATAPAKID</code>, for example, that the framework adds by default. Thus, you need to specify\n<code>\"include_changelog_tech_cols\": False</code> to change this behaviour.\nFinally, you also need to specify the name of the column in the table that can be used to join with the\nactivation requests table to get the timestamp of the several requests/deltas,\nwhich is <code>\"actrequest\"</code> by default (<code>\"request_col_name\": 'request'</code>).</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">LOAD_TYPE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;INIT&quot;</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;DELTA&quot;</span>\n\n<span class=\"k\">if</span> <span class=\"n\">LOAD_TYPE</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;INIT&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;init&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;overwrite&quot;</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">extraction_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;delta&quot;</span>\n    <span class=\"n\">write_type</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;append&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;request_col_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;request&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;include_changelog_tech_cols&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">extraction_type</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;upperBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50000</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"n\">write_type</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"31-extraction-from-write-optimized-dsos-get-actrequest_timestamp-from-activation-requests-table\">3.1 - Extraction from Write Optimized DSOs, Get ACTREQUEST_TIMESTAMP from Activation Requests Table</h4>\n\n<p>By default, the act_request_timestamp has being hardcoded (either assumes a given extraction_timestamp or the\ncurrent timestamp) in the init extraction, however this may be causing problems when merging changes in silver,\nfor write optimised DSOs. So, a new possibility to choose when to retrieve this timestamp from the\nact_req_table was added.</p>\n\n<p>This scenario performs the data extraction from Write Optimized DSOs, forcing the actrequest_timestamp to\nassume the value from the activation requests table (timestamp column).</p>\n\n<p>This feature is only available for WODSOs and to use it you need to specify <code>\"get_timestamp_from_actrequest\": True</code>.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sap_bw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_hana_pwd&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_bw_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;changelog_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;odsobject&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_ods_object&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;request_col_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;request&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;include_changelog_tech_cols&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;latest_timestamp_data_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_ACTREQUEST_TIMESTAMP/&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;extraction_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;init&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;upperBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;get_timestamp_from_act_request&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/my_identifier_ACTREQUEST_TIMESTAMP&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h2 id=\"how-can-we-decide-the-partitioncolumn\">How can we decide the partitionColumn?</h2>\n\n<p><strong>Compatible partitionColumn for upperBound/lowerBound Spark options:</strong></p>\n\n<p>It needs to be <strong>int, date, timestamp</strong> \u2192 <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p>\n\n<p><strong>If you don't have any column to partition on those formats, you can use predicates to partition the table</strong> \u2192 <a href=\"https://docs.databricks.com/en/connect/external-systems/jdbc.html#manage-parallelism\">https://docs.databricks.com/en/connect/external-systems/jdbc.html#manage-parallelism</a></p>\n\n<p>One of the most important parameters to optimise the extraction is the <strong>partitionColumn</strong>, as you can see in the template. Thus, this section helps you figure out if a column is a good candidate or not. </p>\n\n<p>Basically the partition column needs to be a column which is able to adequately split the processing, which means we can use it to \"create\" different queries with intervals/filters, so that the Spark tasks process similar amounts of rows/volume. Usually a good candidate is an integer auto-increment technical column.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"although-record-is-usually-a-good-candidate-it-is-usually-available-on-the-changelog-table-only-meaning-that-you-would-need-to-use-a-different-strategy-for-the-init-in-case-you-dont-have-good-candidates-for-partitioncolumn-you-can-use-the-sample-acon-provided-in-the-scenario-21-in-the-template-above-it-might-make-sense-to-use-scenario-21-for-the-init-and-then-scenario-22-or-23-for-the-subsequent-deltas\">Although RECORD is usually a good candidate, it is usually available on the changelog table only. Meaning that you would need to use a different strategy for the init. In case you don't have good candidates for partitionColumn, you can use the sample acon provided in the <strong>scenario 2.1</strong> in the template above. It might make sense to use <strong>scenario 2.1</strong> for the init and then <strong>scenario 2.2 or 2.3</strong> for the subsequent deltas.</h6>\n\n</div>\n\n<p><strong>When there is no int, date or timestamp good candidate for partitionColumn:</strong></p>\n\n<p>In this case you can opt by the <strong>scenario 2.5 - Generate Predicates</strong>, which supports any kind of column to be defined as <strong>partitionColumn</strong>.</p>\n\n<p>However, you should still analyse if the column you are thinking about is a good candidate or not. In this scenario, Spark will create one query per distinct value of the <strong>partitionColumn</strong>, so you can perform some analysis.</p>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.extract_from_sftp", "modulename": "lakehouse_engine_usage.data_loader.extract_from_sftp", "kind": "module", "doc": "<h1 id=\"extract-from-sftp\">Extract from SFTP</h1>\n\n<p>Secure File Transfer Protocol (SFTP) is a file protocol for transferring files over the web.</p>\n\n<p>This feature is available in the Lakehouse Engine with the purpose of having a mechanism to read data directly from SFTP directories without moving those files manually/physically to a S3 bucket.</p>\n\n<p>The engine uses Pandas to read the files and converts them into a Spark dataframe, which makes the available resources of an Acon usable, such as <code>dq_specs</code>, <code>output_specs</code>, <code>terminator_specs</code> and <code>transform_specs</code>.</p>\n\n<p>Furthermore, this feature provides several filters on the directories that makes easier to control the extractions.</p>\n\n<h4 id=\"introductory-notes\"><strong>Introductory Notes</strong>:</h4>\n\n<p>There are important parameters that must be added to <strong>input specs</strong> in order to make the SFTP extraction work properly:</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"read-type-br-the-engine-supports-only-batch-mode-for-this-feature\"><strong>Read type</strong> <br> The engine supports only <strong>BATCH</strong> mode for this feature.</h6>\n\n</div>\n\n<p><strong>sftp_files_format</strong> - File format that will be used to read data from SFTP. <strong>The engine supports: CSV, FWF, JSON and XML</strong>.</p>\n\n<p><strong>location</strong> - The SFTP directory to be extracted. If it is necessary to filter a specific file, it can be made using the <code>file_name_contains</code> option.</p>\n\n<p><strong>options</strong> - Arguments used to set the Paramiko SSH client connection (hostname, username, password, port...), set the filter to retrieve files and set the file parameters (separators, headers, cols...). For more information about the file parameters, please go to the Pandas link in the useful links section.</p>\n\n<p>The options allowed are:</p>\n\n<table>\n<thead>\n<tr>\n  <th>Property type</th>\n  <th>Detail</th>\n  <th>Example</th>\n  <th>Comment</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>Connection</td>\n  <td>add_auto_policy(str)</td>\n  <td>true of false</td>\n  <td>Indicates to allow an SFTP connection using no host key. When a connection attempt is being made using no host key, then the engine will throw an exception if the auto_add_policy property is false. The purpose of this flag is to make the user conscientiously choose a lesser secure connection.</td>\n</tr>\n<tr>\n  <td>Connection</td>\n  <td>key_type (str)</td>\n  <td>\"Ed25519\" or \"RSA\"</td>\n  <td>Indicates the key type to be used for the connection (SSH, Ed25519).</td>\n</tr>\n<tr>\n  <td>Connection</td>\n  <td>key_filename (str)</td>\n  <td>\"/path/to/private_key/private_key.ppk\"</td>\n  <td>The filename, or list of filenames, of optional private(keys), and/or certs to try for authentication. It must be used with a pkey in order to add a policy. If a pkey is not provided, then use <code>add_auto_policy</code>.</td>\n</tr>\n<tr>\n  <td>Connection</td>\n  <td>pkey (str)</td>\n  <td>\"AAAAC3MidD1lVBI1NTE5AAAAIKssLqd6hjahPi9FBH4GPDqMqwxOMsfxTgowqDCQAeX+\"</td>\n  <td>Value to use for the host key when connecting to the remote SFTP server.</td>\n</tr>\n<tr>\n  <td>Filter</td>\n  <td>date_time_gt (str)</td>\n  <td>\"1900-01-01\" or \"1900-01-01 08:59:59\"</td>\n  <td>Filter the files greater than the string datetime formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\"</td>\n</tr>\n<tr>\n  <td>Filter</td>\n  <td>date_time_lt (str)</td>\n  <td>\"3999-12-31\" or \"3999-12-31 20:59:59\"</td>\n  <td>Filter the files lower than the string datetime formatted as \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\"</td>\n</tr>\n<tr>\n  <td>Filter</td>\n  <td>earliest_file (bool)</td>\n  <td>true or false</td>\n  <td>Filter the earliest dated file in the directory.</td>\n</tr>\n<tr>\n  <td>Filter</td>\n  <td>file_name_contains (str)</td>\n  <td>\"part_of_filename\"</td>\n  <td>Filter files when match the pattern.</td>\n</tr>\n<tr>\n  <td>Filter</td>\n  <td>latest_file (bool)</td>\n  <td>true or false</td>\n  <td>Filter the most recent dated file in the directory.</td>\n</tr>\n<tr>\n  <td>Read data from subdirectories</td>\n  <td>sub_dir (bool)</td>\n  <td>true or false</td>\n  <td>The engine will search files into subdirectories of the <strong>location</strong>. It will consider one level below the root location given.<br>When <code>sub_dir</code> is used with <strong>latest_file/earliest_file</strong> argument, the engine will retrieve the latest/earliest file for each subdirectory.</td>\n</tr>\n<tr>\n  <td>Add metadata info</td>\n  <td>file_metadata (bool)</td>\n  <td>true or false</td>\n  <td>When this option is set as True, the dataframe retrieves the <strong>filename with location</strong> and the <strong>modification_time</strong> from the original files in sftp. It attaches these two columns adding the information to respective records.</td>\n</tr>\n</tbody>\n</table>\n\n<p><strong>Useful Info &amp; Links</strong>:</p>\n\n<ol>\n<li><a href=\"https://docs.paramiko.org/en/latest/api/client.html\">Paramiko SSH Client</a></li>\n<li><a href=\"https://pandas.pydata.org/docs/reference/io.html\">Pandas documentation</a></li>\n</ol>\n\n<h2 id=\"scenario-1\">Scenario 1</h2>\n\n<p>The scenario below shows the extraction of a CSV file using most part of the available filter options. Also, as an example, the column \"created_on\" is created in the transform_specs in order to store the processing date for every record. As the result, it will have in the output table the original file date (provided by the option <code>file_metadata</code>) and the processing date from the engine.</p>\n\n<p>For an incremental load approach, it is advised to use the \"modification_time\" column created by the option <code>file_metadata</code>. Since it has the original file date of modification, this date can be used in the logic to control what is new and has been changed recently.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"below-scenario-uses-add_auto_policy-true-which-is-not-recommended\">Below scenario uses <strong>\"add_auto_policy\": true</strong>, which is <strong>not recommended</strong>.</h6>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n      <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_source&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;sftp_files_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_data_path&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n              <span class=\"s2\">&quot;hostname&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_hostname&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;username&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_username&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_password&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;port&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_port&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;add_auto_policy&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;file_name_contains&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;test_pattern&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;sep&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">},</span>\n              <span class=\"s2\">&quot;latest_file&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;file_metadata&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n          <span class=\"p\">}</span>\n      <span class=\"p\">},</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n      <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_transformations&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_source&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"p\">{</span>\n                  <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;with_literals&quot;</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;literals&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;created_on&quot;</span><span class=\"p\">:</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">()}},</span>\n              <span class=\"p\">},</span>\n          <span class=\"p\">],</span>\n      <span class=\"p\">},</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_transformations&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/dummy_table&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h2 id=\"scenario-2\">Scenario 2</h2>\n\n<p>The following scenario shows the extraction of a JSON file using an RSA pkey authentication instead of auto_add_policy. The engine supports Ed25519Key and RSA for pkeys.</p>\n\n<p>For the pkey file location, it is important to have the file in a location accessible by the cluster. This can be achieved either by mounting the location or with volumes.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-scenario-uses-a-more-secure-authentication-thus-it-is-the-recommended-option-instead-of-the-previous-scenario\">This scenario uses a more secure authentication, thus it is the recommended option, instead of the previous scenario.</h6>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n      <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_source&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;sftp_files_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;json&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_data_path&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n              <span class=\"s2\">&quot;hostname&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_hostname&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;username&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_username&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sftp_password&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;port&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_port&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;key_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RSA&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;key_filename&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dbfs_mount_location/my_file_key.ppk&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;pkey&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_key&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;latest_file&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;file_metadata&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;lines&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s2\">&quot;orient&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;columns&quot;</span><span class=\"p\">},</span>\n          <span class=\"p\">},</span>\n      <span class=\"p\">},</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n      <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_transformations&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_source&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"p\">{</span>\n                  <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;with_literals&quot;</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;literals&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;lh_created_on&quot;</span><span class=\"p\">:</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">()}},</span>\n              <span class=\"p\">},</span>\n          <span class=\"p\">],</span>\n      <span class=\"p\">},</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sftp_transformations&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/dummy_table&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.extract_using_jdbc_connection", "modulename": "lakehouse_engine_usage.data_loader.extract_using_jdbc_connection", "kind": "module", "doc": "<h1 id=\"extract-using-jdbc-connection\">Extract using JDBC connection</h1>\n\n<div class=\"pdoc-alert pdoc-alert-danger\">\n\n<h6 id=\"sap-extraction\"><strong>SAP Extraction</strong></h6>\n\n<p>SAP is only used as an example to demonstrate how we can use a JDBC connection to extract data.</p>\n\n<p><strong>If you are looking to extract data from SAP, please use our sap_b4 or sap_bw reader.</strong></p>\n\n<p>You can find the <strong>sap_b4 reader</strong> documentation: <a href=\"../../lakehouse_engine_usage/data_loader/extract_from_sap_b4_adso.html\">Extract from SAP B4 ADSOs</a> and the <strong>sap_bw reader</strong> documentarion: <a href=\"../../lakehouse_engine_usage/data_loader/extract_from_sap_bw_dso.html\">Extract from SAP BW DSOs</a></p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-danger\">\n\n<h6 id=\"parallel-extraction\"><strong>Parallel Extraction</strong></h6>\n\n<p>Parallel extractions <strong>can bring a jdbc source down</strong> if a lot of stress is put on the system. Be careful choosing the number of partitions. Spark is a distributed system and can lead to many connections.</p>\n\n</div>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Many databases allow a JDBC connection to extract data. Our engine has one reader where you can configure all the necessary definitions to connect to a database using JDBC.</p>\n\n<p>In the next section you will find several examples about how to do it.</p>\n\n<h2 id=\"the-simplest-scenario-using-sqlite\">The Simplest Scenario using sqlite</h2>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<h6 id=\"not-parallel-recommended-for-smaller-datasets-only-or-when-stressing-the-source-system-is-a-high-concern\">Not parallel -  Recommended for smaller datasets only, or when stressing the source system is a high concern</h6>\n\n</div>\n\n<p>This scenario is the simplest one we can have, not taking any advantage of Spark JDBC optimisation techniques and using a single connection to retrieve all the data from the source.</p>\n\n<p>Here we use a sqlite database where any connection is allowed. Due to that, we do not specify any username or password.</p>\n\n<p>Same as spark, we provide two different ways to run jdbc reader.</p>\n\n<p>1 - We can use the <strong>jdbc() function</strong>, passing inside all the arguments needed for Spark to work, and we can even combine this with additional options passed through .options().</p>\n\n<p>2 - Other way is using <strong>.format(\"jdbc\")</strong> and pass all necessary arguments through .options(). It's important to say by choosing jdbc() we can also add options() to the execution.</p>\n\n<p><strong>You can find and run the following code in our local test for the engine.</strong></p>\n\n<h3 id=\"jdbc-function\">jdbc() function</h3>\n\n<p>As we can see in the next cell, all the arguments necessary to establish the jdbc connection are passed inside the <code>jdbc_args</code> object. Here we find the url, the table, and the driver. Besides that, we can add options, such as the partition number. The partition number will impact in the queries' parallelism.</p>\n\n<p>The below code is an example in how to use jdbc() function in our ACON.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;url&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_function/correct_arguments/tests.db&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;table&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;jdbc_function&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;properties&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;driver&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;org.sqlite.JDBC&quot;</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;numPartitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;db_table&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;test_db.jdbc_function_table&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;date&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">],</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/jdbc_reader/jdbc_function/correct_arguments/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>This is same as using the following code in pyspark:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">jdbc</span><span class=\"p\">(</span>\n  <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s2\">&quot;jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_function/correct_arguments/tests.db&quot;</span><span class=\"p\">,</span>\n  <span class=\"n\">table</span><span class=\"o\">=</span><span class=\"s2\">&quot;jdbc_function&quot;</span><span class=\"p\">,</span>\n  <span class=\"n\">properties</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span><span class=\"s2\">&quot;org.sqlite.JDBC&quot;</span><span class=\"p\">})</span>\n  <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"p\">(</span><span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"formatjdbc\">.format(\"jdbc\")</h3>\n\n<p>In this example we do not use the <code>jdbc_args</code> object. All the jdbc connection parameters are inside the dictionary with the object options.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;url&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_format/correct_arguments/tests.db&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;dbtable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;jdbc_format&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;driver&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;org.sqlite.JDBC&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;numPartitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;db_table&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;test_db.jdbc_format_table&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;date&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">],</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/jdbc_reader/jdbc_format/correct_arguments/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>This is same as using the following code in pyspark:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"p\">(</span><span class=\"s2\">&quot;url&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;jdbc:sqlite:/app/tests/lakehouse/in/feature/jdbc_reader/jdbc_format/correct_arguments/tests.db&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"p\">(</span><span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;org.sqlite.JDBC&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"p\">(</span><span class=\"s2\">&quot;dbtable&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;jdbc_format&quot;</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"p\">(</span><span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h2 id=\"template-with-more-complete-and-runnable-examples\">Template with more complete and runnable examples</h2>\n\n<p>In this template we will use a <strong>SAP as example</strong> for a more complete and runnable example.\nThese definitions can be used in several databases that allow JDBC connection.</p>\n\n<p>The following scenarios of extractions are covered:</p>\n\n<ul>\n<li>1 - The Simplest Scenario (Not parallel -  Recommended for smaller datasets only,\nor when stressing the source system is a high concern)</li>\n<li>2 - Parallel extraction \n<ul>\n<li>2.1 - Simplest Scenario </li>\n<li>2.2 - Provide upperBound (Recommended)</li>\n<li>2.3 - Provide predicates (Recommended)</li>\n</ul></li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Disclaimer: This template only uses <strong>SAP as demonstration example for JDBC connection.</strong>\n<strong>This isn't a SAP template!!!</strong>\n<strong>If you are looking to extract data from SAP, please use our sap_b4 reader or the sap_bw reader.</strong></p>\n\n</div>\n\n<p>The JDBC connection has 2 main sections to be filled, the <strong>jdbc_args</strong> and <strong>options</strong>:</p>\n\n<ul>\n<li>jdbc_args - Here you need to fill everything related to jdbc connection itself, like table/query, url, user,\n..., password.</li>\n<li>options - This section is more flexible, and you can provide additional options like \"fetchSize\", \"batchSize\",\n\"numPartitions\", ..., upper and \"lowerBound\".</li>\n</ul>\n\n<p>If you want to know more regarding jdbc spark options you can follow the link below:</p>\n\n<ul>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></li>\n</ul>\n\n<p>If you want to have a better understanding about JDBC Spark optimizations, you can find them in the following:</p>\n\n<ul>\n<li><a href=\"https://docs.databricks.com/en/connect/external-systems/jdbc.html\">https://docs.databricks.com/en/connect/external-systems/jdbc.html</a></li>\n<li><a href=\"https://stackoverflow.com/questions/41085238/what-is-the-meaning-of-partitioncolumn-lowerbound-upperbound-numpartitions-pa\">https://stackoverflow.com/questions/41085238/what-is-the-meaning-of-partitioncolumn-lowerbound-upperbound-numpartitions-pa</a></li>\n<li><a href=\"https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source\">https://newbedev.com/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source</a></li>\n</ul>\n\n<h3 id=\"1-the-simplest-scenario-not-parallel-recommended-for-smaller-datasets-or-for-not-stressing-the-source\">1 - The Simplest Scenario (Not parallel - Recommended for smaller datasets, or for not stressing the source)</h3>\n\n<p>This scenario is the simplest one we can have, not taking any advantage of Spark JDBC optimisation techniques\nand using a single connection to retrieve all the data from the source. It should only be used in case the data\nyou want to extract from is a small one, with no big requirements in terms of performance to fulfill.\nWhen extracting from the source, we can have two options:</p>\n\n<ul>\n<li><strong>Delta Init</strong> - full extraction of the source. You should use it in the first time you extract from the\nsource or any time you want to re-extract completely. Similar to a so-called full load.</li>\n<li><strong>Delta</strong> - extracts the portion of the data that is new or has changed in the source, since the last\nextraction (for that, the logic at the transformation step needs to be applied). On the examples below,\nthe logic using REQTSN column is applied, which means that the maximum value on bronze is filtered\nand its value is used to filter incoming data from the data source.</li>\n</ul>\n\n<h5 id=\"init-load-data-into-the-bronze-bucket\">Init - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/no_parallel/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"delta-load-data-into-the-bronze-bucket\">Delta - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_jdbc_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/no_parallel/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">}}],</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/no_parallel/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"2-parallel-extraction\">2 - Parallel extraction</h3>\n\n<p>On this section we present 3 possible scenarios for parallel extractions from JDBC sources.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Disclaimer for parallel extraction: Parallel extractions can bring a jdbc source down if a lot of stress\nis put on the system. <strong>Be careful when choosing the number of partitions. \nSpark is a distributed system and can lead to many connections.</strong></p>\n\n</div>\n\n<h4 id=\"21-parallel-extraction-simplest-scenario\">2.1 - Parallel Extraction, Simplest Scenario</h4>\n\n<p>This scenario provides the simplest example you can have for a parallel extraction from JDBC sources, only using\nthe property <code>numPartitions</code>. The goal of the scenario is to cover the case in which people do not have\nmuch experience around how to optimize the extraction from JDBC sources or cannot identify a column that can\nbe used to split the extraction in several tasks. This scenario can also be used if the use case does not\nhave big performance requirements/concerns, meaning you do not feel the need to optimize the performance of\nthe extraction to its maximum potential.</p>\n\n<p>On the example bellow, <code>\"numPartitions\": 10</code> is specified, meaning that Spark will open 10 parallel connections\nto the source and automatically decide how to parallelize the extraction upon that requirement. This is the\nonly change compared to the example provided in the scenario 1.</p>\n\n<h5 id=\"delta-init-load-data-into-the-bronze-bucket\">Delta Init - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_1/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"delta-load-data-into-the-bronze-bucket-2\">Delta - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_1/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">}}],</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;REQTSN&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_1/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"22-parallel-extraction-provide-upper_bound-recommended\">2.2 - Parallel Extraction, Provide upper_bound (Recommended)</h4>\n\n<p>This scenario performs the extraction from the JDBC source in parallel, but has more concerns trying to\noptimize and have more control (compared to 2.1 example) on how the extraction is split and performed,\nusing the following options:</p>\n\n<ul>\n<li><code>numPartitions</code> - number of Spark partitions to split the extraction.</li>\n<li><code>partitionColumn</code> - column used to split the extraction. It must be a numeric, date, or timestamp.\nIt should be a column that is able to split the extraction evenly in several tasks. An auto-increment\ncolumn is usually a very good candidate.</li>\n<li><code>lowerBound</code> - lower bound to decide the partition stride.</li>\n<li><code>upperBound</code> - upper bound to decide the partition stride.</li>\n</ul>\n\n<p>This is an adequate example to be followed if there is a column in the data source that is good to\nbe used as the <code>partitionColumn</code>. Comparing with the previous example,\nthe <code>numPartitions</code> and three additional options to fine tune the extraction (<code>partitionColumn</code>, <code>lowerBound</code>,\n<code>upperBound</code>) are provided.</p>\n\n<p>When these 4 properties are used, Spark will use them to build several queries to split the extraction.\n<strong>Example:</strong> for <code>\"numPartitions\": 10</code>, <code>\"partitionColumn\": \"record\"</code>, <code>\"lowerBound: 1\"</code>, <code>\"upperBound: 100\"</code>,\nSpark will generate 10 queries like:</p>\n\n<ul>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &lt; 10 OR RECORD IS NULL</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 10 AND RECORD &lt; 20</code></li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 20 AND RECORD &lt; 30</code></li>\n<li>...</li>\n<li><code>SELECT * FROM dummy_table WHERE RECORD &gt;= 100</code></li>\n</ul>\n\n<h5 id=\"init-load-data-into-the-bronze-bucket-2\">Init - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;upperBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">2000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_2/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"delta-load-data-into-the-bronze-bucket-3\">Delta - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;partitionColumn&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;numPartitions&quot;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;lowerBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;upperBound&quot;</span><span class=\"p\">:</span> <span class=\"mi\">2000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_2/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">}}],</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_2/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h4 id=\"23-parallel-extraction-with-predicates-recommended\">2.3 - Parallel Extraction with Predicates (Recommended)</h4>\n\n<p>This scenario performs the extraction from JDBC source in parallel, useful in contexts where there aren't\nnumeric, date or timestamp columns to parallelize the extraction:</p>\n\n<ul>\n<li><code>partitionColumn</code> - column used to split the extraction (can be of any type).</li>\n</ul>\n\n<ul>\n<li>This is an adequate example to be followed if there is a column in the data source that is good to be\nused as the <code>partitionColumn</code>, specially if these columns are not complying with the scenario 2.2.</li>\n</ul>\n\n<p><strong>When this property is used, all predicates to Spark need to be provided, otherwise it will leave data behind.</strong></p>\n\n<p>Bellow, a lakehouse function to generate predicate list automatically, is presented.</p>\n\n<p><strong>By using this function one needs to be careful specially on predicates_query and predicates_add_null variables.</strong></p>\n\n<p><strong>predicates_query:</strong> At the sample below the whole table (<code>select distinct(x) from table</code>) is being considered,\nbut it is possible to filter using predicates list here, specially if you are applying filter on\ntransformations spec, and you know entire table won't be necessary, so you can change it to something like this:\n<code>select distinct(x) from table where x &gt; y</code>.</p>\n\n<p><strong>predicates_add_null:</strong> One can consider if null on predicates list or not. By default, this property is True.\n<strong>Example:</strong> for <code>\"partitionColumn\": \"record\"</code></p>\n\n<h5 id=\"init-load-data-into-the-bronze-bucket-3\">Init - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.core.exec_env</span> <span class=\"kn\">import</span> <span class=\"n\">ExecEnv</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.utils.extraction.jdbc_extraction_utils</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">,</span>\n    <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">get_or_create</span><span class=\"p\">()</span>\n\n<span class=\"n\">partitionColumn</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_partition_col&quot;</span>\n<span class=\"n\">dbtable</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span>\n\n<span class=\"n\">predicates_query</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;(SELECT DISTINCT(</span><span class=\"si\">{</span><span class=\"n\">partitionColumn</span><span class=\"si\">}</span><span class=\"s2\">) FROM </span><span class=\"si\">{</span><span class=\"n\">dbtable</span><span class=\"si\">}</span><span class=\"s2\">)&quot;&quot;&quot;</span>\n<span class=\"n\">column_for_predicates</span> <span class=\"o\">=</span> <span class=\"n\">partitionColumn</span>\n<span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_user&quot;</span>\n<span class=\"n\">password</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span>\n<span class=\"n\">driver</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span>\n<span class=\"n\">predicates_add_null</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n<span class=\"n\">jdbc_util</span> <span class=\"o\">=</span> <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">(</span>\n        <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">user</span><span class=\"p\">,</span>\n        <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span><span class=\"p\">,</span>\n        <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">url</span><span class=\"p\">,</span>\n        <span class=\"n\">predicates_add_null</span><span class=\"o\">=</span><span class=\"n\">predicates_add_null</span><span class=\"p\">,</span>\n        <span class=\"n\">partition_column</span><span class=\"o\">=</span><span class=\"n\">partitionColumn</span><span class=\"p\">,</span>\n        <span class=\"n\">dbtable</span><span class=\"o\">=</span><span class=\"n\">dbtable</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">predicates</span> <span class=\"o\">=</span> <span class=\"n\">jdbc_util</span><span class=\"o\">.</span><span class=\"n\">get_predicates</span><span class=\"p\">(</span><span class=\"n\">predicates_query</span><span class=\"p\">)</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;predicates&quot;</span><span class=\"p\">:</span> <span class=\"n\">predicates</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_3/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h5 id=\"delta-load-data-into-the-bronze-bucket-4\">Delta - Load data into the Bronze Bucket</h5>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.core.exec_env</span> <span class=\"kn\">import</span> <span class=\"n\">ExecEnv</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.utils.extraction.jdbc_extraction_utils</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">,</span>\n    <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">ExecEnv</span><span class=\"o\">.</span><span class=\"n\">get_or_create</span><span class=\"p\">()</span>\n\n<span class=\"n\">partitionColumn</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_partition_col&quot;</span>\n<span class=\"n\">dbtable</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span>\n\n<span class=\"n\">predicates_query</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;(SELECT DISTINCT(</span><span class=\"si\">{</span><span class=\"n\">partitionColumn</span><span class=\"si\">}</span><span class=\"s2\">) FROM </span><span class=\"si\">{</span><span class=\"n\">dbtable</span><span class=\"si\">}</span><span class=\"s2\">)&quot;&quot;&quot;</span>\n<span class=\"n\">column_for_predicates</span> <span class=\"o\">=</span> <span class=\"n\">partitionColumn</span>\n<span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_user&quot;</span>\n<span class=\"n\">password</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span>\n<span class=\"n\">driver</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span>\n<span class=\"n\">predicates_add_null</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n<span class=\"n\">jdbc_util</span> <span class=\"o\">=</span> <span class=\"n\">JDBCExtractionUtils</span><span class=\"p\">(</span>\n    <span class=\"n\">JDBCExtraction</span><span class=\"p\">(</span>\n        <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">user</span><span class=\"p\">,</span>\n        <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span><span class=\"p\">,</span>\n        <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">url</span><span class=\"p\">,</span>\n        <span class=\"n\">predicates_add_null</span><span class=\"o\">=</span><span class=\"n\">predicates_add_null</span><span class=\"p\">,</span>\n        <span class=\"n\">partition_column</span><span class=\"o\">=</span><span class=\"n\">partitionColumn</span><span class=\"p\">,</span>\n        <span class=\"n\">dbtable</span><span class=\"o\">=</span><span class=\"n\">dbtable</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">predicates</span> <span class=\"o\">=</span> <span class=\"n\">jdbc_util</span><span class=\"o\">.</span><span class=\"n\">get_predicates</span><span class=\"p\">(</span><span class=\"n\">predicates_query</span><span class=\"p\">)</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_sap_b4_url&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;predicates&quot;</span><span class=\"p\">:</span> <span class=\"n\">predicates</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_user&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_b4_hana_pwd&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;com.sap.db.jdbc.Driver&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;fetchSize&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100000</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_3/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">}}],</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;max_my_identifier_bronze_date&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_identifier_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;appended_my_identifier&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;partitions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;RECORD&quot;</span><span class=\"p\">],</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_path/jdbc_template/parallel_3/my_identifier/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.filtered_full_load", "modulename": "lakehouse_engine_usage.data_loader.filtered_full_load", "kind": "module", "doc": "<h1 id=\"filtered-full-load\">Filtered Full Load</h1>\n\n<p>This scenario is very similar to the <a href=\"../../lakehouse_engine_usage/data_loader/full_load.html\">full load</a>, but it filters the data coming from the source, instead of doing a complete full load.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;header&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delimiter&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;inferSchema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/full_load/with_filter/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;filtered_sales&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;expression_filter&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;exp&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date like &#39;2016%&#39;&quot;</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;filtered_sales&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;parquet&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/full_load/with_filter/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes:</h5>\n\n<ul>\n<li>As seen in the ACON, the filtering capabilities are provided by a transformer called <code>expression_filter</code>, where you can provide a custom Spark SQL filter.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.filtered_full_load_with_selective_replace", "modulename": "lakehouse_engine_usage.data_loader.filtered_full_load_with_selective_replace", "kind": "module", "doc": "<h1 id=\"filtered-full-load-with-selective-replace\">Filtered Full Load with Selective Replace</h1>\n\n<p>This scenario is very similar to the <a href=\"filtered_full_load.html\">Filtered Full Load</a>, but we only replace a subset of the partitions, leaving the other ones untouched, so we don't replace the entire table. This capability is very useful for backfilling scenarios.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;header&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delimiter&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;inferSchema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/full_load/with_filter_partition_overwrite/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;filtered_sales&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;expression_filter&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;exp&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date like &#39;2016%&#39;&quot;</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;filtered_sales&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;customer&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">],</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/full_load/with_filter_partition_overwrite/data&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;replaceWhere&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date like &#39;2016%&#39;&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes:</h5>\n\n<ul>\n<li>The key option for this scenario in the ACON is the <code>replaceWhere</code>, which we use to only overwrite a specific period of time, that realistically can match a subset of all the partitions of the table. Therefore, this capability is very useful for backfilling scenarios.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.flatten_schema_and_explode_columns", "modulename": "lakehouse_engine_usage.data_loader.flatten_schema_and_explode_columns", "kind": "module", "doc": "<h1 id=\"flatten-schema-and-explode-columns\">Flatten Schema and Explode Columns</h1>\n\n<p>Related with schema, we can make two kind of operations:</p>\n\n<ul>\n<li><p><strong>Flatten Schema</strong>: transformer named \"flatten_schema\" used to flatten the schema of dataframe.</p>\n\n<ul>\n<li>Parameters to be defined:\n<ul>\n<li>max_level: 2 =&gt; this sets the level until you want to flatten the schema.</li>\n<li>shorten_names: True =&gt; this flag is when you want to shorten the name of the prefixes of the fields.</li>\n<li>alias: True =&gt; this flag is used when you want to define a prefix for the column to be flattened.</li>\n<li>num_chars: 7 =&gt; this sets the number of characters to consider when shortening the names of the fields.</li>\n<li>ignore_cols: True =&gt; this list value should be set to specify the columns you don't want to flatten.</li>\n</ul></li>\n</ul></li>\n<li><p><strong>Explode Columns</strong>: transformer named \"explode_columns\" used to explode columns with types ArrayType and MapType. </p>\n\n<ul>\n<li>Parameters to be defined:\n<ul>\n<li>explode_arrays: True =&gt; this flag should be set to true to explode all array columns present in the dataframe.</li>\n<li>array_cols_to_explode: [\"sample_col\"] =&gt; this list value should be set when to specify the array columns desired to explode.</li>\n<li>explode_maps: True =&gt; this flag should be set to true to explode all map columns present in the dataframe.</li>\n<li>map_cols_to_explode: [\"map_col\"] =&gt; this list value should be set when to specify the map columns desired to explode.</li>\n</ul></li>\n<li>Recommendation: use array_cols_to_explode and map_cols_to_explode to specify the columns desired to explode and do not do it for all of them.</li>\n</ul></li>\n</ul>\n\n<p>The below scenario of <strong>flatten_schema</strong> is transforming one or more columns and dividing the content nested in more columns, as desired. We defined the number of levels we want to flatten in the schema, regarding the nested values. In this case, we are just setting <code>max_level</code> of <code>2</code>.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;schema_path&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_schema/source_schema.json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_schema/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;date&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;customer&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;customer2&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;with_expressions&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols_and_exprs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;constant&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;&#39;just a constant&#39;&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;length_customer2&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;length(customer2)&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;from_json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;input_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sample&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;schema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;struct&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;fields&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">                </span><span class=\"p\">{</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field1&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                </span><span class=\"p\">},</span>\n<span class=\"w\">                </span><span class=\"p\">{</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                </span><span class=\"p\">},</span>\n<span class=\"w\">                </span><span class=\"p\">{</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field3&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;double&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                </span><span class=\"p\">},</span>\n<span class=\"w\">                </span><span class=\"p\">{</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field4&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;struct&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;fields&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">                      </span><span class=\"p\">{</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field1&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                      </span><span class=\"p\">},</span>\n<span class=\"w\">                      </span><span class=\"p\">{</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                      </span><span class=\"p\">}</span>\n<span class=\"w\">                    </span><span class=\"p\">]</span>\n<span class=\"w\">                  </span><span class=\"p\">},</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                </span><span class=\"p\">}</span>\n<span class=\"w\">              </span><span class=\"p\">]</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;to_json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;in_cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;amount&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">],</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;out_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;item_amount_json&quot;</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;flatten_schema&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;max_level&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">2</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/transformations/column_reshapers/flatten_schema/batch/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>The scenario of <strong>explode_arrays</strong> is transforming the arrays columns in one or more rows, depending on the number of elements, so, it replicates the row for each array value. In this case we are using explode to all array columns, using <code>explode_arrays</code> as <code>true</code>.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;schema_path&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/explode_arrays/source_schema.json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/explode_arrays/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;date&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;customer&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;customer2&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;with_expressions&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols_and_exprs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;constant&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;&#39;just a constant&#39;&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;length_customer2&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;length(customer2)&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;to_json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;in_cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;amount&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">],</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;out_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;item_amount_json&quot;</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;explode_columns&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;explode_arrays&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/transformations/column_reshapers/explode_arrays/batch/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>The scenario of <strong>flatten_and_explode_arrays_and_maps</strong> is using <code>flatten_schema</code> and <code>explode_columns</code> to have the desired output. In this case, the desired output is to flatten all schema and explode maps and arrays, even having an array inside a struct. Steps:</p>\n\n<pre><code>1. In this case, we have an array column inside a struct column, so first we need to use the `flatten_schema` transformer to extract the columns inside that struct;\n2. Then, we are able to explode all the array columns desired and map columns, using `explode_columns` transformer.\n3. To be able to have the map column in 2 columns, we use again the `flatten_schema` transformer.\n</code></pre>\n\n<p>As for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;schema_path&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_and_explode_arrays_and_maps/source_schema.json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/transformations/column_reshapers/flatten_and_explode_arrays_and_maps/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;date&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;customer&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;customer2&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;with_expressions&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols_and_exprs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;constant&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;&#39;just a constant&#39;&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;length_customer2&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;length(customer2)&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;from_json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;input_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;agg_fields&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;schema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;struct&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"nt\">&quot;fields&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">                </span><span class=\"p\">{</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field1&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{},</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;containsNull&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;elementType&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;array&quot;</span>\n<span class=\"w\">                  </span><span class=\"p\">}</span>\n<span class=\"w\">                </span><span class=\"p\">},</span>\n<span class=\"w\">                </span><span class=\"p\">{</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;struct&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                    </span><span class=\"nt\">&quot;fields&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">                      </span><span class=\"p\">{</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field1&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                      </span><span class=\"p\">},</span>\n<span class=\"w\">                      </span><span class=\"p\">{</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;field2&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                        </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                      </span><span class=\"p\">}</span>\n<span class=\"w\">                    </span><span class=\"p\">]</span>\n<span class=\"w\">                  </span><span class=\"p\">},</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">                  </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">                </span><span class=\"p\">}</span>\n<span class=\"w\">              </span><span class=\"p\">]</span>\n<span class=\"w\">            </span><span class=\"p\">}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;to_json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;in_cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;amount&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">],</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;out_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;item_amount_json&quot;</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;flatten_schema&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;max_level&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">2</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;explode_columns&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;explode_arrays&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;map_cols_to_explode&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;sample&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">]</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;flatten_schema&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;max_level&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">2</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/transformations/column_reshapers/flatten_and_explode_arrays_and_maps/batch/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.full_load", "modulename": "lakehouse_engine_usage.data_loader.full_load", "kind": "module", "doc": "<h1 id=\"full-load\">Full Load</h1>\n\n<p>This scenario reads CSV data from a path and writes in full to another path with delta lake files.</p>\n\n<h5 id=\"relevant-notes\">Relevant notes</h5>\n\n<ul>\n<li>This ACON infers the schema automatically through the option <code>inferSchema</code> (we use it for local tests only). This is usually not a best practice using CSV files, and you should provide a schema through the InputSpec variables <code>schema_path</code>, <code>read_schema_from_table</code> or <code>schema</code>.</li>\n<li>The <code>transform_specs</code> in this case are purely optional, and we basically use the repartition transformer to create one partition per combination of date and customer. This does not mean you have to use this in your algorithm.</li>\n<li>A full load is also adequate for an init load (initial load).</li>\n</ul>\n\n<p>As for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;header&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delimiter&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;inferSchema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/full_load/full_overwrite/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;repartitioned_sales&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;repartition&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;num_partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;cols&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">&quot;customer&quot;</span><span class=\"p\">]</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;customer&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">],</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/full_load/full_overwrite/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.read_from_dataframe", "modulename": "lakehouse_engine_usage.data_loader.read_from_dataframe", "kind": "module", "doc": "<h1 id=\"read-from-dataframe\">Read from Dataframe</h1>\n\n<div class=\"pdoc-alert pdoc-alert-danger\">\n\n<h6 id=\"dont-use-this-feature-if-the-lakehouse-engine-already-has-a-supported-data-format-for-your-use-case-as-in-that-case-it-is-preferred-to-use-the-dedicated-data-formats-which-are-more-extensively-tested-and-predictable-check-the-supported-data-formats-herelakehouse_enginecoredefinitionshtmlinputformat\">Don't use this feature if the Lakehouse Engine already has a supported data format for your use case, as in that case it is preferred to use the dedicated data formats which are more extensively tested and predictable. Check the supported data formats <a href=\"../../lakehouse_engine/core/definitions.html#InputFormat\">here</a>.</h6>\n\n</div>\n\n<p>Reading from a Spark DataFrame is very simple using our framework. You just need to define the input_specs as follows: </p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_df&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;df_name&quot;</span><span class=\"p\">:</span> <span class=\"n\">df</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"why-is-it-relevant\"><strong>Why is it relevant?</strong></h6>\n\n<p>With this capability of reading a dataframe you can deal with sources that do not yet officially have a reader (e.g., REST api, XML files, etc.).</p>\n\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.streaming_append_load_with_malformed", "modulename": "lakehouse_engine_usage.data_loader.streaming_append_load_with_malformed", "kind": "module", "doc": "<h1 id=\"streaming-append-load-with-dropmalformed\">Streaming Append Load with DROPMALFORMED</h1>\n\n<p>This scenario illustrates an append load done via streaming instead of batch, providing an efficient way of picking up new files from an S3 folder, instead of relying on the incremental filtering from the source needed from a batch based process (see append loads in batch from a JDBC source to understand the differences between streaming and batch append loads). However, not all sources (e.g., JDBC) allow streaming.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;header&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delimiter&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;mode&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;DROPMALFORMED&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/append_load/streaming_dropmalformed/data&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;schema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;struct&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;fields&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;customer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;article&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">]</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;db_table&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;test_db.streaming_dropmalformed_table&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;date&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">],</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/append_load/streaming_dropmalformed/checkpoint&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/append_load/streaming_dropmalformed/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes:</h5>\n\n<ul>\n<li>In this scenario, we use DROPMALFORMED read mode, which drops rows that do not comply with the provided schema;</li>\n<li>In this scenario, the schema is provided through the <code>input_spec</code> \"schema\" variable. This removes the need of a separate JSON Spark schema file, which may be more convenient in certain cases.</li>\n<li>As can be seen, we use the <code>output_spec</code> Spark option <code>checkpointLocation</code> to specify where to save the checkpoints indicating what we have already consumed from the input data. This allows fault-tolerance if the streaming job fails, but more importantly, it allows us to run a streaming job using <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\">AvailableNow</a> and the next job automatically picks up the stream state since the last checkpoint, allowing us to do efficient append loads without having to manually specify incremental filters as we do for batch append loads.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.streaming_append_load_with_terminator", "modulename": "lakehouse_engine_usage.data_loader.streaming_append_load_with_terminator", "kind": "module", "doc": "<h1 id=\"streaming-append-load-with-optimize-dataset-terminator\">Streaming Append Load with Optimize Dataset Terminator</h1>\n\n<p>This scenario includes a terminator which optimizes a dataset (table), being able of vacuuming the table, optimising it with z-order or not, computing table statistics and more. You can find more details on the Terminator <a href=\"../../lakehouse_engine/terminators/dataset_optimizer.html\">here</a>.</p>\n\n<p>As for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;header&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delimiter&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;mode&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;DROPMALFORMED&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/append_load/streaming_with_terminators/data&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;schema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;struct&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;fields&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;customer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;article&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;string&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">},</span>\n<span class=\"w\">          </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;name&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;amount&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;integer&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;nullable&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;metadata&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{}</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">]</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;db_table&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;test_db.streaming_with_terminators_table&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">&quot;date&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">],</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/append_load/streaming_with_terminators/checkpoint&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/append_load/streaming_with_terminators/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;terminate_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;optimize_dataset&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;db_table&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;test_db.streaming_with_terminators_table&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;debug&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.streaming_delta_load_with_group_and_rank_condensation", "modulename": "lakehouse_engine_usage.data_loader.streaming_delta_load_with_group_and_rank_condensation", "kind": "module", "doc": "<h1 id=\"streaming-delta-load-with-group-and-rank-condensation\">Streaming Delta Load with Group and Rank Condensation</h1>\n\n<p>This scenario is useful for when we want to do delta loads based on changelogs that need to be first condensed based on a group by and then a rank only, instead of the record mode logic in the record mode based change data capture.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;input_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;read_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;schema_path&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/source_schema.json&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;with_filepath&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;mode&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;FAILFAST&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;header&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delimiter&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;|&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/data&quot;</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;transform_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze_with_extraction_date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;transformers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;with_regex_value&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;input_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;lhe_extraction_filepath&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;output_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;extraction_date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;drop_input_col&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;regex&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;.*WE_SO_SCL_(\\\\d+).csv&quot;</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;with_auto_increment_id&quot;</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;group_and_rank&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;group_key&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;item&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">],</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;ranking_key&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;extraction_date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;changed_on&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">              </span><span class=\"s2\">&quot;lhe_row_id&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">]</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;function&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;repartition&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">          </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;num_partitions&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span>\n<span class=\"w\">          </span><span class=\"p\">}</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">      </span><span class=\"p\">]</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">],</span>\n<span class=\"w\">  </span><span class=\"nt\">&quot;output_specs&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">    </span><span class=\"p\">{</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;spec_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_silver&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;input_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;sales_bronze_with_extraction_date&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;write_type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;data_format&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;location&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/data&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;options&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/group_and_rank/with_duplicates_in_same_file/streaming/checkpoint&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">},</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;with_batch_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">      </span><span class=\"nt\">&quot;merge_opts&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;current.salesorder = new.salesorder and current.item = new.item&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;update_predicate&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;new.extraction_date &gt;= current.extraction_date and new.changed_on &gt;= current.changed_on&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;delete_predicate&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;new.extraction_date &gt;= current.extraction_date and new.changed_on &gt;= current.changed_on and new.event = &#39;deleted&#39;&quot;</span>\n<span class=\"w\">      </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"w\">  </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes:</h5>\n\n<ul>\n<li>This type of delta load with this type of condensation is useful when the source changelog can be condensed based on dates, instead of technical fields like <code>datapakid</code>, <code>record</code>, <code>record_mode</code>, etc., as we see in SAP BW DSOs.An example of such system is Omnihub Tibco orders and deliveries files.</li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.streaming_delta_with_late_arriving_and_out_of_order_events", "modulename": "lakehouse_engine_usage.data_loader.streaming_delta_with_late_arriving_and_out_of_order_events", "kind": "module", "doc": "<h1 id=\"streaming-delta-load-with-late-arriving-and-out-of-order-events-with-and-without-watermarking\">Streaming Delta Load with Late Arriving and Out of Order Events (with and without watermarking)</h1>\n\n<h2 id=\"how-to-deal-with-late-arriving-data-without-using-watermark\">How to Deal with Late Arriving Data without using Watermark</h2>\n\n<p>This scenario covers a delta load in streaming mode that is able to deal with late arriving and out of order events.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"p\">{</span>\n  <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"n\">true</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/in/feature/delta_load/record_mode_cdc/late_arriving_changes/streaming/data&quot;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;transformed_sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n          <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;condense_record_mode_cdc&quot;</span><span class=\"p\">,</span>\n          <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;business_key&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;item&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;ranking_key_desc&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;extraction_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;actrequest_timestamp&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;datapakid&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;partno&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;record&quot;</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;record_mode_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;recordmode&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;valid_record_modes&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n              <span class=\"s2\">&quot;&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;N&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;R&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;D&quot;</span><span class=\"p\">,</span>\n              <span class=\"s2\">&quot;X&quot;</span>\n            <span class=\"p\">]</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n      <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;transformed_sales_source&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;merge&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/late_arriving_changes/streaming/data&quot;</span><span class=\"p\">,</span>\n      <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file:///app/tests/lakehouse/out/feature/delta_load/record_mode_cdc/late_arriving_changes/streaming/checkpoint&quot;</span>\n      <span class=\"p\">},</span>\n      <span class=\"s2\">&quot;merge_opts&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;merge_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;current.salesorder = new.salesorder and current.item = new.item and current.date &lt;=&gt; new.date&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;update_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.extraction_timestamp &gt; current.extraction_timestamp or new.actrequest_timestamp &gt; current.actrequest_timestamp or ( new.actrequest_timestamp = current.actrequest_timestamp and new.datapakid &gt; current.datapakid) or ( new.actrequest_timestamp = current.actrequest_timestamp and new.datapakid = current.datapakid and new.partno &gt; current.partno) or ( new.actrequest_timestamp = current.actrequest_timestamp and new.datapakid = current.datapakid and new.partno = current.partno and new.record &gt;= current.record)&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;delete_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.recordmode in (&#39;R&#39;,&#39;D&#39;,&#39;X&#39;)&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;insert_predicate&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;new.recordmode is null or new.recordmode not in (&#39;R&#39;,&#39;D&#39;,&#39;X&#39;)&quot;</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;spark.sql.streaming.schemaInference&quot;</span><span class=\"p\">:</span> <span class=\"n\">true</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h5 id=\"relevant-notes\">Relevant notes:</h5>\n\n<ul>\n<li>First question we can impose is: Do we need such complicated update predicate to handle late arriving and out of order events? Simple answer is no. Because we expect that the latest event (e.g., latest status of a record in the source) will eventually arrive, and therefore the target delta lake table will eventually be consistent. However, when will that happen? Do we want to have our target table inconsistent until the next update comes along? This of course is only true when your source cannot ensure the order of the changes and cannot avoid late arriving changes (e.g., some changes that should have come in this changelog extraction, will only arrive in the next changelog extraction). From previous experiences, this is not the case with SAP BW, for example (as SAP BW is ACID compliant, and it will extract data from an SAP source and only have the updated changelog available when the extraction goes through, so theoretically we should not be able to extract data from the SAP BW changelog while SAP BW is still extracting data). </li>\n<li>However, when the source cannot fully ensure ordering (e.g., Kafka) and we want to make sure we don't load temporarily inconsistent data into the target table, we can pay extra special attention, as we do here, to our update and insert predicates, that will enable us to only insert or update data if the new event meets the respective predicates:\n<ul>\n<li>In this scenario, we will only update if the <code>update_predicate</code> is true, and that long predicate we have here ensures that the change that we are receiving is likely the latest one;</li>\n<li>In this scenario, we will only insert the record if the record is not marked for deletion (this can happen if the new event is a record that is marked for deletion, but the record was not in the target table (late arriving changes where the delete came before the insert), and therefore, without the <code>insert_predicate</code>, the algorithm would still try to insert the row, even if the <code>record_mode</code> indicates that that row is for deletion. By using the <code>insert_predicate</code> above we avoid that to happen. However, even in such scenario, to prevent the algorithm to insert the data that comes later (which is old, as we said, the delete came before the insert and was actually the latest status), we would even need a more complex predicate based on your data's nature. Therefore, please read the disclaimer below.</li>\n</ul></li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"disclaimer-the-scenario-illustrated-in-this-page-is-purely-fictional-designed-for-the-lakehouse-engine-local-tests-specifically-your-data-source-changelogs-may-be-different-and-the-scenario-and-predicates-discussed-here-may-not-make-sense-to-you-consequently-the-data-product-team-should-reason-about-the-adequate-merge-predicate-and-insert-update-and-delete-predicates-that-better-reflect-how-they-want-to-handle-the-delta-loads-for-their-data\"><strong>Disclaimer</strong>! The scenario illustrated in this page is purely fictional, designed for the Lakehouse Engine local tests specifically. Your data source changelogs may be different and the scenario and predicates discussed here may not make sense to you. Consequently, the data product team should reason about the adequate merge predicate and insert, update and delete predicates, that better reflect how they want to handle the delta loads for their data.</h6>\n\n</div>\n\n<ul>\n<li>We use spark.sql.streaming.schemaInference in our local tests only. We don't encourage you to use it in your data product.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"documentation\"><strong>Documentation</strong></h6>\n\n<p><a href=\"https://www.databricks.com/blog/feature-deep-dive-watermarking-apache-spark-structured-streaming\">Feature Deep Dive: Watermarking in Apache Spark Structured Streaming - The Databricks Blog</a>\n<a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming Programming Guide - Spark 3.4.0 Documentation</a></p>\n\n</div>\n\n<h2 id=\"how-to-deal-with-late-arriving-data-using-watermark\">How to Deal with Late Arriving Data using Watermark</h2>\n\n<p>When building real-time pipelines, one of the realities that teams have to work with is that distributed data ingestion is inherently unordered. Additionally, in the context of stateful streaming operations, teams need to be able to properly track event time progress in the stream of data they are ingesting for the proper calculation of time-window aggregations and other stateful operations. While working with real-time streaming data there will be delays between event time and processing time due to how data is ingested and whether the overall application experiences issues like downtime. Due to these potential variable delays, the engine that you use to process this data needs to have some mechanism to decide when to close the aggregate windows and produce the aggregate result.</p>\n\n<p>Imagine a scenario where we will need to perform stateful aggregations on the streaming data to understand and identify problems in the machines. <strong>This is where we need to leverage Structured Streaming and Watermarking to produce the necessary stateful aggregations.</strong></p>\n\n<h5 id=\"approach-1-use-a-pre-defined-fixed-window-bad\">Approach 1 - Use a pre-defined fixed window (Bad)</h5>\n\n<p><img src=\"../../assets/img/fixed_window.png?raw=true\" style=\"max-width: 800px; height: auto; \"/></p>\n\n<p>Credits: <a href=\"https://www.databricks.com/blog/feature-deep-dive-watermarking-apache-spark-structured-streaming\">Image source</a></p>\n\n<p>To explain this visually let\u2019s take a scenario where we are receiving data at various times from around 10:50 AM \u2192 11:20 AM. We are creating 10-minute tumbling windows that calculate the average of the temperature and pressure readings that came in during the windowed period.</p>\n\n<p>In this first picture, we have the tumbling windows trigger at 11:00 AM, 11:10 AM and 11:20 AM leading to the result tables shown at the respective times. When the second batch of data comes around 11:10 AM with data that has an event time of 10:53 AM this gets incorporated into the temperature and pressure averages calculated for the 11:00 AM \u2192 11:10 AM window that closes at 11:10 AM, which does not give the correct result.</p>\n\n<h5 id=\"approach-2-watermark\">Approach 2 - Watermark</h5>\n\n<p>We can define a <strong>watermark</strong> that will allow Spark to understand when to close the aggregate window and produce the correct aggregate result. In Structured Streaming applications, we can ensure that all relevant data for the aggregations we want to calculate is collected by using a feature called <strong>watermarking</strong>. In the most basic sense, by defining a <strong>watermark</strong> Spark Structured Streaming then knows when it has ingested all data up to some time, <strong>T</strong>, (based on a set lateness expectation) so that it can close and produce windowed aggregates up to timestamp <strong>T</strong>.</p>\n\n<p><img src=\"../../assets/img/watermarking.png?raw=true\" style=\"max-width: 800px; height: auto; \"/></p>\n\n<p>Credits: <a href=\"https://www.databricks.com/blog/feature-deep-dive-watermarking-apache-spark-structured-streaming\">Image source</a></p>\n\n<p>Unlike the first scenario where Spark will emit the windowed aggregation for the previous ten minutes every ten minutes (i.e. emit the 11:00 AM \u219211:10 AM window at 11:10 AM), Spark now waits to close and output the windowed aggregation once <strong>the max event time seen minus the specified watermark is greater than the upper bound of the window</strong>.</p>\n\n<p>In other words, Spark needed to wait until it saw data points where the latest event time seen minus 10 minutes was greater than 11:00 AM to emit the 10:50 AM \u2192 11:00 AM aggregate window. At 11:00 AM, it does not see this, so it only initialises the aggregate calculation in Spark\u2019s internal state store. At 11:10 AM, this condition is still not met, but we have a new data point for 10:53 AM so the internal state gets updated, just <strong>not emitted</strong>. Then finally by 11:20 AM Spark has seen a data point with an event time of 11:15 AM and since 11:15 AM minus 10 minutes is 11:05 AM which is later than 11:00 AM the 10:50 AM \u2192 11:00 AM window can be emitted to the result table.</p>\n\n<p>This produces the correct result by properly incorporating the data based on the expected lateness defined by the watermark. Once the results are emitted the corresponding state is removed from the state store.</p>\n\n<h6 id=\"watermarking-and-different-output-modes\">Watermarking and Different Output Modes</h6>\n\n<p>It is important to understand how state, late-arriving records, and the different output modes could lead to different behaviours of your application running on Spark. The main takeaway here is that in both append and update modes, once the watermark indicates that all data is received for an aggregate time window, the engine can trim the window state. In append mode the aggregate is produced only at the closing of the time window plus the watermark delay while in update mode it is produced on every update to the window.</p>\n\n<p>Lastly, by increasing your watermark delay window you will cause the pipeline to wait longer for data and potentially drop less data \u2013 higher precision, but also higher latency to produce the aggregates. On the flip side, smaller watermark delay leads to lower precision but also lower latency to produce the aggregates.</p>\n\n<p>Watermarks can only be used when you are running your streaming application in <strong>append</strong> or <strong>update</strong> output modes. There is a third output mode, complete mode, in which the entire result table is written to storage. This mode cannot be used because it requires all aggregate data to be preserved, and hence cannot use watermarking to drop intermediate state.</p>\n\n<h6 id=\"joins-with-watermark\">Joins With Watermark</h6>\n\n<p>There are three types of stream-stream joins that can be implemented in Structured Streaming: <strong>inner, outer, and semi joins</strong>. The main problem with doing joins in streaming applications is that you may have an incomplete picture of one side of the join. Giving Spark an understanding of when there are no future matches to expect is similar to the earlier problem with aggregations where Spark needed to understand when there were no new rows to incorporate into the calculation for the aggregation before emitting it.</p>\n\n<p>To allow Spark to handle this, we can leverage a combination of watermarks and event-time constraints within the join condition of the stream-stream join. This combination allows Spark to filter out late records and trim the state for the join operation through a time range condition on the join.</p>\n\n<p>Spark has a policy for handling multiple watermark definitions. Spark maintains <strong>one global watermark</strong> that is based on the slowest stream to ensure the highest amount of safety when it comes to not missing data.</p>\n\n<p>We can change this behaviour by changing <em>spark.sql.streaming.multipleWatermarkPolicy</em> to max; however, this means that data from the slower stream will be dropped.</p>\n\n<h6 id=\"state-store-performance-considerations\">State Store Performance Considerations</h6>\n\n<p>As of Spark 3.2, Spark offers RocksDB state store provider.</p>\n\n<p>If you have stateful operations in your streaming query (for example, streaming aggregation, streaming dropDuplicates, stream-stream joins, mapGroupsWithState, or flatMapGroupsWithState) and you want to maintain millions of keys in the state, then you may face issues related to large JVM garbage collection (GC) pauses causing high variations in the micro-batch processing times. This occurs because, by the implementation of HDFSBackedStateStore, the state data is maintained in the JVM memory of the executors and large number of state objects puts memory pressure on the JVM causing high GC pauses.</p>\n\n<p>In such cases, you can choose to use a more optimized state management solution based on RocksDB. Rather than keeping the state in the JVM memory, this solution uses RocksDB to efficiently manage the state in the native memory and the local disk. Furthermore, any changes to this state are automatically saved by Structured Streaming to the checkpoint location you have provided, thus providing full fault-tolerance guarantees (the same as default state management).</p>\n\n<p>To enable the new build-in state store implementation, <em>set <code>spark.sql.streaming.stateStore.providerClass</code> to <code>org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider</code></em>.</p>\n\n<p>For more details please visit Spark documentation: <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#rocksdb-state-store-implementation\">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#rocksdb-state-store-implementation</a></p>\n\n<p>You can enable this in your acons, by specifying it as part of the exec_env properties like below:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"nt\">&quot;exec_env&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;spark.sql.streaming.stateStore.providerClass&quot;</span><span class=\"p\">:</span><span class=\"s2\">&quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.write_and_read_dataframe", "modulename": "lakehouse_engine_usage.data_loader.write_and_read_dataframe", "kind": "module", "doc": "<h1 id=\"write-and-read-dataframe\">Write and Read Dataframe</h1>\n\n<p>DataFrame writer can give us some advantages by returning a dictionary containing the <code>spec_id</code> and the computed dataframe.\nIn these examples we will cover the following scenarios of using the output <code>dataframe</code> format:</p>\n\n<ol>\n<li><a href=\"#1-write-to-dataframe-consuming-the-output-spec-as-dataframe\"><strong>Write to dataframe</strong>: Consuming the output spec as DataFrame;</a></li>\n<li><a href=\"#2-write-all-dataframes-consuming-all-dataframes-generated-per-specs\"><strong>Write all dataframes</strong>: Consuming all DataFrames generated per specs;</a></li>\n<li><a href=\"#3-read-from-and-write-to-dataframe-making-use-of-the-dataframe-output-spec-to-compose-silver-data\"><strong>Read from and Write to dataframe</strong>: Making use of the DataFrame output spec to compose silver data.</a></li>\n</ol>\n\n<h4 id=\"main-advantages-of-using-this-output-writer\">Main advantages of using this output writer:</h4>\n\n<ul>\n<li><strong>Debugging purposes</strong>: as we can access any dataframe used in any part of our ACON\nwe can observe what is happening with the computation and identify what might be wrong\nor can be improved.</li>\n<li><strong>Flexibility</strong>: in case we have some very specific need not covered yet by the lakehouse\nengine capabilities, example: return the Dataframe for further processing like using a machine\nlearning model/prediction.</li>\n<li><strong>Simplify ACONs</strong>: instead developing a single complex ACON, using the Dataframe writer,\nwe can compose our ACON from the output of another ACON. This allows us to identify\nand split the notebook logic across ACONs.</li>\n</ul>\n\n<p>If you want/need, you can add as many dataframes as you want in the output spec\nreferencing the spec_id you want to add.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p><strong>This is not intended to replace the other capabilities offered by the\nlakehouse-engine</strong> and in case <strong>other feature can cover your use case</strong>,\nyou should <strong>use it instead of using the Dataframe writer</strong>, as they\nare much <strong>more extensively tested on different type of operations</strong>.</p>\n\n<p><em>Additionally, please always introspect if the problem that you are trying to resolve and for which no lakehouse-engine feature is available, could be a common problem and thus deserve a common solution and feature.</em></p>\n\n<p>Moreover, <strong>Dataframe writer is not supported for the streaming trigger\ntypes <code>processing time</code> and <code>continuous</code>.</strong></p>\n\n</div>\n\n<h2 id=\"1-write-to-dataframe-consuming-the-output-spec-as-dataframe\">1. Write to dataframe: Consuming the output spec as DataFrame</h2>\n\n<h3 id=\"silver-dummy-sales-write-to-dataframe\">Silver Dummy Sales Write to DataFrame</h3>\n\n<p>In this example we will cover the Dummy Sales write to a result containing the output DataFrame.</p>\n\n<ul>\n<li>An ACON is used to read from bronze, apply silver transformations and write to a dictionary\ncontaining the output spec as key and the dataframe as value through the following steps:\n<ul>\n<li>1 - Definition of how to read data (input data location, read type and data format);</li>\n<li>2 - Transformation of data (rename relevant columns);</li>\n<li>3 - Write the data to dict containing the dataframe;</li>\n</ul></li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"if-you-are-trying-to-retrieve-more-than-once-the-same-data-using-checkpoint-it-will-return-an-empty-dataframe-with-empty-schema-as-we-dont-have-new-data-to-read\">If you are trying to retrieve more than once the same data using checkpoint it will return an empty dataframe with empty schema as we don't have new data to read.</h6>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">cols_to_rename</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;ordered_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;date&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;order_date&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;article&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">}</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_sales&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;cols&quot;</span><span class=\"p\">:</span> <span class=\"n\">cols_to_rename</span><span class=\"p\">,</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;checkpointLocation&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints/bronze/dummy_sales&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h3 id=\"run-the-load-and-return-the-dictionary-with-the-dataframes-by-outputspec\">Run the Load and Return the Dictionary with the DataFrames by OutputSpec</h3>\n\n<p>This exploratory test will return a dictionary with the output spec and the dataframe\nthat will be stored after transformations.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n<span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span>\n<span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;dummy_sales_silver&quot;</span><span class=\"p\">))</span>\n</code></pre>\n</div>\n\n<h2 id=\"2-write-all-dataframes-consuming-all-dataframes-generated-per-specs\">2. Write all dataframes: Consuming all DataFrames generated per specs</h2>\n\n<h3 id=\"silver-dummy-sales-write-to-dataframe-2\">Silver Dummy Sales Write to DataFrame</h3>\n\n<p>In this example we will cover the Dummy Sales write to a result containing the specs and related DataFrame.</p>\n\n<ul>\n<li>An ACON is used to read from bronze, apply silver transformations and write to a dictionary\ncontaining the spec id as key and the DataFrames as value through the following steps:\n<ul>\n<li>Definition of how to read data (input data location, read type and data format);</li>\n<li>Transformation of data (rename relevant columns);</li>\n<li>Write the data to a dictionary containing all the spec ids and DataFrames computed per step;</li>\n</ul></li>\n</ul>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">cols_to_rename</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;ordered_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;date&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;order_date&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;article&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">}</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_sales&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;cols&quot;</span><span class=\"p\">:</span> <span class=\"n\">cols_to_rename</span><span class=\"p\">,</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<h3 id=\"run-the-load-and-return-the-dictionary-with-the-related-dataframes-by-spec\">Run the Load and Return the Dictionary with the related DataFrames by Spec</h3>\n\n<p>This exploratory test will return a dictionary with all specs and the related dataframe.\nYou can access the DataFrame you need by <code>output.get(&lt;spec_id&gt;)</code> for future developments and tests.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n<span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span>\n<span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;sales_bronze&quot;</span><span class=\"p\">))</span>\n<span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;sales_silver&quot;</span><span class=\"p\">))</span>\n</code></pre>\n</div>\n\n<h2 id=\"3-read-from-and-write-to-dataframe-making-use-of-the-dataframe-output-spec-to-compose-silver-data\">3. Read from and Write to dataframe: Making use of the DataFrame output spec to compose silver data</h2>\n\n<h3 id=\"silver-load-dummy-deliveries\">Silver Load Dummy Deliveries</h3>\n\n<p>In this example we will cover the Dummy Deliveries table read and incremental load to silver composing the silver data to write using the DataFrame output spec:</p>\n\n<ul>\n<li>First ACON is used to get the latest data from bronze, in this step we are using more than one output because we will need the bronze data with the latest data in the next step.</li>\n<li>Second ACON is used to consume the bronze data and the latest data to perform silver transformation, in this ACON we are using as <strong>input the two dataframes computed by the first ACON.</strong></li>\n<li>Third ACON is used to write the silver computed data from the previous ACON to the target.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-example-is-not-a-recommendation-on-how-to-deal-with-incremental-loads-the-acon-was-split-in-3-for-demo-purposes\">This example is not a recommendation on how to deal with incremental loads, the ACON was split in 3 for demo purposes.</h6>\n\n</div>\n\n<p>Consume bronze data, generate the latest data and return a dictionary with bronze and transformed dataframes:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_sales&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_silver_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_table_max_value&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_silver_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;get_max_value&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;output_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;latest&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;with_expressions&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;cols_and_exprs&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;latest&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;CASE WHEN latest IS NULL THEN 0 ELSE latest END&quot;</span><span class=\"p\">},</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_transformed&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_table_max_value&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">dummy_deliveries_transformed</span> <span class=\"o\">=</span> <span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n\n<span class=\"n\">dummy_deliveries_transformed_df</span> <span class=\"o\">=</span> <span class=\"n\">dummy_deliveries_transformed</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;dummy_deliveries_transformed&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">dummy_deliveries_bronze_df</span> <span class=\"o\">=</span> <span class=\"n\">dummy_deliveries_transformed</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;deliveries_bronze&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Consume previous dataframes generated by the first ACON (bronze and latest bronze data) to generate the silver data. In this acon we are only using <strong>just one output</strong> because we only need the dataframe from the output for the next step.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">cols_to_rename</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;delivery_note_header&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_note&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;article&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">}</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;df_name&quot;</span><span class=\"p\">:</span> <span class=\"n\">dummy_deliveries_bronze_df</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_table_max_value&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;df_name&quot;</span><span class=\"p\">:</span> <span class=\"n\">dummy_deliveries_transformed_df</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;cols&quot;</span><span class=\"p\">:</span> <span class=\"n\">cols_to_rename</span><span class=\"p\">,</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;incremental_filter&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;input_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;increment_df&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_table_max_value&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;increment_col&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;latest&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;greater_or_equal&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">dummy_deliveries_silver</span> <span class=\"o\">=</span> <span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n<span class=\"n\">dummy_deliveries_silver_df</span> <span class=\"o\">=</span> <span class=\"n\">dummy_deliveries_silver</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;dummy_deliveries_silver&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Write the silver data generated by previous ACON into the target</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">write_silver_acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;df_name&quot;</span><span class=\"p\">:</span> <span class=\"n\">dummy_deliveries_silver_df</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_quality&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;expectations_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/expectations/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;validations_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/validations/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;checkpoint_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/checkpoints/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries_dq&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket/dq/dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_note&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">19</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_max_to_be_between&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_quality&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;append&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/silver/dummy_deliveries_df_writer&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;exec_env&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.schema.autoMerge.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.optimizeWrite.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;spark.databricks.delta.autoCompact.enabled&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">write_silver_acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_loader.write_to_console", "modulename": "lakehouse_engine_usage.data_loader.write_to_console", "kind": "module", "doc": "<h1 id=\"write-to-console\">Write to Console</h1>\n\n<p>Console writer is an interesting feature to debug / validate what have been done on lakehouse engine. Before moving forward and store data somewhere, it is possible to show / print the final dataframe to the console, which means it is possible to transform the data as many times as you want and display the final result to validate if it is as expected.</p>\n\n<h2 id=\"silver-dummy-sales-write-to-console-example\">Silver Dummy Sales Write to Console Example</h2>\n\n<p>In this template we will cover the Dummy Sales write to console. An ACON is used to read from bronze, apply silver transformations and write on console through the following steps:</p>\n\n<ol>\n<li>Definition of how to read data (input data location, read type and data format);</li>\n<li>Transformation of data (rename relevant columns);</li>\n<li>Definition of how to print to console (limit, truncate, vertical options);</li>\n</ol>\n\n<p>For this, the ACON specs are :</p>\n\n<ul>\n<li><strong>input_specs</strong> (MANDATORY): specify how to read data;</li>\n<li><strong>transform specs</strong> (OPTIONAL): specify how to transform data;</li>\n<li><strong>output_specs</strong> (MANDATORY): specify how to write data to the target.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"writer-to-console-is-a-wrapper-for-sparkshow-function-if-you-want-to-know-more-about-the-function-itself-or-the-available-options-please-check-the-spark-documentation-herehttpssparkapacheorgdocslatestapipythonreferencepysparksqlapipysparksqldataframeshowhtml\">Writer to console <strong>is a wrapper for spark.show() function</strong>, if you want to know more about the function itself or the available options, <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html\">please check the spark documentation here</a>.</h6>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">cols_to_rename</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;item&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;ordered_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;date&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;order_date&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;article&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">}</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_sales&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;transform_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;rename&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;cols&quot;</span><span class=\"p\">:</span> <span class=\"n\">cols_to_rename</span><span class=\"p\">,</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_silver&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_sales_transform&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;console&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;limit&quot;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s2\">&quot;truncate&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"s2\">&quot;vertical&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">},</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>And then, <strong>Run the Load and Exit the Notebook</strong>: This exploratory test will write to the console, which means the final\ndataframe will be displayed.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_quality", "modulename": "lakehouse_engine_usage.data_quality", "kind": "module", "doc": "<h1 id=\"data-quality\">Data Quality</h1>\n\n<p>The Data Quality framework is based on <a href=\"https://greatexpectations.io/\">Great Expectations (GX)</a> and other custom-made \ndevelopments, providing a very light abstraction on top of the GX open source framework and the Spark framework.</p>\n\n<h2 id=\"how-to-use-data-quality\">How to use Data Quality?</h2>\n\n<h3 id=\"data-loader\">Data Loader</h3>\n\n<p>You can define data quality rules inside the DataLoader algorithm that you use to load data.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>The DataLoader algorithm allows you to store the results of the data quality checks inside your custom location\nusing the <strong>result_sink</strong> options (e.g., a delta table on your data product). Using result sink unlocks the \ncapability to store DQ results having history over all the DQ executions, which can be used for debugging, \nto create <strong>DQ dashboards</strong> on top of the data, and much more.</p>\n\n</div>\n\n<p><strong>Examples</strong>:\nIn these examples, dummy sales local data is used to cover a few example usages of the DQ Framework\n(based on Great Expectations).\nThe main difference between the sample acons is on the usage of <code>dq_specs</code>.</p>\n\n<ul>\n<li>1 - <a href=\"data_quality/minimal_example.html\">Minimal Example applying DQ with the Required Parameters</a></li>\n<li>2 - <a href=\"data_quality/result_sink.html\">Configure Result Sink</a></li>\n<li>3 - <a href=\"data_quality/validations_failing.html\">Validations Failing</a></li>\n<li>4 - <a href=\"data_quality/row_tagging.html\">Row Tagging</a></li>\n</ul>\n\n<h3 id=\"data-quality-validator\">Data Quality Validator</h3>\n\n<p>The DQValidator algorithm focuses on validating data (e.g., spark DataFrames, Files or Tables).\nIn contrast to the <code>dq_specs</code> inside the DataLoader algorithm, the DQValidator focuses on <strong>validating data at rest \n(post-mortem)</strong> instead of validating data in-transit (before it is loaded to the destination).</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>The DQValidator algorithm allows you to store the results of the data quality checks inside your custom location\nusing the <strong>result_sink</strong> options (e.g., a delta table on your data product). Using result sink unlocks the\ncapability to store DQ results having history over all the DQ executions, which can be used for debugging,\nto create <strong>DQ dashboards</strong> on top of the data, and much more.</p>\n\n</div>\n\n<p><a href=\"data_quality/data_quality_validator.html\">Here you can find more information regarding DQValidator and examples</a>.</p>\n\n<h3 id=\"reconciliator\">Reconciliator</h3>\n\n<p>Similarly to the <a href=\"#data-quality-validator\">Data Quality Validator</a> algorithm, the Reconciliator algorithm focuses on \nvalidating data at rest (post-mortem). In contrast to the DQValidator algorithm, the Reconciliator always compares a \ntruth dataset (e.g., spark DataFrames, Files or Tables) with the current dataset (e.g., spark DataFrames, Files or \nTables), instead of executing DQ rules defined by the teams. \n<a href=\"reconciliator.html\">Here you can find more information regarding reconciliator and examples</a>.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"reconciliator-does-not-use-great-expectations-therefore-data-docs-and-result-sink-and-others-native-methods-are-not-available\">Reconciliator does not use Great Expectations, therefore Data Docs and Result Sink and others native methods are not available.</h6>\n\n</div>\n\n<h3 id=\"custom-expectations\">Custom Expectations</h3>\n\n<p>If your data has a data quality check that cannot be done with the expectations provided by Great Expectations you \ncan create a custom expectation to make this verification.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Before creating a custom expectation check if there is an expectation already created to address your needs, \nboth in Great Expectations and the Lakehouse Engine.\nAny Custom Expectation that is too specific (using hardcoded table/column names) will be rejected.\n<strong>Expectations should be generic by definition.</strong></p>\n\n</div>\n\n<p><a href=\"data_quality/custom_expectations.html\">Here you can find more information regarding custom expectations and examples</a>.</p>\n\n<h3 id=\"row-tagging\">Row Tagging</h3>\n\n<p>The row tagging strategy allows users to tag the rows that failed to be easier to identify the problems \nin the validations. <a href=\"data_quality/row_tagging.html\">Here you can find all the details and examples</a>.</p>\n\n<h2 id=\"how-to-check-the-results-of-the-data-quality-process\">How to check the results of the Data Quality Process?</h2>\n\n<h3 id=\"1-tablelocation-analysis\">1. Table/location analysis</h3>\n\n<p>The possibility to configure a <strong>Result Sink</strong> allows you to store the history of executions of the DQ process. \nYou can query the table or the location to search through data and analyse history.</p>\n\n<h3 id=\"2-power-bi-dashboard\">2. Power BI Dashboard</h3>\n\n<p>With the information expanded, interactive analysis can be built on top of the history of the DQ process.\nA dashboard can be created with the results that we have in <code>dq_specs</code>. To be able to have this information you \nneed to use arguments <code>result_sink_db_table</code> and/or <code>result_sink_location</code>.</p>\n\n<p>Through having a dashboard, the runs and expectations can be analysed, filtered by year, month, source and \nrun name, and you will have information about the number of runs, some statistics, status of expectations and more. \nAnalysis such as biggest failures per expectation type, biggest failures by columns, biggest failures per source, \nand others can be made, using the information in the <code>result_sink_db_table</code>/<code>result_sink_location</code>.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>The recommendation is to use the same result sink table/location for all your dq_specs and \nin the dashboard you will get a preview of the status of all of them.</p>\n\n</div>\n\n<p><img src=\"../assets/img/dq_dashboard.png?raw=true\" style=\"max-width: 800px; height: auto; \"/></p>\n\n<h3 id=\"3-data-docs-website\">3. Data Docs Website</h3>\n\n<p>A site that is auto generated to present you all the relevant information can also be used. If you choose to define \nthe parameter <code>data_docs_bucket</code> you will be able to store the GX documentation in the defined bucket,\nand therefore make your data docs available in the DQ Web App (GX UI) visible to everyone. \nThe <code>data_docs_bucket</code> property supersedes the <code>bucket</code> property only for data docs storage.</p>\n"}, {"fullname": "lakehouse_engine_usage.data_quality.custom_expectations", "modulename": "lakehouse_engine_usage.data_quality.custom_expectations", "kind": "module", "doc": "<h1 id=\"custom-expectations\">Custom Expectations</h1>\n\n<h2 id=\"defining-custom-expectations\">Defining Custom Expectations</h2>\n\n<p>Custom expectations are defined in python and need to follow a structure to correctly integrate with Great Expectations.</p>\n\n<p>Follow the <a href=\"https://docs.greatexpectations.io/docs/oss/guides/expectations/custom_expectations_lp/\">documentation of GX on Creating Custom Expectations</a> \nand find information about <a href=\"https://docs.greatexpectations.io/docs/conceptual_guides/expectation_classes\">the existing types of expectations</a>. </p>\n\n<p>Here is an example of custom expectation.\nAs for other cases, the acon configuration should be executed with <code>load_data</code> using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"o\">...</span><span class=\"p\">}</span>\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"sd\">&quot;&quot;&quot;Expectation to check if column &#39;a&#39; is lower or equal than column &#39;b&#39;.&quot;&quot;&quot;</span>\n<span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">,</span> <span class=\"n\">Optional</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">great_expectations.core</span> <span class=\"kn\">import</span> <span class=\"n\">ExpectationConfiguration</span>\n<span class=\"kn\">from</span> <span class=\"nn\">great_expectations.execution_engine</span> <span class=\"kn\">import</span> <span class=\"n\">ExecutionEngine</span><span class=\"p\">,</span> <span class=\"n\">SparkDFExecutionEngine</span>\n<span class=\"kn\">from</span> <span class=\"nn\">great_expectations.expectations.expectation</span> <span class=\"kn\">import</span> <span class=\"n\">ColumnPairMapExpectation</span>\n<span class=\"kn\">from</span> <span class=\"nn\">great_expectations.expectations.metrics.map_metric_provider</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">ColumnPairMapMetricProvider</span><span class=\"p\">,</span>\n    <span class=\"n\">column_pair_condition_partial</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.utils.expectations_utils</span> <span class=\"kn\">import</span> <span class=\"n\">validate_result</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ColumnPairCustom</span><span class=\"p\">(</span><span class=\"n\">ColumnPairMapMetricProvider</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Asserts that column &#39;A&#39; is lower or equal than column &#39;B&#39;.</span>\n\n<span class=\"sd\">    Additionally, the &#39;margin&#39; parameter can be used to add a margin to the</span>\n<span class=\"sd\">    check between column &#39;A&#39; and &#39;B&#39;: &#39;A&#39; &lt;= &#39;B&#39; + &#39;margin&#39;.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"n\">condition_metric_name</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;column_pair_values.a_smaller_or_equal_than_b&quot;</span>\n    <span class=\"n\">condition_domain_keys</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"s2\">&quot;batch_id&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;ignore_row_if&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">condition_value_keys</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s2\">&quot;margin&quot;</span><span class=\"p\">,)</span>\n\n    <span class=\"nd\">@column_pair_condition_partial</span><span class=\"p\">(</span><span class=\"n\">engine</span><span class=\"o\">=</span><span class=\"n\">SparkDFExecutionEngine</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">_spark</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"n\">ColumnPairMapMetricProvider</span><span class=\"p\">,</span>\n        <span class=\"n\">column_A</span><span class=\"p\">:</span> <span class=\"n\">Any</span><span class=\"p\">,</span>\n        <span class=\"n\">column_B</span><span class=\"p\">:</span> <span class=\"n\">Any</span><span class=\"p\">,</span>\n        <span class=\"n\">margin</span><span class=\"p\">:</span> <span class=\"n\">Any</span><span class=\"p\">,</span>\n        <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Any</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Implementation of the expectation&#39;s logic.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            column_A: Value of the row of column_A.</span>\n<span class=\"sd\">            column_B: Value of the row of column_B.</span>\n<span class=\"sd\">            margin: margin value to be added to column_b.</span>\n<span class=\"sd\">            kwargs: dict with additional parameters.</span>\n\n<span class=\"sd\">        Returns:</span>\n<span class=\"sd\">            If the condition is met.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"k\">if</span> <span class=\"n\">margin</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">approx</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"k\">elif</span> <span class=\"ow\">not</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">margin</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">complex</span><span class=\"p\">)):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">TypeError</span><span class=\"p\">(</span>\n                <span class=\"sa\">f</span><span class=\"s2\">&quot;margin must be one of int, float, complex.&quot;</span>\n                <span class=\"sa\">f</span><span class=\"s2\">&quot; Found: </span><span class=\"si\">{</span><span class=\"n\">margin</span><span class=\"si\">}</span><span class=\"s2\"> as </span><span class=\"si\">{</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">margin</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n            <span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">approx</span> <span class=\"o\">=</span> <span class=\"n\">margin</span>  <span class=\"c1\"># type: ignore</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">column_A</span> <span class=\"o\">&lt;=</span> <span class=\"n\">column_B</span> <span class=\"o\">+</span> <span class=\"n\">approx</span>  <span class=\"c1\"># type: ignore</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ExpectColumnPairAToBeSmallerOrEqualThanB</span><span class=\"p\">(</span><span class=\"n\">ColumnPairMapExpectation</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Expect values in column A to be lower or equal than column B.</span>\n\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        column_A: The first column name.</span>\n<span class=\"sd\">        column_B: The second column name.</span>\n<span class=\"sd\">        margin: additional approximation to column B value.</span>\n\n<span class=\"sd\">    Keyword Args:</span>\n<span class=\"sd\">        - allow_cross_type_comparisons: If True, allow</span>\n<span class=\"sd\">            comparisons between types (e.g. integer and string).</span>\n<span class=\"sd\">            Otherwise, attempting such comparisons will raise an exception.</span>\n<span class=\"sd\">        - ignore_row_if: &quot;both_values_are_missing&quot;,</span>\n<span class=\"sd\">            &quot;either_value_is_missing&quot;, &quot;neither&quot; (default).</span>\n<span class=\"sd\">        - result_format: Which output mode to use:</span>\n<span class=\"sd\">            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.</span>\n<span class=\"sd\">        - include_config: If True (default), then include the expectation config</span>\n<span class=\"sd\">            as part of the result object.</span>\n<span class=\"sd\">        - catch_exceptions: If True, then catch exceptions and</span>\n<span class=\"sd\">            include them as part of the result object. Default: False.</span>\n<span class=\"sd\">        - meta: A JSON-serializable dictionary (nesting allowed)</span>\n<span class=\"sd\">            that will be included in the output without modification.</span>\n\n<span class=\"sd\">    Returns:</span>\n<span class=\"sd\">        An ExpectationSuiteValidationResult.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"n\">examples</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;dataset_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Test Dataset&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;data&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">22</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">],</span>\n                        <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">],</span>\n                        <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">],</span>\n                    <span class=\"p\">},</span>\n                    <span class=\"s2\">&quot;schemas&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;spark&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                            <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;IntegerType&quot;</span><span class=\"p\">,</span>\n                            <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;IntegerType&quot;</span><span class=\"p\">,</span>\n                            <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;IntegerType&quot;</span><span class=\"p\">,</span>\n                        <span class=\"p\">}</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;tests&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;title&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;negative_test&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;exact_match_out&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;include_in_gallery&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;in&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                            <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;COMPLETE&quot;</span><span class=\"p\">,</span>\n                            <span class=\"s2\">&quot;unexpected_index_column_names&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">],</span>\n                        <span class=\"p\">},</span>\n                    <span class=\"p\">},</span>\n                    <span class=\"s2\">&quot;out&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;success&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;unexpected_index_list&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                            <span class=\"p\">{</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"mi\">11</span><span class=\"p\">},</span>\n                            <span class=\"p\">{</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"mi\">22</span><span class=\"p\">},</span>\n                            <span class=\"p\">{</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">},</span>\n                        <span class=\"p\">],</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;title&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;positive_test&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;exact_match_out&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;include_in_gallery&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;in&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;margin&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                            <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;COMPLETE&quot;</span><span class=\"p\">,</span>\n                            <span class=\"s2\">&quot;unexpected_index_column_names&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;a&quot;</span><span class=\"p\">],</span>\n                        <span class=\"p\">},</span>\n                    <span class=\"p\">},</span>\n                    <span class=\"s2\">&quot;out&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;success&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;unexpected_index_list&quot;</span><span class=\"p\">:</span> <span class=\"p\">[],</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"n\">map_metric</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;column_pair_values.a_smaller_or_equal_than_b&quot;</span>\n    <span class=\"n\">success_keys</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;ignore_row_if&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;margin&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;mostly&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">default_kwarg_values</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;mostly&quot;</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;ignore_row_if&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;neither&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;BASIC&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;include_config&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;catch_exceptions&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_validate</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"n\">ExpectationConfiguration</span><span class=\"p\">,</span>\n        <span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">,</span>\n        <span class=\"n\">runtime_configuration</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">execution_engine</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">ExecutionEngine</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Custom implementation of the GE _validate method.</span>\n\n<span class=\"sd\">        This method is used on the tests to validate both the result</span>\n<span class=\"sd\">        of the tests themselves and if the unexpected index list</span>\n<span class=\"sd\">        is correctly generated.</span>\n<span class=\"sd\">        The GE test logic does not do this validation, and thus</span>\n<span class=\"sd\">        we need to make it manually.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            configuration: Configuration used in the test.</span>\n<span class=\"sd\">            metrics: Test result metrics.</span>\n<span class=\"sd\">            runtime_configuration: Configuration used when running the expectation.</span>\n<span class=\"sd\">            execution_engine: Execution Engine where the expectation was run.</span>\n\n<span class=\"sd\">        Returns:</span>\n<span class=\"sd\">            Dictionary with the result of the validation.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"k\">return</span> <span class=\"n\">validate_result</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">configuration</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">)</span>\n\n\n<span class=\"sd\">&quot;&quot;&quot;Mandatory block of code. If it is removed the expectation will not be available.&quot;&quot;&quot;</span>\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n    <span class=\"c1\"># test the custom expectation with the function `print_diagnostic_checklist()`</span>\n    <span class=\"n\">ExpectColumnPairAToBeSmallerOrEqualThanB</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">print_diagnostic_checklist</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<h3 id=\"naming-conventions\">Naming Conventions</h3>\n\n<p>Your expectation's name <strong>should</strong> start with expect.</p>\n\n<p>The name of the file <strong>must</strong> be the name of the expectation written in snake case. Ex: <code>expect_column_length_match_input_length</code></p>\n\n<p>The name of the class <strong>must</strong> be the name of the expectation written in camel case. Ex: <code>ExpectColumnLengthMatchInputLength</code></p>\n\n<h3 id=\"file-structure\">File Structure</h3>\n\n<p>The file contains two main sections:</p>\n\n<ul>\n<li>the definition of the metric that we are tracking (where we define the logic of the expectation);</li>\n<li>the definition of the expectation</li>\n</ul>\n\n<h3 id=\"metric-definition\">Metric Definition</h3>\n\n<p>In this section we define the logic of the expectation. This needs to follow a certain structure:</p>\n\n<h4 id=\"code-structure\">Code Structure</h4>\n\n<p>1) The class you define needs to extend one of the Metric Providers defined by Great Expectations that corresponds \nto your expectation's type. More info on the <a href=\"https://docs.greatexpectations.io/docs/conceptual_guides/metricproviders\">metric providers</a>. </p>\n\n<p>2) You need to define the name of your metric. This name <strong>must</strong> be unique and <strong>must</strong> follow the following structure: \ntype of expectation.name of metric. Ex.: <code>column_pair_values.a_smaller_or_equal_than_b</code>\n<strong>Types of expectations:</strong>  <code>column_values</code>, <code>multicolumn_values</code>, <code>column_pair_values</code>, <code>table_rows</code>, <code>table_columns</code>.</p>\n\n<p>3) Any <a href=\"#parameters\">GX default parameters</a> that are necessary to calculate your metric <strong>must</strong> be defined as \"condition_domain_keys\".</p>\n\n<p>4) Any <a href=\"#parameters\">additional parameters</a> that are necessary to calculate your metric <strong>must</strong> be defined as \"condition_value_keys\".</p>\n\n<p>5) The logic of your expectation <strong>must</strong> be defined for the SparkDFExecutionEngine in order to be run on the Lakehouse.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"k\">class</span> <span class=\"nc\">ColumnMapMetric</span><span class=\"p\">(</span><span class=\"n\">ColumnMapMetricProvider</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Asserts that a column matches a pattern.&quot;&quot;&quot;</span>\n\n    <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"n\">condition_metric_name</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;column_pair_values.a_smaller_or_equal_than_b&quot;</span>\n    <span class=\"mi\">3</span><span class=\"p\">)</span> <span class=\"n\">condition_domain_keys</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"s2\">&quot;batch_id&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;ignore_row_if&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"mi\">4</span><span class=\"p\">)</span> <span class=\"n\">condition_value_keys</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s2\">&quot;margin&quot;</span><span class=\"p\">,)</span>\n\n    <span class=\"mi\">5</span><span class=\"p\">)</span> <span class=\"nd\">@column_pair_condition_partial</span><span class=\"p\">(</span><span class=\"n\">engine</span><span class=\"o\">=</span><span class=\"n\">SparkDFExecutionEngine</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">_spark</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"n\">ColumnPairMapMetricProvider</span><span class=\"p\">,</span>\n        <span class=\"n\">column_A</span><span class=\"p\">:</span> <span class=\"n\">Any</span><span class=\"p\">,</span>\n        <span class=\"n\">column_B</span><span class=\"p\">:</span> <span class=\"n\">Any</span><span class=\"p\">,</span>\n        <span class=\"n\">margin</span><span class=\"p\">:</span> <span class=\"n\">Any</span><span class=\"p\">,</span>\n        <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Any</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Implementation of the expectation&#39;s logic.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            column_A: Value of the row of column_A.</span>\n<span class=\"sd\">            column_B: Value of the row of column_B.</span>\n<span class=\"sd\">            margin: margin value to be added to column_b.</span>\n<span class=\"sd\">            kwargs: dict with additional parameters.</span>\n\n<span class=\"sd\">        Returns:</span>\n<span class=\"sd\">            If the condition is met.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"k\">if</span> <span class=\"n\">margin</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">approx</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"k\">elif</span> <span class=\"ow\">not</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">margin</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">complex</span><span class=\"p\">)):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">TypeError</span><span class=\"p\">(</span>\n                <span class=\"sa\">f</span><span class=\"s2\">&quot;margin must be one of int, float, complex.&quot;</span>\n                <span class=\"sa\">f</span><span class=\"s2\">&quot; Found: </span><span class=\"si\">{</span><span class=\"n\">margin</span><span class=\"si\">}</span><span class=\"s2\"> as </span><span class=\"si\">{</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">margin</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n            <span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">approx</span> <span class=\"o\">=</span> <span class=\"n\">margin</span>  <span class=\"c1\"># type: ignore</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">column_A</span> <span class=\"o\">&lt;=</span> <span class=\"n\">column_B</span> <span class=\"o\">+</span> <span class=\"n\">approx</span>  <span class=\"c1\"># type: ignore</span>\n</code></pre>\n</div>\n\n<h3 id=\"expectation-definition\">Expectation Definition</h3>\n\n<p>In this section we define the expectation. This needs to follow a certain structure:</p>\n\n<h4 id=\"code-structure-2\">Code Structure</h4>\n\n<p>1) The class you define needs to extend one of the Expectations defined by Great Expectations that corresponds to your expectation's type. </p>\n\n<p>2) You must define an \"examples\" object where you define at least one success and one failure of your expectation to \ndemonstrate its logic. The result format must be set to complete, and you must set the <a href=\"#result-format\">unexpected_index_name</a> variable.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>For any examples where you will have unexpected results you must define  unexpected_index_list in your \"out\" element.\nThis will be validated during the testing phase.</p>\n\n</div>\n\n<p>3) The metric <strong>must</strong> be the same you defined in the metric definition.</p>\n\n<p>4) You <strong>must</strong> define all <a href=\"#parameters\">additional parameters</a> that the user has to/should provide to the expectation. </p>\n\n<p>5) You <strong>should</strong> define any default values for your expectations parameters. </p>\n\n<p>6) You must <strong>define</strong> the <code>_validate</code> method like shown in the example. You <strong>must</strong> call the <code>validate_result</code> function \ninside your validate method, this process adds a validation to the unexpected index list in the examples.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>If your custom expectation requires any extra validations, or you require additional fields to be returned on \nthe final dataframe, you can add them in this function. \nThe validate_result method has two optional parameters (<code>partial_success</code> and `partial_result) that can be used to \npass the result of additional validations and add more information to the result key of the returned dict respectively.</p>\n\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"k\">class</span> <span class=\"nc\">ExpectColumnPairAToBeSmallerOrEqualThanB</span><span class=\"p\">(</span><span class=\"n\">ColumnPairMapExpectation</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Expect values in column A to be lower or equal than column B.</span>\n\n<span class=\"sd\">    Args:</span>\n<span class=\"sd\">        column_A: The first column name.</span>\n<span class=\"sd\">        column_B: The second column name.</span>\n<span class=\"sd\">        margin: additional approximation to column B value.</span>\n\n<span class=\"sd\">    Keyword Args:</span>\n<span class=\"sd\">        allow_cross_type_comparisons: If True, allow</span>\n<span class=\"sd\">            comparisons between types (e.g. integer and string).</span>\n<span class=\"sd\">            Otherwise, attempting such comparisons will raise an exception.</span>\n<span class=\"sd\">        ignore_row_if: &quot;both_values_are_missing&quot;,</span>\n<span class=\"sd\">            &quot;either_value_is_missing&quot;, &quot;neither&quot; (default).</span>\n<span class=\"sd\">        result_format: Which output mode to use:</span>\n<span class=\"sd\">            `BOOLEAN_ONLY`, `BASIC` (default), `COMPLETE`, or `SUMMARY`.</span>\n<span class=\"sd\">        include_config: If True (default), then include the expectation config</span>\n<span class=\"sd\">            as part of the result object.</span>\n<span class=\"sd\">        catch_exceptions: If True, then catch exceptions and</span>\n<span class=\"sd\">            include them as part of the result object. Default: False.</span>\n<span class=\"sd\">        meta: A JSON-serializable dictionary (nesting allowed)</span>\n<span class=\"sd\">            that will be included in the output without modification.</span>\n\n<span class=\"sd\">    Returns:</span>\n<span class=\"sd\">        An ExpectationSuiteValidationResult.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"n\">examples</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;dataset_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Test Dataset&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">22</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">],</span>\n                <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">],</span>\n                <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">],</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;schemas&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;spark&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;IntegerType&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;IntegerType&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;IntegerType&quot;</span><span class=\"p\">}</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;tests&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;title&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;negative_test&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;exact_match_out&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;include_in_gallery&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;in&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                            <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;COMPLETE&quot;</span><span class=\"p\">,</span>\n                            <span class=\"s2\">&quot;unexpected_index_column_names&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">],</span>\n                            <span class=\"s2\">&quot;include_unexpected_rows&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                        <span class=\"p\">},</span>\n                    <span class=\"p\">},</span>\n                    <span class=\"s2\">&quot;out&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;success&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;unexpected_index_list&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                            <span class=\"p\">{</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"mi\">11</span><span class=\"p\">},</span>\n                            <span class=\"p\">{</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"mi\">22</span><span class=\"p\">},</span>\n                            <span class=\"p\">{</span><span class=\"s2\">&quot;c&quot;</span><span class=\"p\">:</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">},</span>\n                        <span class=\"p\">],</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;title&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;positive_test&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;exact_match_out&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;include_in_gallery&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;in&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;a&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;margin&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n                        <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                            <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;COMPLETE&quot;</span><span class=\"p\">,</span>\n                            <span class=\"s2\">&quot;unexpected_index_column_names&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;a&quot;</span><span class=\"p\">],</span>\n                        <span class=\"p\">},</span>\n                    <span class=\"p\">},</span>\n                    <span class=\"s2\">&quot;out&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;success&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"mi\">3</span><span class=\"p\">)</span> <span class=\"n\">map_metric</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;column_values.pattern_match&quot;</span>\n    <span class=\"mi\">4</span><span class=\"p\">)</span> <span class=\"n\">success_keys</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"s2\">&quot;validation_regex&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;mostly&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"mi\">5</span><span class=\"p\">)</span> <span class=\"n\">default_kwarg_values</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;ignore_row_if&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;never&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;result_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;BASIC&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;include_config&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;catch_exceptions&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;mostly&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"mi\">6</span><span class=\"p\">)</span> <span class=\"k\">def</span> <span class=\"nf\">_validate</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"n\">ExpectationConfiguration</span><span class=\"p\">,</span>\n        <span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">,</span>\n        <span class=\"n\">runtime_configuration</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">execution_engine</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">ExecutionEngine</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Custom implementation of the GX _validate method.</span>\n\n<span class=\"sd\">        This method is used on the tests to validate both the result</span>\n<span class=\"sd\">        of the tests themselves and if the unexpected index list</span>\n<span class=\"sd\">        is correctly generated.</span>\n<span class=\"sd\">        The GX test logic does not do this validation, and thus</span>\n<span class=\"sd\">        we need to make it manually.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            configuration: Configuration used in the test.</span>\n<span class=\"sd\">            metrics: Test result metrics.</span>\n<span class=\"sd\">            runtime_configuration: Configuration used when running the expectation.</span>\n<span class=\"sd\">            execution_engine: Execution Engine where the expectation was run.</span>\n\n<span class=\"sd\">        Returns:</span>\n<span class=\"sd\">            Dictionary with the result of the validation.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"k\">return</span> <span class=\"n\">validate_result</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">configuration</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"printing-the-expectation-diagnostics\">Printing the Expectation Diagnostics</h3>\n\n<p>Your expectations <strong>must</strong> include the ability to call the Great Expectations diagnostic function in order to be validated.</p>\n\n<p>In order to do this code <strong>must</strong> be present.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"sd\">&quot;&quot;&quot;Mandatory block of code. If it is removed the expectation will not be available.&quot;&quot;&quot;</span>\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n    <span class=\"c1\"># test the custom expectation with the function `print_diagnostic_checklist()`</span>\n    <span class=\"n\">ExpectColumnPairAToBeSmallerOrEqualThanB</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">print_diagnostic_checklist</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<h2 id=\"creation-process\">Creation Process</h2>\n\n<p>1) Create a branch from lakehouse engine.</p>\n\n<p>2) Create a custom expectation with your specific logic:</p>\n\n<ol>\n<li>All new expectations must be placed inside folder <code>/lakehouse_engine/dq_processors/custom_expectations</code>.</li>\n<li>The name of the expectation must be added to the file <code>/lakehouse_engine/core/definitions.py</code>, to the variable: <code>CUSTOM_EXPECTATION_LIST</code>.</li>\n<li>All new expectations must be tested on <code>/tests/feature/custom_expectations/test_custom_expectations.py</code>.\nIn order to create a new test for your custom expectation it is necessary to:</li>\n</ol>\n\n<ul>\n<li>Copy one of the expectation folders in <code>tests/resources/feature/custom_expectations</code> renaming it to your custom expectation.</li>\n<li>Make any necessary changes on the data/schema file present.</li>\n<li>On <code>/tests/feature/custom_expectations/test_custom_expectations.py</code> add a scenario to test your expectation, all expectations \nmust be tested on batch and streaming. The test is implemented to generate an acon based on each scenario data. </li>\n<li>Test your developments to check that everything is working as intended.</li>\n</ul>\n\n<p>3) When the development is completed, create a pull request with your changes.</p>\n\n<p>4) Your expectation will be available with the next release of the lakehouse engine that happens after you pull request is approved. \nThis means that you need to upgrade your version of the lakehouse engine in order to use it.</p>\n\n<h2 id=\"usage\">Usage</h2>\n\n<p>Custom Expectations are available to use like any other expectations provided by Great Expectations.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>Depending on the type of expectation you are defining some parameters are expected by default. \nEx: A ColumnMapExpectation has a default \"column\" parameter.</p>\n\n<h3 id=\"mostly\">Mostly</h3>\n\n<p><a href=\"https://docs.greatexpectations.io/docs/reference/learn/expectations/standard_arguments/#mostly\">Mostly</a> is a standard \nparameter for a subset of expectations that is used to define a threshold for the failure of an expectation. \nEx: A mostly value of 0.7 makes it so that the expectation only fails if more than 70% of records have \na negative result.</p>\n\n<h2 id=\"result-format\">Result Format</h2>\n\n<p>Great Expectations has several different types of <a href=\"https://docs.greatexpectations.io/docs/reference/learn/expectations/result_format/\">result formats</a> \nfor the expectations results. The lakehouse engine requires the result format to be set to \"COMPLETE\" in order to tag \nthe lines where the expectations failed.</p>\n\n<h3 id=\"unexpected_index_column_names\"><code>unexpected_index_column_names</code></h3>\n\n<p>Inside this key you must define what columns are used as an index inside your data. If this is set and the result \nformat is set to \"COMPLETE\" a list with the indexes of the lines that failed the validation will be returned by \nGreat Expectations.\nThis information is used by the Lakehouse Engine to tag the lines in error after the fact. The additional tests \ninside the <code>_validate</code> method verify that the custom expectation is tagging these lines correctly.</p>\n"}, {"fullname": "lakehouse_engine_usage.data_quality.data_quality_validator", "modulename": "lakehouse_engine_usage.data_quality.data_quality_validator", "kind": "module", "doc": "<h1 id=\"data-quality-validator\">Data Quality Validator</h1>\n\n<p>DQValidator algorithm allows DQ Validations isolated from the data load (only read and apply data quality validations).\nWith this algorithm you have the capacity to apply the Lakehouse-Engine Data Quality Process,\nusing <a href=\"https://greatexpectations.io/expectations/\">Great Expectations</a> functions directly into a specific dataset also\nmaking use of all the <a href=\"../../lakehouse_engine/core/definitions.html#InputSpec\">InputSpecs</a> available in the engine.</p>\n\n<p>Validating the Data Quality, using this algorithm, is a matter of defining the data you want to read and the validations you want to do to your data, detailing the great expectations functions you want to apply on the data to assess its quality.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p><strong>This algorithm also gives the possibility to restore a previous version of a delta table or delta files in case the DQ\nprocess raises any exception. Please use it carefully!!</strong> You may lose important commits and data. Moreover, this will\nhighly depend on the frequency that you run your Data Quality validations. If you run your data loads daily and Data\nQuality validations weekly, and you define the restore_prev_version to true, this means that the table will be restored\nto the previous version, but the error could have happened 4 or 5 versions before.</p>\n\n</div>\n\n<h2 id=\"when-to-use\">When to use?</h2>\n\n<ul>\n<li><strong>Post-Load validation</strong>: check quality of data already loaded to a table/location</li>\n<li><strong>Pre-Load validation</strong>: check quality of the data you want to load (check DQ by reading a set of files in a specific\nlocation...)</li>\n<li><strong>Validation of a DataFrame computed in the notebook itself</strong> (e.g. check data quality after joining or filtering\ndatasets, using the computed DataFrame as input for the validation)</li>\n</ul>\n\n<p>This algorithm also gives teams some freedom to:</p>\n\n<ul>\n<li><strong>Schedule isolated DQ Validations to run periodically</strong>, with the frequency they need;</li>\n<li>Define a DQ Validation process <strong>as an end-to-end test</strong> of the respective data product.</li>\n</ul>\n\n<h2 id=\"how-to-use\">How to use?</h2>\n\n<p>All of these configurations are passed via the ACON to instantiate\na <a href=\"../../lakehouse_engine/core/definitions.html#DQValidatorSpec\">DQValidatorSpec object</a>. The DQValidator algorithm uses an\nACON to configure its execution. In <a href=\"../../lakehouse_engine/core/definitions.html#DQValidatorSpec\">DQValidatorSpec</a> you can\nfind the meaning of each ACON property.</p>\n\n<p>Here is an example of ACON configuration:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.my_table&quot;</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;dq_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_sales&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sales_source&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;store_backend&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;file_system&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;local_fs_root_dir&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;/app/tests/lakehouse/in/feature/dq_validator/dq&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;result_sink_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;json&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article&quot;</span><span class=\"p\">}},</span>\n            <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">11</span><span class=\"p\">},</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">],</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;restore_prev_version&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>On this page you will also find the following examples of usage:</p>\n\n<ol>\n<li>Dataframe as input &amp; Success on the DQ Validation</li>\n<li>Table as input &amp; Failure on DQ Validation &amp; Restore previous version</li>\n<li>Files as input &amp; Failure on DQ Validation &amp; Fail_on_error disabled</li>\n<li>Files as input &amp; Failure on DQ Validation &amp; Critical functions defined</li>\n<li>Files as input &amp; Failure on DQ Validation &amp; Max failure percentage defined</li>\n</ol>\n\n<h3 id=\"example-1-dataframe-as-input-success-on-the-dq-validation\">Example 1 : Dataframe as input &amp; Success on the DQ Validation</h3>\n\n<p>This example focuses on using a dataframe, computed in this notebook, directly in the input spec. First, a new\nDataFrame is generated as a result of the join of data from two tables (dummy_deliveries and dummy_pd_article) and\nsome DQ Validations are applied on top of this dataframe.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_dq_validation</span>\n\n<span class=\"n\">input_df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">        SELECT a.*, b.article_category, b.article_color</span>\n<span class=\"s2\">        FROM my_database.dummy_deliveries a</span>\n<span class=\"s2\">        JOIN my_database.dummy_pd_article b</span>\n<span class=\"s2\">            ON a.article_id = b.article_id</span>\n<span class=\"s2\">        &quot;&quot;&quot;</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_article_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dataframe&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;df_name&quot;</span><span class=\"p\">:</span> <span class=\"n\">input_df</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;dq_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_article_dq&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_article_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_validator_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_path/dq_validator/dq_validator_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;expectations_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/dq_validator/expectations/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;validations_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/dq_validator/validations/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/dq_validator/data_docs/site/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;checkpoint_store_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/dq_validator/checkpoints/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;unexpected_rows_pk&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;delivery_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;article_id&quot;</span><span class=\"p\">],</span>\n        <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">}}],</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;restore_prev_version&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_dq_validation</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"example-2-table-as-input-failure-on-dq-validation-restore-previous-version\">Example 2: Table as input &amp; Failure on DQ Validation &amp; Restore previous version</h3>\n\n<p>In this example we are using a table as input to validate the data that was loaded. Here, we are forcing the DQ Validations to fail in order to show the possibility of restoring the table to the previous version.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p><strong>Be careful when using the feature of restoring a previous version of a delta table or delta files.</strong> You may\nlose important commits and data. Moreover, this will highly depend on the frequency that you run your Data Quality\nvalidations. If you run your data loads daily and Data Quality validations weekly, and you define the\nrestore_prev_version to true, this means that the table will be restored to the previous version, but the error\ncould have happened 4 or 5 versions before (because loads are daily, validations are weekly).</p>\n\n</div>\n\n<p>Steps followed in this example to show how the restore_prev_version feature works.</p>\n\n<ol>\n<li><strong>Insert rows into the dummy_deliveries table</strong> to adjust the total numbers of rows and <strong>make the DQ process fail</strong>.</li>\n<li><strong>Use the \"DESCRIBE HISTORY\" statement to check the number of versions available on the table</strong> and check the version\nnumber resulting from the insertion to the table.</li>\n<li><strong>Execute the DQ Validation</strong>, using the configured acon (based on reading the dummy_deliveries table and setting the \n<code>restore_prev_version</code> to <code>true</code>). Checking the logs of the process, you can see that the data did not pass all the \nexpectations defined and that the table version restore process was triggered.</li>\n<li><strong>Re-run a \"DESCRIBE HISTORY\" statement to check that the previous version of the table was restored</strong> and thus, the row inserted in the beginning of the process is no longer present in the table.</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_dq_validation</span>\n\n<span class=\"c1\"># Force failure of data quality by adding new row</span>\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">&quot;&quot;&quot;INSERT INTO my_database.dummy_deliveries VALUES (7, 1, 20180601, 71, &quot;article1&quot;, &quot;delivered&quot;)&quot;&quot;&quot;</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Check history of the table</span>\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">&quot;&quot;&quot;DESCRIBE HISTORY my_database.dummy_deliveries&quot;&quot;&quot;</span><span class=\"p\">)</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;dq_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">}},</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">19</span><span class=\"p\">}},</span>\n        <span class=\"p\">],</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;restore_prev_version&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_dq_validation</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Check that the previous version of the table was restored</span>\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">&quot;&quot;&quot;DESCRIBE HISTORY my_database.dummy_deliveries&quot;&quot;&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"example-3-files-as-input-failure-on-dq-validation-fail_on_error-disabled\">Example 3: Files as input &amp; Failure on DQ Validation &amp; Fail_on_error disabled</h3>\n\n<p>In this example we are using a location as input to validate the files in a specific folder.\nHere, we are forcing the DQ Validations to fail, however disabling the \"fail_on_error\" configuration,\nso the algorithm warns about the expectations that failed but the process/the execution of the algorithm doesn't fail.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_dq_validation</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/silver/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;dq_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">}},</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">17</span><span class=\"p\">}},</span>\n        <span class=\"p\">],</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;restore_prev_version&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_dq_validation</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"example-4-files-as-input-failure-on-dq-validation-critical-functions-defined\">Example 4: Files as input &amp; Failure on DQ Validation &amp; Critical functions defined</h3>\n\n<p>In this example we are using a location as input to validate the files in a specific folder.\nHere, we are forcing the DQ Validations to fail by using the critical functions feature, which will throw an error\nif any of the functions fails.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_dq_validation</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/silver/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;dq_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">}},</span>\n        <span class=\"p\">],</span>\n        <span class=\"s2\">&quot;critical_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">17</span><span class=\"p\">}},</span>\n        <span class=\"p\">],</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;restore_prev_version&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_dq_validation</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"example-5-files-as-input-failure-on-dq-validation-max-failure-percentage-defined\">Example 5: Files as input &amp; Failure on DQ Validation &amp; Max failure percentage defined</h3>\n\n<p>In this example we are using a location as input to validate the files in a specific folder.\nHere, we are forcing the DQ Validations to fail by using the max_percentage_failure,\nwhich will throw an error if the percentage of failures surpasses the defined maximum threshold.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_dq_validation</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/silver/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;dq_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_input&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_not_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_date&quot;</span><span class=\"p\">}},</span>\n            <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">17</span><span class=\"p\">}},</span>\n        <span class=\"p\">],</span>\n        <span class=\"s2\">&quot;max_percentage_failure&quot;</span><span class=\"p\">:</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;restore_prev_version&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_dq_validation</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h2 id=\"limitations\">Limitations</h2>\n\n<p>Unlike DataLoader, this new DQValidator algorithm only allows, for now, one input_spec (instead of a list of input_specs) and one dq_spec (instead of a list of dq_specs). There are plans and efforts already initiated to make this available in the input_specs and one dq_spec (instead of a list of dq_specs). However, you can prepare a Dataframe which joins more than a source, and use it as input, in case you need to assess the Data Quality from different sources at the same time. Alternatively, you can also show interest on any enhancement on this feature, as well as contributing yourself.</p>\n"}, {"fullname": "lakehouse_engine_usage.data_quality.minimal_example", "modulename": "lakehouse_engine_usage.data_quality.minimal_example", "kind": "module", "doc": "<h1 id=\"minimal-example\">Minimal Example</h1>\n\n<p>This scenario illustrates the minimal configuration that you can have to use <code>dq_specs</code>, in which\nit uses required parameters: <code>spec_id, input_id, dq_type, bucket, dq_functions</code> and the optional\nparameter <code>data_docs_bucket</code>. This parameter allows you to store the GX documentation in another\nbucket that can be used to make your data docs available, in DQ Web App (GX UI), without giving users access to your bucket.\nThe<code>data_docs_bucket</code> property supersedes the <code>bucket</code> property only for data docs storage.</p>\n\n<p>Regarding the dq_functions, it uses 3 functions (retrieved from the expectations supported by GX), which check:</p>\n\n<ul>\n<li><strong>expect_column_to_exist</strong> - if a column exist in the data;</li>\n<li><strong>expect_table_row_count_to_be_between</strong> - if the row count of the data is between the defined interval;</li>\n<li><strong>expect_table_column_count_to_be_between</strong> - if the number of columns in the data is bellow the max value defined.</li>\n</ul>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">25</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_column_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">7</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_quality.result_sink", "modulename": "lakehouse_engine_usage.data_quality.result_sink", "kind": "module", "doc": "<h1 id=\"result-sink\">Result Sink</h1>\n\n<p>These scenarios store the results of the dq_specs into a result sink. For that, both scenarios include parameters defining\nthe specific table and location (<code>result_sink_db_table</code> and <code>result_sink_location</code>) where the results\nare expected to be stored. With this configuration, people can, later on, check the history of the DQ\nexecutions using the configured table/location, as shown bellow. You can configure saving the output of the\nresults in the result sink following two approaches:</p>\n\n<ul>\n<li><a href=\"#1-result-sink-exploded-recommended\"><strong>Denormalized/exploded Data Model (recommended)</strong></a> - the results are stored in a detailed format in which\npeople are able to analyse them by Data Quality Run, by expectation_type and by keyword arguments.</li>\n</ul>\n\n<table>\n<thead>\n<tr>\n  <th>...</th>\n  <th>source</th>\n  <th>column</th>\n  <th>max_value</th>\n  <th>min_value</th>\n  <th>expectation_type</th>\n  <th>expectation_success</th>\n  <th>observed_value</th>\n  <th>run_time_year</th>\n  <th>...</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>all columns from raw + more</td>\n  <td>deliveries</td>\n  <td>salesorder</td>\n  <td>null</td>\n  <td>null</td>\n  <td>expect_column_to_exist</td>\n  <td>TRUE</td>\n  <td>null</td>\n  <td>2023</td>\n  <td>...</td>\n</tr>\n<tr>\n  <td>all columns from raw + more</td>\n  <td>deliveries</td>\n  <td>null</td>\n  <td>null</td>\n  <td>null</td>\n  <td>expect_table_row_count_to_be_between</td>\n  <td>TRUE</td>\n  <td>23</td>\n  <td>2023</td>\n  <td>...</td>\n</tr>\n<tr>\n  <td>all columns from raw + more</td>\n  <td>deliveries</td>\n  <td>null</td>\n  <td>null</td>\n  <td>null</td>\n  <td>expect_table_column_count_to_be_between</td>\n  <td>TRUE</td>\n  <td>6</td>\n  <td>2023</td>\n  <td>...</td>\n</tr>\n</tbody>\n</table>\n\n<ul>\n<li><a href=\"#2-raw-result-sink\"><strong>Raw Format Data Model (not recommended)</strong></a> - the results are stored in the raw format that Great\nExpectations outputs. This is not recommended as the data will be highly nested and in a\nstring format (to prevent problems with schema changes), which makes analysis and the creation of a dashboard on top way \nharder.</li>\n</ul>\n\n<table>\n<thead>\n<tr>\n  <th>checkpoint_config</th>\n  <th>run_name</th>\n  <th>run_time</th>\n  <th>run_results</th>\n  <th>success</th>\n  <th>validation_result_identifier</th>\n  <th>spec_id</th>\n  <th>input_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>entire configuration</td>\n  <td>20230323-...-dq_validation</td>\n  <td>2023-03-23T15:11:32.225354+00:00</td>\n  <td>results of the 3 expectations</td>\n  <td>true/false for the run</td>\n  <td>identifier</td>\n  <td>spec_id</td>\n  <td>input_id</td>\n</tr>\n</tbody>\n</table>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<ul>\n<li>More configurations can be applied in the result sink, as the file format and partitions.</li>\n<li>It is recommended to:\n<ul>\n<li>Use the same result sink table/location for all dq_specs across different data loads, from different \nsources, in the same Data Product.</li>\n<li>Use the parameter <code>source</code> (only available with <code>\"result_sink_explode\": True</code>), in the dq_specs, as\nused in both scenarios, with the name of the data source, to be easier to distinguish sources in the\nanalysis. If not specified, the <code>input_id</code> of the dq_spec will be considered as the <code>source</code>.</li>\n<li>These recommendations will enable more rich analysis/dashboard at Data Product level, considering\nall the different sources and data loads that the Data Product is having.</li>\n</ul></li>\n</ul>\n\n</div>\n\n<h2 id=\"1-result-sink-exploded-recommended\">1. Result Sink Exploded (Recommended)</h2>\n\n<p>This scenario stores DQ Results (results produces by the execution of the dq_specs) in the Result Sink,\nin a detailed format, in which people are able to analyse them by Data Quality Run, by expectation_type and\nby keyword arguments. This is the recommended approach since it makes the analysis on top of the result\nsink way easier and faster.</p>\n\n<p>For achieving the exploded data model, this scenario introduces the parameter <code>result_sink_explode</code>, which\nis a flag to determine if the output table/location should have the columns exploded (as <code>True</code>) or\nnot (as <code>False</code>). <strong>Default:</strong> <code>True</code>, but it is still provided explicitly in this scenario for demo purposes.\nThe table/location will include a schema which contains general columns, statistic columns, arguments of\nexpectations, and others, thus part of the schema will be always with values and other part will depend on\nthe expectations chosen.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_result_sink&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_path/dq_result_sink/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_explode&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;source&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_success&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">25</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_column_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">7</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>To check the history of the DQ results, you can run commands like:</p>\n\n<ul>\n<li>the table: <code>display(spark.table(\"my_database.dq_result_sink\"))</code></li>\n<li>the location: <code>display(spark.read.format(\"delta\").load(\"my_dq_path/dq_result_sink/\"))</code></li>\n</ul>\n\n<h2 id=\"2-raw-result-sink\">2. Raw Result Sink</h2>\n\n<p>This scenario is very similar to the previous one, but it changes the parameter <code>result_sink_explode</code> to <code>False</code> so that\nit produces a raw result sink output containing only one row representing the full run of <code>dq_specs</code> (no\nmatter the amount of expectations/dq_functions defined there). Being a raw output, <strong>it is not a\nrecommended approach</strong>, as it will be more complicated to analyse and make queries on top of it.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_result_sink_raw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_path/dq_result_sink_raw/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_explode&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;{{ configs.database }}.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;source&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_success_raw&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">25</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_column_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">7</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>To check the history of the DQ results, you can run commands like:</p>\n\n<ul>\n<li>the table: <code>display(spark.table(\"my_database.dq_result_sink_raw\"))</code></li>\n<li>the location: <code>display(spark.read.format(\"delta\").load(\"my_dq_path/dq_result_sink_raw/\"))</code></li>\n</ul>\n"}, {"fullname": "lakehouse_engine_usage.data_quality.row_tagging", "modulename": "lakehouse_engine_usage.data_quality.row_tagging", "kind": "module", "doc": "<h1 id=\"row-tagging\">Row Tagging</h1>\n\n<p>Data quality is essential for any organisation that relies on data to make informed decisions. \nHigh-quality data provides accurate, reliable, and timely information that enables organisations to identify\nopportunities, mitigate risks, and optimize their operations. In contrast, low-quality data can lead to incorrect\nconclusions, faulty decisions, and wasted resources.</p>\n\n<p>There are several common issues that can compromise data quality, such as:</p>\n\n<ul>\n<li>data entry errors; </li>\n<li>data duplication; </li>\n<li>incomplete / inconsistent data; </li>\n<li>changes where data is collected (e.g. sources); </li>\n<li>faulty data processing, such as inaccurate data cleansing or transformations.</li>\n</ul>\n\n<p>Therefore, implementing data quality controls, such as data validation rules, and regularly monitoring data for \naccuracy and completeness is key for any organisation.</p>\n\n<p>One of these controls that can be applied is the <strong>DQ Row Tagging Strategy</strong> so that you not only apply validations on \nyour data to ensure Data Quality, but you also tag your data with the results of the Data Quality validations \nproviding advantages like:</p>\n\n<ul>\n<li>Transparency for downstream and upstream consumers; </li>\n<li>Data Observability and Reliability; </li>\n<li>More trust over the data; </li>\n<li>Anomaly Detection; </li>\n<li>Easier and faster discovery of Data Quality problems, and, consequently faster resolution; </li>\n<li>Makes it easier to deal with integrations with other systems and migrations (you can have validations capturing that a column was changed or simply disappeared);</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>When using the DQ Row Tagging approach data availability will take precedence over Data Quality, meaning \nthat all the data will be introduced into the final target (e.g. table or location) no matter what Data Quality\nissues it is having.</p>\n\n</div>\n\n<p>Different Types of Expectations:</p>\n\n<ul>\n<li>Table Level </li>\n<li>Column Aggregated Level </li>\n<li>Query Level </li>\n<li>Column Values (<strong>row level</strong>)</li>\n<li>Column Pair Value (<strong>row level</strong>)</li>\n<li>Multicolumn Values (<strong>row level</strong>)</li>\n</ul>\n\n<p>The expectations highlighted as <strong>row level</strong> will be the ones enabling to Tag failures on specific rows and adding \nthe details about each failure (they affect the field <strong>run_row_result</strong> inside <strong>dq_validations</strong>). The expectations \nwith other levels (not row level) influence the overall result of the Data Quality execution, but won't be used to tag\nspecific rows (they affect the field <strong>run_success</strong> only, so you can even have situations for which you get \n<strong>run_success False</strong> and <strong>run_row_success True</strong> for all rows).</p>\n\n<h2 id=\"how-does-the-strategy-work\">How does the Strategy work?</h2>\n\n<p>The strategy relies mostly on the 6 below arguments.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>When you specify <code>\"tag_source_data\": True</code> the arguments <strong>fail_on_error</strong>, <strong>gx_result_format</strong> and \n<strong>result_sink_explode</strong> are set to the expected values.</p>\n\n</div>\n\n<ul>\n<li><strong>unexpected_rows_pk</strong> - the list columns composing the primary key of the source data to use to identify the rows \nfailing the DQ validations. </li>\n<li><strong>tbl_to_derive_pk</strong> - <code>db.table</code> to automatically derive the unexpected_rows_pk from. </li>\n<li><strong>gx_result_format</strong> - great expectations result format. Default: <code>COMPLETE</code>. </li>\n<li><strong>tag_source_data</strong> - flag to enable the tagging strategy in the source data, adding the information of \nthe DQ results in a column <code>dq_validations</code>. This column makes it possible to identify if the DQ run was\nsucceeded in general and, if not, it unlocks the insights to know what specific rows have made the DQ validations\nfail and why. Default: <code>False</code>.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>It only works if result_sink_explode is <code>True</code>, result_format is <code>COMPLETE</code> and \nfail_on_error is `False.</p>\n\n</div>\n\n<ul>\n<li><strong>fail_on_error</strong> - whether to fail the algorithm if the validations of your data in the DQ process failed. </li>\n<li><strong>result_sink_explode</strong> - flag to determine if the output table/location should have the columns exploded (as <code>True</code>)\nor not (as <code>False</code>). Default: <code>True</code>.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>It is mandatory to provide one of the arguments (<strong>unexpected_rows_pk</strong> or <strong>tbl_to_derive_pk</strong>) when using \n<strong>tag_source_data</strong> as <strong>True</strong>. \nWhen <strong>tag_source_data</strong> is <strong>False</strong>, this is not mandatory, but <strong>still recommended</strong>.</p>\n\n</div>\n\n<p><img src=\"../../assets/img/row_tagging.png?raw=true\" style=\"max-width: 800px; height: auto; \"/></p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>The tagging strategy only works when <code>tag_source_data</code> is <code>True</code>, which automatically\nassigns the expected values for the parameters <code>result_sink_explode</code> (True), <code>fail_on_error</code> (False)\nand <code>gx_result_format</code> (\"COMPLETE\").</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>For the DQ Row Tagging to work, in addition to configuring the aforementioned arguments in the dq_specs, \nyou will also need to add the <strong>dq_validations</strong> field into your table (your DDL statements, <strong>recommended</strong>) or \nenable schema evolution.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Kwargs field is a string, because it can assume different schemas for different expectations and runs. \nIt is useful to provide the complete picture of the <strong>row level failure</strong> and to allow filtering/joining with \nthe result sink table, when there is one. Some examples of kwargs bellow:</p>\n\n<ul>\n<li><code>{\"column\": \"country\", \"min_value\": 1, \"max_value\": 2, \"batch_id\": \"o723491yyr507ho4nf3\"}</code> \u2192 example for \nexpectations starting with <code>expect_column_values</code> (they always make use of \"column\", the other arguments vary). </li>\n<li><code>{\"column_A: \"country\", \"column_B\": \"city\", \"batch_id\": \"o723491yyr507ho4nf3\"}</code> \u2192 example for expectations \nstarting with <code>expect_column_pair</code> (they make use of \"column_A\" and \"column_B\", the other arguments vary). </li>\n<li><code>{\"column_list\": [\"col1\", \"col2\", \"col3\"], \"batch_id\": \"o723491yyr507ho4nf3\"}</code> \u2192 example for expectations \nstarting with <code>expect_multicolumn</code> (they make use of \"column_list\", the other arguments vary).\n<code>batch_id</code> is common to all expectations, and it is an identifier for the batch of data being validated by\nGreat Expectations.</li>\n</ul>\n\n</div>\n\n<h3 id=\"example\">Example</h3>\n\n<p>This scenario uses the row tagging strategy which allow users to tag the rows that failed to be easier to\nidentify the problems in the validations.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket }}&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_result_sink&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_path/dq_result_sink/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tag_source_data&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;source&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_tag&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">25</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_be_in_set&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;value_set&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;37&quot;</span><span class=\"p\">]},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_pair_a_to_be_smaller_or_equal_than_b&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column_A&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;column_B&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_item&quot;</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_multicolumn_sum_to_equal&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column_list&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;delivery_item&quot;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;sum_total&quot;</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;critical_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_column_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">6</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Running bellow cell shows the new column created, named <code>dq_validations</code> with information about DQ validations.\n<code>display(spark.read.format(\"delta\").load(\"s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/\"))</code></p>\n\n<h2 id=\"performance-and-limitations-trade-offs\">Performance and Limitations Trade-offs</h2>\n\n<p>When using the DQ Row Tagging Strategy, by default we are using Great Expectations Result Format \"Complete\" with \nUnexpected Index Column Names (a primary key for the failures), meaning that for each failure, we are getting all \nthe distinct values for the primary key. After getting all the failures, we are applying some needed transformations \nand joining them with the source data, so that it can be tagged by filling the \"dq_validations\" column.</p>\n\n<p>Hence, this can definitely be a heavy and time-consuming operation on your data loads. To reduce this disadvantage \nyou can cache the dataframe by passing the <code>\"cache_df\": True</code> in your DQ Specs. In addition to this, always have in \nmind that each expectation (dq_function) that you add into your DQ Specs, is more time that you are adding into your \ndata loads, so always balance performance vs amount of validations that you need.</p>\n\n<p>Moreover, Great Expectations is currently relying on the driver node to capture the results of the execution and \nreturn/store them. Thus, in case you have huge amounts of rows failing (let's say 500k or more) Great Expectations \nmight raise exceptions.</p>\n\n<p>On these situations, the data load will still happen and the data will still be tagged with the Data Quality \nvalidations information, however you won't have the complete picture of the failures, so the raised_exceptions \nfield is filled as True, so that you can easily notice it and debug it.</p>\n\n<p>Most of the time, if you have such an amount of rows failing, it will probably mean that you did something wrong \nand want to fix it as soon as possible (you are not really caring about tagging specific rows, because you will \nnot want your consumers to be consuming a million of defective rows). However, if you still want to try to make it \npass, you can try to increase your driver and play with some spark configurations like: </p>\n\n<ul>\n<li><code>spark.driver.maxResultSize</code></li>\n<li><code>spark.task.maxFailures</code></li>\n</ul>\n\n<p>For debugging purposes, you can also use a different <a href=\"a href=&quot;https://docs.greatexpectations.io/docs/reference/expectations/result_format/&quot;&gt;https://docs.greatexpectations.io/docs/reference/expectations/result_format/&lt;/a\">Great Expectations Result Format</a> like \"SUMMARY\" (adding in your DQ Spec\n<code>\"gx_result_format\": \"SUMMARY\"</code>), so that you get only a partial list of the failures, avoiding surpassing the driver\ncapacity. </p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>When using a Result Format different from the default (\"COMPLETE\"), the flag \"tag_source_data\" will be \noverwritten to <code>False</code>, as the results of the tagging wouldn't be complete which could lead to erroneous \nconclusions from stakeholders (but you can always get the details about the result of the DQ execution in\nthe <code>result_sink_location</code> or <code>result_sink_db_table</code> that you have configured).</p>\n\n</div>\n"}, {"fullname": "lakehouse_engine_usage.data_quality.validations_failing", "modulename": "lakehouse_engine_usage.data_quality.validations_failing", "kind": "module", "doc": "<h1 id=\"validations-failing\">Validations Failing</h1>\n\n<p>The scenarios presented on this page are similar, but their goal is to show what happens when a DQ expectation fails the validations.\nThe logs generated by the execution of the code will contain information regarding which expectation(s) have failed and why.</p>\n\n<h2 id=\"1-fail-on-error\">1. Fail on Error</h2>\n\n<p>In this scenario is specified below two parameters:</p>\n\n<ul>\n<li><code>\"fail_on_error\": False</code> - this parameter is what controls what happens if a DQ expectation fails. In case\nthis is set to <code>true</code> (default), your job will fail/be aborted and an exception will be raised.\nIn case this is set to <code>false, a log message will be printed about the error (as shown in this\nscenario) and the result status will also be available in result sink (if configured) and in the\n[data docs great expectation site](../data_quality.html#3-data-docs-website). On this scenario it is set to</code>false` \nto avoid failing the execution of the notebook.</li>\n<li>the <code>max_value</code> of the function <code>expect_table_column_count_to_be_between</code> is defined with specific value so that\nthis expectation fails the validations.</li>\n</ul>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_result_sink&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_path/dq_result_sink/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;source&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_fail&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;fail_on_error&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">20</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_column_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_be_null&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;article&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_values_to_be_unique&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;status&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_min_to_be_between&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_max_to_be_between&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delivery_item&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">30</span><span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>If you run bellow command, you would be able to see the <code>success</code> column has the value <code>false</code>\nfor the last execution.\n<code>display(spark.table(RENDER_UTILS.render_content(\"my_database.dq_result_sink\")))</code></p>\n\n<h2 id=\"2-critical-functions\">2. Critical Functions</h2>\n\n<p>In this scenario, alternative parameters to <code>fail_on_error</code> are used:</p>\n\n<ul>\n<li><code>critical_functions</code> - this parameter defaults to <code>None</code> if not defined.\nIt controls what DQ functions are considered a priority and as such, it stops the validation\nand throws an execution error whenever a function defined as critical doesn't pass the test.\nIf any other function that is not defined in this parameter fails, an error message is printed in the logs.\nThis parameter has priority over <code>fail_on_error</code>.\nIn this specific example, after defining the <code>expect_table_column_count_to_be_between</code> as critical,\nit is made sure that the execution is stopped whenever the conditions for the function are not met.</li>\n</ul>\n\n<p>Additionally, it can also be defined additional parameters like:</p>\n\n<ul>\n<li><code>max_percentage_failure</code> - this parameter defaults to <code>None</code> if not defined.\nIt controls what percentage of the total functions can fail without stopping the execution of the validation.\nIf the threshold is surpassed the execution stops and a failure error is thrown.\nThis parameter has priority over <code>fail_on_error</code> and <code>critical_functions</code>.</li>\n</ul>\n\n<p>You can also pair <code>critical_functions</code> with <code>max_percentage_failure</code> by defining something like\na 0.6 max percentage of failure and also defining some critical function.\nIn this case even if the threshold is respected, the list defined on <code>critical_functions</code> still is checked.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">load_data</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;input_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;header&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;|&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;inferSchema&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/dummy_deliveries/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;dq_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_source&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_data_product_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_bucket&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_data_docs_bucket&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_docs_prefix&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq/my_data_product/data_docs/site/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dq_result_sink&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;result_sink_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_dq_path/dq_result_sink/&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;source&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;deliveries_critical&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;tbl_to_derive_pk&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.dummy_deliveries&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dq_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_column_to_exist&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;column&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;salesorder&quot;</span><span class=\"p\">}},</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_row_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;min_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">25</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n            <span class=\"s2\">&quot;critical_functions&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span><span class=\"s2\">&quot;function&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;expect_table_column_count_to_be_between&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;max_value&quot;</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">}},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"s2\">&quot;output_specs&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dummy_deliveries_bronze&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;input_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;dq_validator&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;write_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;overwrite&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/dummy_deliveries_dq_template/&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">load_data</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.reconciliator", "modulename": "lakehouse_engine_usage.reconciliator", "kind": "module", "doc": "<h1 id=\"reconciliator\">Reconciliator</h1>\n\n<p>Checking if data reconciles, using this algorithm, is a matter of reading the <strong>truth</strong> data and the <strong>current</strong> data.\nYou can use any input specification compatible with the lakehouse engine to read <strong>truth</strong> or <strong>current</strong> data. On top\nof that, you can pass a <code>truth_preprocess_query</code> and a <code>current_preprocess_query</code> so you can preprocess the data before\nit goes into the actual reconciliation process. The reconciliation process is focused on joining <strong>truth</strong>\nwith <code>current</code> by all provided columns except the ones passed as <code>metrics</code>.</p>\n\n<p>In the table below, we present how a simple reconciliation would look like:</p>\n\n<table>\n<thead>\n<tr>\n  <th>current_country</th>\n  <th>current_count</th>\n  <th>truth_country</th>\n  <th>truth_count</th>\n  <th>absolute_diff</th>\n  <th>perc_diff</th>\n  <th>yellow</th>\n  <th>red</th>\n  <th>recon_type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>Sweden</td>\n  <td>123</td>\n  <td>Sweden</td>\n  <td>120</td>\n  <td>3</td>\n  <td>0.025</td>\n  <td>0.1</td>\n  <td>0.2</td>\n  <td>percentage</td>\n</tr>\n<tr>\n  <td>Germany</td>\n  <td>2946</td>\n  <td>Sweden</td>\n  <td>2946</td>\n  <td>0</td>\n  <td>0</td>\n  <td>0.1</td>\n  <td>0.2</td>\n  <td>percentage</td>\n</tr>\n<tr>\n  <td>France</td>\n  <td>2901</td>\n  <td>France</td>\n  <td>2901</td>\n  <td>0</td>\n  <td>0</td>\n  <td>0.1</td>\n  <td>0.2</td>\n  <td>percentage</td>\n</tr>\n<tr>\n  <td>Belgium</td>\n  <td>426</td>\n  <td>Belgium</td>\n  <td>425</td>\n  <td>1</td>\n  <td>0.002</td>\n  <td>0.1</td>\n  <td>0.2</td>\n  <td>percentage</td>\n</tr>\n</tbody>\n</table>\n\n<p>The Reconciliator algorithm uses an ACON to configure its execution. You can find the meaning of each ACON property\nin <a href=\"../lakehouse_engine/core/definitions.html#ReconciliatorSpec\">ReconciliatorSpec object</a>.</p>\n\n<p>Below there is an example of usage of reconciliator.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_reconciliation</span>\n\n<span class=\"n\">truth_query</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">  SELECT</span>\n<span class=\"s2\">    shipping_city,</span>\n<span class=\"s2\">    sum(sales_order_qty) as qty,</span>\n<span class=\"s2\">    order_date_header</span>\n<span class=\"s2\">  FROM (</span>\n<span class=\"s2\">    SELECT</span>\n<span class=\"s2\">      ROW_NUMBER() OVER (</span>\n<span class=\"s2\">        PARTITION BY sales_order_header, sales_order_schedule, sales_order_item, shipping_city</span>\n<span class=\"s2\">        ORDER BY changed_on desc</span>\n<span class=\"s2\">      ) as rank1,</span>\n<span class=\"s2\">      sales_order_header,</span>\n<span class=\"s2\">      sales_order_item,</span>\n<span class=\"s2\">      sales_order_qty,</span>\n<span class=\"s2\">      order_date_header,</span>\n<span class=\"s2\">      shipping_city</span>\n<span class=\"s2\">    FROM truth -- truth is a locally accessible temp view created by the lakehouse engine</span>\n<span class=\"s2\">    WHERE order_date_header = &#39;2021-10-01&#39;</span>\n<span class=\"s2\">  ) a</span>\n<span class=\"s2\">WHERE a.rank1 = 1</span>\n<span class=\"s2\">GROUP BY a.shipping_city, a.order_date_header</span>\n<span class=\"s2\">&quot;&quot;&quot;</span>\n\n<span class=\"n\">current_query</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">  SELECT</span>\n<span class=\"s2\">    shipping_city,</span>\n<span class=\"s2\">    sum(sales_order_qty) as qty,</span>\n<span class=\"s2\">    order_date_header</span>\n<span class=\"s2\">  FROM (</span>\n<span class=\"s2\">    SELECT</span>\n<span class=\"s2\">      ROW_NUMBER() OVER (</span>\n<span class=\"s2\">        PARTITION BY sales_order_header, sales_order_schedule, sales_order_item, shipping_city</span>\n<span class=\"s2\">        ORDER BY changed_on desc</span>\n<span class=\"s2\">      ) as rank1,</span>\n<span class=\"s2\">      sales_order_header,</span>\n<span class=\"s2\">      sales_order_item,</span>\n<span class=\"s2\">      sales_order_qty,</span>\n<span class=\"s2\">      order_date_header,</span>\n<span class=\"s2\">      shipping_city</span>\n<span class=\"s2\">    FROM current -- current is a locally accessible temp view created by the lakehouse engine</span>\n<span class=\"s2\">    WHERE order_date_header = &#39;2021-10-01&#39;</span>\n<span class=\"s2\">  ) a</span>\n<span class=\"s2\">WHERE a.rank1 = 1</span>\n<span class=\"s2\">GROUP BY a.shipping_city, a.order_date_header</span>\n<span class=\"s2\">&quot;&quot;&quot;</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;metrics&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;metric&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;qty&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;percentage&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;aggregation&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;avg&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;yellow&quot;</span><span class=\"p\">:</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"s2\">&quot;red&quot;</span><span class=\"p\">:</span> <span class=\"mf\">0.1</span><span class=\"p\">}],</span>\n    <span class=\"s2\">&quot;truth_input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;truth&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;schema_path&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/artefacts/metadata/schemas/bronze/orders.json&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;delimiter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;^&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;dateFormat&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;yyyyMMdd&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/bronze/orders&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;truth_preprocess_query&quot;</span><span class=\"p\">:</span> <span class=\"n\">truth_query</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;current_input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;current&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.orders&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;current_preprocess_query&quot;</span><span class=\"p\">:</span> <span class=\"n\">current_query</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_reconciliation</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor", "modulename": "lakehouse_engine_usage.sensor", "kind": "module", "doc": "<h1 id=\"sensor\">Sensor</h1>\n\n<h2 id=\"what-is-it\">What is it?</h2>\n\n<p>The lakehouse engine sensors are an abstraction to otherwise complex spark code that can be executed in very small\nsingle-node clusters to check if an upstream system or data product contains new data since the last execution of our\njob. With this feature, we can trigger a job to run in more frequent intervals and if the upstream does not contain new\ndata, then the rest of the job exits without creating bigger clusters to execute more intensive data ETL (Extraction,\nTransformation, and Loading).</p>\n\n<h2 id=\"how-do-sensor-based-jobs-work\">How do Sensor-based jobs work?</h2>\n\n<p><img src=\"../assets/img/sensor_os.png\" alt=\"image\" width=\"1000px\" height=\"auto\"></p>\n\n<p>With the sensors capability, data products in the lakehouse can sense if another data product or an upstream system (source\nsystem) have new data since the last successful job. We accomplish this through the approach illustrated above, which\ncan be interpreted as follows:</p>\n\n<ol>\n<li>A Data Product can check if Kafka, JDBC or any other Lakehouse Engine Sensors supported sources, contains new data using the respective sensors;</li>\n<li>The Sensor task may run in a very tiny single-node cluster to ensure cost\nefficiency (<a href=\"#are-sensor-based-jobs-cost-efficient\">check sensor cost efficiency</a>);</li>\n<li>If the sensor has recognised that there is new data in the upstream, then you can start a different ETL Job Cluster\nto process all the ETL tasks (data processing tasks).</li>\n<li>In the same way, a different Data Product can sense if an upstream Data Product has new data by using 1 of 2 options:\n<ol>\n<li><strong>(Preferred)</strong> Sense the upstream Data Product sensor control delta table;</li>\n<li>Sense the upstream Data Product data files in s3 (files sensor) or any of their delta tables (delta table\nsensor);</li>\n</ol></li>\n</ol>\n\n<h2 id=\"the-structure-and-relevance-of-the-data-products-sensors-control-table\">The Structure and Relevance of the Data Product\u2019s Sensors Control Table</h2>\n\n<p>The concept of a lakehouse engine sensor is based on a special delta table stored inside the data product that chooses\nto opt in for a sensor-based job. That table is used to control the status of the various sensors implemented by that\ndata product. You can refer to the below table to understand the sensor delta table structure:</p>\n\n<table>\n<thead>\n<tr>\n  <th>Column Name</th>\n  <th>Type</th>\n  <th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td><strong>sensor_id</strong></td>\n  <td>STRING</td>\n  <td>A unique identifier of the sensor in a specific job. This unique identifier is really important because it is used by the engine to identify if there is new data in the upstream.<br />Each sensor in each job should have a different sensor_id.<br />If you attempt to create 2 sensors with the same sensor_id, the engine will fail.</td>\n</tr>\n<tr>\n  <td><strong>assets</strong></td>\n  <td>ARRAY&lt;STRING></td>\n  <td>A list of assets (e.g., tables or dataset folder) that are considered as available to consume downstream after the sensor has status <em>PROCESSED_NEW_DATA</em>.</td>\n</tr>\n<tr>\n  <td><strong>status</strong></td>\n  <td>STRING</td>\n  <td>Status of the sensor. Can either be:<br /><ul><li><em>ACQUIRED_NEW_DATA</em> \u2013 when the sensor in a job has recognised that there is new data from the upstream but, the job where the sensor is, was still not successfully executed.</li><li><em>PROCESSED_NEW_DATA</em> - when the job where the sensor is located has processed all the tasks in that job.</li></ul></td>\n</tr>\n<tr>\n  <td><strong>status_change_timestamp</strong></td>\n  <td>STRING</td>\n  <td>Timestamp when the status has changed for the last time.</td>\n</tr>\n<tr>\n  <td><strong>checkpoint_location</strong></td>\n  <td>STRING</td>\n  <td>Base location of the Spark streaming checkpoint location, when applicable (i.e., when the type of sensor uses Spark streaming checkpoints to identify if the upstream has new data). E.g. Spark streaming checkpoints are used for Kafka, Delta and File sensors.</td>\n</tr>\n<tr>\n  <td><strong>upstream_key</strong></td>\n  <td>STRING</td>\n  <td>Upstream key (e.g., used to store an attribute name from the upstream so that new data can be detected automatically).<br />This is useful for sensors that do not rely on Spark streaming checkpoints, like the JDBC sensor, as it stores the name of a field in the JDBC upstream that contains the values that will allow us to identify new data (e.g., a timestamp in the upstream that tells us when the record was loaded into the database).</td>\n</tr>\n<tr>\n  <td><strong>upstream_value</strong></td>\n  <td>STRING</td>\n  <td>Upstream value (e.g., used to store the max attribute value from the upstream so that new data can be detected automatically). This is the value for upstream_key. <br />This is useful for sensors that do not rely on Spark streaming checkpoints, like the JDBC sensor, as it stores the value of a field in the JDBC upstream that contains the maximum value that was processed by the sensor, and therefore useful for recognizing that there is new data in the upstream (e.g., the value of a timestamp attribute in the upstream that tells us when the record was loaded into the database).</td>\n</tr>\n</tbody>\n</table>\n\n<p><strong>Note:</strong> to make use of the sensors you will need to add this table to your data product.</p>\n\n<h2 id=\"how-is-it-different-from-scheduled-jobs\">How is it different from scheduled jobs?</h2>\n\n<p>Sensor-based jobs are still scheduled, but they can be scheduled with higher frequency, as they are more cost-efficient\nthan ramping up a multi-node cluster supposed to do heavy ETL, only to figure out that the upstream does not have new\ndata.</p>\n\n<h2 id=\"are-sensor-based-jobs-cost-efficient\">Are sensor-based jobs cost-efficient?</h2>\n\n<p>For the same schedule (e.g., 4 times a day), sensor-based jobs are more cost-efficient than scheduling a regular job, because with sensor-based jobs you can start a <strong>very tiny single-node cluster</strong>, and only if there is new data in the upstream the bigger ETL cluster is spin up. For this reason, they are considered more cost-efficient.\nMoreover, if you have very hard SLAs to comply with, you can also play with alternative architectures where you can have several sensors in a continuous (always running) cluster, which then keeps triggering the respective data processing jobs, whenever there is new data.</p>\n\n<h2 id=\"sensor-steps\">Sensor Steps</h2>\n\n<ol>\n<li>Create your sensor task for the upstream source. Examples of available sources:\n<ul>\n<li><a href=\"sensor/delta_table.html\">Delta Table</a></li>\n<li><a href=\"sensor/delta_upstream_sensor_table.html\">Delta Upstream Sensor Table</a></li>\n<li><a href=\"sensor/file.html\">File</a></li>\n<li><a href=\"sensor/jdbc_table.html\">JDBC</a></li>\n<li><a href=\"sensor/kafka.html\">Kafka</a></li>\n<li><a href=\"sensor/sap_bw_b4.html\">SAP BW/B4</a></li>\n</ul></li>\n<li>Setup/Execute your ETL task based in the Sensor Condition</li>\n<li>Update the Sensor Control table status with the <a href=\"sensor/update_sensor_status.html\">Update Sensor Status</a></li>\n</ol>\n"}, {"fullname": "lakehouse_engine_usage.sensor.delta_table", "modulename": "lakehouse_engine_usage.sensor.delta_table", "kind": "module", "doc": "<h1 id=\"sensor-from-delta-table\">Sensor from Delta Table</h1>\n\n<p>This shows how to create a <strong>Sensor to detect new data from a Delta Table</strong>.</p>\n\n<h2 id=\"configuration-required-to-have-a-sensor\">Configuration required to have a Sensor</h2>\n\n<ul>\n<li><strong>sensor_id</strong>: A unique identifier of the sensor in a specific job.</li>\n<li><strong>assets</strong>: List of assets considered for the sensor, which are considered as available once the\nsensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li>\n<li><strong>control_db_table_name</strong>: Name of the sensor control table.</li>\n<li><strong>input_spec</strong>: Input spec with the upstream source.</li>\n<li><strong>preprocess_query</strong>: Query to filter data returned by the upstream.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-parameter-is-only-needed-when-the-upstream-data-have-to-be-filtered-in-this-case-a-custom-query-should-be-created-with-the-source-table-as-sensor_new_data\">This parameter is only needed when the upstream data have to be filtered, in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</h6>\n\n<p>If you want to view some examples of usage you can visit the <a href=\"../delta_upstream_sensor_table/delta_upstream_sensor_table.md\">delta upstream sensor table</a> or the <a href=\"../jdbc_table/jdbc_table.md\">jdbc sensor</a>.</p>\n\n</div>\n\n<ul>\n<li><strong>base_checkpoint_location</strong>: Spark streaming checkpoints to identify if the upstream has new data.</li>\n<li><strong>fail_on_empty_result</strong>: Flag representing if it should raise <code>NoNewDataException</code> when\nthere is no new data detected from upstream.</li>\n</ul>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n\n<h2 id=\"scenarios\">Scenarios</h2>\n\n<p>This covers the following scenarios of using the Sensor:</p>\n\n<ol>\n<li><a href=\"#fail_on_empty_result-as-true-default-and-suggested\">The <code>fail_on_empty_result=True</code> (the default and <strong>SUGGESTED</strong> behaviour).</a></li>\n<li><a href=\"#fail_on_empty_result-as-false\">The <code>fail_on_empty_result=False</code>.</a></li>\n</ol>\n\n<p>Data will be consumed from a delta table in streaming mode,\nso if there is any new data it will give condition to proceed to the next task.</p>\n\n<h3 id=\"fail_on_empty_result-as-true-default-and-suggested\"><code>fail_on_empty_result</code> as True (default and SUGGESTED)</h3>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;sensor_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;assets&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;control_db_table_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sensor_upstream&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;upstream_database.source_delta_table&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;readChangeFeed&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;true&quot;</span><span class=\"p\">,</span> <span class=\"c1\"># to read changes in upstream table</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;base_checkpoint_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"fail_on_empty_result-as-false\"><code>fail_on_empty_result</code> as False</h3>\n\n<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it \nhas acquired new data. This value can be used to execute or not the next steps.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">acquired_data</span> <span class=\"o\">=</span> <span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor.delta_upstream_sensor_table", "modulename": "lakehouse_engine_usage.sensor.delta_upstream_sensor_table", "kind": "module", "doc": "<h1 id=\"sensor-from-other-sensor-delta-table\">Sensor from other Sensor Delta Table</h1>\n\n<p>This shows how to create a <strong>Sensor to detect new data from another Sensor Delta Table</strong>.</p>\n\n<h2 id=\"configuration-required-to-have-a-sensor\">Configuration required to have a Sensor</h2>\n\n<ul>\n<li><strong>sensor_id</strong>: A unique identifier of the sensor in a specific job.</li>\n<li><strong>assets</strong>: List of assets considered for the sensor, which are considered as available once the\nsensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li>\n<li><strong>control_db_table_name</strong>: Name of the sensor control table.</li>\n<li><strong>input_spec</strong>: Input spec with the upstream source.</li>\n<li><strong>preprocess_query</strong>: Query to filter data returned by the upstream.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-parameter-is-only-needed-when-the-upstream-data-have-to-be-filtered\">This parameter is only needed when the upstream data have to be filtered,</h6>\n\n<p>in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p>\n\n</div>\n\n<ul>\n<li><strong>base_checkpoint_location</strong>: Spark streaming checkpoints to identify if the upstream has new data.</li>\n<li><strong>fail_on_empty_result</strong>: Flag representing if it should raise <code>NoNewDataException</code> when\nthere is no new data detected from upstream.</li>\n</ul>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n\n<h2 id=\"scenarios\">Scenarios</h2>\n\n<p>This covers the following scenarios of using the Sensor:</p>\n\n<ol>\n<li><a href=\"#fail_on_empty_result-as-true-default-and-suggested\">The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</a></li>\n<li><a href=\"#fail_on_empty_result-as-false\">The <code>fail_on_empty_result=False</code>.</a></li>\n</ol>\n\n<p>It makes use of <code>generate_sensor_query</code> to generate the <code>preprocess_query</code>,\ndifferent from <a href=\"../sensor/delta_table.html\">delta_table</a>.</p>\n\n<p>Data from other sensor delta table, in streaming mode, will be consumed. If there is any new data it will trigger \nthe condition to proceed to the next task.</p>\n\n<h3 id=\"fail_on_empty_result-as-true-default-and-suggested\"><code>fail_on_empty_result</code> as True (default and SUGGESTED)</h3>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span><span class=\"p\">,</span> <span class=\"n\">generate_sensor_query</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;sensor_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;assets&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;control_db_table_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sensor_upstream&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;delta&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;db_table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;upstream_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;readChangeFeed&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;true&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;preprocess_query&quot;</span><span class=\"p\">:</span> <span class=\"n\">generate_sensor_query</span><span class=\"p\">(</span><span class=\"s2\">&quot;UPSTREAM_SENSOR_ID&quot;</span><span class=\"p\">),</span>\n    <span class=\"s2\">&quot;base_checkpoint_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"fail_on_empty_result-as-false\"><code>fail_on_empty_result</code> as False</h3>\n\n<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it\nhas acquired new data. This value can be used to execute or not the next steps.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">acquired_data</span> <span class=\"o\">=</span> <span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor.file", "modulename": "lakehouse_engine_usage.sensor.file", "kind": "module", "doc": "<h1 id=\"sensor-from-files\">Sensor from Files</h1>\n\n<p>This shows how to create a <strong>Sensor to detect new data from a File Location</strong>.</p>\n\n<h2 id=\"configuration-required-to-have-a-sensor\">Configuration required to have a Sensor</h2>\n\n<ul>\n<li><strong>sensor_id</strong>: A unique identifier of the sensor in a specific job.</li>\n<li><strong>assets</strong>: List of assets considered for the sensor, which are considered as available once the sensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li>\n<li><strong>control_db_table_name</strong>: Name of the sensor control table.</li>\n<li><strong>input_spec</strong>: Input spec with the upstream source.</li>\n<li><strong>preprocess_query</strong>: Query to filter data returned by the upstream.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-parameter-is-only-needed-when-the-upstream-data-have-to-be-filtered\">This parameter is only needed when the upstream data have to be filtered,</h6>\n\n<p>in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p>\n\n</div>\n\n<ul>\n<li><strong>base_checkpoint_location</strong>: Spark streaming checkpoints to identify if the upstream has new data.</li>\n<li><strong>fail_on_empty_result</strong>: Flag representing if it should raise <code>NoNewDataException</code> when\nthere is no new data detected from upstream.</li>\n</ul>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n\n<h2 id=\"scenarios\">Scenarios</h2>\n\n<p>This covers the following scenarios of using the Sensor:</p>\n\n<ol>\n<li><a href=\"#fail_on_empty_result-as-true-default-and-suggested\">The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</a></li>\n<li><a href=\"#fail_on_empty_result-as-false\">The <code>fail_on_empty_result=False</code>.</a></li>\n</ol>\n\n<p>Using these sensors and consuming the data in streaming mode, if any new file is added to the file location, \nit will automatically trigger the proceeding task.</p>\n\n<h3 id=\"fail_on_empty_result-as-true-default-and-suggested\"><code>fail_on_empty_result</code> as True (default and SUGGESTED)</h3>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;sensor_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;assets&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;control_db_table_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sensor_upstream&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;csv&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># You can use any of the data formats supported by the lakehouse engine, e.g: &quot;avro|json|parquet|csv|delta|cloudfiles&quot;</span>\n        <span class=\"s2\">&quot;location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/path&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;base_checkpoint_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"fail_on_empty_result-as-false\"><code>fail_on_empty_result</code> as False</h3>\n\n<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it\nhas acquired new data. This value can be used to execute or not the next steps.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n<span class=\"p\">}</span>\n<span class=\"n\">acquired_data</span> <span class=\"o\">=</span> <span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor.jdbc_table", "modulename": "lakehouse_engine_usage.sensor.jdbc_table", "kind": "module", "doc": "<h1 id=\"sensor-from-jdbc\">Sensor from JDBC</h1>\n\n<p>This shows how to create a <strong>Sensor to detect new data from a JDBC table</strong>.</p>\n\n<h2 id=\"configuration-required-to-have-a-sensor\">Configuration required to have a Sensor</h2>\n\n<ul>\n<li><strong>jdbc_args</strong>: Arguments of the JDBC upstream.</li>\n<li><strong>generate_sensor_query</strong>: Generates a Sensor query to consume data from the upstream, this function can be used on <code>preprocess_query</code> ACON option.\n<ul>\n<li><strong>sensor_id</strong>: The unique identifier for the Sensor.</li>\n<li><strong>filter_exp</strong>: Expression to filter incoming new data.\nA placeholder <code>?upstream_key</code> and <code>?upstream_value</code> can be used, example: <code>?upstream_key &gt; ?upstream_value</code> so that it can be replaced by the respective values from the sensor <code>control_db_table_name</code> for this specific sensor_id.</li>\n<li><strong>control_db_table_name</strong>: Sensor control table name.</li>\n<li><strong>upstream_key</strong>: the key of custom sensor information to control how to identify new data from the upstream (e.g., a time column in the upstream).</li>\n<li><strong>upstream_value</strong>: the <strong>first</strong> upstream value to identify new data from the upstream (e.g., the value of a time present in the upstream). <strong><em>Note:</em></strong> This parameter will have effect just in the first run to detect if the upstream have new data. If it's empty the default value applied is <code>-2147483647</code>.</li>\n<li><strong>upstream_table_name</strong>: Table name to consume the upstream value. If it's empty the default value applied is <code>sensor_new_data</code>.</li>\n</ul></li>\n</ul>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n\n<h2 id=\"scenarios\">Scenarios</h2>\n\n<p>This covers the following scenarios of using the Sensor:</p>\n\n<ol>\n<li><a href=\"#fail_on_empty_result-as-true-default-and-suggested\">Generic JDBC template with <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</a></li>\n<li><a href=\"#fail_on_empty_result-as-false\">Generic JDBC template with <code>fail_on_empty_result=False</code>.</a></li>\n</ol>\n\n<p>Data from JDBC, in batch mode, will be consumed. If there is new data based in the preprocess query from the source table, it will trigger the condition to proceed to the next task.</p>\n\n<h3 id=\"fail_on_empty_result-as-true-default-and-suggested\"><code>fail_on_empty_result</code> as True (default and SUGGESTED)</h3>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span><span class=\"p\">,</span> <span class=\"n\">generate_sensor_query</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;sensor_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;assets&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;control_db_table_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sensor_upstream&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;jdbc_args&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_URL&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;table&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_DB_TABLE&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;properties&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_USERNAME&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_PWD&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_DRIVER&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">},</span>\n        <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;preprocess_query&quot;</span><span class=\"p\">:</span> <span class=\"n\">generate_sensor_query</span><span class=\"p\">(</span>\n        <span class=\"n\">sensor_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">filter_exp</span><span class=\"o\">=</span><span class=\"s2\">&quot;?upstream_key &gt; &#39;?upstream_value&#39;&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">control_db_table_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">upstream_key</span><span class=\"o\">=</span><span class=\"s2\">&quot;UPSTREAM_COLUMN_TO_IDENTIFY_NEW_DATA&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">),</span>\n    <span class=\"s2\">&quot;base_checkpoint_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"fail_on_empty_result-as-false\"><code>fail_on_empty_result</code> as False</h3>\n\n<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it\nhas acquired new data. This value can be used to execute or not the next steps.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">acquired_data</span> <span class=\"o\">=</span> <span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor.kafka", "modulename": "lakehouse_engine_usage.sensor.kafka", "kind": "module", "doc": "<h1 id=\"sensor-from-kafka\">Sensor from Kafka</h1>\n\n<p>This shows how to create a <strong>Sensor to detect new data from Kafka</strong>.</p>\n\n<h2 id=\"configuration-required-to-have-a-sensor\">Configuration required to have a Sensor</h2>\n\n<ul>\n<li><strong>sensor_id</strong>: A unique identifier of the sensor in a specific job.</li>\n<li><strong>assets</strong>: List of assets considered for the sensor, which are considered as available once the\nsensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li>\n<li><strong>control_db_table_name</strong>: Name of the sensor control table.</li>\n<li><strong>input_spec</strong>: Input spec with the upstream source.</li>\n<li><strong>preprocess_query</strong>: Query to filter data returned by the upstream.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-parameter-is-only-needed-when-the-upstream-data-have-to-be-filtered\">This parameter is only needed when the upstream data have to be filtered,</h6>\n\n<p>in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p>\n\n</div>\n\n<ul>\n<li><strong>base_checkpoint_location</strong>: Spark streaming checkpoints to identify if the upstream has new data.</li>\n<li><strong>fail_on_empty_result</strong>: Flag representing if it should raise <code>NoNewDataException</code> when\nthere is no new data detected from upstream.</li>\n</ul>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n\n<h2 id=\"scenarios\">Scenarios</h2>\n\n<p>This covers the following scenarios of using the Sensor:</p>\n\n<ol>\n<li><a href=\"#fail_on_empty_result-as-true-default-and-suggested\">The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</a></li>\n<li><a href=\"#fail_on_empty_result-as-false\">The <code>fail_on_empty_result=False</code>.</a></li>\n</ol>\n\n<p>Data from Kafka, in streaming mode, will be consumed, so if there is any new data in the kafka topic it will give condition to proceed to the next task.</p>\n\n<h3 id=\"fail_on_empty_result-as-true-default-and-suggested\"><code>fail_on_empty_result</code> as True (default and SUGGESTED)</h3>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;sensor_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;assets&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;control_db_table_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sensor_upstream&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;streaming&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;kafka&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;kafka.bootstrap.servers&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;KAFKA_SERVER&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;subscribe&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;KAFKA_TOPIC&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;startingOffsets&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;earliest&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;kafka.security.protocol&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;SSL&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;kafka.ssl.truststore.location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;TRUSTSTORE_LOCATION&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;kafka.ssl.truststore.password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;TRUSTSTORE_PWD&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;kafka.ssl.keystore.location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;KEYSTORE_LOCATION&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;kafka.ssl.keystore.password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;KEYSTORE_PWD&quot;</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;base_checkpoint_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"fail_on_empty_result-as-false\"><code>fail_on_empty_result</code> as False</h3>\n\n<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it\nhas acquired new data. This value can be used to execute or not the next steps.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">acquired_data</span> <span class=\"o\">=</span> <span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor.sap_bw_b4", "modulename": "lakehouse_engine_usage.sensor.sap_bw_b4", "kind": "module", "doc": "<h1 id=\"sensor-from-sap\">Sensor from SAP</h1>\n\n<p>This shows how to create a <strong>Sensor to detect new data from a SAP LOGCHAIN table</strong>.</p>\n\n<h2 id=\"configuration-required-to-have-a-sensor\">Configuration required to have a Sensor</h2>\n\n<ul>\n<li><strong>sensor_id</strong>: A unique identifier of the sensor in a specific job.</li>\n<li><strong>assets</strong>: List of assets considered for the sensor, which are considered as available once the\nsensor detects new data and status is <code>ACQUIRED_NEW_DATA</code>.</li>\n<li><strong>control_db_table_name</strong>: Name of the sensor control table.</li>\n<li><strong>input_spec</strong>: Input spec with the upstream source.</li>\n<li><strong>preprocess_query</strong>: Query to filter data returned by the upstream.</li>\n</ul>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-parameter-is-only-needed-when-the-upstream-data-have-to-be-filtered\">This parameter is only needed when the upstream data have to be filtered,</h6>\n\n<p>in this case a custom query should be created with the source table as <code>sensor_new_data</code>.</p>\n\n</div>\n\n<ul>\n<li><strong>base_checkpoint_location</strong>: Spark streaming checkpoints to identify if the upstream has new data.</li>\n<li><strong>fail_on_empty_result</strong>: Flag representing if it should raise <code>NoNewDataException</code> when\nthere is no new data detected from upstream.</li>\n</ul>\n\n<p>Specific configuration required to have a Sensor consuming a SAP BW/B4 upstream.\nThe Lakehouse Engine provides two utility functions to make easier to consume SAP as upstream:\n<code>generate_sensor_sap_logchain_query</code> and <code>generate_sensor_query</code>.</p>\n\n<ul>\n<li><p><strong>generate_sensor_sap_logchain_query</strong>: This function aims\nto create a temporary table with timestamp from the SAP LOGCHAIN table, which is a process control table.</p>\n\n<p><strong>Note</strong>: this temporary table only lives during runtime, and it is related with the\nsap process control table but has no relationship or effect on the sensor control table.</p>\n\n<ul>\n<li><strong>chain_id</strong>: SAP Chain ID process.</li>\n<li><strong>dbtable</strong>: SAP LOGCHAIN db table name, default: <code>my_database.RSPCLOGCHAIN</code>.</li>\n<li><strong>status</strong>: SAP Chain Status of your process, default: <code>G</code>.</li>\n<li><strong>engine_table_name</strong>: Name of the temporary table created from the upstream data,\ndefault: <code>sensor_new_data</code>.\nThis temporary table will be used as source in the <code>query</code> option.</li>\n</ul></li>\n<li><p><strong>generate_sensor_query</strong>: Generates a Sensor query to consume data from the temporary table created in the <code>prepareQuery</code>.</p>\n\n<ul>\n<li><strong>sensor_id</strong>: The unique identifier for the Sensor.</li>\n<li><strong>filter_exp</strong>: Expression to filter incoming new data.\nA placeholder <code>?upstream_key</code> and <code>?upstream_value</code> can be used, example: <code>?upstream_key &gt; ?upstream_value</code>\nso that it can be replaced by the respective values from the sensor <code>control_db_table_name</code>\nfor this specific sensor_id.</li>\n<li><strong>control_db_table_name</strong>: Sensor control table name.</li>\n<li><strong>upstream_key</strong>: the key of custom sensor information to control how to identify\nnew data from the upstream (e.g., a time column in the upstream).</li>\n<li><p><strong>upstream_value</strong>: the <strong>first</strong> upstream value to identify new data from the\nupstream (e.g., the value of a time present in the upstream).</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"this-parameter-will-have-effect-just-in-the-first-run-to-detect-if-the-upstream-have-new-data-if-its-empty-the-default-value-applied-is-2147483647\">This parameter will have effect just in the first run to detect if the upstream have new data. If it's empty the default value applied is <code>-2147483647</code>.</h6>\n\n</div></li>\n<li><p><strong>upstream_table_name</strong>: Table name to consume the upstream value.\nIf it's empty the default value applied is <code>sensor_new_data</code>.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<h6 id=\"in-case-of-using-the-generate_sensor_sap_logchain_query-the-default-value-for-the-temp-table-is-sensor_new_data-so-if-passing-a-different-value-in-the-engine_table_name-this-parameter-should-have-the-same-value\">In case of using the <code>generate_sensor_sap_logchain_query</code> the default value for the temp table is <code>sensor_new_data</code>, so if passing a different value in the <code>engine_table_name</code> this parameter should have the same value.</h6>\n\n</div></li>\n</ul></li>\n</ul>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n\n<h2 id=\"scenarios\">Scenarios</h2>\n\n<p>This covers the following scenarios of using the Sensor:</p>\n\n<ol>\n<li><a href=\"#fail_on_empty_result-as-true-default-and-suggested\">The <code>fail_on_empty_result=True</code> (the default and SUGGESTED behaviour).</a></li>\n<li><a href=\"#fail_on_empty_result-as-false\">The <code>fail_on_empty_result=False</code>.</a></li>\n</ol>\n\n<p>Data from SAP, in streaming mode, will be consumed, so if there is any new data in the kafka topic it will give condition to proceed to the next task.</p>\n\n<h3 id=\"fail_on_empty_result-as-true-default-and-suggested\"><code>fail_on_empty_result</code> as True (default and SUGGESTED)</h3>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span><span class=\"p\">,</span> <span class=\"n\">generate_sensor_query</span><span class=\"p\">,</span> <span class=\"n\">generate_sensor_sap_logchain_query</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;sensor_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;assets&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;control_db_table_name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input_spec&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;spec_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;sensor_upstream&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;read_type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;batch&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;data_format&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;jdbc&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;options&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s2\">&quot;compress&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;driver&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_DRIVER&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;url&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_URL&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_USERNAME&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;password&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;JDBC_PWD&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;prepareQuery&quot;</span><span class=\"p\">:</span> <span class=\"n\">generate_sensor_sap_logchain_query</span><span class=\"p\">(</span><span class=\"n\">chain_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;CHAIN_ID&quot;</span><span class=\"p\">,</span> <span class=\"n\">dbtable</span><span class=\"o\">=</span><span class=\"s2\">&quot;JDBC_DB_TABLE&quot;</span><span class=\"p\">),</span>\n            <span class=\"s2\">&quot;query&quot;</span><span class=\"p\">:</span> <span class=\"n\">generate_sensor_query</span><span class=\"p\">(</span>\n                <span class=\"n\">sensor_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n                <span class=\"n\">filter_exp</span><span class=\"o\">=</span><span class=\"s2\">&quot;?upstream_key &gt; &#39;?upstream_value&#39;&quot;</span><span class=\"p\">,</span>\n                <span class=\"n\">control_db_table_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n                <span class=\"n\">upstream_key</span><span class=\"o\">=</span><span class=\"s2\">&quot;UPSTREAM_COLUMN_TO_IDENTIFY_NEW_DATA&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">),</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">&quot;base_checkpoint_location&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;s3://my_data_product_bucket/checkpoints&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h3 id=\"fail_on_empty_result-as-false\"><code>fail_on_empty_result</code> as False</h3>\n\n<p>Using <code>fail_on_empty_result=False</code>, in which the <code>execute_sensor</code> function returns a <code>boolean</code> representing if it\nhas acquired new data. This value can be used to execute or not the next steps.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">execute_sensor</span>\n\n<span class=\"n\">acon</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">],</span>\n    <span class=\"s2\">&quot;fail_on_empty_result&quot;</span><span class=\"p\">:</span> <span class=\"kc\">False</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">acquired_data</span> <span class=\"o\">=</span> <span class=\"n\">execute_sensor</span><span class=\"p\">(</span><span class=\"n\">acon</span><span class=\"o\">=</span><span class=\"n\">acon</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "lakehouse_engine_usage.sensor.update_sensor_status", "modulename": "lakehouse_engine_usage.sensor.update_sensor_status", "kind": "module", "doc": "<h1 id=\"update-sensor-control-delta-table-after-processing-the-data\">Update Sensor control delta table after processing the data</h1>\n\n<p>This shows how to <strong>update the status of your Sensor after processing the new data</strong>.</p>\n\n<p>Here is an example on how to update the status of your sensor in the Sensors Control Table:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">lakehouse_engine.engine</span> <span class=\"kn\">import</span> <span class=\"n\">update_sensor_status</span>\n\n<span class=\"n\">update_sensor_status</span><span class=\"p\">(</span>\n    <span class=\"n\">sensor_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;MY_SENSOR_ID&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">control_db_table_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;my_database.lakehouse_engine_sensors&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">status</span><span class=\"o\">=</span><span class=\"s2\">&quot;PROCESSED_NEW_DATA&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">assets</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;MY_SENSOR_ASSETS&quot;</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>If you want to know more please visit the definition of the class <a href=\"../../lakehouse_engine/core/definitions.html#SensorSpec\">here</a>.</p>\n"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();